[
 {
  "url": "https://www.aaai.org/Papers/AAAI/1992/AAAI92-119.pdf",
  "title": "An Empirical Analysis of Terminological Representation Systems",
  "year": 1992,
  "venue": "AAAI",
  "abstract": "The family of terminological representation systems has its roots in the representation system KLONE. Since the development of this system more than a dozen similar representation systems have been developed by various research groups. These systems vary along a number of dimensions. In this paper, we present the results of an empirical analysis of six such systems. Surprisingly, the systems turned out to be quite diverse leading to problems when transporting knowledge bases from one system to another. Additionally, the runtime performance between different systems and knowledge bases varied more than we expected. Finally, our empirical runtime performance results give an idea of what runtime performance to expect from such representation systems. These findings complement previously reported analytical results about the computational complexity of reasoning in such systems.",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1994/AAAI94-014.pdf",
  "title": "Experimentally Evaluating Communicative Strategies: The Effect of the Task",
  "year": 1994,
  "venue": "AAAI",
  "abstract": "Effective problem solving among multiple agents requires a better understanding of the role of communication in collaboration. In this paper we show that there are communicative strategies that greatly improve the performance of resource-bounded agents, but that these strategies are highly sensitive to the task requirements, situation parameters and agents’ resource limitations. We base our argument on two sources of evidence: (1) an analysis of a corpus of 55 problem solving dialogues, and (2) experimental simulations of collaborative problem solving dialogues in an experimental world, Design-World, where we parameterize task requirements, agents’ resources and communicative strategies.",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1994/AAAI94-129.pdf",
  "title": "Unclear Distinctions Lead to Unnecessary Shortcomings: Examining the Rule Versus Fact, Role versus Filler, and Type Versus Predicate Distinctions from a Connectionist Representation and Reasoning Perspective",
  "year": 1994,
  "venue": "AAAI",
  "abstract": "This paper deals with three distinctions pertaining to knowledge representation, namely, the rules vs facts distinction, roles vs fillers distinction, and predicates vs types distinction. Though these distinctions may indeed have some intuitive appeal, the exact natures of these distinctions are not entirely clear. This paper discusses some of the problems that arise when one accords these distinctions a prominent status in a connectionist system by choosing the representational structures so as to reflect these distinctions. The example we will look at in this paper is the connectionist reasoning system developed by Ajjanagadde & Shastri(Ajjanagadde & Shastri 1991; Shastri & Ajjanagadde 1993). Their1 system performs an interesting class of inferences using activation synchrony to represent dynamic bindings. The rule/fact, role/filler, type/predicate distinctions figure predominantly in the way knowledge is encoded in their system. We will discuss some significant shortcomings this leads to. Then, we will propose a much more uniform scheme for representing knowledge. The resulting system enjoys some significant advantages over Ajjanagadde & Shastri’s system, while retaining the idea of using synchrony to represent bindings.",
  "stance": 0.5
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1994/AAAI94-139.pdf",
  "title": "On the Relation between the Coherence and Foundations Theories of Belief Revision",
  "year": 1994,
  "venue": "AAAI",
  "abstract": "Two recent, papers, (Ggrdenfors 1990; Doyle 1992), try to assess the relative merits of the two main approaches to belief revision, the foundations and coherence theories, but leave open the question of the mathematical connections between them. We answer this question by showing that the foundations and coherence theories of belief revision are mathematically equivalent. The result also has consequences for nonmonotonic reasoning, as it, entails that Poole’s system of default, reasoning and Shoham’s preferential logic are expressively equivalent, in that they can represent the same set of non monotonic consequence relations.",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1994/AAAI94-160.pdf",
  "title": "The First Law of Robotics (A Call to Arms)",
  "year": 1994,
  "venue": "AAAI",
  "abstract": "Even before the advent of Artificial Intelligence, science fiction writer Isaac Asimov recognized that an agent must place the protection of humans from harm at a higher priority than obeying human orders. Inspired by Asimov, we pose the following fundamental questions: (1) How should one formalize the rich, but informal, notion of “harm”? (2) How can an agent avoid performing harmful actions, and do so in a computationally tractable manner? (3) How should an agent resolve conflict between its goals and the need to avoid harm? (4) When should an agent prevent a human from harming herself? While we address some of these questions in technical detail, the primary goal of this paper is to focus attention on Asimov’s concern: society will reject autonomous agents unless we have some credible means of making them safe! The Three Laws of Robotics: A robot may not injure a human being, or, through inaction, allow a human being to come to harm. A robot must obey orders given it by human beings except where such orders would conflict with the First Law. A robot must protect its own existence as long as such protection does not conflict with the First or Second Law. Isaac Asimov (Asimov 1942).",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1994/AAAI94-162.pdf",
  "title": "On the Nature of Modal Truth Criteria in Planning",
  "year": 1994,
  "venue": "AAAI",
  "abstract": "Chapman’s paper, “Planning for Conjunctive Goals,” has been widely acknowledged for its contribution toward understanding the nature of nonlinear (partial-order) planning, and it has been one of the bases of later work by others---but it is not free of problems. This paper addresses some problems involving modal truth and the Modal Truth Criterion (MTC). Our results are as follows: Even though modal duality is a fundamental axiom of classical modal logics, it does not hold for modal truth in Chapman’s plans; i.e., “necessarily p” is not equivalent to “not possibly lp.” Although the MTC for necessary truth is correct, the MTC for possible truth is incorrect: it provides necessary but insz#kient conditions for ensuring possible truth. Furthermore, even though necessary truth can be determined in polynomial time, possible truth is NP-hard. If we rewrite the MTC to talk about modal conditional truth (i.e., modal truth conditional on executability) rather than modal truth, then both the MTC for necessary conditional truth and the MTC for possible conditional truth are correct; and both can be computed in polynomial time.",
  "stance": -0.8
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1994/AAAI94-270.pdf",
  "title": "Model-Based Sensor Diagnosis: When Monitoring Should be Monitored",
  "year": 1994,
  "venue": "AAAI",
  "abstract": "A complex industrial plant, such as a nuclear power plant, is monitored thanks to a number of sensors. The instrumentation may be itself a complex system liable to failures. We propose a model-based sensor diagnosis system which relies on the topological description of the plant and on a set of component models. This model implicitly conceals relations involving only sensor data. Such relations must always be verified if components behave normally; thus, the detection task consists of verifying these relations. So, this work is a first step in extending the scope of model-based diagnosis, since we question here the information stemming from the plant and normally considered as safe. As further studies, we wish to monitor this detection system itself; i.e., whenever the instrumentation is supposed to behave correctly, nonverified constraints point out to errors in the plant model.",
  "stance": 0.2
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1996/AAAI96-013.pdf",
  "title": "Nearly Monotonic Problems: A Key to Effective FA/C Distributed Sensor Interpretation?",
  "year": 1996,
  "venue": "AAAI",
  "abstract": "The functionally-accurate, cooperative (FA/C) distributed problem-solving paradigm is one approach for organizing distributed problem solving among homogeneous, cooperating agents. A key assumption of the FA/C model has been that the agents’ local solutions can substitute for the raw data in determining the global solutions. This is not the case in general, however. Does this mean that researchers’ intuitions have been wrong and/or that FA/C problem solving is not likely to be effective ? We suggest that some domains have a characteristic that can account for the success of exchanging mainly local solutions. We call such problems nearly monotonic. This concept is discussed in the context of FA/C-based distributed sensor interpretation.",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1996/AAAI96-097.pdf",
  "title": "Formalizing Narratives Using Nested Circumscription",
  "year": 1996,
  "venue": "AAAI",
  "abstract": "The representation of narratives of actions and observations is a current issue in Knowledge Representation, where traditional plan-oriented treatments of action seem to fall short. To address narratives, Pinto and Reiter have extended Situation Calculus axioms, Kowalski and Sergot have introduced the Event Calculus in Logic Programming, and Baral et al. have defined the specification language C which allows to express actual and hypothetical situations in a uniform setting. The L entailment relation can formalize several forms of reasoning about actions and change. In this paper we illustrate a translation of L theories into Nested Abnormality Theories, a novel form of circumscription. The proof of soundness and completeness of the translation is the main technical result of the paper, but attention is also devoted to the features of Nested Abnormality Theories to capture commonsense reasoning in general and to clarify which assumptions a logical formalization forces upon a domain. These results also help clarifying the relationship between L and other recent circumscriptive formalizations for narratives, such as Miller and Shanahan’s. Content Areas Temporal Reasoning, Nonmonotonic Reasoning, Knowledge Representation.",
  "stance": 0.5
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1996/AAAI96-099.pdf",
  "title": "On the Range of Applicability of Baker’s Approach to the Frame Problem",
  "year": 1996,
  "venue": "AAAI",
  "abstract": "We investigate the range of applicability of Baker’s approach to the frame problem using an action language. We show that for temporal projection and deterministic domains, Baker’s approach gives the intuitively expected results.",
  "stance": 0.2
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1996/AAAI96-118.pdf",
  "title": "Testing the Robustness of the Genetic Algorithm on the Floating Building Block Representation",
  "year": 1996,
  "venue": "AAAI",
  "abstract": "Recent studies on a floating building block representation for the genetic algorithm (GA) suggest that there are many advantages to using the floating representation. This paper investigates the behavior of the GA on floating representation problems in response to three different types of pressures: (1) a reduction in the amount of genetic material available to the GA during the problem solving process, (2) functions which have negative-valued building blocks, and (3) randomizing non-coding segments. Results indicate that the GA’s performance on floating representation problems is very robust. Significant reductions in genetic material (genome length) may be made with relatively small decrease in performance. The GA can effectively solve problems with negative building blocks. Randomizing non-coding segments appears to improve rather than harm GA performance.",
  "stance": 0.5
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1996/AAAI96-168.pdf",
  "title": "A Bias towards Relevance: Recognizing Plans where Goal Minimization Fails",
  "year": 1996,
  "venue": "AAAI",
  "abstract": "Domains such as multiple trauma management, in which there are multiple interacting goals that change over time, are ones in which plan recognition’s standard inductive bias towards a single explanatory goal is inappropriate. In this paper we define and argue for an alternative bias based on identifying contextually “relevant” goals. We support this claim by showing how a complementary planning system in TraumAID 2.0, a decision-support system for the management of multiple trauma, allows us to define a four-level scale of relevance and therefore, of measurable deviations from relevance. This in turn allows definition of a bias towards relevance in the incremental recognition of physician plans by TraumAID’s critiquing interface, TraumaTIQ.",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1996/AAAI96-175.pdf",
  "title": "On the Size of Reactive Plans",
  "year": 1996,
  "venue": "AAAI",
  "abstract": "One of the most widespread approaches to reactive planning is Schoppers’ universal plans. We propose a stricter definition of universal plans which guarantees a weak notion of soundness not present in the original definition. Furthermore, we isolate three different types of completeness which capture different behaviours exhibited by universal plans. We show that universal plans which run in polynomial time and are of polynomial size cannot satisfy even the weakest type of completeness unless the polynomial hierarchy collapses. However, by relaxing either the polynomial time or the polynomial space requirement, the construction of universal plans satisfying the strongest type of completeness becomes trivial.",
  "stance": 0.5
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1996/AAAI96-194.pdf",
  "title": "A Counterexample to Theorems of Cox and Fine",
  "year": 1996,
  "venue": "AAAI",
  "abstract": "Cox’s well-known theorem justifying the use of probability is shown not to hold in finite domains. The counterexample also suggests that Cox’s assumptions are insufficient to prove the result even in infinite domains. The same counterexample is used to disprove a result of Fine on comparative conditional probability.",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1998/AAAI98-042.pdf",
  "title": "The Branching Factor of Regular Search Spaces",
  "year": 1998,
  "venue": "AAAI",
  "abstract": "Many problems, such as the sliding-tile puzzles, generate search trees where different nodes have different numbers of children, in this case depending on the position of the blank. We show how to calculate the asymptotic branching factors of such problems, and how to efficiently compute the exact numbers of nodes at a given depth. This information is important for determining the complexity of various search algorithms on these problems. In addition to the sliding-tile puzzles, we also apply our technique to Rubik’s Cube. While our techniques are fairly straightforward, the literature is full of incorrect branching factors for these problems, and the errors in several incorrect methods are fairly subtle.",
  "stance": -1.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1998/AAAI98-078.pdf",
  "title": "Cooperating with People: The Intelligent Classroom",
  "year": 1998,
  "venue": "AAAI",
  "abstract": "People frequently complain that it is too difficult to figure out how to get computers to do what they want. However, with a computer system that actually tries to understand what its users are doing, people can interact in ways that are more natural to them. We have been developing a system, the Intelligent Classroom, that does exactly this. The Intelligent Classroom uses cameras and microphones to sense a speaker’s actions and then infers his intentions from those actions. Finally, it uses these intentions to decide what to do to best cooperate with the speaker. In the Intelligent Classroom, the speaker need not worry about how to operate the Classroom; he may simply go about his lecture and trust the Classroom to assist him at the appropriate moments.",
  "stance": 0.5
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1998/AAAI98-120.pdf",
  "title": "Experimenting with Power Default Reasoning",
  "year": 1998,
  "venue": "AAAI",
  "abstract": "In this paper we explore the computational aspects of Propositional Power Default Reasoning (PDR), a form of non-monotonic reasoning which the underlying logic is Kleene’s 3-valued propositional logic. PDR leads to a concise meaning of the problem of skeptical entailment which has better complexity characteristics than the usual formalisms (co-NP(3)-Complete instead [[p-Complete). We take advantage of this in an implementation called powder to encode and solve hard graph problems and explore randomly generated instances of skeptical entailment.",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2000/AAAI00-020.pdf",
  "title": "Anchoring Symbols to Sensor Data: Preliminary Report",
  "year": 2000,
  "venue": "AAAI",
  "abstract": "Anchoring is the process of creating and maintaining the correspondence between symbols and percepts that refer to the same physical objects. Although this process must necessarily be present in any physically embedded system that includes a symbolic component (e.g., an autonomous robot), no systematic study of anchoring as a problem per se has been reported in the literature on intelligent systems. In this paper, we propose a domain-independent definition of the anchoring problem, and identify its three basic functionalities: find, reacquire, and track. We illustrate our definition on two systems operating in two different domains: an unmanned airborne vehicle for traffic surveillance; and a mobile robot for office navigation.",
  "stance": 0.5
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2000/AAAI00-064.pdf",
  "title": "Solving Advanced Reasoning Tasks Using Quantified Boolean Formulas",
  "year": 2000,
  "venue": "AAAI",
  "abstract": "We consider the compilation of different reasoning tasks into the evaluation problem of quantified boolean formulas (QBFs) as an approach to develop prototype reasoning systems useful for, e.g., experimental purposes. Such a method is a natural generalization of a similar technique applied to NP-problems and has been recently proposed by other researchers. More specifically, we present translations of several well-known reasoning tasks from the area of nonmonotonic reasoning into QBFs, and compare their implementation in the prototype system QUIP with established NMRprovers. The results show reasonable performance, and document that the QBF approach is an attractive tool for rapid prototyping of experimental knowledge-representation systems.",
  "stance": 0.5
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2000/AAAI00-078.pdf",
  "title": "GeoRep: A Flexible Tool for Spatial Representation of Line Drawings",
  "year": 2000,
  "venue": "AAAI",
  "abstract": "A key problem in diagrammatic reasoning is understanding how people reason about qualitative relationships in diagrams. We claim that progress in diagrammatic reasoning is slowed by two problems: (1) researchers tend to start from scratch, creating new spatial reasoners for each new problem area, and (2) constraints from human visual processing are rarely considered. To address these problems, we created GeoRep, a spatial reasoning engine that generates qualitative spatial descriptions from line drawings. GeoRep has been successfully used in several research projects, including cognitive simulation studies of human vision. In this paper, we outline GeoRep’s architecture, explain the domain-independent and domain-specific aspects of its processing, and motivate the representations it produces. We then survey how GeoRep has been used in three different projects–a model of symmetry, a model of understanding juxtaposition diagrams of physical situations, and a system for reasoning about military courses of action.",
  "stance": 0.5
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2000/AAAI00-169.pdf",
  "title": "Combining Classification and Temporal Learning",
  "year": 2000,
  "venue": "AAAI",
  "abstract": "This introduces TRACA (Temporal Reinforcement-learning and Classification Architecture), a connectionist learning system for solving problems in large state spaces. These types of problems, such as robot control, commonly include the presence of irrelevant attributes and hidden-state. TRACA is capable of dealing with both irrelevant information and hidden-state while addressing two common shortcomings of other learning systems. The first shortcoming is requiring a large number of training examples which is unrealistic for learning in the real world. The second is having to pre-determine or constrain network structure and size.",
  "stance": 0.4
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2002/AAAI02-051.pdf",
  "title": "The Design of Collectives of Agents to Control Non-Markovian Systems",
  "year": 2002,
  "venue": "AAAI",
  "abstract": "The “Collective Intelligence” (COIN) framework concerns the design of collectives of reinforcement-learning agents such that their interaction causes a provided “world” utility function concerning the entire collective to be maximized. Previously, we applied that framework to scenarios involving Markovian dynamics where no re-evolution of the system from counter-factual initial conditions (an often expensive calculation) is permitted. This approach sets the individual utility function of each agent to be both aligned with the world utility, and at the same time, easy for the associated agents to optimize. Here we extend that approach to systems involving non-Markovian dynamics. In computer simulations, we compare our techniques with each other and with conventional “team games” We show whereas in team games performance often degrades badly with time, it steadily improves when our techniques are used. We also investigate situations where the system’s dimensionality is effectively reduced. We show that this leads to difficulties in the agents’ ability to learn. The implication is that “learning” is a property only of high-enough dimensional systems.",
  "stance": -0.7
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2002/AAAI02-119.pdf",
  "title": "Reviewing the Design of DAML+OIL: An Ontology Language for the Semantic Web",
  "year": 2002,
  "venue": "AAAI",
  "abstract": "In the current “Syntactic Web”, uninterpreted syntactic constructs are given meaning only by private off-line agreements that are inaccessible to computers. In the Semantic Web vision, this is replaced by a web where both data and its semantic definition are accessible and manipulable by computer software. DAML+OIL is an ontology language specifically designed for this use in the Web; it exploits existing Web standards (XML and RDF), adding the familiar ontological primitives of object oriented and frame based systems, and the formal rigor of a very expressive description logic. The definition of DAML+OIL is now over a year old, and the language has been in fairly widespread use. In this paper, we review DAML+OIL’s relation with its key ingredients (XML, RDF, OIL, DAML-ONT, Description Logics), we discuss the design decisions and trade-offs that were the basis for the language definition, and identify a number of implementation challenges posed by the current language. These issues are important for designers of other representation languages for the Semantic Web, be they competitors or successors of DAML+OIL, such as the language currently under definition by W3C.",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2004/AAAI04-022.pdf",
  "title": "Modeling Choices in Quasigroup Completion: SAT Versus CSP",
  "year": 2004,
  "venue": "AAAI",
  "abstract": "We perform a systematic comparison of SAT and CSP models for a challenging combinatorial problem, quasigroup completion (QCP). Our empirical results clearly indicate the superiority of the 3D SAT encoding (Kautz et al. 2001), with various solvers, over other SAT and CSP models. We propose a partial explanation of the observed performance. Analytically, we focus on the relative conciseness of the 3D model and the pruning power of unit propagation. Empirically, the focus is on the role of the unit-propagation heuristic of the best performing solver, Satz (Li & Anbulagan 1997), which proves crucial to its success, and results in a significant improvement in scalability when imported into the CSP solvers. Our results strongly suggest that SAT encodings of permutation problems (Hnich, Smith, & Walsh 2004) may well prove quite competitive in other domains, in particular when compared with the currently preferred channeling CSP models.",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2004/AAAI04-056.pdf",
  "title": "Learning and Inferring Transportation Routines",
  "year": 2004,
  "venue": "AAAI",
  "abstract": "This paper introduces a hierarchical Markov model that can learn and infer a user’s daily movements through the community. The model uses multiple levels of abstraction in order to bridge the gap between raw GPS sensor measurements and high level information such as a user’s mode of transportation or her goal. We apply Rao-Blackwellised particle filters for efficient inference both at the low level and at the higher levels of the hierarchy. Significant locations such as goals or locations where the user frequently changes mode of transportation are learned from GPS data logs without requiring any manual labeling. We show how to detect abnormal behaviors (e.g. taking a wrong bus) by concurrently tracking his activities with a trained and a prior model. Experiments show that our model is able to accurately predict the goals of a person and to recognize situations in which the user performs unknown activities.",
  "stance": 0.7
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2006/AAAI06-239.pdf",
  "title": "Machine Reading",
  "year": 2006,
  "venue": "AAAI",
  "abstract": "Over the last two decades or so, Natural Language Processing (NLP) has developed powerful methods for low-level syntactic and semantic text processing tasks such as parsing, semantic role labeling, and text categorization. Over the same period, the fields of machine learning and probabilistic reasoning have yielded important breakthroughs as well. It is now time to investigate how to leverage these advances to understand text.",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2006/AAAI06-244.pdf",
  "title": "Turing’s Dream and the Knowledge Challenge",
  "year": 2006,
  "venue": "AAAI",
  "abstract": "There is a set of clear-cut challenges, all centering around knowledge, that have received insufficient attention in AI, and whose solution could bring the realization of Turing’s dream – the dream of a machine we can talk with just like a person, and which is therefore (at least) our intellectual equal. These challenges have to do with the representation of linguistically expressible knowledge, the role of knowledge in language understanding, the use of knowledge for several sorts of commonsense reasoning, and knowledge accumulation. Concerning the last topic, I briefly present preliminary results of some of our recent efforts to extract “shallow” general knowledge about the world from large text corpora.",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2008/AAAI08-028.pdf",
  "title": "Semantical Considerations on Dialectical and Practical Commitments",
  "year": 2008,
  "venue": "AAAI",
  "abstract": "This paper studies commitments in multiagent systems. A dialectical commitment corresponds to an agent taking a position about a putative fact, including for the sake of argument. A practical commitment corresponds to an agent being obliged to another to bring about a condition. Although commitments have been used in many works, an adequate formal semantics and axiomatization for them does not yet exist. This paper presents a logic of commitments that illustrates the commonalities and differences of the two kinds of commitments. In this manner, it generalizes the developments of previous papers, precisely delineates the meanings of commitments, and identifies important postulates used informally or semiformally in previous work.",
  "stance": 0.3
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2008/AAAI08-150.pdf",
  "title": "How Good is Almost Perfect?",
  "year": 2008,
  "venue": "AAAI",
  "abstract": "Heuristic search using algorithms such as A* and IDA* is the prevalent method for obtaining optimal sequential solutions for classical planning tasks. Theoretical analyses of these classical search algorithms, such as the well-known results of Pohl, Gaschnig and Pearl, suggest that such heuristic search algorithms can obtain better than exponential scaling behaviour, provided that the heuristics are accurate enough. Here, we show that for a number of common planning benchmark domains, including ones that admit optimal solution in polynomial time, general search algorithms such as A* must necessarily explore an exponential number of search nodes even under the optimistic assumption of almost perfect heuristic estimators, whose heuristic error is bounded by a small additive constant. Our results shed some light on the comparatively bad performance of optimal heuristic search approaches in “simple” planning domains such as GRIPPER. They suggest that in many applications, further improvements in run-time require changes to other parts of the search algorithm than the heuristic estimator.",
  "stance": 0.3
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2008/AAAI08-216.pdf",
  "title": "CIGAR: Concurrent and Interleaving Goal and Activity Recognition",
  "year": 2008,
  "venue": "AAAI",
  "abstract": "In artificial intelligence and pervasive computing research, inferring users’ high-level goals from activity sequences is an important task. A major challenge in goal recognition is that users often pursue several high-level goals in a concurrent and interleaving manner, where the pursuit of goals may spread over different parts of an activity sequence and may be pursued in parallel. Existing approaches to recognizing multiple goals often formulate this problem either as a single-goal recognition problem or in a deterministic way, ignoring uncertainty. In this paper, we propose CIGAR (Concurrent and Interleaving Goal and Activity Recognition) a novel and simple two-level probabilistic framework for multiple-goal recognition where we can recognize both concurrent and interleaving goals. We use skip-chain conditional random fields (SCCRF) for modeling interleaving goals and we model concurrent goals by adjusting inferred probabilities through a correlation graph, which is a major advantage in that we are able to reason about goal interactions explicitly through the correlation graph. The two-level framework also avoids the high training complexity when modeling concurrency and interleaving together in a unified CRF model. Experimental results show that our method can effectively improve recognition accuracies on several real-world datasets collected from various wireless and sensor networks.",
  "stance": 1.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2008/AAAI08-273.pdf",
  "title": "Game Theory Pragmatics: A Challenge for AI",
  "year": 2008,
  "venue": "AAAI",
  "abstract": "Game theory has been playing an increasingly visible role in computer science in general and AI in particular, most notably in the area of multiagent systems. I briefly list the areas where most of the action has been in the past decade or so. I then suggest that going forward, the most dramatic interaction between computer science and game theory – with a special role for AI – could be around what might be called game theory pragmatics.",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/7845/7704",
  "title": "Preferred Explanations: Theory and Generation via Planning",
  "year": 2011,
  "venue": "AAAI",
  "abstract": "In this paper we examine the general problem of generating preferred explanations for observed behavior with respect to a model of the behavior of a dynamical system. This problem arises in a diversity of applications including diagnosis of dynamical systems and activity recognition. We provide a logical characterization of the notion of an explanation. To generate explanations we identify and exploit a correspondence between explanation generation and planning. The determination of good explanations requires additional domainspecific knowledge which we represent as preferences over explanations. The nature of explanations requires us to formulate preferences in a somewhat retrodictive fashion by utilizing Past Linear Temporal Logic. We propose methods for exploiting these somewhat unique preferences effectively within state-of-the-art planners and illustrate the feasibility of generating (preferred) explanations via planning.",
  "stance": 0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/7856/7715",
  "title": "A Closer Look at the Probabilistic Description Logic Prob-EL",
  "year": 2011,
  "venue": "AAAI",
  "abstract": "We study probabilistic variants of the description logic EL. For the case where probabilities apply only to concepts, we provide a careful analysis of the borderline between tractability and EXPTIME-completeness. One outcome is that any probability value except zero and one leads to intractability in the presence of general TBoxes, while this is not the case for classical TBoxes. For the case where probabilities can also be applied to roles, we show PSPACE-completeness. This result is (positively) surprising as the best previously known upper bound was 2-EXPTIME and there were reasons to believe in completeness for this class.",
  "stance": 0.9
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/7861/7720",
  "title": "Transportability of Causal and Statistical Relations: A Formal Approach",
  "year": 2011,
  "venue": "AAAI",
  "abstract": "We address the problem of transferring information learned from experiments to a different environment, in which only passive observations can be collected. We introduce a formal representation called “selection diagrams” for expressing knowledge about differences and commonalities between environments and, using this representation, we derive procedures for deciding whether effects in the target environment can be inferred from experiments conducted elsewhere. When the answer is affirmative, the procedures identify the set of experiments and observations that need be conducted to license the transport. We further discuss how transportability analysis can guide the transfer of knowledge in non-experimental learning to minimize re-measurement cost and improve prediction power.",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/7902/7761",
  "title": "Size Adaptive Selection of Most Informative Features",
  "year": 2011,
  "venue": "AAAI",
  "abstract": "In this paper, we propose a novel method to select the most informative subset of features, which has little redundancy and very strong discriminating power. Our proposed approach automatically determines the optimal number of features and selects the best subset accordingly by maximizing the average pairwise informativeness, thus has obvious advantage over traditional filter methods. By relaxing the essential combinatorial optimization problem into the standard quadratic programming problem, the most informative feature subset can be obtained efficiently, and a strategy to dynamically compute the redundancy between feature pairs further greatly accelerates our method through avoiding unnecessary computations of mutual information. As shown by the extensive experiments, the proposed method can successfully select the most informative subset of features, and the obtained classification results significantly outperform the state-of-the-art results on most test datasets.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/8111/7969",
  "title": "A Data-Driven Approach to Question Subjectivity Identification in Community Question Answering",
  "year": 2012,
  "venue": "AAAI",
  "abstract": "Automatic Subjective Question Answering (ASQA), which aims at answering users’ subjective questions using summaries of multiple opinions, becomes increasingly important. One challenge of ASQA is that expected answers for subjective questions may not readily exist in the Web. The rising and popularity of Community Question Answering (CQA) sites, which provide platforms for people to post and answer questions, provides an alternative to ASQA. One important task of ASQA is question subjectivity identification, which identifies whether a user is asking a subjective question. Unfortunately, there has been little labeled training data available for this task. In this paper, we propose an approach to collect training data automatically by utilizing social signals in CQA sites without involving any manual labeling. Experimental results show that our data-driven approach achieves 9.37% relative improvement over the supervised approach using manually labeled data, and achieves 5.15% relative gain over a stateof-the-art semi-supervised approach. In addition, we propose several heuristic features for question subjectivity identification. By adding these features, we achieve 11.23% relative improvement over word n-gram feature under the same experimental setting.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/8221/8079",
  "title": "Basing Decisions on Sentences in Decision Diagrams",
  "year": 2012,
  "venue": "AAAI",
  "abstract": "The Sentential Decision Diagram (SDD) is a recently proposed representation of Boolean functions, containing Ordered Binary Decision Diagrams (OBDDs) as a distinguished subclass. While OBDDs are characterized by total variable orders, SDDs are characterized by dissections of variable orders, known as vtrees. Despite this generality, SDDs retain a number of properties, such as canonicity and a polytime Apply operator, that have been critical to the practical success of OBDDs. Moreover, upper bounds on the size of SDDs were also given, which are tighter than comparable upper bounds on the size of OBDDs. In this paper, we analyze more closely some of the theoretical properties of SDDs and their size. In particular, we consider the impact of basing decisions on sentences (using dissections as in SDDs), in comparison to basing decisions on variables (using total variable orders as in OBDDs). Here, we identify a class of Boolean functions where basing decisions on sentences using dissections of a variable order can lead to exponentially more compact SDDs, compared to OBDDs based on the same variable order. Moreover, we identify a fundamental property of the decompositions that underlie SDDs and use it to show how certain changes to a vtree can also lead to exponential differences in the size of an SDD.",
  "stance": 0.2
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/8453/8312",
  "title": "Learning to Rank Effective Paraphrases from Query Logs for Community Question Answering",
  "year": 2013,
  "venue": "AAAI",
  "abstract": "We present a novel method for ranking query paraphrases for effective search in community question answering (cQA). The method uses query logs from Yahoo! Search and Yahoo! Answers for automatically extracting a corpus of paraphrases of queries and questions using the query-question click history. Elements of this corpus are automatically ranked according to recall and mean reciprocal rank, and then used for learning two independent learning to rank models (SVMRank), whereby a set of new query paraphrases can be scored according to recall and MRR. We perform several automatic evaluation procedures using cross-validation for analyzing the behavior of various aspects of our learned ranking functions, which show that our method is useful and effective for search in cQA.",
  "stance": 0.9
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/8464/8323",
  "title": "Heterogeneous Metric Learning with Joint Graph Regularization for Cross-Media Retrieval",
  "year": 2013,
  "venue": "AAAI",
  "abstract": "As the major component of big data, unstructured heterogeneous multimedia content such as text, image, audio, video and 3D increasing rapidly on the Internet. User demand a new type of cross-media retrieval where user can search results across various media by submitting query of any media. Since the query and the retrieved results can be of different media, how to learn a heterogeneous metric is the key challenge. Most existing metric learning algorithms only focus on a single media where all of the media objects share the same data representation. In this paper, we propose a joint graph regularized heterogeneous metric learning (JGRHML) algorithm, which integrates the structure of different media into a joint graph regularization. In JGRHML, different media are complementary to each other and optimizing them simultaneously can make the solution smoother for both media and further improve the accuracy of the final metric. Based on the heterogeneous metric, we further learn a high-level semantic metric through label propagation. JGRHML is effective to explore the semantic relationship hidden across different modalities. The experimental results on two datasets with up to five media types show the effectiveness of our proposed approach.",
  "stance": 0.7
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/8564/8423",
  "title": "Reasoning about Saturated Conditional Independence Under Uncertainty: Axioms, Algorithms, and Levesque's Situations to the Rescue",
  "year": 2013,
  "venue": "AAAI",
  "abstract": "The implication problem of probabilistic conditional independencies is investigated in the presence of missing data. Here, graph separation axioms fail to hold for saturated conditional independencies, unlike the known idealized case with no missing data. Several axiomatic, algorithmic, and logical characterizations of the implication problem for saturated conditional independencies are established. In particular, equivalences are shown to the implication problem of a propositional fragment under Levesque’s situations, and that of Lien’s class of multivalued database dependencies under null values.",
  "stance": -0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/8566/8425",
  "title": "A Topic-Based Coherence Model for Statistical Machine Translation",
  "year": 2013,
  "venue": "AAAI",
  "abstract": "Coherence that ties sentences of a text into a meaningfully connected structure is of great importance to text generation and translation. In this paper, we propose a topic-based coherence model to produce coherence for document translation, in terms of the continuity of sentence topics in a text. We automatically extract a coherence chain for each source text to be translated. Based on the extracted source coherence chain, we adopt a maximum entropy classifier to predict the target coherence chain that defines a linear topic structure for the target document. The proposed topic-based coherence model then uses the predicted target coherence chain to help decoder select coherent word/phrase translations. Our experiments show that incorporating the topic-based coherence model into machine translation achieves substantial improvement over both the baseline and previous methods that integrate document topics rather than coherence chains into machine translation.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/8567/8426",
  "title": "An Extended GHKM Algorithm for Inducing Lambda-SCFG",
  "year": 2013,
  "venue": "AAAI",
  "abstract": "Semantic parsing, which aims at mapping a natural language (NL) sentence into its formal meaning representation (e.g., logical form), has received increasing attention in recent years. While synchronous context-free grammar (SCFG) augmented with lambda calculus (λSCFG) provides an effective mechanism for semantic parsing, how to learn such λ-SCFG rules still remains a challenge because of the difficulty in determining the correspondence between NL sentences and logical forms. To alleviate this structural divergence problem, we extend the GHKM algorithm, which is a state-ofthe-art algorithm for learning synchronous grammars in statistical machine translation, to induce λ-SCFG from pairs of NL sentences and logical forms. By treating logical forms as trees, we reformulate the theory behind GHKM that gives formal semantics to the alignment between NL words and logical form tokens. Experiments on the GEOQUERY dataset show that our semantic parser achieves an F-measure of 90.2%, the best result published to date.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/8702/8561",
  "title": "Fraudulent Support Telephone Number Identification Based on Co-Occurrence Information on the Web",
  "year": 2014,
  "venue": "AAAI",
  "abstract": "“Fraudulent support phones” refers to the misleading telephone numbers placed on Web pages or other media that claim to provide services with which they are not associated. Most fraudulent support phone information is found on search engine result pages (SERPs), and such information substantially degrades the search engine user experience. In this paper, we propose an approach to identify fraudulent support telephone numbers on the Web based on the co-occurrence relations between telephone numbers that appear on SERPs. We start from a small set of seed official support phone numbers and seed fraudulent numbers. Then, we construct a co-occurrence graph according to the co-occurrence relationships of the telephone numbers that appear on Web pages. Additionally, we take the page layout information into consideration on the assumption that telephone numbers that appear in nearby page blocks should be regarded as more closely related. Finally, we develop a propagation algorithm to diffuse the trust scores of seed official support phone numbers and the distrust scores of the seed fraudulent numbers on the co-occurrence graph to detect additional fraudulent numbers. Experimental results based on over 1.5 million SERPs produced by a popular Chinese commercial search engine indicate that our approach outperforms TrustRank, Anti-TrustRank and Good-Bad Rank algorithms by achieving an AUC value of over 0.90.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/8704/8563",
  "title": "A Joint Optimization Model for Image Summarization Based on Image Content and Tags",
  "year": 2014,
  "venue": "AAAI",
  "abstract": "As an effective technology for navigating a large number of images, image summarization is becoming a promising task with the rapid development of image sharing sites and social networks. Most existing summarization approaches use the visual-based features for image representation without considering tag information. In this paper, we propose a novel framework, named JOINT, which employs both image content and tag information to summarize images. Our model generates the summary images which can best reconstruct the original collection. Based on the assumption that an image with representative content should also have typical tags, we introduce a similarity-inducing regularizer to our model. Furthermore, we impose the lasso penalty on the objective function to yield a concise summary set. Extensive experiments demonstrate our model outperforms the state-of-the-art approaches.",
  "stance": 0.9
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/8745/8604",
  "title": "Synthesis of Geometry Proof Problems",
  "year": 2014,
  "venue": "AAAI",
  "abstract": "This paper presents a semi-automated methodology for generating geometric proof problems of the kind found in a highschool curriculum. We formalize the notion of a geometry proof problem and describe an algorithm for generating such problems over a user-provided figure. Our experimental results indicate that our problem generation algorithm can effectively generate proof problems in elementary geometry. On a corpus of 110 figures taken from popular geometry textbooks, our system generated an average of about 443 problems per figure in an average time of 4.7 seconds per figure.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/8751/8610",
  "title": "Learning Unknown Event Models",
  "year": 2014,
  "venue": "AAAI",
  "abstract": "Agents with incomplete environment models are likely to be surprised, and this represents an opportunity to learn. We investigate approaches for situated agents to detect surprises, discriminate among different forms of surprise, and hypothesize new models for the unknown events that surprised them. We instantiate these approaches in a new goal reasoning agent (named FOOLMETWICE), investigate its performance in simulation studies, and report that it produces plans with significantly reduced execution cost in comparison to not learning models for surprising events.",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/8799/8658",
  "title": "Lazy Defenders Are Almost Optimal against Diligent Attackers",
  "year": 2014,
  "venue": "AAAI",
  "abstract": "Most work building on the Stackelberg security games model assumes that the attacker can perfectly observe the defender’s randomized assignment of resources to targets. This assumption has been challenged by recent papers, which designed tailor-made algorithms that compute optimal defender strategies for security games with limited surveillance. We analytically demonstrate that in zero-sum security games, lazy defenders, who simply keep optimizing against perfectly informed attackers, are almost optimal against diligent attackers, who go to the effort of gathering a reasonable number of observations. This result implies that, in some realistic situations, limited surveillance may not need to be explicitly addressed.",
  "stance": -0.7
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/8819/8678",
  "title": "Equilibria in Epidemic Containment Games",
  "year": 2014,
  "venue": "AAAI",
  "abstract": "The spread of epidemics and malware is commonly modeled by diffusion processes on networks. Protective interventions such as vaccinations or installing anti-virus software are used to contain their spread. Typically, each node in the network has to decide its own strategy of securing itself, and its benefit depends on which other nodes are secure, making this a natural game-theoretic setting. There has been a lot of work on network security game models, but most of the focus has been either on simplified epidemic models or homogeneous",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/9162/9021",
  "title": "Clustering-Based Collaborative Filtering for Link Prediction",
  "year": 2015,
  "venue": "AAAI",
  "abstract": "In this paper, we propose a novel collaborative filtering approach for predicting the unobserved links in a network (or graph) with both topological and node features. Our approach improves the well-known compressed sensing based matrix completion method by introducing a new multipleindependent-Bernoulli-distribution model as the data sampling mask. It makes better link predictions since the model is more general and better matches the data distributions in many real-world networks, such as social networks like Facebook. As a result, a satisfying stability of the prediction can be guaranteed. To obtain an accurate multiple-independentBernoulli-distribution model of the topological feature space, our approach adjusts the sampling of the adjacency matrix of the network (or graph) using the clustering information in the node feature space. This yields a better performance than those methods which simply combine the two types of features. Experimental results on several benchmark datasets suggest that our approach outperforms the best existing link prediction methods.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/9166/9025",
  "title": "Mining Query Subtopics from Questions in Community Question Answering",
  "year": 2015,
  "venue": "AAAI",
  "abstract": "This paper proposes mining query subtopics from questions in community question answering (CQA). The subtopics are represented as a number of clusters of questions with keywords summarizing the clusters. The task is unique in that the subtopics from questions can not only facilitate user browsing in CQA search, but also describe aspects of queries from a question-answering perspective. The challenges of the task include how to group semantically similar questions and how to find keywords capable of summarizing the clusters. We formulate the subtopic mining task as a non-negative matrix factorization (NMF) problem and further extend the model of NMF to incorporate question similarity estimated from metadata of CQA into learning. Compared with existing methods, our method can jointly optimize question clustering and keyword extraction and encourage the former task to enhance the latter. Experimental results on large scale real world CQA datasets show that the proposed method significantly outperforms the existing methods in terms of keyword extraction, while achieving a comparable performance to the state-ofthe-art methods for question clustering.",
  "stance": 0.9
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/9176/9035",
  "title": "Extracting Bounded-Level Modules from Deductive RDF Triplestores",
  "year": 2015,
  "venue": "AAAI",
  "abstract": "We present a novel semantics for extracting bounded-level modules from RDF ontologies and databases augmented with safe inference rules, à la Datalog. Dealing with a recursive rule language poses challenging issues for defining the module semantics, and also makes module extraction algorithmically unsolvable in some cases. Our results include a set of module extraction algorithms compliant with the novel semantics. Experimental results show that the resulting framework is effective in extracting expressive modules from RDF datasets with formal guarantees, whilst controlling their suc-",
  "stance": 0.8
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/9293/9152",
  "title": "Assessing the Robustness of Cremer-McLean with Automated Mechanism Design",
  "year": 2015,
  "venue": "AAAI",
  "abstract": "In a classic result in the mechanism design literature, Cremer and McLean (1985) show that if buyers’ valuations are sufficiently correlated, a mechanism exists that allows the seller to extract the full surplus from efficient allocation as revenue. This result is commonly seen as “too good to be true” (in practice), casting doubt on its modeling assumptions. In this paper, we use an automated mechanism design approach to assess how sensitive the Cremer-McLean result is to relaxing its main technical assumption. That assumption implies that each valuation that a bidder can have results in a unique conditional distribution over the external signal(s). We relax this, allowing multiple valuations to be consistent with the same distribution over the external signal(s). Using similar insights to Cremer-McLean, we provide a highly efficient algorithm for computing the optimal revenue in this more general case. Using this algorithm, we observe that indeed, as the number of valuations consistent with a distribution grows, the optimal revenue quickly drops to that of a reserve-price mechanism. Thus, automated mechanism design allows us to gain insight into the precise sense in which Cremer-McLean is “too good to be true.”",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/9305/9164",
  "title": "Hedonic Coalition Formation in Networks",
  "year": 2015,
  "venue": "AAAI",
  "abstract": "Coalition formation is a fundamental problem in the organization of many multi-agent systems. In large populations, the formation of coalitions is often restricted by structural visibility and locality constraints under which agents can reorganize. We capture and study this aspect using a novel network-based model for dynamic locality within the popular framework of hedonic coalition formation games. We analyze the effects of network-based visibility and structure on the convergence of coalition formation processes to stable states. Our main result is a tight characterization of the structures based on which dynamic coalition formation can stabilize quickly. Maybe surprisingly, polynomial-time convergence can be achieved if and only if coalition formation is based on complete or star graphs.",
  "stance": 0.1
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/10033/9892",
  "title": "Refining Subgames in Large Imperfect Information Games",
  "year": 2016,
  "venue": "AAAI",
  "abstract": "The leading approach to solving large imperfect information games is to pre-calculate an approximate solution using a simplified abstraction of the full game; that solution is then used to play the original, full-scale game. The abstraction step is necessitated by the size of the game tree. However, as the original game progresses, the remaining portion of the tree (the subgame) becomes smaller. An appealing idea is to use the simplified abstraction to play the early parts of the game and then, once the subgame becomes tractable, to calculate a solution using a finer-grained abstraction in real time, creating a combined final strategy. While this approach is straightforward for perfect information games, it is a much more complex problem for imperfect information games. If the subgame is solved locally, the opponent can alter his play in prior to this subgame to exploit our combined strategy. To prevent this, we introduce the notion of subgame margin, a simple value with appealing properties. If any best response reaches the subgame, the improvement of exploitability of the combined strategy is (at least) proportional to the subgame margin. This motivates subgame refinements resulting in large positive margins. Unfortunately, current techniques either neglect subgame margin (potentially leading to a large negative subgame margin and drastically more exploitable strategies), or guarantee only non-negative subgame margin (possibly producing the original, unrefined strategy, even if much stronger strategies are possible). Our technique remedies this problem by maximizing the subgame margin and is guaranteed to find the optimal solution. We evaluate our technique using one of the top participants of the AAAI-14 Computer Poker Competition, the leading playground for agents in imperfect information settings.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/9992/9851",
  "title": "Commonsense in Parts: Mining Part-Whole Relations from the Web and Image Tags",
  "year": 2016,
  "venue": "AAAI",
  "abstract": "Commonsense knowledge about part-whole relations (e.g., screen partOf notebook) is important for interpreting user input in web search and question answering, or for object detection in images. Prior work on knowledge base construction has compiled part-whole assertions, but with substantial limitations: i) semantically different kinds of part-whole relations are conflated into a single generic relation, ii) the arguments of a part-whole assertion are merely words with ambiguous meaning, iii) the assertions lack additional attributes like visibility (e.g., a nose is visible but a kidney is not) and cardinality information (e.g., a bird has two legs while a spider eight), iv) limited coverage of only tens of thousands of assertions. This paper presents a new method for automatically acquiring part-whole commonsense from Web contents and image tags at an unprecedented scale, yielding many millions of assertions, while specifically addressing the four shortcomings of prior work. Our method combines pattern-based information extraction methods with logical reasoning. We carefully distinguish different relations: physicalPartOf, memberOf, substanceOf. We consistently map the arguments of all assertions onto WordNet senses, eliminating the ambiguity of wordlevel assertions. We identify whether the parts can be visually perceived, and infer cardinalities for the assertions. The resulting commonsense knowledge base has very high quality and high coverage, with an accuracy of 89% determined by extensive sampling, and is publicly available.",
  "stance": 0.8
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/9994/9853",
  "title": "Inferring a Personalized Next Point-of-Interest Recommendation Model with Latent Behavior Patterns",
  "year": 2016,
  "venue": "AAAI",
  "abstract": "In this paper, we address the problem of personalized next Point-of-interest (POI) recommendation which has become an important and very challenging task in location-based social networks (LBSNs), but not well studied yet. With the conjecture that, under different contextual scenario, human exhibits distinct mobility patterns, we attempt here to jointly model the next POI recommendation under the influence of user’s latent behavior pattern. We propose to adopt a third-rank tensor to model the successive check-in behaviors. By incorporating softmax function to fuse the personalized Markov chain with latent pattern, we furnish a Bayesian Personalized Ranking (BPR) approach and derive the optimization criterion accordingly. Expectation Maximization (EM) is then used to estimate the model parameters. Extensive experiments on two large-scale LBSNs datasets demonstrate the significant improvements of our model over several state-of-the-art methods.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/10479/10338",
  "title": "CLARE: A Joint Approach to Label Classification and Tag Recommendation",
  "year": 2017,
  "venue": "AAAI",
  "abstract": "Data classification and tag recommendation are both important and challenging tasks in social media. These two tasks are often considered independently and most efforts have been made to tackle them separately. However, labels in data classification and tags in tag recommendation are inherently related. For example, a Youtube video annotated with NCAA, stadium, pac12 is likely to be labeled as football, while a video/image with the class label of coast is likely to be tagged with beach, sea, water and sand. The existence of relations between labels and tags motivates us to jointly perform classification and tag recommendation for social media data in this paper. In particular, we provide a principled way to capture the relations between labels and tags, and propose a novel framework CLARE, which fuses data CLAssification and tag REcommendation into a coherent model. With experiments on three social media datasets, we demonstrate that the proposed framework CLARE achieves superior performance on both tasks compared to the state-of-the-art methods.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/10490/10349",
  "title": "Web-Based Semantic Fragment Discovery for On-Line Lingual-Visual Similarity",
  "year": 2017,
  "venue": "AAAI",
  "abstract": "In this paper, we present an automatic approach for on-line discovery of visual-lingual semantic fragments from weakly labeled Internet images. Instead of learning region-entity correspondences from well-labeled image-sentence pairs, our approach directly collects and enhances the weakly labeled visual contents from the Web and constructs an adaptive visual representation which automatically links generic lingual phrases to their related visual contents. To ensure reliable and efficient semantic discovery, we adopt non-parametric density estimation to re-rank the related visual instances and proposed a fast self-similarity-based quality assessment method to identify the high-quality semantic fragments. The discovered semantic fragments provide an adaptive joint representation for texts and images, based on which lingual-visual similarity can be defined for further co-analysis of heterogeneous multimedia data. Experimental results on semantic fragment quality assessment, sentence-based image retrieval, automatic multimedia insertion and ordering demonstrated the effectiveness of the proposed framework.The experiments show that the proposed methods can make effective use of the Web knowledge, and are able to generate competitive results compared to state-of-the-art approaches in various tasks.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/10499/10358",
  "title": "A Dependency-Based Neural Reordering Model for Statistical Machine Translation",
  "year": 2017,
  "venue": "AAAI",
  "abstract": "In machine translation (MT) that involves translating between two languages with significant differences in word order, determining the correct word order of translated words is a major challenge. The dependency parse tree of a source sentence can help to determine the correct word order of the translated words. In this paper, we present a novel reordering approach utilizing a neural network and dependency-based embeddings to predict whether the translations of two source words linked by a dependency relation should remain in the same order or should be swapped in the translated sentence. Experiments on Chinese-to-English translation show that our approach yields a statistically significant improvement of 0.57 BLEU point on benchmark NIST test sets, compared to our prior state-of-theart statistical MT system that uses sparse dependency-based reordering features.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/10508/10367",
  "title": "SnapNETS: Automatic Segmentation of Network Sequences with Node Labels",
  "year": 2017,
  "venue": "AAAI",
  "abstract": "Given a sequence of snapshots of flu propagating over a population network, can we find a segmentation when the patterns of the disease spread change, possibly due to interventions? In this paper, we study the problem of segmenting graph sequences with labeled nodes. Memes on the Twitter network, diseases over a contact network, movie-cascades over a social network, etc. are all graph sequences with labeled nodes. Most related work is on plain graphs (and hence ignore the label dynamics) or fix parameters or require much feature engineering. Instead, we propose SNAPNETS, to automatically find segmentations of such graph sequences, with different characteristics of nodes of each label in adjacent segments. It satisfies all the desired properties (being parameter-free, comprehensive and scalable) by leveraging a principled, multilevel, flexible framework which maps the problem to a path optimization problem over a weighted DAG. Extensive experiments on several diverse real datasets show that it finds cut points matching ground-truth or meaningful external signals outperforming non-trivial baselines. We also show that SNAPNETS scales near-linearly with the size of the input.",
  "stance": 0.8
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/10588/10447",
  "title": "Obvious Strategyproofness Needs Monitoring for Good Approximations",
  "year": 2017,
  "venue": "AAAI",
  "abstract": "Obvious strategyproofness (OSP) is an appealing concept as it allows to maintain incentive compatibility even in the presence of agents that are not fully rational, e.g., those who struggle with contingent reasoning (Li 2015). However, it has been shown to impose some limitations, e.g., no OSP mechanism can return a stable matching (Ashlagi and Gonczarowski",
  "stance": -1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/10607/10466",
  "title": "On Pareto Optimality in Social Distance Games",
  "year": 2017,
  "venue": "AAAI",
  "abstract": "We investigate Pareto stability in Social Distance Games, that are coalition forming games in which agents utilities are proportional to their harmonic centralities in the respective coalitions, i.e., to the average inverse distance from the other agents. Pareto optimal solutions have been already considered in the literature as outcomes arising from the strategic interaction of the agents. In particular, they are stable under the deviation of the grand coalition, as they do not permit a simultaneous deviation by all the agents making all of them weakly better off and some strictly better off. We first show that, while computing a Pareto stable solution maximizing the social welfare is NP-hard in bounded degree graphs, a 2min{Δ,√n}-approximating one can be determined in polynomial time, where n is the number of agents and Δ the maximum node degree. We then determine asymptotically tight bounds on the Price of Pareto Optimality for several classes of social graphs arising from the following combinations: unbounded and bounded node degree, undirected and directed edges, unweighted and weighted edges.",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/10618/10477",
  "title": "Resource Graph Games: A Compact Representation for Games with Structured Strategy Spaces",
  "year": 2017,
  "venue": "AAAI",
  "abstract": "In many real-world systems, strategic agents’ decisions can be understood as complex—i.e., consisting of multiple subdecisions—and hence can give rise to an exponential number of pure strategies. Examples include network congestion games, simultaneous auctions, and security games. However, agents’ sets of strategies are often structured, allowing them to be represented compactly. There currently exists no general modeling language that captures a wide range of commonly seen strategy structure and utility structure. We propose Resource Graph Games (RGGs), the first general compact representation for games with structured strategy spaces, which is able to represent a wide range of games studied in literature. We leverage recent results about multilinearity, a key property of games that allows us to represent the mixed strategies compactly, and, as a result, to compute various equilibrium concepts efficiently. While not all RGGs are multilinear, we provide a general method of converting RGGs to those that are multilinear, and identify subclasses of RGGs whose converted version allow efficient computation.",
  "stance": 0.8
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/10630/10489",
  "title": "The Benefit in Free Information Disclosure When Selling Information to People",
  "year": 2017,
  "venue": "AAAI",
  "abstract": "This paper studies the benefit for information providers in free public information disclosure in settings where the prospective information buyers are people. The underlying model, which applies to numerous real-life situations, considers a standard decision making setting where the decision maker is uncertain about the outcomes of her decision. The information provider can fully disambiguate this uncertainty and wish to maximize her profit from selling such information. We use a series of AMT-based experiments with people to test the benefit for the information provider from reducing some of the uncertainty associated with the decision maker’s problem, for free. Free information disclosure of this kind can be proved to be ineffective when the buyer is a fullyrational agent. Yet, when it comes to people we manage to demonstrate that a substantial improvement in the information provider’s profit can be achieved with such an approach. The analysis of the results reveals that the primary reason for this phenomena is people’s failure to consider the strategic nature of the interaction with the information provider. Peoples’ inability to properly calculate the value of information is found to be secondary in its influence.",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/10645/10504",
  "title": "A Generic Bet-and-Run Strategy for Speeding Up Stochastic Local Search",
  "year": 2017,
  "venue": "AAAI",
  "abstract": "A common strategy for improving optimization algorithms is to restart the algorithm when it is believed to be trapped in an inferior part of the search space. However, while specific restart strategies have been developed for specific problems (and specific algorithms), restarts are typically not regarded as a general tool to speed up an optimization algorithm. In fact, many optimization algorithms do not employ restarts at",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/11255/11114",
  "title": "Discovering and Distinguishing Multiple Visual Senses for Polysemous Words",
  "year": 2018,
  "venue": "AAAI",
  "abstract": "To reduce the dependence on labeled data, there have been increasing research efforts on learning visual classifiers by exploiting web images. One issue that limits their performance is the problem of polysemy. To solve this problem, in this work, we present a novel framework that solves the problem of polysemy by allowing sense-specific diversity in search results. Specifically, we first discover a list of possible semantic senses to retrieve sense-specific images. Then we merge visual similar semantic senses and prune noises by using the retrieved images. Finally, we train a visual classifier for each selected semantic sense and use the learned sense-specific classifiers to distinguish multiple visual senses. Extensive experiments on classifying images into sense-specific categories and re-ranking search results demonstrate the superiority of our proposed approach.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/11256/11115",
  "title": "Representation Learning for Scale-Free Networks",
  "year": 2018,
  "venue": "AAAI",
  "abstract": "Network embedding aims to learn the low-dimensional representations of vertexes in a network, while structure and inherent properties of the network is preserved. Existing network embedding works primarily focus on preserving the microscopic structure, such as the firstand second-order proximity of vertexes, while the macroscopic scale-free property is largely ignored. Scale-free property depicts the fact that vertex degrees follow a heavy-tailed distribution (i.e., only a few vertexes have high degrees) and is a critical property of realworld networks, such as social networks. In this paper, we study the problem of learning representations for scale-free networks. We first theoretically analyze the difficulty of embedding and reconstructing a scale-free network in the Euclidean space, by converting our problem to the sphere packing problem. Then, we propose the “degree penalty” principle for designing scale-free property preserving network embedding algorithm: punishing the proximity between high-degree vertexes. We introduce two implementations of our principle by utilizing the spectral techniques and a skip-gram model respectively. Extensive experiments on six datasets show that our algorithms are able to not only reconstruct heavy-tailed distributed degree distribution, but also outperform state-ofthe-art embedding models in various network mining tasks, such as vertex classification and link prediction.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/11277/11136",
  "title": "Deep Region Hashing for Generic Instance Search from Images",
  "year": 2018,
  "venue": "AAAI",
  "abstract": "Instance Search (INS) is a fundamental problem for many applications, while it is more challenging comparing to traditional image search since the relevancy is defined at the instance level. Existing works have demonstrated the success of many complex ensemble systems that are typically conducted by firstly generating object proposals, and then extracting handcrafted and/or CNN features of each proposal for matching. However, object bounding box proposals and feature extraction are often conducted in two separated steps, thus the effectiveness of these methods collapses. Also, due to the large amount of generated proposals, matching speed becomes the bottleneck that limits its application to largescale datasets. To tackle these issues, in this paper we propose an effective and efficient Deep Region Hashing (DRH) approach for large-scale INS using an image patch as the query. Specifically, DRH is an end-to-end deep neural network which consists of object proposal, feature extraction, and hash code generation. DRH shares full-image convolutional feature map with the region proposal network, thus enabling nearly cost-free region proposals. Also, each highdimensional, real-valued region features are mapped onto a low-dimensional, compact binary codes for the efficient object region level matching on large-scale dataset. Experimental results on four datasets show that our DRH can achieve even better performance than the state-of-the-arts in terms of mAP, while the efficiency is improved by nearly 100 times.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/11307/11166",
  "title": "Learning the Joint Representation of Heterogeneous Temporal Events for Clinical Endpoint Prediction",
  "year": 2018,
  "venue": "AAAI",
  "abstract": "The availability of a large amount of electronic health records (EHR) provides huge opportunities to improve health care service by mining these data. One important application is clinical endpoint prediction, which aims to predict whether a disease, a symptom or an abnormal lab test will happen in the future according to patients’ history records. This paper develops deep learning techniques for clinical endpoint prediction, which are effective in many practical applications. However, the problem is very challenging since patients’ history records contain multiple heterogeneous temporal events such as lab tests, diagnosis, and drug administrations. The visiting patterns of different types of events vary significantly, and there exist complex nonlinear relationships between different events. In this paper, we propose a novel model for learning the joint representation of heterogeneous temporal events. The model adds a new gate to control the visiting rates of different events which effectively models the irregular patterns of different events and their nonlinear correlations. Experiment results with real-world clinical data on the tasks of predicting death and abnormal lab tests prove the effectiveness of our proposed approach over competitive baselines.",
  "stance": 0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/11308/11167",
  "title": "Generating an Event Timeline About Daily Activities From a Semantic Concept Stream",
  "year": 2018,
  "venue": "AAAI",
  "abstract": "Recognizing activities of daily living (ADLs) in the real world is an important task for understanding everyday human life. However, even though our life events consist of chronological ADLs with the corresponding places and objects (e.g., drinking coffee in the living room after making coffee in the kitchen and walking to the living room), most existing works focus on predicting individual activity labels from sensor data. In this paper, we introduce a novel framework that produces an event timeline of ADLs in a home environment. The proposed method combines semantic concepts such as action, object, and place detected by sensors for generating stereotypical event sequences with the following three realworld properties. First, we use temporal interactions among concepts to remove objects and places unrelated to each action. Second, we use commonsense knowledge mined from a language resource to find a possible combination of concepts in the real world. Third, we use temporal variations of events to filter repetitive events, since our daily life changes over time. We use cross-place validation to evaluate our proposed method on a daily-activities dataset with manually labeled event descriptions. The empirical evaluation demonstrates that our method using real-world properties improves the performance of generating an event timeline over diverse",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/11320/11179",
  "title": "Learning Nonlinear Dynamics in Efficient, Balanced Spiking Networks Using Local Plasticity Rules",
  "year": 2018,
  "venue": "AAAI",
  "abstract": "The brain uses spikes in neural circuits to perform many dynamical computations. The computations are performed with properties such as spiking efficiency, i.e. minimal number of spikes, and robustness to noise. A major obstacle for learning computations in artificial spiking neural networks with such desired biological properties is due to lack of our understanding of how biological spiking neural networks learn",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/11508/11367",
  "title": "Understanding Over Participation in Simple Contests",
  "year": 2018,
  "venue": "AAAI",
  "abstract": "One key motivation for using contests in real-life is the substantial evidence reported in empirical contest-design literature for people’s tendency to act more competitively in contests than predicted by the Nash Equilibrium. This phenomenon has been traditionally explained by people’s eagerness to win and maximize their relative (rather than absolute) payoffs. In this paper we make use of “simple contests”, where contestants only need to strategize on whether to participate in the contest or not, as an infrastructure for studying whether indeed more effort is exerted in contests due to competitiveness, or perhaps this can be attributed to other factors that hold also in non-competitive settings. The experimental methodology we use compares contestants’ participation decisions in eight contest settings differing in the nature of the contest used, the number of contestants used and the theoretical participation predictions to those obtained (whenever applicable) by subjects facing equivalent non-competitive decision situations in the form of a lottery. We show that indeed people tend to over-participate in contests compared to the theoretical predictions, yet the same phenomenon holds (to a similar extent) also in the equivalent non-competitive settings. Meaning that many of the contests used nowadays as a means for inducing extra human effort, that are often complex to organize and manage, can be replaced by a simpler non-competitive mechanism that uses probabilistic prizes.",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/11512/11371",
  "title": "A Voting-Based System for Ethical Decision Making",
  "year": 2018,
  "venue": "AAAI",
  "abstract": "We present a general approach to automating ethical decisions, drawing on machine learning and computational social choice. In a nutshell, we propose to learn a model of societal preferences, and, when faced with a specific ethical dilemma at runtime, efficiently aggregate those preferences to identify a desirable choice. We provide a concrete algorithm that instantiates our approach; some of its crucial steps are informed by a new theory of swap-dominance efficient voting rules. Finally, we implement and evaluate a system for ethical decision making in the autonomous vehicle domain, using preference data collected from 1.3 million people through the Moral",
  "stance": 0.8
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/11537/11396",
  "title": "Stream Reasoning in Temporal Datalog",
  "year": 2018,
  "venue": "AAAI",
  "abstract": "In recent years, there has been an increasing interest in extending traditional stream processing engines with logical, rule-based, reasoning capabilities. This poses significant theoretical and practical challenges since rules can derive new information and propagate it both towards past and future time points; as a result, streamed query answers can depend on data that has not yet been received, as well as on data that arrived far in the past. Stream reasoning algorithms, however, must be able to stream out query answers as soon as possible, and can only keep a limited number of previous input facts in memory. In this paper, we propose novel reasoning problems to deal with these challenges, and study their computational properties on Datalog extended with a temporal sort and the successor function—a core rule-based language for stream reasoning applications.",
  "stance": 0.3
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/11553/11412",
  "title": "Fair Inference on Outcomes",
  "year": 2018,
  "venue": "AAAI",
  "abstract": "In this paper, we consider the problem of fair statistical inference involving outcome variables. Examples include classification and regression problems, and estimating treatment effects in randomized trials or observational data. The issue of fairness arises in such problems where some covariates or treatments are “sensitive,” in the sense of having potential of creating discrimination. In this paper, we argue that the presence of discrimination can be formalized in a sensible way as the presence of an effect of a sensitive covariate on the outcome along certain causal pathways, a view which generalizes (Pearl 2009). A fair outcome model can then be learned by solving a constrained optimization problem. We discuss a number of complications that arise in classical statistical inference due to this view and provide workarounds based on recent work in causal and semi-parametric inference.",
  "stance": -0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/3765/3643",
  "title": "ColNet: Embedding the Semantics of Web Tables for Column Type Prediction",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "Automatically annotating column types with knowledge base (KB) concepts is a critical task to gain a basic understanding of web tables. Current methods rely on either table metadata like column name or entity correspondences of cells in the KB, and may fail to deal with growing web tables with incomplete meta information. In this paper we propose a neural network based column type annotation framework named ColNet which is able to integrate KB reasoning and lookup with machine learning and can automatically train Convolutional Neural Networks for prediction. The prediction model not only considers the contextual semantics within a cell using word representation, but also embeds the semantics of a column by learning locality features from multiple cells. The method is evaluated with DBPedia and two different web table datasets, T2Dv2 from the general Web and Limaye from Wikipedia pages, and achieves higher performance than the state-of-the-art approaches.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/3792/3670",
  "title": "Mining Entity Synonyms with Efficient Neural Set Generation",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "Mining entity synonym sets (i.e., sets of terms referring to the same entity) is an important task for many entity-leveraging applications. Previous work either rank terms based on their similarity to a given query term, or treats the problem as a two-phase task (i.e., detecting synonymy pairs, followed by organizing these pairs into synonym sets). However, these approaches fail to model the holistic semantics of a set and suffer from the error propagation issue. Here we propose a new framework, named SynSetMine, that efficiently generates entity synonym sets from a given vocabulary, using example sets from external knowledge bases as distant supervision. SynSetMine consists of two novel modules: (1) a set-instance classifier that jointly learns how to represent a permutation invariant synonym set and whether to include a new instance (i.e., a term) into the set, and (2) a set generation algorithm that enumerates the vocabulary only once and applies the learned set-instance classifier to detect all entity synonym sets in it. Experiments on three real datasets from different domains demonstrate both effectiveness and efficiency of SynSetMine for mining entity synonym sets.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/3795/3673",
  "title": "Learning from Web Data Using Adversarial Discriminative Neural Networks for Fine-Grained Classification",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "Fine-grained classification is absorbed in recognizing the subordinate categories of one field, which need a large number of labeled images, while it is expensive to label these images. Utilizing web data has been an attractive option to meet the demands of training data for convolutional neural networks (CNNs), especially when the well-labeled data is not enough. However, directly training on such easily obtained images often leads to unsatisfactory performance due to factors such as noisy labels. This has been conventionally addressed by reducing the noise level of web data. In this paper, we take a fundamentally different view and propose an adversarial discriminative loss to advocate representation coherence between standard and web data. This is further encapsulated in a simple, scalable and end-to-end trainable multi-task learning framework. We experiment on three public datasets using large-scale web data to evaluate the effectiveness and generalizability of the proposed approach. Extensive experiments demonstrate that our approach performs favorably against the state-of-the-art methods.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/3845/3723",
  "title": "Image Aesthetic Assessment Assisted by Attributes through Adversarial Learning",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "The inherent connections among aesthetic attributes and aesthetics are crucial for image aesthetic assessment, but have not been thoroughly explored yet. In this paper, we propose a novel image aesthetic assessment assisted by attributes through both representation-level and label-level. The attributes are used as privileged information, which is only required during training. Specifically, we first propose a multitask deep convolutional rating network to learn the aesthetic score and attributes simultaneously. The attributes are explored to construct better feature representations for aesthetic assessment through multi-task learning. After that, we introduce a discriminator to distinguish the predicted attributes and aesthetics of the multi-task deep network from the ground truth label distribution embedded in the training data. The multi-task deep network wants to output aesthetic score and attributes as close to the ground truth labels as possible. Thus the deep network and the discriminator compete with each other. Through adversarial learning, the attributes are explored to enforce the distribution of the predicted attributes and aesthetics to converge to the ground truth label distribution. Experimental results on two benchmark databases demonstrate the superiority of the proposed method to state of the art work.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/3848/3726",
  "title": "Multi3Net: Segmenting Flooded Buildings via Fusion of Multiresolution, Multisensor, and Multitemporal Satellite Imagery",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "We propose a novel approach for rapid segmentation of flooded buildings by fusing multiresolution, multisensor, and multitemporal satellite imagery in a convolutional neural network. Our model significantly expedites the generation of satellite imagery-based flood maps, crucial for first responders and local authorities in the early stages of flood events. By incorporating multitemporal satellite imagery, our model allows for rapid and accurate post-disaster damage assessment and can be used by governments to better coordinate mediumand long-term financial assistance programs for affected areas. The network consists of multiple streams of encoder-decoder architectures that extract spatiotemporal information from medium-resolution images and spatial information from high-resolution images before fusing the resulting representations into a single medium-resolution segmentation map of flooded buildings. We compare our model to state-of-the-art methods for building footprint segmentation as well as to alternative fusion approaches for the segmentation of flooded buildings and find that our model performs best on both tasks. We also demonstrate that our model produces highly accurate segmentation maps of flooded buildings using only publicly available medium-resolution data instead of significantly more detailed but sparsely available very high-resolution data. We release the first open-source dataset of fully preprocessed and labeled multiresolution, multispectral, and multitemporal satellite images of disaster sites along with our source code.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/3892/3770",
  "title": "DeepSTN+: Context-Aware Spatial-Temporal Neural Network for Crowd Flow Prediction in Metropolis",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "Crowd flow prediction is of great importance in a wide range of applications from urban planning, traffic control to public safety. It aims to predict the inflow (the traffic of crowds entering a region in a given time interval) and outflow (the traffic of crowds leaving a region for other places) of each region in the city with knowing the historical flow data. In this paper, we propose DeepSTN+, a deep learning-based convolutional model, to predict crowd flows in the metropolis. First, DeepSTN+ employs the ConvPlus structure to model the longrange spatial dependence among crowd flows in different regions. Further, PoI distributions and time factor are combined to express the effect of location attributes to introduce prior knowledge of the crowd movements. Finally, we propose an effective fusion mechanism to stabilize the training process, which further improves the performance. Extensive experimental results based on two real-life datasets demonstrate the superiority of our model, i.e., DeepSTN+ reduces the error of the crowd flow prediction by approximately 8%∼13% compared with the state-of-the-art baselines.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/3928/3806",
  "title": "Cognitive Deficit of Deep Learning in Numerosity",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "Subitizing, or the sense of small natural numbers, is an innate cognitive function of humans and primates; it responds to visual stimuli prior to the development of any symbolic skills, language or arithmetic. Given successes of deep learning (DL) in tasks of visual intelligence and given the primitivity of number sense, a tantalizing question is whether DL can comprehend numbers and perform subitizing. But somewhat disappointingly, extensive experiments of the type of cognitive psychology demonstrate that the examples-driven black box DL cannot see through superficial variations in visual representations and distill the abstract notion of natural number, a task that children perform with high accuracy and confidence. The failure is apparently due to the learning method not the CNN computational machinery itself. A recurrent neural network capable of subitizing does exist, which we construct by encoding a mechanism of mathematical morphology into the CNN convolutional kernels. Also, we investigate, using subitizing as a test bed, the ways to aid the black box DL by cognitive priors derived from human insight. Our findings are mixed and interesting, pointing to both cognitive deficit of pure DL, and some measured successes of boosting DL by predetermined cognitive implements. This case study of DL in cognitive computing is meaningful for visual numerosity represents a minimum level of human intelligence.",
  "stance": -0.3
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/3975/3853",
  "title": "Algorithms for Average Regret Minimization",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "In this paper, we study a problem from the realm of multicriteria decision making in which the goal is to select from a given set S of d-dimensional objects a minimum sized subset S′ with bounded regret. Thereby, regret measures the unhappiness of users which would like to select their favorite object from set S but now can only select their favorite object from the subset S′. Previous work focused on bounding the maximum regret which is determined by the most unhappy user. We propose to consider the average regret instead which is determined by the sum of (un)happiness of all possible users. We show that this regret measure comes with desirable properties as supermodularity which allows to construct approximation algorithms. Furthermore, we introduce the regret minimizing permutation problem and discuss extensions of our algorithms to the recently proposed k-regret measure. Our theoretical results are accompanied with experiments on a variety of inputs with d up to 7.",
  "stance": 0.4
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/3977/3855",
  "title": "Bayesian Functional Optimisation with Shape Prior",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "Real world experiments are expensive, and thus it is important to reach a target in a minimum number of experiments. Experimental processes often involve control variables that change over time. Such problems can be formulated as functional optimisation problem. We develop a novel Bayesian optimisation framework for such functional optimisation of expensive black-box processes. We represent the control function using Bernstein polynomial basis and optimise in the coefficient space. We derive the theory and practice required to dynamically adjust the order of the polynomial degree, and show how prior information about shape can be integrated. We demonstrate the effectiveness of our approach for short polymer fibre design and optimising learning rate schedules for deep networks.",
  "stance": 0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/4021/3899",
  "title": "Very Hard Electoral Control Problems",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "It is important to understand how the outcome of an election can be modified by an agent with control over the structure of the election. Electoral control has been studied for many election systems, but for all these systems the winner problem is in P, and so control is in NP. There are election systems, such as Kemeny, that have many desirable properties, but whose winner problems are not in NP. Thus for such systems control is not in NP, and in fact we show that it is typically complete for Σp2 (i.e., NP , the second level of the polynomial hierarchy). This is a very high level of complexity. Approaches that perform quite well for solving NP problems do not necessarily work for Σp2-complete problems. However, answer set programming is suited to express problems in Σp2 , and we present an encoding for Kemeny control.",
  "stance": -0.1
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/4042/3920",
  "title": "When Do Envy-Free Allocations Exist?",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "We consider a fair division setting in which m indivisible items are to be allocated among n agents, where the agents have additive utilities and the agents’ utilities for individual items are independently sampled from a distribution. Previous work has shown that an envy-free allocation is likely to exist when m = Ω(n logn) but not when m = n+o(n), and left open the question of determining where the phase transition from non-existence to existence occurs. We show that, surprisingly, there is in fact no universal point of transition— instead, the transition is governed by the divisibility relation between m and n. On the one hand, if m is divisible by n, an envy-free allocation exists with high probability as long as m ≥ 2n. On the other hand, if m is not “almost” divisible by n, an envy-free allocation is unlikely to exist even when m = Θ(n logn/ log log n).",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/4075/3953",
  "title": "Pareto Optimization for Subset Selection with Dynamic Cost Constraints",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "In this paper, we consider the subset selection problem for function f with constraint boundB which changes over time. We point out that adaptive variants of greedy approaches commonly used in the area of submodular optimization are not able to maintain their approximation quality. Investigating the recently introduced POMC Pareto optimization approach, we show that this algorithm efficiently computes a φ = (αf/2)(1 − 1 ef )-approximation, where αf is the submodularity ratio of f , for each possible constraint bound b ≤ B. Furthermore, we show that POMC is able to adapt its set of solutions quickly in the case that B increases. Our experimental investigations for the influence maximization in social networks show the advantage of POMC over generalized greedy algorithms.",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/4096/3974",
  "title": "Geometry-Aware Face Completion and Editing",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "Face completion is a challenging generation task because it requires generating visually pleasing new pixels that are semantically consistent with the unmasked face region. This paper proposes a geometry-aware Face Completion and Editing NETwork (FCENet) by systematically studying facial geometry from the unmasked region. Firstly, a facial geometry estimator is learned to estimate facial landmark heatmaps and parsing maps from the unmasked face image. Then, an encoder-decoder structure generator serves to complete a face image and disentangle its mask areas conditioned on both the masked face image and the estimated facial geometry images. Besides, since low-rank property exists in manually labeled masks, a low-rank regularization term is imposed on the disentangled masks, enforcing our completion network to manage occlusion area with various shape and size. Furthermore, our network can generate diverse results from the same masked input by modifying estimated facial geometry, which provides a flexible mean to edit the completed face appearance. Extensive experimental results qualitatively and quantitatively demonstrate that our network is able to generate visually pleasing face completion results and edit face attributes",
  "stance": 0.7
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/4108/3986",
  "title": "Be Inaccurate but Don’t Be Indecisive: How Error Distribution Can Affect User Experience",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "System accuracy is a crucial factor influencing user experience in intelligent interactive systems. Although accuracy is known to be important, little is known about the role of the system’s error distribution in user experience. In this paper we study, in the context of background music selection for tabletop games, how the error distribution of an intelligent system affects the user’s perceived experience. In particular, we show that supervised learning algorithms that solely optimize for prediction accuracy can make the system “indecisive”. That is, it can make the system’s errors sparsely distributed throughout the game session. We hypothesize that sparsely distributed errors can harm the users’ perceived experience and it is preferable to use a model that is somewhat inaccurate but decisive, than a model that is accurate but often indecisive. In order to test our hypothesis we introduce an ensemble approach with a restrictive voting rule that instead of erring sparsely through time, it errs consistently for a period of time. A user study in which people watched videos of Dungeons and Dragons sessions supports our hypothesis.",
  "stance": 0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/4114/3992",
  "title": "Relaxing and Restraining Queries for OBDA",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "We advocate the use of ontologies for relaxing and restraining queries, so that they retrieve either more or less answers, enabling the exploration of a given dataset. We propose a set of rewriting rules to relax and restrain conjunctive queries (CQs) over datasets mediated by an ontology written in a dialect of DL-Lite with complex role inclusions (CRIs). The addition of CRI enables the representation of knowledge about data involving ordered hierarchies of categories, in the style of multi-dimensional data models. Although CRIs in general destroy the first-order rewritability of CQs, we identify settings in which CQs remain rewritable.",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/4129/4007",
  "title": "Qualitative Spatial Logic over 2D Euclidean Spaces Is Not Finitely Axiomatisable",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "Several qualitative spatial logics used in reasoning about geospatial data have a sound and complete axiomatisation over metric spaces. It has been open whether the same axiomatisation is also sound and complete for 2D Euclidean spaces. We answer this question negatively by showing that the axiomatisations presented in (Du et al. 2013; Du and Alechina 2016) are not complete for 2D Euclidean spaces and, moreover, the logics are not finitely axiomatisable.",
  "stance": -0.6
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/4186/4064",
  "title": "Adversarial Label Learning",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "We consider the task of training classifiers without labels. We propose a weakly supervised method—adversarial label learning—that trains classifiers to perform well against an adversary that chooses labels for training data. The weak supervision constrains what labels the adversary can choose. The method therefore minimizes an upper bound of the classifier’s error rate using projected primal-dual subgradient descent. Minimizing this bound protects against bias and dependencies in the weak supervision. Experiments on real datasets show that our method can train without labels and outperforms other approaches for weakly supervised learning.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/4191/4069",
  "title": "Enhanced Random Forest Algorithms for Partially Monotone Ordinal Classification",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "One of the factors hindering the use of classification models in decision making is that their predictions may contradict expectations. In domains such as finance and medicine, the ability to include knowledge of monotone (nondecreasing) relationships is sought after to increase accuracy and user satisfaction. As one of the most successful classifiers, attempts have been made to do so for Random Forest. Ideally a solution would (a) maximise accuracy; (b) have low complexity and scale well; (c) guarantee global monotonicity; and (d) cater for multi-class. This paper first reviews the state-of-theart from both the literature and statistical libraries, and identifies opportunities for improvement. A new rule-based method is then proposed, with a maximal accuracy variant and a faster approximate variant. Simulated and real datasets are then used to perform the most comprehensive ordinal classification benchmarking in the monotone forest literature. The proposed approaches are shown to reduce the bias induced by monotonisation and thereby improve accuracy.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/5359/5215",
  "title": "Who Likes What? — SplitLBI in Exploring Preferential Diversity of Ratings",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "In recent years, learning user preferences has received significant attention. A shortcoming of existing learning to rank work lies in that they do not take into account the multilevel hierarchies from social choice to individuals. In this paper, we propose a multi-level model which learns both the common preference or utility function over the population based on features of alternatives to-be-compared, and preferential diversity functions conditioning on user categories. Such a multi-level model, enables us to simultaneously learn a coarse-grained social preference function together with a fine-grained personalized diversity. It provides us prediction power for the choices of new users on new alternatives. The key algorithm in this paper is based on Split Linearized Bregman Iteration (SplitLBI) algorithm which generates a dynamic path from the common utility to personalized preferential diversity, at different levels of sparsity on personalization. A synchronized parallel version of SplitLBI is proposed to meet the needs of fast analysis of large-scale data. The validity of the methodology are supported by experiments with both simulated and real-world datasets such as movie and dining restaurant ratings which provides us a coarse-to-fine grained preference learning.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/5414/5270",
  "title": "A Graph Auto-Encoder for Haplotype Assembly and Viral Quasispecies Reconstruction",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "Reconstructing components of a genomic mixture from data obtained by means of DNA sequencing is a challenging problem encountered in a variety of applications including single individual haplotyping and studies of viral communities. Highthroughput DNA sequencing platforms oversample mixture components to provide massive amounts of reads whose relative positions can be determined by mapping the reads to a known reference genome; assembly of the components, however, requires discovery of the reads’ origin – an NP-hard problem that the existing methods struggle to solve with the required level of accuracy. In this paper, we present a learning framework based on a graph auto-encoder designed to exploit structural properties of sequencing data. The algorithm is a neural network which essentially trains to ignore sequencing errors and infers the posterior probabilities of the origin of sequencing reads. Mixture components are then reconstructed by finding consensus of the reads determined to originate from the same genomic component. Results on realistic synthetic as well as experimental data demonstrate that the proposed framework reliably assembles haplotypes and reconstructs viral communities, often significantly outperforming state-ofthe-art techniques. Source codes, datasets and supplementary document are available at https://github.com/WuLoli/GAEseq.",
  "stance": 0.8
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/5492/5348",
  "title": "M3ER: Multiplicative Multimodal Emotion Recognition using Facial, Textual, and Speech Cues",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "We present M3ER, a learning-based method for emotion recognition from multiple input modalities. Our approach combines cues from multiple co-occurring modalities (such as face, text, and speech) and also is more robust than other methods to sensor noise in any of the individual modalities. M3ER models a novel, data-driven multiplicative fusion method to combine the modalities, which learn to emphasize the more reliable cues and suppress others on a persample basis. By introducing a check step which uses Canonical Correlational Analysis to differentiate between ineffective and effective modalities, M3ER is robust to sensor noise. M3ER also generates proxy features in place of the ineffectual modalities. We demonstrate the efficiency of our network through experimentation on two benchmark datasets, IEMOCAP and CMU-MOSEI. We report a mean accuracy of 82.7% on IEMOCAP and 89.0% on CMU-MOSEI, which, collectively, is an improvement of about 5% over prior work.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/5527/5383",
  "title": "Hard Examples for Common Variable Decision Heuristics",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "The CDCL algorithm for SAT is equivalent to the resolution proof system under a few assumptions, one of them being an optimal non-deterministic procedure for choosing the next variable to branch on. In practice this task is left to a variable decision heuristic, and since the so-called VSIDS decision heuristic is considered an integral part of CDCL, whether CDCL with a VSIDS-like heuristic is also equivalent to resolution remained a significant open question. We give a negative answer by building a family of formulas that have resolution proofs of polynomial size but require exponential time to decide in CDCL with common heuristics such as VMTF, CHB, and certain implementations of VSIDS and LRB.",
  "stance": -0.1
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/5626/5482",
  "title": "A Human-AI Loop Approach for Joint Keyword Discovery and Expectation Estimation in Micropost Event Detection",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "Microblogging platforms such as Twitter are increasingly being used in event detection. Existing approaches mainly use machine learning models and rely on event-related keywords to collect the data for model training. These approaches make strong assumptions on the distribution of the relevant microposts containing the keyword – referred to as the expectation of the distribution – and use it as a posterior regularization parameter during model training. Such approaches are, however, limited as they fail to reliably estimate the informativeness of a keyword and its expectation for model training. This paper introduces a Human-AI loop approach to jointly discover informative keywords for model training while estimating their expectation. Our approach iteratively leverages the crowd to estimate both keyword-specific expectation and the disagreement between the crowd and the model in order to discover new keywords that are most beneficial for model training. These keywords and their expectation not only improve the resulting performance but also make the model training process more transparent. We empirically demonstrate the merits of our approach, both in terms of accuracy and interpretability, on multiple real-world datasets and show that our approach improves the state of the art by 24.3%.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/5628/5484",
  "title": "Asymptotically Unambitious Artificial General Intelligence",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "General intelligence, the ability to solve arbitrary solvable problems, is supposed by many to be artificially constructible. Narrow intelligence, the ability to solve a given particularly difficult problem, has seen impressive recent development. Notable examples include self-driving cars, Go engines, image classifiers, and translators. Artificial General Intelligence (AGI) presents dangers that narrow intelligence does not: if something smarter than us across every domain were indifferent to our concerns, it would be an existential threat to humanity, just as we threaten many species despite no ill will. Even the theory of how to maintain the alignment of an AGI’s goals with our own has proven highly elusive. We present the first algorithm we are aware of for asymptotically unambitious AGI, where “unambitiousness” includes not seeking arbitrary power. Thus, we identify an exception to the Instrumental Convergence Thesis, which is roughly that by default, an AGI would seek power, including over us.",
  "stance": 0.3
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/5631/5487",
  "title": "Explainable Reinforcement Learning through a Causal Lens",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "Prominent theories in cognitive science propose that humans understand and represent the knowledge of the world through causal relationships. In making sense of the world, we build causal models in our mind to encode cause-effect relations of events and use these to explain why new events happen by referring to counterfactuals — things that did not happen. In this paper, we use causal models to derive causal explanations of the behaviour of model-free reinforcement learning agents. We present an approach that learns a structural causal model during reinforcement learning and encodes causal relationships between variables of interest. This model is then used to generate explanations of behaviour based on counterfactual analysis of the causal model. We computationally evaluate the model in 6 domains and measure performance and task prediction accuracy. We report on a study with 120 participants who observe agents playing a real-time strategy game (Starcraft II) and then receive explanations of the agents’ behaviour. We investigate: 1) participants’ understanding gained by explanations through task prediction; 2) explanation satisfaction and 3) trust. Our results show that causal model explanations perform better on these measures compared to two other baseline explanation models. ",
  "stance": 0.8
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/5696/5552",
  "title": "COTSAE: CO-Training of Structure and Attribute Embeddings for Entity Alignment",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "Entity alignment is a fundamental and vital task in Knowledge Graph (KG) construction and fusion. Previous works mainly focus on capturing the structural semantics of entities by learning the entity embeddings on the relational triples and pre-aligned ”seed entities”. Some works also seek to incorporate the attribute information to assist refining the entity embeddings. However, there are still many problems not considered, which dramatically limits the utilization of attribute information in the entity alignment. Different KGs may have lots of different attribute types, and even the same attribute may have diverse data structures and value granularities. Most importantly, attributes may have various ”contributions” to the entity alignment. To solve these problems, we propose COTSAE that combines the structure and attribute information of entities by co-training two embedding learning components, respectively. We also propose a joint attention method in our model to learn the attentions of attribute types and values cooperatively. We verified our COTSAE on several datasets from real-world KGs, and the results showed that it is significantly better than the latest entity alignment methods. The structure and attribute information can complement each other and both contribute to performance improvement.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/5712/5568",
  "title": "Detecting Semantic Anomalies",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "We critically appraise the recent interest in out-of-distribution (OOD) detection and question the practical relevance of existing benchmarks. While the currently prevalent trend is to consider different datasets as OOD, we argue that outdistributions of practical interest are ones where the distinction is semantic in nature for a specified context, and that evaluative tasks should reflect this more closely. Assuming a context of object recognition, we recommend a set of benchmarks, motivated by practical applications. We make progress on these benchmarks by exploring a multi-task learning based approach, showing that auxiliary objectives for improved semantic awareness result in improved semantic anomaly detection, with accompanying generalization benefits.",
  "stance": 0.3
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/5793/5649",
  "title": "On the Discrepancy between the Theoretical Analysis and Practical Implementations of Compressed Communication for Distributed Deep Learning",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "Compressed communication, in the form of sparsification or quantization of stochastic gradients, is employed to reduce communication costs in distributed data-parallel training of deep neural networks. However, there exists a discrepancy between theory and practice: while theoretical analysis of most existing compression methods assumes compression is applied to the gradients of the entire model, many practical implementations operate individually on the gradients of each layer of the model. In this paper, we prove that layer-wise compression is, in theory, better, because the convergence rate is upper bounded by that of entire-model compression for a wide range of biased and unbiased compression methods. However, despite the theoretical bound, our experimental study of six well-known methods shows that convergence, in practice, may or may not be better, depending on the actual trained model and compression ratio. Our findings suggest that it would be advantageous for deep learning frameworks to include support for both layerwise and entire-model compression.",
  "stance": 0.4
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/6218/6074",
  "title": "Fair Procedures for Fair Stable Marriage Outcomes",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "Given a two-sided market where each agent ranks those on the other side by preference, the stable marriage problem calls for finding a perfect matching such that no pair of agents prefer each other to their matches. Recent studies show that the number of stable solutions can be large in practice. Yet the classical solution to the problem, the Gale-Shapley (GS) algorithm, assigns an optimal match to each agent on one side, and a pessimal one to each on the other side; such a solution may fare well in terms of equity only in highly asymmetric markets. Finding a stable matching that minimizes the sex equality cost, an equity measure expressing the discrepancy of mean happiness among the two sides, is strongly NP-hard. Extant heuristics either (a) oblige some agents to involuntarily abandon their matches, or (b) bias the outcome in favor of some agents, or (c) need high-polynomial or unbounded time. We provide the first procedurally fair algorithms that output equitable stable marriages and are guaranteed to terminate in at most cubic time; the key to this breakthrough is the monitoring of a monotonic state function and the use of a selective criterion for accepting proposals. Our experiments with diverse simulated markets show that: (a) extant heuristics fail to yield high equity; (b) the best solution found by the GS algorithm can be very far from optimal equity; and (c) our procedures stand out in both efficiency and equity, even when compared to a non-procedurally fair approximation scheme.",
  "stance": 0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/6253/6109",
  "title": "Learning to Map Frequent Phrases to Sub-Structures of Meaning Representation for Neural Semantic Parsing",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "Neural semantic parsers usually generate meaning representation tokens from natural language tokens via an encoderdecoder model. However, there is often a vocabularymismatch problem between natural language utterances and logical forms. That is, one word maps to several atomic logical tokens, which need to be handled as a whole, rather than individual logical tokens at multiple steps. In this paper, we propose that the vocabulary-mismatch problem can be effectively resolved by leveraging appropriate logical tokens. Specifically, we exploit macro actions, which are of the same granularity of words/phrases, and allow the model to learn mappings from frequent phrases to corresponding substructures of meaning representation. Furthermore, macro actions are compact, and therefore utilizing them can significantly reduce the search space, which brings a great benefit to weakly supervised semantic parsing. Experiments show that our method leads to substantial performance improvement on three benchmarks, in both supervised and weakly supervised settings.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/6254/6110",
  "title": "Attending to Entities for Better Text Understanding",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "Recent progress in NLP witnessed the development of largescale pre-trained language models (GPT, BERT, XLNet, etc.) based on Transformer (Vaswani et al. 2017), and in a range of end tasks, such models have achieved state-of-the-art results, approaching human performance. This clearly demonstrates the power of the stacked self-attention architecture when paired with a sufficient number of layers and a large amount of pre-training data. However, on tasks that require complex and long-distance reasoning where surface-level cues are not enough, there is still a large gap between the pre-trained models and human performance. Strubell et al. (2018) recently showed that it is possible to inject knowledge of syntactic structure into a model through supervised self-attention. We conjecture that a similar injection of semantic knowledge, in particular, coreference information, into an existing model would improve performance on such complex problems. On the LAMBADA (Paperno et al. 2016) task, we show that a model trained from scratch with coreference as auxiliary supervision for self-attention outperforms the largest GPT-2 model, setting the new state-of-the-art, while only containing a tiny fraction of parameters compared to GPT-2. We also conduct a thorough analysis of different variants of model architectures and supervision configurations, suggesting future directions on applying similar techniques to other problems.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/6541/6397",
  "title": "Novel Is Not Always Better: On the Relation between Novelty and Dominance Pruning",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "Novelty pruning is a planning technique that focuses on exploring states that are novel, i.e., those containing facts that have not been seen before. This seemingly simple idea has had a huge impact on the state of the art in planning though its effectiveness is not entirely understood yet. We relate novelty to dominance pruning, which compares states to previously seen states to eliminate those that are provably worse in terms of goal distance. Novelty can be interpreted as an unsafe approximation of dominance, where states containing novel facts are relevant because they enable new paths to the goal and, therefore, they are less likely to be dominated by others. This provides a framework to understand the success of novelty, resulting in new variants that combine both techniques.",
  "stance": 0.3
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/6559/6415",
  "title": "Deep Bayesian Nonparametric Learning of Rules and Plans from Demonstrations with a Learned Automaton Prior",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "We introduce a method to learn imitative policies from expert demonstrations that are interpretable and manipulable. We achieve interpretability by modeling the interactions between high-level actions as an automaton with connections to formal logic. We achieve manipulability by integrating this automaton into planning, so that changes to the automaton have predictable effects on the learned behavior. These qualities allow a human user to first understand what the model has learned, and then either correct the learned behavior or zeroshot generalize to new, similar tasks. We build upon previous work by no longer requiring additional supervised information which is hard to collect in practice. We achieve this by using a deep Bayesian nonparametric hierarchical model. We test our model on several domains and also show results for a real-world implementation on a mobile robotic arm platform.",
  "stance": 0.6
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/6596/6452",
  "title": "Modeling Probabilistic Commitments for Maintenance Is Inherently Harder than for Achievement",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "Most research on probabilistic commitments focuses on commitments to achieve enabling preconditions for other agents. Our work reveals that probabilistic commitments to instead maintain preconditions for others are surprisingly harder to use well than their achievement counterparts, despite strong semantic similarities. We isolate the key difference as being not in how the commitment provider is constrained, but rather in how the commitment recipient can locally use the commitment specification to approximately model the provider’s effects on the preconditions of interest. Our theoretic analyses show that we can more tightly bound the potential suboptimality due to approximate modeling for achievement than for maintenance commitments. We empirically evaluate alternative approximate modeling strategies, confirming that probabilistic maintenance commitments are qualitatively more challenging for the recipient to model well, and indicating the need for more detailed specifications that can sacrifice some of the agents’ autonomy.",
  "stance": -0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/6616/6470",
  "title": "Detecting Human-Object Interactions via Functional Generalization",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "We present an approach for detecting human-object interactions (HOIs) in images, based on the idea that humans interact with functionally similar objects in a similar manner. The proposed model is simple and efficiently uses the data, visual features of the human, relative spatial orientation of the human and the object, and the knowledge that functionally similar objects take part in similar interactions with humans. We provide extensive experimental validation for our approach and demonstrate state-of-the-art results for HOI detection. On the HICO-Det dataset our method achieves a gain of over 2.5% absolute points in mean average precision (mAP) over stateof-the-art. We also show that our approach leads to significant performance gains for zero-shot HOI detection in the seen object setting. We further demonstrate that using a generic object detector, our model can generalize to interactions involving previously unseen objects.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/6622/6476",
  "title": "Learning Deep Relations to Promote Saliency Detection",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "Though saliency detectors has made stunning progress recently. The performances of the state-of-the-art saliency detectors are not acceptable in some confusing areas, e.g., object boundary. We argue that the feature spatial independence should be one of the root cause. This paper explores the ubiquitous relations on the deep features to promote the existing saliency detectors efficiently. We establish the relation by maximizing the mutual information of the deep features of the same category via deep neural networks to break this independence. We introduce a threshold-constrained training pair construction strategy to ensure that we can accurately estimate the relations between different image parts in a selfsupervised way. The relation can be utilized to further excavate the salient areas and inhibit confusing backgrounds. The experiments demonstrate that our method can significantly boost the performance of the state-of-the-art saliency detectors on various benchmark datasets. Besides, our model is label-free and extremely efficient. The inference speed is 140 FPS on a single GTX1080 GPU.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/6636/6490",
  "title": "A Coarse-to-Fine Adaptive Network for Appearance-Based Gaze Estimation",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "Human gaze is essential for various appealing applications. Aiming at more accurate gaze estimation, a series of recent works propose to utilize face and eye images simultaneously. Nevertheless, face and eye images only serve as independent or parallel feature sources in those works, the intrinsic correlation between their features is overlooked. In this paper we make the following contributions: 1) We propose a coarseto-fine strategy which estimates a basic gaze direction from face image and refines it with corresponding residual predicted from eye images. 2) Guided by the proposed strategy, we design a framework which introduces a bi-gram model to bridge gaze residual and basic gaze direction, and an attention component to adaptively acquire suitable fine-grained feature. 3) Integrating the above innovations, we construct a coarse-to-fine adaptive network named CA-Net and achieve state-of-the-art performances on MPIIGaze and EyeDiap.",
  "stance": 0.9
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/6705/6559",
  "title": "CIAN: Cross-Image Affinity Net for Weakly Supervised Semantic Segmentation",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "Weakly supervised semantic segmentation with only imagelevel labels saves large human effort to annotate pixel-level labels. Cutting-edge approaches rely on various innovative constraints and heuristic rules to generate the masks for every single image. Although great progress has been achieved by these methods, they treat each image independently and do not take account of the relationships across different images. In this paper, however, we argue that the cross-image relationship is vital for weakly supervised segmentation. Because it connects related regions across images, where supplementary representations can be propagated to obtain more consistent and integral regions. To leverage this information, we propose an end-to-end cross-image affinity module, which exploits pixel-level cross-image relationships with only imagelevel labels. By means of this, our approach achieves 64.3% and 65.3% mIoU on Pascal VOC 2012 validation and test set respectively, which is a new state-of-the-art result by only using image-level labels for weakly supervised semantic segmentation, demonstrating the superiority of our approach.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/6775/6629",
  "title": "Semantics-Aligned Representation Learning for Person Re-Identification",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "Person re-identification (reID) aims to match person images to retrieve the ones with the same identity. This is a challenging task, as the images to be matched are generally semantically misaligned due to the diversity of human poses and capture viewpoints, incompleteness of the visible bodies (due to occlusion), etc. In this paper, we propose a framework that drives the reID network to learn semantics-aligned feature representation through delicate supervision designs. Specifically, we build a Semantics Aligning Network (SAN) which consists of a base network as encoder (SA-Enc) for reID, and a decoder (SA-Dec) for reconstructing/regressing the densely semantics aligned full texture image. We jointly train the SAN under the supervisions of person re-identification and aligned texture generation. Moreover, at the decoder, besides the reconstruction loss, we add Triplet ReID constraints over the feature maps as the perceptual losses. The decoder is discarded in the inference and thus our scheme is computationally efficient. Ablation studies demonstrate the effectiveness of our design. We achieve the state-of-the-art performances on the benchmark datasets CUHK03, Market1501, MSMT17, and the partial person reID dataset Partial REID.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/6779/6633",
  "title": "Real-Time Object Tracking via Meta-Learning: Efficient Model Adaptation and One-Shot Channel Pruning",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "We propose a novel meta-learning framework for real-time object tracking with efficient model adaptation and channel pruning. Given an object tracker, our framework learns to fine-tune its model parameters in only a few gradient-descent iterations during tracking while pruning its network channels using the target ground-truth at the first frame. Such a learning problem is formulated as a meta-learning task, where a meta-tracker is trained by updating its meta-parameters for initial weights, learning rates, and pruning masks through carefully designed tracking simulations. The integrated metatracker greatly improves tracking performance by accelerating the convergence of online learning and reducing the cost of feature computation. Experimental evaluation on the standard datasets demonstrates its outstanding accuracy and speed compared to the state-of-the-art methods.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/7042/6896",
  "title": "A System for Medical Information Extraction and Verification from Unstructured Text",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "A wealth of medical knowledge has been encoded in terminologies like SNOMED CT, NCI, FMA, and more. However, these resources are usually lacking information like relations between diseases, symptoms, and risk factors preventing their use in diagnostic or other decision making applications. In this paper we present a pipeline for extracting such information from unstructured text and enriching medical knowledge bases. Our approach uses Semantic Role Labelling and is unsupervised. We show how we dealt with several deficiencies of SRL-based extraction, like copula verbs, relations expressed through nouns, and assigning scores to extracted triples. The system have so far extracted about 120K relations and in-house doctors verified about 5k relationships. We compared the output of the system with a manually constructed network of diseases, symptoms and risk factors build by doctors in the course of a year. Our results show that our pipeline extracts good quality and precise relations and speeds up the knowledge acquisition process considerably.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/7044/6898",
  "title": "Machine-Learning-Based Functional Microcirculation Analysis",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "Analysis of microcirculation is an important clinical and research task. Functional analysis of the microcirculation allows researchers to understand how blood flowing in a tissues’ smallest vessels affects disease progression, organ function, and overall health. Current methods of manual analysis of microcirculation are tedious and timeconsuming, limiting the quick turnover of results. There has been limited research on automating functional analysis of microcirculation. As such, in this paper, we propose a twostep machine-learning-based algorithm to functionally assess microcirculation videos. The first step uses a modified vessel segmentation algorithm to extract the location of vessel-like structures. While the second step uses a 3D-CNN to assess whether the vessel-like structures contained flowing blood. To our knowledge, this is the first application of machine learning for functional analysis of microcirculation. We use real-world labelled microcirculation videos to train and test our algorithm and assess its performance. More precisely, we demonstrate that our two-step algorithm can efficiently analyze real data with high accuracy (90%).",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/7056/6910",
  "title": "AI Trust in Business Processes: The Need for Process-Aware Explanations",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "Business processes underpin a large number of enterprise operations including processing loan applications, managing invoices, and insurance claims. There is a large opportunity for infusing AI to reduce cost or provide better customer experience and the BPM literature is rich in machine learning solutions. More recently, deep learning models have been applied to process predictions. Unfortunately, companies have applied or adopted very few of these innovations. We assert that a reason for this lack of adoption is that business users are risk-averse and do not implicitly trust AI models. We challenge the BPM community to build on the AI interpretability literature, and the AI Trust community to understand what it means to take advantage of business process artifacts in order to provide business level explanations.",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16109/15916",
  "title": "In-game Residential Home Planning via Visual Context-aware Global Relation Learning",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "In this paper, we propose an effective global relation learning algorithm to recommend an appropriate location of a building unit for in-game customization of residential home complex. Given a construction layout, we propose a visual contextaware graph generation network that learns the implicit global relations among the scene components and infers the location of a new building unit. The proposed network takes as input the scene graph and the corresponding top-view depth image. It provides the location recommendations for a newlyadded building units by learning an auto-regressive edge distribution conditioned on existing scenes. We also introduce a global graph-image matching loss to enhance the awareness of essential geometry semantics of the site. Qualitative and quantitative experiments demonstrate that the recommended location well reflects the implicit spatial rules of components in the residential estates, and it is instructive and practical to locate the building units in the 3D scene of the complex con-",
  "stance": 0.6
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16131/15938",
  "title": "GTA: Graph Truncated Attention for Retrosynthesis",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Retrosynthesis is the task of predicting reactant molecules from a given product molecule and is, important in organic chemistry because the identification of a synthetic path is as demanding as the discovery of new chemical compounds. Recently, the retrosynthesis task has been solved automatically without human expertise using powerful deep learning models. Recent deep models are primarily based on seq2seq or graph neural networks depending on the function of molecular representation, sequence, or graph. Current state-of-theart models represent a molecule as a graph, but they require joint training with auxiliary prediction tasks, such as the most probable reaction template or reaction center prediction. Furthermore, they require additional labels by experienced chemists, thereby incurring additional cost. Herein, we propose a novel template-free model, i.e., Graph Truncated Attention (GTA), which leverages both sequence and graph representations by inserting graphical information into a seq2seq model. The proposed GTA model masks the self-attention layer using the adjacency matrix of product molecule in the encoder and applies a new loss using atom mapping acquired from an automated algorithm to the cross-attention layer in the decoder. Our model achieves new state-of-the-art records, i.e., exact match top-1 and top-10 accuracies of 51.1% and 81.6% on the USPTO-50k benchmark dataset, respectively, and 46.0% and 70.0% on the USPTO-full dataset, respectively, both without any reaction class information. The GTA model surpasses prior graph-based template-free models by 2% and 7% in terms of the top-1 and top-10 accuracies on the USPTO-50k dataset, respectively, and by over 6% for both the top-1 and top-10 accuracies on the USPTO-full dataset.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16140/15947",
  "title": "Sketch Generation with Drawing Process Guided by Vector Flow and Grayscale",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "We propose a novel image-to-pencil translation method that could not only generate high-quality pencil sketches but also offer the drawing process. Existing pencil sketch algorithms are based on texture rendering rather than the direct imitation of strokes, making them unable to show the drawing process but only a final result. To address this challenge, we first establish a pencil stroke imitation mechanism. Next, we develop a framework with three branches to guide stroke drawing: the first branch guides the direction of the strokes, the second branch determines the shade of the strokes, and the third branch enhances the details further. Under this framework’s guidance, we can produce a pencil sketch by drawing one stroke every time. Our method is fully interpretable. Comparison with existing pencil drawing algorithms shows that our method is superior to others in terms of texture quality, style, and user evaluation. Our code and supplementary material are now available at: https://github.com/TZYSJTU/Sketch-Generation-withDrawing-Process-Guided-by-Vector-Flow-and-Grayscale",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16194/16001",
  "title": "Deductive Learning for Weakly-Supervised 3D Human Pose Estimation via Uncalibrated Cameras",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Without prohibitive and laborious 3D annotations, weakly-supervised 3D human pose methods mainly employ the model regularization with geometric projection consistency or geometry estimation from multi-view images. Nevertheless, those approaches explicitly need known parameters of calibrated cameras, exhibiting a limited model generalization in various realistic scenarios. To mitigate this issue, in this paper, we propose a Deductive Weakly-Supervised Learning (DWSL) for 3D human pose machine. Our DWSL firstly learns latent representations on depth and camera pose for 3D pose reconstruction. Since weak supervision usually causes ill-conditioned learning or inferior estimation, our DWSL introduces deductive reasoning to make an inference for human pose from a view to another and develops a reconstruction loss to demonstrate what the model learns and infers is reliable. This learning by deduction strategy employs the view-transform demonstration and structural rules derived from depth, geometry and angle constraints, which improves the reliability of the model training with weak supervision. On three 3D human pose benchmarks, we conduct extensive experiments to evaluate our proposed method, which achieves superior performance in comparison with state-of-the-art weak-supervised methods. Particularly, our model shows an appealing potential for learning from 2D data captured in dynamic outdoor scenes, which demonstrates promising robustness and generalization in realistic scenarios. Our code is publicly available at https://github.com/XipengChen/DWSL-3D-pose. ",
  "stance": 0.7
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16226/16033",
  "title": "Deep Metric Learning with Self-Supervised Ranking",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Deep metric learning aims to learn a deep embedding space, where similar objects are pushed towards together and different objects are repelled against. Existing approaches typically use inter-class characteristics, e.g., class-level information or instance-level similarity, to obtain semantic relevance of data points and get a large margin between different classes in the embedding space. However, the intra-class characteristics, e.g., local manifold structure or relative relationship within the same class, are usually overlooked in the learning process. Hence the data structure cannot be fully exploited and the output embeddings have limitation in retrieval. More importantly, retrieval results lack in a good ranking. This paper presents a novel self-supervised ranking auxiliary framework, which captures intra-class characteristics as well as inter-class characteristics for better metric learning. Our method defines specific transform functions to simulates the local structure change of intra-class in the initial image domain, and formulates a self-supervised learning procedure to fully exploit this property and preserve it in the embedding space. Extensive experiments on three standard benchmarks show that our method significantly improves and outperforms the state-of-the-art methods on the performances of both retrieval and ranking by 2%-4%.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16229/16036",
  "title": "Learning Local Neighboring Structure for Robust 3D Shape Representation",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Mesh is a powerful data structure for 3D shapes. Representation learning for 3D meshes is important in many computer vision and graphics applications. The recent success of convolutional neural networks (CNNs) for structured data (e.g., images) suggests the value of adapting insight from CNN for 3D shapes. However, 3D shape data are irregular since each node’s neighbors are unordered. Various graph neural networks for 3D shapes have been developed with isotropic filters or predefined local coordinate systems to overcome the node inconsistency on graphs. However, isotropic filters or predefined local coordinate systems limit the representation power. In this paper, we propose a local structure-aware anisotropic convolutional operation (LSA-Conv) that learns adaptive weighting matrices for each node according to the local neighboring structure and performs shared anisotropic filters. In fact, the learnable weighting matrix is similar to the attention matrix in the random synthesizer – a new Transformer model for natural language processing (NLP). Comprehensive experiments demonstrate that our model produces significant improvement in 3D shape reconstruction compared to state-of-the-art methods.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16253/16060",
  "title": "Text-Guided Graph Neural Networks for Referring 3D Instance Segmentation",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "This paper addresses a new task called referring 3D instance segmentation, which aims to segment out the target instance in a 3D scene given a query sentence. Previous work on scene understanding has explored visual grounding with natural language guidance, yet the emphasis is mostly constrained on images and videos. We propose a Text-guided Graph Neural Network (TGNN) for referring 3D instance segmentation on point clouds. Given a query sentence and the point cloud of a 3D scene, our method learns to extract per-point features and predicts an offset to shift each point toward its object center. Based on the point features and the offsets, we cluster the points to produce fused features and coordinates for the candidate objects. The resulting clusters are modeled as nodes in a Graph Neural Network to learn the representations that encompass the relation structure for each candidate object. The GNN layers leverage each object’s features and its relations with neighbors to generate an attention heatmap for the input sentence expression. Finally, the attention heatmap is used to “guide” the aggregation of information from neighborhood nodes. Our method achieves state-of-the-art performance on referring 3D instance segmentation and 3D localization on ScanRefer, Nr3D, and Sr3D benchmarks, respectively.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16254/16061",
  "title": "Initiative Defense against Facial Manipulation",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Benefiting from the development of generative adversarial networks (GAN), facial manipulation has achieved significant progress in both academia and industry recently. It inspires an increasing number of entertainment applications but also incurs severe threats to individual privacy and even political security meanwhile. To mitigate such risks, many countermeasures have been proposed. However, the great majority methods are designed in a passive manner, which is to detect whether the facial images or videos are tampered after their wide propagation. These detection-based methods have a fatal limitation, that is, they only work for ex-post forensics but can not prevent the engendering of malicious behavior. To address the limitation, in this paper, we propose a novel framework of initiative defense to degrade the performance of facial manipulation models controlled by malicious users. The basic idea is to actively inject imperceptible venom into target facial data before manipulation. To this end, we first imitate the target manipulation model with a surrogate model, and then devise a poison perturbation generator to obtain the desired venom. An alternating training strategy are further leveraged to train both the surrogate model and the perturbation generator. Two typical facial manipulation tasks: face attribute editing and face reenactment, are considered in our initiative defense framework. Extensive experiments demonstrate the effectiveness and robustness of our framework in different settings. Finally, we hope this work can shed some light on initiative countermeasures against more adversarial scenarios.",
  "stance": 0.7
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16269/16076",
  "title": "Discriminative Region Suppression for Weakly-Supervised Semantic Segmentation",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Weakly-supervised semantic segmentation (WSSS) using image-level labels has recently attracted much attention for reducing annotation costs. Existing WSSS methods utilize localization maps from the classification network to generate pseudo segmentation labels. However, since localization maps obtained from the classifier focus only on sparse discriminative object regions, it is difficult to generate highquality segmentation labels. To address this issue, we introduce discriminative region suppression (DRS) module that is a simple yet effective method to expand object activation regions. DRS suppresses the attention on discriminative regions and spreads it to adjacent non-discriminative regions, generating dense localization maps. DRS requires few or no additional parameters and can be plugged into any network. Furthermore, we introduce an additional learning strategy to give a self-enhancement of localization maps, named localization map refinement learning. Benefiting from this refinement learning, localization maps are refined and enhanced by recovering some missing parts or removing noise itself. Due to its simplicity and effectiveness, our approach achieves mIoU 71.4% on the PASCAL VOC 2012 segmentation benchmark using only image-level labels. Extensive experiments demonstrate the effectiveness of our approach.",
  "stance": 0.8
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16281/16088",
  "title": "Learning Monocular Depth in Dynamic Scenes via Instance-Aware Projection Consistency",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "We present an end-to-end joint training framework that explicitly models 6-DoF motion of multiple dynamic objects, ego-motion, and depth in a monocular camera setup without supervision. Our technical contributions are three-fold. First, we highlight the fundamental difference between inverse and forward projection while modeling the individual motion of each rigid object, and propose a geometrically correct projection pipeline using a neural forward projection module. Second, we design a unified instance-aware photometric and geometric consistency loss that holistically imposes self-supervisory signals for every background and object region. Lastly, we introduce a general-purpose auto-annotation scheme using any off-the-shelf instance segmentation and optical flow models to produce video instance segmentation maps that will be utilized as input to our training pipeline. These proposed elements are validated in a detailed ablation study. Through extensive experiments conducted on the KITTI and Cityscapes dataset, our framework is shown to outperform the state-of-the-art depth and motion estimation methods. Our code, dataset, and models are publicly available.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16291/16098",
  "title": "Joint Semantic-geometric Learning for Polygonal Building Segmentation",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Building extraction from aerial or satellite images has been an important research problem in remote sensing and computer vision domains for decades. Compared with pixel-wise semantic segmentation models that output raster building segmentation map, polygonal building segmentation approaches produce more realistic building polygons that are in the desirable vector format for practical applications. Despite the substantial efforts over recent years, state-of-the-art polygonal building segmentation methods still suffer from several limitations, e.g., (1) relying on a perfect segmentation map to guarantee the vectorization quality; (2) requiring a complex post-processing procedure; (3) generating inaccurate vertices with a fixed quantity, a wrong sequential order, self-intersections, etc. To tackle the above issues, in this paper, we propose a polygonal building segmentation approach and make the following contributions: (1) We design a multitask segmentation network for joint semantic and geometric learning via three tasks, i.e., pixel-wise building segmentation, multi-class corner prediction, and edge orientation prediction. (2) We propose a simple but effective vertex generation module for transforming the segmentation contour into high-quality polygon vertices. (3) We further propose a polygon refinement network that automatically moves the polygon vertices into more accurate locations. Results on two popular building segmentation datasets demonstrate that our approach achieves significant improvements for both building instance segmentation (with 2% F1-score gain) and polygon vertex prediction (with 6% F1-score gain) compared with current state-of-the-art methods.",
  "stance": 0.9
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16295/16102",
  "title": "Inference Fusion with Associative Semantics for Unseen Object Detection",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "We study the problem of object detection when training and test objects are disjoint, i.e. no training examples of the target classes are available. Existing unseen object detection approaches usually combine generic detection frameworks with a single-path unseen classifier, by aligning object regions with semantic class embeddings. In this paper, inspired from human cognitive experience, we propose a simple but effective dual-path detection model that further explores associative semantics to supplement the basic visual-semantic knowledge transfer. We use a novel target-centric multipleassociation strategy to establish concept associations, to ensure that the predictor generalized to unseen domain can be learned during training. In this way, through a reasonable inference fusion mechanism, those two parallel reasoning paths can strengthen the correlation between seen and unseen objects, thus improving detection performance. Experiments show that our inductive method can significantly boost the performance by 7.42% over inductive models, and even 5.25% over transductive models on MSCOCO dataset.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16298/16105",
  "title": "SD-Pose: Semantic Decomposition for Cross-Domain 6D Object Pose Estimation",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "The current leading 6D object pose estimation methods rely heavily on annotated real data, which is highly costly to acquire. To overcome this, many works have proposed to introduce computer-generated synthetic data. However, bridging the gap between the synthetic and real data remains a severe problem. Images depicting different levels of realism/semantics usually have different transferability between the synthetic and real domains. Inspired by this observation, we introduce an approach, SD-Pose, that explicitly decomposes the input image into multi-level semantic representations and then combines the merits of each representation to bridge the domain gap. Our comprehensive analyses and experiments show that our semantic decomposition strategy can fully utilize the different domain similarities of different representations, thus allowing us to outperform the state of the art on modern 6D object pose datasets without accessing any real data during training.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16307/16114",
  "title": "Spatiotemporal Graph Neural Network based Mask Reconstruction for Video Object Segmentation",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "This paper addresses the task of segmenting class-agnostic objects in semi-supervised setting. Although previous detection based methods achieve relatively good performance, these approaches extract the best proposal by a greedy strategy, which may lose the local patch details outside the chosen candidate. In this paper, we propose a novel spatiotemporal graph neural network (STG-Net) to reconstruct more accurate masks for video object segmentation, which captures the local contexts by utilizing all proposals. In the spatial graph, we treat object proposals of a frame as nodes and represent their correlations with an edge weight strategy for mask context aggregation. To capture temporal information from previous frames, we use a memory network to refine the mask of current frame by retrieving historic masks in a temporal graph. The joint use of both local patch details and temporal relationships allow us to better address the challenges such as object occlusion and missing. Without online learning and finetuning, our STG-Net achieves state-of-the-art performance on four large benchmarks (DAVIS, YouTube-VOS, SegTrackv2, and YouTube-Objects), demonstrating the effectiveness of the proposed approach.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16345/16152",
  "title": "Dual Adversarial Graph Neural Networks for Multi-label Cross-modal Retrieval",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Cross-modal retrieval has become an active study field with the expanding scale of multimodal data. To date, most existing methods transform multimodal data into a common representation space where semantic similarities between items can be directly measured across different modalities. However, these methods typically suffer from following limitations: 1) They usually attempt to bridge the modality gap by designing losses in the common representation space which may not be sufficient to eliminate potential heterogeneity of different modalities in the common space. 2) They typically treat labels as independent individuals and ignore label relationships which are important for constructing semantic links between multimodal data. In this work, we propose a novel Dual Adversarial Graph Neural Networks (DAGNN) composed of the dual generative adversarial networks and the multi-hop graph neural networks, which learn modality-invariant and discriminative common representations for cross-modal retrieval. Firstly, we construct the dual generative adversarial networks to project multimodal data into a common representation space. Secondly, we leverage the multi-hop graph neural networks, in which a layer aggregation mechanism is proposed to exploit multi-hop propagation information, to capture the label correlation dependency and learn inter-dependent classifiers. Comprehensive experiments conducted on two cross-modal retrieval benchmark datasets, NUS-WIDE and MIRFlickr, indicate the superiority of DAGNN.",
  "stance": 0.8
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16357/16164",
  "title": "Social-DPF: Socially Acceptable Distribution Prediction of Futures",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "We consider long-term path forecasting problems in crowds, where future sequence trajectories are generated given a short observation. Recent methods for this problem have focused on modeling social interactions and predicting multi-modal futures. However, it is not easy for machines to successfully consider social interactions, such as avoiding collisions while considering the uncertainty of futures under a highly interactive and dynamic scenario. In this paper, we propose a model that incorporates multiple interacting motion sequences jointly and predicts multi-modal socially acceptable distributions of futures. Specifically, we introduce a new aggregation mechanism for social interactions, which selectively models long-term inter-related dynamics between movements in a shared environment through a message passing mechanism. Moreover, we propose a loss function that not only accesses how accurate the estimated distributions of the futures are but also considers collision avoidance. We further utilize mixture density functions to describe the trajectories and learn multi-modality of future paths. Extensive experiments over several trajectory prediction benchmarks demonstrate that our method is able to forecast socially acceptable distributions in complex scenarios.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16431/16238",
  "title": "ERNIE-ViL: Knowledge Enhanced Vision-Language Representations through Scene Graphs",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "We propose a knowledge-enhanced approach, ERNIE-ViL, which incorporates structured knowledge obtained from scene graphs to learn joint representations of vision-language. ERNIE-ViL tries to build the detailed semantic connections (objects, attributes of objects and relationships between objects) across vision and language, which are essential to vision-language cross-modal tasks. Utilizing scene graphs of visual scenes, ERNIE-ViL constructs Scene Graph Prediction tasks, i.e., Object Prediction, Attribute Prediction and Relationship Prediction tasks in the pre-training phase. Specifically, these prediction tasks are implemented by predicting nodes of different types in the scene graph parsed from the sentence. Thus, ERNIE-ViL can learn the joint representations characterizing the alignments of the detailed semantics across vision and language. After pre-training on large scale image-text aligned datasets, we validate the effectiveness of ERNIE-ViL on 5 cross-modal downstream tasks. ERNIE-ViL achieves state-of-the-art performances on all these tasks and ranks the first place on the VCR leaderboard with an absolute improvement of 3.7%.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16469/16276",
  "title": "RESA: Recurrent Feature-Shift Aggregator for Lane Detection",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Lane detection is one of the most important tasks in selfdriving. Due to various complex scenarios (e.g., severe occlusion, ambiguous lanes, etc.) and the sparse supervisory signals inherent in lane annotations, lane detection task is still challenging. Thus, it is difficult for the ordinary convolutional neural network (CNN) to train in general scenes to catch subtle lane feature from the raw image. In this paper, we present a novel module named REcurrent Feature-Shift Aggregator (RESA) to enrich lane feature after preliminary feature extraction with an ordinary CNN. RESA takes advantage of strong shape priors of lanes and captures spatial relationships of pixels across rows and columns. It shifts sliced feature map recurrently in vertical and horizontal directions and enables each pixel to gather global information. RESA can conjecture lanes accurately in challenging scenarios with weak appearance clues by aggregating sliced feature map. Moreover, we propose a Bilateral Up-Sampling Decoder that combines coarse-grained and fine-detailed features in the upsampling stage. It can recover the low-resolution feature map into pixel-wise prediction meticulously. Our method achieves state-of-the-art results on two popular lane detection benchmarks (CULane and Tusimple). Code has been made available at: https://github.com/ZJULearning/resa.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16483/16290",
  "title": "A SAT-based Resolution of Lam's Problem",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "In 1989, computer searches by Lam, Thiel, and Swiercz experimentally resolved Lam’s problem from projective geometry—the long-standing problem of determining if a projective plane of order ten exists. Both the original search and an independent verification in 2011 discovered no such projective plane. However, these searches were each performed using highly specialized custom-written code and did not produce nonexistence certificates. In this paper, we resolve Lam’s problem by translating the problem into Boolean logic and use satisfiability (SAT) solvers to produce nonexistence certificates that can be verified by a third party. Our work uncovered consistency issues in both previous searches—highlighting the difficulty of relying on specialpurpose search code for nonexistence results.",
  "stance": -0.6
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16508/16315",
  "title": "Turbocharging Treewidth-Bounded Bayesian Network Structure Learning",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "We present a new approach for learning the structure of a treewidth-bounded Bayesian Network (BN). The key to our approach is applying an exact method (based on MaxSAT) locally, to improve the score of a heuristically computed BN. This approach allows us to scale the power of exact methods— so far only applicable to BNs with several dozens of random variables—to large BNs with several thousands of random variables. Our experiments show that our method improves the score of BNs provided by state-of-the-art heuristic methods, often significantly.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16555/16362",
  "title": "Learning Accurate and Interpretable Decision Rule Sets from Neural Networks",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "This paper proposes a new paradigm for learning a set of independent logical rules in disjunctive normal form as an interpretable model for classification. We consider the problem of learning an interpretable decision rule set as training a neural network in a specific, yet very simple two-layer architecture. Each neuron in the first layer directly maps to an interpretable if-then rule after training, and the output neuron in the second layer directly maps to a disjunction of the firstlayer rules to form the decision rule set. Our representation of neurons in this first rules layer enables us to encode both the positive and the negative association of features in a decision rule. State-of-the-art neural net training approaches can be leveraged for learning highly accurate classification models. Moreover, we propose a sparsity-based regularization approach to balance between classification accuracy and the simplicity of the derived rules. Our experimental results show that our method can generate more accurate decision rule sets than other state-of-the-art rule-learning algorithms with better accuracy-simplicity trade-offs. Further, when compared with uninterpretable black-box machine learning approaches such as random forests and full-precision deep neural networks, our approach can easily find interpretable decision rule sets that have comparable predictive performance.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16562/16369",
  "title": "A Hybrid Probabilistic Approach for Table Understanding",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Tables of data are used to record vast amounts of socioeconomic, scientific, and governmental information. Although humans create tables using underlying organizational principles, unfortunately AI systems struggle to understand the contents of these tables. This paper introduces an end-to-end system for table understanding, the process of capturing the relational structure of data in tables. We introduce models that identify cell types, group these cells into blocks of data that serve a similar functional role, and predict the relationships between these blocks. We introduce a hybrid, neuro-symbolic approach, combining embedded representations learned from thousands of tables with probabilistic constraints that capture regularities in how humans organize tables. Our neurosymbolic model is better able to capture positional invariants of headers and enforce homogeneity of data types. One limitation in this research area is the lack of rich datasets for evaluating end-to-end table understanding, so we introduce a new benchmark dataset comprised of 431 diverse tables from data.gov. The evaluation results show that our system achieves the state-of-the-art performance on cell type classification, block identification, and relationship prediction, improving over prior efforts by up to 7% of macro F1 score.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16625/16432",
  "title": "Dynamic Neuro-Symbolic Knowledge Graph Construction for Zero-shot Commonsense Question Answering",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Understanding narratives requires reasoning about implicit world knowledge related to the causes, effects, and states of situations described in text. At the core of this challenge is how to access contextually relevant knowledge on demand and reason over it. In this paper, we present initial studies toward zero-shot commonsense question answering by formulating the task as inference over dynamically generated commonsense knowledge graphs. In contrast to previous studies for knowledge integration that rely on retrieval of existing knowledge from static knowledge graphs, our study requires commonsense knowledge integration where contextually relevant knowledge is often not present in existing knowledge bases. Therefore, we present a novel approach that generates contextually-relevant symbolic knowledge structures on demand using generative neural commonsense knowledge models. Empirical results on two datasets demonstrate the efficacy of our neuro-symbolic approach for dynamically constructing knowledge graphs for reasoning. Our approach achieves significant performance boosts over pretrained language models and vanilla knowledge models, all while providing interpretable reasoning paths for its predictions.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16630/16437",
  "title": "Answering Complex Queries in Knowledge Graphs with Bidirectional Sequence Encoders",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Representation learning for knowledge graphs (KGs) has focused on the problem of answering simple link prediction queries. In this work we address the more ambitious challenge of predicting the answers of conjunctive queries with multiple missing entities. We propose Bidirectional Query Embedding (BIQE), a method that embeds conjunctive queries with models based on bi-directional attention mechanisms. Contrary to prior work, bidirectional selfattention can capture interactions among all the elements of a query graph. We introduce two new challenging datasets for studying conjunctive query inference and conduct experiments on several benchmark datasets that demonstrate BIQE significantly outperforms state of the art baselines.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16633/16440",
  "title": "A Scalable Reasoning and Learning Approach for Neural-Symbolic Stream Fusion",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Driven by deep neural networks (DNN), the recent development of computer vision makes vision sensors such as stereo cameras and Lidars ubiquitous in autonomous cars, robotics and traffic monitoring. However, a traditional DNN-based data fusion pipeline like object tracking has to hard-wire an engineered set of DNN models to a fixed processing logic, which makes it difficult to infuse new models to that pipeline. To overcome this, we propose a novel neural-symbolic stream reasoning approach realised by semantic stream reasoning programs which specify DNN-based data fusion pipelines via logic rules with learnable probabilistic degrees as weights. The reasoning task over this program is governed by a novel incremental reasoning algorithm, which lends itself also as a core building block for a scalable and parallel algorithm to learn the weights for such program. Extensive experiments with our first prototype on multi-object tracking benchmarks for autonomous driving and traffic monitoring show that our flexible approach can considerably improve both accuracy and processing throughput compared to the DNN-based counterparts.",
  "stance": 0.9
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16642/16449",
  "title": "A Few Queries Go a Long Way: Information-Distortion Tradeoffs in Matching",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Georgios Amanatidis,1,2 Georgios Birmpas,3 Aris Filos-Ratsikas,4 Alexandros A. Voudouris5 1Department of Mathematical Sciences, University of Essex 2ILLC, University of Amsterdam 3Department of Computer, Control and Management Engineering, Sapienza University of Rome 4Department of Computer Science, University of Liverpool 5School of Computer Science and Electronic Engineering, University of Essex georgios.amanatidis@essex.ac.uk, george.birbas@diag.uniroma1.it, aris.filos-ratsikas@liverpool.ac.uk, alexandros.voudouris@essex.ac.uk Abstract",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16671/16478",
  "title": "PoA of Simple Auctions with Interdependent Values",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "We expand the literature on the price of anarchy (PoA) of simultaneous item auctions by considering settings with correlated values; we do this via the fundamental economic model of interdependent values (IDV). It is well-known that in multi-item settings with private values, correlated values can lead to bad PoA, which can be polynomially large in the number of agents n. In the more general model of IDV, we show that the PoA can be polynomially large even in singleitem settings. On the positive side, we identify a natural condition on information dispersion in the market, which enables good PoA guarantees. Under this condition, we show that for single-item settings, the PoA of standard mechanisms degrades gracefully. For settings with multiple items we show a separation between two domains: If there are more buyers, we devise a new simultaneous item auction with good PoA, under limited information asymmetry. To the best of our knowledge, this is the first positive PoA result for correlated values in multi-item settings. The main technical difficulty in establishing this result is that the standard tool for establishing PoA results — the smoothness framework — is unsuitable for IDV settings, and so we must introduce new techniques to address the unique challenges imposed by such settings. In the domain of more items, we establish impossibility results even for surprisingly simple scenarios.",
  "stance": 0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16686/16493",
  "title": "An Analysis of Approval-Based Committee Rules for 2D-Euclidean Elections",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "We study approval-based committee elections for the case where the voters’ preferences come from a 2D-Euclidean model. We consider two main issues: First, we ask for the complexity of computing election results. Second, we evaluate election outcomes experimentally, following the visualization technique of Elkind et al. (2017). Regarding the first issue, we find that many NP-hard rules remain intractable for 2D-Euclidean elections. For the second one, we observe that the behavior and nature of many rules strongly depend on the exact protocol for choosing the approved candidates.",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16705/16512",
  "title": "Majority Opinion Diffusion in Social Networks: An Adversarial Approach",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "We introduce and study a novel majority based opinion diffusion model. Consider a graph G, which represents a social network. Assume that initially a subset of nodes, called seed nodes or early adopters, are colored either black or white, which correspond to positive or negative opinion regarding a consumer product or a technological innovation. Then, in each round an uncolored node, which is adjacent to at least one colored node, chooses the most frequent color among its",
  "stance": 0.7
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16734/16541",
  "title": "MARTA: Leveraging Human Rationales for Explainable Text Classification",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Explainability is a key requirement for text classification in many application domains ranging from sentiment analysis to medical diagnosis or legal reviews. Existing methods often rely on “attention” mechanisms for explaining classification results by estimating the relative importance of input units. However, recent studies have shown that such mechanisms tend to mis-identify irrelevant input units in their explanation. In this work, we propose a hybrid human-AI approach that incorporates human rationales into attention-based text classification models to improve the explainability of classification results. Specifically, we ask workers to provide rationales for their annotation by selecting relevant pieces of text. We introduce MARTA, a Bayesian framework that jointly learns an attention-based model and the reliability of workers while injecting human rationales into model training. We derive a principled optimization algorithm based on variational inference with efficient updating rules for learning MARTA parameters. Extensive validation on real-world datasets shows that our framework significantly improves the state of the art both in terms of classification explainability and accuracy.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16758/16565",
  "title": "Consistent Right-Invariant Fixed-Lag Smoother with Application to Visual Inertial SLAM",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "State estimation problems without absolute position measurements routinely arise in navigation of unmanned aerial vehicles, autonomous ground vehicles, etc. whose proper operation relies on accurate state estimates and reliable covariances. Unaware of absolute positions, these problems have immanent unobservable directions. Traditional causal estimators, however, usually gain spurious information on the unobservable directions, leading to over-confident covariance inconsistent with actual estimator errors. The consistency problem of fixed-lag smoothers (FLSs) has only been attacked by the first estimate Jacobian (FEJ) technique because of the complexity to analyze their observability property. But the FEJ has several drawbacks hampering its wide adoption. To ensure the consistency of a FLS, this paper introduces the right invariant error formulation into the FLS framework. To our knowledge, we are the first to analyze the observability of a FLS with the right invariant error. Our main contributions are twofold. As the first novelty, to bypass the complexity of analysis with the classic observability matrix, we show that observability analysis of FLSs can be done equivalently on the linearized system. Second, we prove that the inconsistency issue in the traditional FLS can be elegantly solved by the right invariant error formulation without artificially correcting Jacobians. By applying the proposed FLS to the monocular visual inertial simultaneous localization and mapping (SLAM) problem, we confirm that the method consistently estimates covariance similarly to a batch smoother in simulation and that our method achieved comparable accuracy as traditional FLSs on real data.",
  "stance": 0.7
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16760/16567",
  "title": "DenserNet: Weakly Supervised Visual Localization Using Multi-Scale Feature Aggregation",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "In this work, we introduce a Denser Feature Network (DenserNet) for visual localization. Our work provides three principal contributions. First, we develop a convolutional neural network (CNN) architecture which aggregates feature maps at different semantic levels for image representations. Using denser feature maps, our method can produce more keypoint features and increase image retrieval accuracy. Second, our model is trained end-to-end without pixel-level annotation other than positive and negative GPS-tagged image pairs. We use a weakly supervised triplet ranking loss to learn discriminative features and encourage keypoint feature repeatability for image representation. Finally, our method is computationally efficient as our architecture has shared features and parameters during forwarding propagation. Our method is flexible and can be crafted on a light-weighted backbone architecture to achieve appealing efficiency with a small penalty on accuracy. Extensive experiment results indicate that our method sets a new state-of-the-art on four challenging large-scale localization benchmarks and three image retrieval benchmarks with the same level of supervision. The code is available at https://github.com/goodproj13/",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16769/16576",
  "title": "A General Setting for Gradual Semantics Dealing with Similarity",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "The paper discusses theoretical foundations that describe principles and processes involved in defining semantics that deal with similarity between arguments. Such semantics compute the strength of an argument on the basis of the strengths of its attackers, similarities between those attackers, and an initial weight ascribed to the argument. We define a semantics by three functions: an adjustment function that updates the strengths of attackers on the basis of their similarities, an aggregation function that computes the strength of the group of attackers, and an influence function that evaluates the impact of the group on the argument’s initial weight. We propose intuitive constraints for the three functions and key rationality principles for semantics, and show how the former lead to the satisfaction of the latter. Then, we propose a broad family of semantics whose instances satisfy the principles. Finally, we analyse the existing adjustment functions and show that they violate some properties, then we propose novel ones and use them for generalizing h-Categorizer.",
  "stance": 0.2
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16808/16615",
  "title": "Focused Inference and System P",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "We bring in the concept of focused inference into the field of qualitative nonmonotonic reasoning by applying focused inference to System P. The idea behind drawing focused inferences is to concentrate on knowledge which seems to be relevant for answering a query while completely disregarding the remaining knowledge even at the risk of missing some meaningful information. Focused inference is motivated by mimicking snap decisions of human reasoners and aims on rapidly drawing still reasonable inferences from large sets of knowledge. In this paper, we define a series of query-dependent, syntactically-driven focused inference relations, elaborate on their formal properties, and show that the series converges against System P. We take advantage of this result in form of an anytime algorithm for drawing inferences which is accompanied by a thorough complexity analysis.",
  "stance": 0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16819/16626",
  "title": "Does Explainable Artificial Intelligence Improve Human Decision-Making?",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Explainable AI provides insights to users into the why for model predictions, offering potential for users to better understand and trust a model, and to recognize and correct AI predictions that are incorrect. Prior research on human and explainable AI interactions has typically focused on measures such as interpretability, trust, and usability of the explanation. There are mixed findings whether explainable AI can improve actual human decisionmaking and the ability to identify the problems with the underlying model. Using real datasets, we compare objective human decision accuracy without AI (control), with an AI prediction (no explanation), and AI prediction with explanation. We find providing any kind of AI prediction tends to improve user decision accuracy, but no conclusive evidence that explainable AI has a meaningful impact. Moreover, we observed the strongest predictor for human decision accuracy was AI accuracy and that users were somewhat able to detect when the AI was correct vs. incorrect, but this was not significantly affected by including an explanation. Our results indicate that, at least in some situations, the why information provided in explainable AI may not enhance user decisionmaking, and further research may be needed to understand how to integrate explainable AI into real systems.",
  "stance": -0.8
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16837/16644",
  "title": "Understanding Decoupled and Early Weight Decay",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Weight decay (WD) is a traditional regularization technique in deep learning, but despite its ubiquity, its behavior is still an area of active research. Golatkar et al. have recently shown that WD only matters at the start of the training in computer vision, upending traditional wisdom. Loshchilov et al. show that for adaptive optimizers, manually decaying weights can outperform adding an l2 penalty to the loss. This technique has become increasingly popular and is referred to as decoupled WD. The goal of this paper is to investigate these two recent empirical observations. We demonstrate that by applying WD only at the start, the network norm stays small throughout training. This has a regularizing effect as the effective gradient updates become larger. However, traditional generalizations metrics fail to capture this effect of WD, and we show how a simple scale-invariant metric can. We also show how the growth of network weights is heavily influenced by the dataset and its generalization properties. For decoupled WD, we perform experiments in NLP and RL where adaptive optimizers are the norm. We demonstrate that the primary issue that decoupled WD alleviates is the mixing of gradients from the objective function and the l2 penalty in the buffers of Adam (which stores the estimates of the first-order moment). Adaptivity itself is not problematic and decoupled WD ensures that the gradients from the l2 term cannot ”drown out” the true objective, facilitating easier hyperparameter tuning.",
  "stance": -0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16853/16660",
  "title": "Frivolous Units: Wider Networks Are Not Really That Wide",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "A remarkable characteristic of overparameterized deep neural networks (DNNs) is that their accuracy does not degrade when the network width is increased. Recent evidence suggests that developing compressible representations allows the complexity of large networks to be adjusted for the learning task at hand. However, these representations are poorly understood. A promising strand of research inspired from biology involves studying representations at the unit level as it offers a more granular interpretation of the neural mechanisms. In order to better understand what facilitates increases in width without decreases in accuracy, we ask: Are there mechanisms at the unit level by which networks control their effective complexity? If so, how do these depend on the architecture, dataset, and hyperparameters? We identify two distinct types of “frivolous” units that proliferate when the network’s width increases: prunable units which can be dropped out of the network without significant change to the output and redundant units whose activities can be expressed as a linear combination of others. These units imply complexity constraints as the function the network computes could be expressed without them. We also identify how the development of these units can be influenced by architecture and a number of training factors. Together, these results help to explain why the accuracy of DNNs does not degrade when width is increased and highlight the importance of frivolous units toward understanding implicit regularization in DNNs.",
  "stance": 0.2
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16873/16680",
  "title": "Neighborhood Consensus Networks for Unsupervised Multi-view Outlier Detection",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Multi-view outlier detection recently attracted rapidly growing attention with the development of multi-view learning. Although promising performance demonstrated, we observe that identifying outliers in multi-view data is still a challenging task due to the complicated characteristics of multi-view data. Specifically, an effective multi-view outlier detection method should be able to handle (1) different types of outliers; (2) two or more views; (3) samples without clusters; (4) high dimensional data. Unfortunately, little is known about how these four issues can be handled simultaneously. In this paper, we propose an unsupervised multi-view outlier detection method to address these issues. Our method is based on the proposed novel neighborhood consensus networks termed NC-Nets, which automatically encodes intrinsic information into a comprehensive latent space for each view (for issue (4)) and uniforms the neighborhood structures among different views (for issue (2)). Accordingly, we propose an outlier score measurement which consists of two parts: the withinview reconstruction score and the cross-view neighborhood consensus score. The measurement is designed based on the characteristics of the different outlier types (for issue (1)) and no cluster assumption is needed (for issue (3)). Experimental results show that our method significantly outperforms state-of-the-art methods. On average, our method achieves 11.2% ∼ 96.2% improvement in term of AUC and 33.5% ∼ 352.7% improvement in term of F1-Score.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16883/16690",
  "title": "Generalized Adversarially Learned Inference",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Allowing effective inference of latent vectors while training GANs can greatly increase their applicability in various downstream tasks. Recent approaches, such as ALI and BiGAN frameworks, develop methods of inference of latent variables in GANs by adversarially training an image generator along with an encoder to match two joint distributions of image and latent vector pairs. We generalize these approaches to incorporate multiple layers of feedback on reconstructions, self-supervision, and other forms of supervision based on prior or learned knowledge about the desired solutions. We achieve this by modifying the discriminator’s objective to correctly identify more than two joint distributions of tuples of an arbitrary number of random variables consisting of images, latent vectors, and other variables generated through auxiliary tasks, such as reconstruction and inpainting or as outputs of suitable pre-trained models. We design a non-saturating maximization objective for the generator-encoder pair and prove that the resulting adversarial game corresponds to a global optimum that simultaneously matches all the distributions. Within our proposed framework, we introduce a novel set of techniques for providing self-supervised feedback to the model based on properties, such as patch-level correspondence and cycle consistency of reconstructions. Through comprehensive experiments, we demonstrate the efficacy, scalability, and flexibility of the proposed approach for a variety of tasks. The appendix of the paper can be found at the following link: https://drive.google.com/file/ d/1i99e682CqYWMEDXlnqkqrctGLVA9viiz/view?usp= sharing",
  "stance": 0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16887/16694",
  "title": "Differentially Private and Communication Efficient Collaborative Learning",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Collaborative learning has received huge interests due to its capability of exploiting the collective computing power of the wireless edge devices. However, during the learning process, model updates using local private samples and large-scale parameter exchanges among agents impose severe privacy concerns and communication bottleneck. In this paper, to address these problems, we propose two differentially private (DP) and communication efficient algorithms, called Q-DPSGD-1 and Q-DPSGD-2. In Q-DPSGD-1, each agent first performs local model updates by a DP gradient descent method to provide the DP guarantee and then quantizes the local model before transmitting it to neighbors to improve communication efficiency. In Q-DPSGD-2, each agent injects discrete Gaussian noise to enforce DP guarantee after first quantizing the local model. Moreover, we track the privacy loss of both approaches under the Rényi DP and provide convergence analysis for both convex and non-convex loss functions. The proposed methods are evaluated in extensive experiments on real-world datasets and the empirical results validate our theoretical findings.",
  "stance": 0.7
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16906/16713",
  "title": "Learning to Reweight with Deep Interactions",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Recently, the concept of teaching has been introduced into machine learning, in which a teacher model is used to guide the training of a student model (which will be used in real tasks) through data selection, loss function design, etc. Learning to reweight, which is a specific kind of teaching that reweights training data using a teacher model, receives much attention due to its simplicity and effectiveness. In existing learning to reweight works, the teacher model only utilizes shallow/surface information such as training iteration number and loss/accuracy of the student model from training/validation sets, but ignores the internal states of the student model, which limits the potential of learning to reweight. In this work, we propose an improved data reweighting algorithm, in which the student model provides its internal states to the teacher model, and the teacher model returns adaptive weights of training samples to enhance the training of the student model. The teacher model is jointly trained with the student model using meta gradients propagated from a validation set. Experiments on image classification with clean/noisy labels and neural machine translation empirically demonstrate that our algorithm makes significant improvement over previous methods.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16935/16742",
  "title": "DeepSynth: Automata Synthesis for Automatic Task Segmentation in Deep Reinforcement Learning",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "This paper proposes DeepSynth, a method for effective training of deep Reinforcement Learning (RL) agents when the reward is sparse and non-Markovian, but at the same time progress towards the reward requires achieving an unknown sequence of high-level objectives. Our method employs a novel algorithm for synthesis of compact automata to uncover this sequential structure automatically. We synthesise a humaninterpretable automaton from trace data collected by exploring the environment. The state space of the environment is then enriched with the synthesised automaton so that the generation of a control policy by deep RL is guided by the discovered structure encoded in the automaton. The proposed approach is able to cope with both high-dimensional, low-level features and unknown sparse non-Markovian rewards. We have evaluated DeepSynth’s performance in a set of experiments that includes the Atari game Montezuma’s Revenge. Compared to existing approaches, we obtain a reduction of two orders of magnitude in the number of iterations required for policy synthesis, and also a significant improvement in scalability.",
  "stance": 0.8
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16987/16794",
  "title": "A Flexible Framework for Communication-Efficient Machine Learning",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "With the increasing scale of machine learning tasks, it has become essential to reduce the communication between computing nodes. Early work on gradient compression focused on the bottleneck between CPUs and GPUs, but communicationefficiency is now needed in a variety of different system architectures, from high-performance clusters to energyconstrained IoT devices. In the current practice, compression levels are typically chosen before training and settings that work well for one task may be vastly sub-optimal for another dataset on another architecture. In this paper, we propose a flexible framework which adapts the compression level to the true gradient at each iteration, maximizing the improvement in the objective function that is achieved per communicated bit. Our framework is easy to adapt from one technology to the next by modeling how the communication cost depends on the compression level for the specific technology. Theoretical results and practical experiments indicate that the automatic tuning strategies significantly increase communication efficiency on several state-of-the-art compression schemes.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17037/16844",
  "title": "Contrastive Clustering",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "In this paper, we propose an online clustering method called Contrastive Clustering (CC) which explicitly performs the instanceand cluster-level contrastive learning. To be specific, for a given dataset, the positive and negative instance pairs are constructed through data augmentations and then projected into a feature space. Therein, the instanceand cluster-level contrastive learning are respectively conducted in the row and column space by maximizing the similarities of positive pairs while minimizing those of negative ones. Our key observation is that the rows of the feature matrix could be regarded as soft labels of instances, and accordingly the columns could be further regarded as cluster representations. By simultaneously optimizing the instanceand cluster-level contrastive loss, the model jointly learns representations and cluster assignments in an end-to-end manner. Besides, the proposed method could timely compute the cluster assignment for each individual, even when the data is presented in streams. Extensive experimental results show that CC remarkably outperforms 17 competitive clustering methods on six challenging image benchmarks. In particular, CC achieves an NMI of 0.705 (0.431) on the CIFAR-10 (CIFAR-100) dataset, which is an up to 19% (39%) performance improvement compared with the best baseline. The code is available at https://github.com/XLearning-SCU/2021-AAAI-CC.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17098/16905",
  "title": "Modular Graph Transformer Networks for Multi-Label Image Classification",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "With the recent advances in graph neural networks, there is a rising number of studies on graph-based multi-label classification with the consideration of object dependencies within visual data. Nevertheless, graph representations can become indistinguishable due to the complex nature of label relationships. We propose a multi-label image classification framework based on graph transformer networks to fully exploit inter-label interactions. The paper presents a modular learning scheme to enhance the classification performance by segregating the computational graph into multiple sub-graphs based on modularity. Our approach, named Modular Graph Transformer Networks (MGTN), is capable of employing multiple backbones for better information propagation over different sub-graphs guided by graph transformers and convolutions. We validate our framework on MS-COCO and Fashion550K datasets to demonstrate improvements for multilabel image classification. The source code is available at https://github.com/ReML-AI/MGTN.",
  "stance": 0.7
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17112/16919",
  "title": "Learning Deep Generative Models for Queuing Systems",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Modern society is heavily dependent on large scale client-server systems with applications ranging from Internet and Communication Services to sophisticated logistics and deployment of goods. To maintain and improve such a system, a careful study of client and server dynamics is needed – e.g. response/service times, average number of clients at given times, etc. To this end, one traditionally relies, within the queuing theory formalism, on parametric analysis and explicit distribution forms. However, parametric forms limit the model’s expressiveness and could struggle on extensively large",
  "stance": -0.7
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17175/16982",
  "title": "PAC Learning of Causal Trees with Latent Variables",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Learning causal probabilistic models with latent variables from observational and experimental data is an important problem. In this paper we present a polynomial-time algorithm that PAC-learns the structure and parameters of a rooted, tree-structured causal network of bounded degree where the internal nodes of the tree cannot be observed or manipulated. Our algorithm is the first of its kind to provably learn the structure and parameters of tree-structured causal models with latent internal variables from random examples and active experiments.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17234/17041",
  "title": "Peer Collaborative Learning for Online Knowledge Distillation",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Traditional knowledge distillation uses a two-stage training strategy to transfer knowledge from a high-capacity teacher model to a compact student model, which relies heavily on the pre-trained teacher. Recent online knowledge distillation alleviates this limitation by collaborative learning, mutual learning and online ensembling, following a one-stage end-to-end training fashion. However, collaborative learning and mutual learning fail to construct an online high-capacity teacher, whilst online ensembling ignores the collaboration among branches and its logit summation impedes the further optimisation of the ensemble teacher. In this work, we propose a novel Peer Collaborative Learning method for online knowledge distillation, which integrates online ensembling and network collaboration into a unified framework. Specifically, given a target network, we construct a multi-branch network for training, in which each branch is called a peer. We perform random augmentation multiple times on the inputs to peers and assemble feature representations outputted from peers with an additional classifier as the peer ensemble teacher. This helps to transfer knowledge from a highcapacity teacher to peers, and in turn further optimises the ensemble teacher. Meanwhile, we employ the temporal mean model of each peer as the peer mean teacher to collaboratively transfer knowledge among peers, which helps each peer to learn richer knowledge and facilitates to optimise a more stable model with better generalisation. Extensive experiments on CIFAR-10, CIFAR-100 and ImageNet show that the proposed method significantly improves the generalisation of various backbone networks and outperforms the state-of-theart methods.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17291/17098",
  "title": "Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Data heterogeneity has been identified as one of the key features in federated learning but often overlooked in the lens of robustness to adversarial attacks. This paper focuses on characterizing and understanding its impact on backdooring attacks in federated learning through comprehensive experiments using synthetic and the LEAF benchmarks. The initial impression driven by our experimental results suggests that data heterogeneity is the dominant factor in the effectiveness of attacks and it may be a redemption for defending against backdooring as it makes the attack less efficient, more challenging to design effective attack strategies, and the attack result also becomes less predictable. However, with further investigations, we found data heterogeneity is more of a curse than a redemption as the attack effectiveness can be significantly boosted by simply adjusting the client-side backdooring timing. More importantly, data heterogeneity may result in overfitting at the local training of benign clients, which can be utilized by attackers to disguise themselves and fool skewed-feature based defenses. In addition, effective attack strategies can be made by adjusting attack data distribution. Finally, we discuss the potential directions of defending the curses brought by data heterogeneity. The results and lessons learned from our extensive experiments and analysis offer new insights for designing robust federated learning methods and systems.",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17296/17103",
  "title": "CloudLSTM: A Recurrent Neural Model for Spatiotemporal Point-cloud Stream Forecasting",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "This paper introduces CloudLSTM, a new branch of recurrent neural models tailored to forecasting over data streams generated by geospatial point-cloud sources. We design a Dynamic Point-cloud Convolution (DConv) operator as the core component of CloudLSTMs, which performs convolution directly over point-clouds and extracts local spatial features from sets of neighboring points that surround different elements of the input. This operator maintains the permutation invariance of sequence-to-sequence learning frameworks, while representing neighboring correlations at each time step – an important aspect in spatiotemporal predictive learning. The DConv operator resolves the grid-structural data requirements of existing spatiotemporal forecasting models and can be easily plugged into traditional LSTM architectures with sequenceto-sequence learning and attention mechanisms. We apply our proposed architecture to two representative, practical use cases that involve point-cloud streams, i.e., mobile service traffic forecasting and air quality indicator forecasting. Our results, obtained with real-world datasets collected in diverse scenarios for each use case, show that CloudLSTM delivers accurate long-term predictions, outperforming a variety of competitor neural network models.",
  "stance": 0.6
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17313/17120",
  "title": "Exploratory Machine Learning with Unknown Unknowns",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "In conventional supervised learning, a training dataset is given with ground-truth labels from a known label set, and the learned model will classify unseen instances to known labels. In real situations, when the learned models do not work well, learners generally attribute the model failure to the inadequate selection of learning algorithms or the lack of enough labeled training samples. In this paper, we point out that there is an important category of failure, which owes to the fact that there are unknown classes in the training data misperceived as other labels, and thus their existence is unknown from the given supervision. Such problems of unknown unknown classes can hardly be addressed by common re-selection of algorithms or accumulation of training samples. For this purpose, we propose the exploratory machine learning, where in this paradigm once learner encounters unsatisfactory learning performance, she can examine the possibility and, if unknown unknowns really exist, deploy the optimal strategy of feature space augmentation to make unknown classes observable and be enabled for learning. Theoretical analysis and empirical study on both synthetic and real datasets validate the efficacy of our proposal.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17341/17148",
  "title": "Learning to Resolve Conflicts for Multi-Agent Path Finding with Conflict-Based Search",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Conflict-Based Search (CBS) is a state-of-the-art algorithm for multi-agent path finding. On the high level, CBS repeatedly detects conflicts and resolves one of them by splitting the current problem into two subproblems. Previous work chooses the conflict to resolve by categorizing conflicts into three classes and always picking one from the highest-priority class. In this work, we propose an oracle for conflict selection that results in smaller search tree sizes than the one used in previous work. However, the computation of the oracle is slow. Thus, we propose a machine-learning (ML) framework for conflict selection that observes the decisions made by the oracle and learns a conflict-selection strategy represented by a linear ranking function that imitates the oracle’s decisions accurately and quickly. Experiments on benchmark maps indicate that our approach, ML-guided CBS, significantly improves the success rates, search tree sizes and runtimes of the current state-of-the-art CBS solver.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17359/17166",
  "title": "Is the Most Accurate AI the Best Teammate? Optimizing AI for Teamwork",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "AI practitioners typically strive to develop the most accurate systems, making an implicit assumption that the AI system will function autonomously. However, in practice, AI systems often are used to provide advice to people in domains ranging from criminal justice and finance to healthcare. In such AI-advised decision making, humans and machines form a team, where the human is responsible for making final decisions. But is the most accurate AI the best teammate? We argue “not necessarily” — predictable performance may be worth a slight sacrifice in AI accuracy. Instead, we argue that AI systems should be trained in a human-centered manner, directly optimized for team performance. We study this proposal for a specific type of human-AI teaming, where the human overseer chooses to either accept the AI recommendation or solve the task themselves. To optimize the team performance for this setting we maximize the team’s expected utility, expressed in terms of the quality of the final decision, cost of verifying, and individual accuracies of people and machines. Our experiments with linear and non-linear models on real-world, high-stakes datasets show that the most accuracy AI may not lead to highest team performance and show the benefit of modeling teamwork during training through improvements in expected team utility across datasets, considering parameters such as human skill and the cost of mistakes. We discuss the shortcoming of current optimization approaches beyond well-studied loss functions such as log-loss, and encourage future work on AI optimization problems motivated by human-AI collaboration.",
  "stance": -0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17417/17224",
  "title": "Minimax Regret Optimisation for Robust Planning in Uncertain Markov Decision Processes",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "The parameters for a Markov Decision Process (MDP) often cannot be specified exactly. Uncertain MDPs (UMDPs) capture this model ambiguity by defining sets which the parameters belong to. Minimax regret has been proposed as an objective for planning in UMDPs to find robust policies which are not overly conservative. In this work, we focus on planning for Stochastic Shortest Path (SSP) UMDPs with uncertain cost and transition functions. We introduce a Bellman equation to compute the regret for a policy. We propose a dynamic programming algorithm that utilises the regret Bellman equation, and show that it optimises minimax regret exactly for UMDPs with independent uncertainties. For coupled uncertainties, we extend our approach to use options to enable a trade off between computation and solution quality. We evaluate our approach on both synthetic and real-world domains, showing that it significantly outperforms existing baselines.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17451/17258",
  "title": "Generalization in Portfolio-Based Algorithm Selection",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Portfolio-based algorithm selection has seen tremendous practical success over the past two decades. This algorithm configuration procedure works by first selecting a portfolio of diverse algorithm parameter settings, and then, on a given problem instance, using an algorithm selector to choose a parameter setting from the portfolio with strong predicted performance. Oftentimes, both the portfolio and the algorithm selector are chosen using a training set of typical problem instances from the application domain at hand. In this paper, we provide the first provable guarantees for portfolio-based algorithm selection. We analyze how large the training set should be to ensure that the resulting algorithm selector’s average performance over the training set is close to its future (expected) performance. This involves analyzing three key reasons why these two quantities may diverge: 1) the learningtheoretic complexity of the algorithm selector, 2) the size of the portfolio, and 3) the learning-theoretic complexity of the algorithm’s performance as a function of its parameters. We introduce an end-to-end learning-theoretic analysis of the portfolio construction and algorithm selection together. We prove that if the portfolio is large, overfitting is inevitable, even with an extremely simple algorithm selector. With experiments, we illustrate a tradeoff exposed by our theoretical analysis: as we increase the portfolio size, we can hope to include a well-suited parameter setting for every possible problem instance, but it becomes impossible to avoid overfitting.",
  "stance": -0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17457/17264",
  "title": "Escaping Local Optima with Non-Elitist Evolutionary Algorithms",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Most discrete evolutionary algorithms (EAs) implement elitism, meaning that they make the biologically implausible assumption that the fittest individuals never die. While elitism favours exploitation and ensures that the best seen solutions are not lost, it has been widely conjectured that non-elitism is necessary to explore promising fitness valleys without getting stuck in local optima. Determining when non-elitist EAs outperform elitist EAs has been one of the most fundamental open problems in evolutionary computation. A recent analysis of a non-elitist EA shows that this algorithm does not outperform its elitist counterparts on the benchmark problem JUMP. We solve this open problem through rigorous runtime analysis of elitist and non-elitist population-based EAs on a class of multi-modal problems. We show that with 3-tournament selection and appropriate mutation rates, the non-elitist EA optimises the multi-modal problem in expected polynomial time, while an elitist EA requires exponential time with overwhelmingly high probability. A key insight in our analysis is the non-linear selection profile of the tournament selection mechanism which, with appropriate mutation rates, allows a small sub-population to reside on the local optimum while the rest of the population explores the fitness valley. In contrast, we show that the comma-selection mechanism which does not have this non-linear profile, fails to optimise this problem in polynomial time. The theoretical analysis is complemented with an empirical investigation on instances of the set cover problem, showing that non-elitist EAs can perform better than the elitist ones. We also provide examples where usage of mutation rates close to the error thresholds is beneficial when employing non-elitist population-based EAs.",
  "stance": -1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17483/17290",
  "title": "Multi-Dimensional Explanation of Target Variables from Documents",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Automated predictions require explanations to be interpretable by humans. Past work used attention and rationale mechanisms to find words that predict the target variable of a document. Often though, they result in a tradeoff between noisy explanations or a drop in accuracy. Furthermore, rationale methods cannot capture the multi-faceted nature of justifications for multiple targets, because of the non-probabilistic nature of the mask. In this paper, we propose the Multi-Target Masker (MTM) to address these shortcomings. The novelty lies in the soft multi-dimensional mask that models a relevance probability distribution over the set of target variables to handle ambiguities. Additionally, two regularizers guide MTM to induce long, meaningful explanations. We evaluate MTM on two datasets and show, using standard metrics and human annotations, that the resulting masks are more accurate and coherent than those generated by the state-of-the-art methods. Moreover, MTM is the first to also achieve the highest F1 scores for all the target variables simultaneously.",
  "stance": 0.8
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17499/17306",
  "title": "A Lightweight Neural Model for Biomedical Entity Linking",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Biomedical entity linking aims to map biomedical mentions, such as diseases and drugs, to standard entities in a given knowledge base. The specific challenge in this context is that the same biomedical entity can have a wide range of names, including synonyms, morphological variations, and names with different word orderings. Recently, BERT-based methods have advanced the state-of-the-art by allowing for rich representations of word sequences. However, they often have hundreds of millions of parameters and require heavy computing resources, which limits their applications in resource-limited scenarios. Here, we propose a lightweight neural method for biomedical entity linking, which needs just a fraction of the parameters of a BERT model and much less computing resources. Our method uses a simple alignment layer with attention mechanisms to capture the variations between mention and entity names. Yet, we show that our model is competitive with previous work on standard evaluation benchmarks.",
  "stance": 0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17505/17312",
  "title": "How Linguistically Fair Are Multilingual Pre-Trained Language Models?",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Massively multilingual pre-trained language models, such as mBERT and XLM-RoBERTa, have received significant attention in the recent NLP literature for their excellent capability towards crosslingual zero-shot transfer of NLP tasks. This is especially promising because a large number of languages have no or very little labeled data for supervised learning. Moreover, a substantially improved performance on low resource languages without any significant degradation of accuracy for high resource languages lead us to believe that these models will help attain a fairer distribution of language technologies despite the prevalent unfair and extremely skewed distribution of resources across the world’s languages. Nevertheless, these models, and the experimental approaches adopted by the researchers to arrive at those, have been criticised by some for lacking a nuanced and thorough comparison of benefits across languages and tasks. A related and important question that has received little attention is how to choose from a set of models, when no single model significantly outperforms the others on all tasks and languages. As we discuss in this paper, this is often the case, and the choices are usually made without a clear articulation of reasons or underlying fairness assumptions. In this work, we scrutinize the choices made in previous work, and propose a few different strategies for fair and efficient model selection based on the principles of fairness in economics and social choice theory. In particular, we emphasize Rawlsian fairness, which provides an appropriate framework for making fair (with respect to languages, or tasks, or both) choices while selecting multilingual pre-trained language models for a practical or scientific set-up.",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17512/17319",
  "title": "FILTER: An Enhanced Fusion Method for Cross-lingual Language Understanding",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Large-scale cross-lingual language models (LM), such as mBERT, Unicoder and XLM, have achieved great success in cross-lingual representation learning. However, when applied to zero-shot cross-lingual transfer tasks, most existing methods use only single-language input for LM finetuning, without leveraging the intrinsic cross-lingual alignment between different languages that proves essential for multilingual tasks. In this paper, we propose FILTER, an enhanced fusion method that takes cross-lingual data as input for XLM finetuning. Specifically, FILTER first encodes text input in the source language and its translation in the target language independently in the shallow layers, then performs crosslanguage fusion to extract multilingual knowledge in the intermediate layers, and finally performs further languagespecific encoding. During inference, the model makes predictions based on the text input in the target language and its translation in the source language. For simple tasks such as classification, translated text in the target language shares the same label as the source language. However, this shared label becomes less accurate or even unavailable for more complex tasks such as question answering, NER and POS tagging. To tackle this issue, we further propose an additional KL-divergence self-teaching loss for model training, based on auto-generated soft pseudo-labels for translated text in the target language. Extensive experiments demonstrate that FILTER achieves new state of the art on two challenging multilingual multi-task benchmarks, XTREME and XGLUE.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17531/17338",
  "title": "BERT & Family Eat Word Salad: Experiments with Text Understanding",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "In this paper, we study the response of large models from the BERT family to incoherent inputs that should confuse any model that claims to understand natural language. We define simple heuristics to construct such examples. Our experiments show that state-of-the-art models consistently fail to recognize them as ill-formed, and instead produce high confidence predictions on them. As a consequence of this phenomenon, models trained on sentences with randomly permuted word order perform close to state-of-the-art models. To alleviate these issues, we show that if models are explicitly trained to recognize invalid inputs, they can be robust to such attacks without a drop in performance.",
  "stance": -0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17534/17341",
  "title": "Humor Knowledge Enriched Transformer for Understanding Multimodal Humor",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Recognizing humor from a video utterance requires understanding the verbal and non-verbal components as well as incorporating the appropriate context and external knowledge. In this paper, we propose Humor Knowledge enriched Transformer (HKT) that can capture the gist of a multimodal humorous expression by integrating the preceding context and external knowledge. We incorporate humor centric external knowledge into the model by capturing the ambiguity and sentiment present in the language. We encode all the language, acoustic, vision, and humor centric features separately using Transformer based encoders, followed by a cross attention layer to exchange information among them. Our model achieves 77.36% and 79.41% accuracy in humorous punchline detection on UR-FUNNY and MUStaRD datasets – achieving a new state-of-the-art on both datasets with the margin of 4.93% and 2.94% respectively. Furthermore, we demonstrate that our model can capture interpretable, humorinducing patterns from all modalities.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17564/17371",
  "title": "Towards Topic-Aware Slide Generation For Academic Papers With Unsupervised Mutual Learning",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Slides are commonly used to present information and tell stories. In academic and research communities, slides are typically used to summarize findings in accepted papers for presentation in meetings and conferences. These slides for academic papers usually contain common and essential topics such as major contributions, model design, experiment details and future work. In this paper, we aim to automatically generate slides for academic papers. We first conducted an in-depth analysis of how humans create slides. We then mined frequently used slide topics. Given a topic, our approach extracts relevant sentences in the paper to provide the draft slides. Due to the lack of labeling data, we integrate prior knowledge of ground truth sentences into a log-linear model to create an initial pseudo-target distribution. Two sentence extractors are learned collaboratively and bootstrap the performance of each other. Evaluation results on a labeled test set show that our model can extract more relevant sentences than baseline methods. Human evaluation also shows slides generated by our model can serve as a good basis for preparing the final",
  "stance": 0.8
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17566/17373",
  "title": "ACT: an Attentive Convolutional Transformer for Efficient Text Classification",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Recently, Transformer has been demonstrating promising performance in many NLP tasks and showing a trend of replacing Recurrent Neural Network (RNN). Meanwhile, less attention is drawn to Convolutional Neural Network (CNN) due to its weak ability in capturing sequential and longdistance dependencies, although it has excellent local feature extraction capability. In this paper, we introduce an Attentive Convolutional Transformer (ACT) that takes the advantages of both Transformer and CNN for efficient text classification. Specifically, we propose a novel attentive convolution mechanism that utilizes the semantic meaning of convolutional filters attentively to transform text from complex word space to a more informative convolutional filter space where important n-grams are captured. ACT is able to capture both local and global dependencies effectively while preserving sequential information. Experiments on various text classification tasks and detailed analyses show that ACT is a lightweight, fast, and effective universal text classifier, outperforming CNNs, RNNs, and attentive models including Transformer.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17575/17382",
  "title": "Infusing Multi-Source Knowledge with Heterogeneous Graph Neural Network for Emotional Conversation Generation",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "The success of emotional conversation systems depends on sufficient perception and appropriate expression of emotions. In a real-world conversation, we firstly instinctively perceive emotions from multi-source information, including the emotion flow of dialogue history, facial expressions, and personalities of speakers, and then express suitable emotions according to our personalities, but these multiple types of information are insufficiently exploited in emotional conversation fields. To address this issue, we propose a heterogeneous graph-based model for emotional conversation generation. Specifically, we design a Heterogeneous Graph-Based Encoder to represent the conversation content (i.e., the dialogue history, its emotion flow, facial expressions, and speakers’ personalities) with a heterogeneous graph neural network, and then predict suitable emotions for feedback. After that, we employ an Emotion-Personality-Aware Decoder to generate a response not only relevant to the conversation context but also with appropriate emotions, by taking the encoded graph representations, the predicted emotions from the encoder and the personality of the current speaker as inputs. Experimental results show that our model can effectively perceive emotions from multi-source knowledge and generate a satisfactory response, which significantly outperforms previous state-of-the-art models.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17608/17415",
  "title": "On the Softmax Bottleneck of Recurrent Language Models",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Recent research has pointed to a limitation of word-level neural language models with softmax outputs. This limitation, known as the “softmax bottleneck” refers to the inability of these models to produce high-rank log probability (logP ) matrices. Various solutions have been proposed to break this bottleneck, including Mixture of Softmaxes, SigSoftmax, and Linear Monotonic Softmax with Piecewise Linear Increasing Functions. They were reported to offer better performance in terms of perplexity on test data. A natural perception from these results is a strong positive correlation between the rank of the logP matrix and the model’s performance. In this work, we show via an extensive empirical study that such a correlation is fairly weak and that the high-rank of the logP matrix is neither necessary nor sufficient for better test perplexity. Although our results are empirical, they are established in part via the construction of a rich family of models, which we call Generalized SigSoftmax. They are able to create diverse ranks for the logP matrices. We also present an investigation as to why the proposed solutions achieve better performance.",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17623/17430",
  "title": "Learning from the Best: Rationalizing Predictions by Adversarial Information Calibration",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Explaining the predictions of AI models is paramount in safety-critical applications, such as in legal or medical domains. One form of explanation for a prediction is an extractive rationale, i.e., a subset of features of an instance that lead the model to give its prediction on the instance. Previous works on generating extractive rationales usually employ a two-phase model: a selector that selects the most important features (i.e., the rationale) followed by a predictor that makes the prediction based exclusively on the selected features. One disadvantage of these works is that the main signal for learning to select features comes from the comparison of the answers given by the predictor and the ground-truth answers. In this work, we propose to squeeze more information from the predictor via an information calibration method. More precisely, we train two models jointly: one is a typical neural model that solves the task at hand in an accurate but black-box manner, and the other is a selector-predictor model that additionally produces a rationale for its prediction. The first model is used as a guide to the second model. We use an adversarial-based technique to calibrate the information extracted by the two models such that the difference between them is an indicator of the missed or over-selected features. In addition, for natural language tasks, we propose to use a language-model-based regularizer to encourage the extraction of fluent rationales. Experimental results on a sentiment analysis task as well as on three tasks from the legal domain show the effectiveness of our approach to rationale extraction.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17624/17431",
  "title": "Nutri-bullets: Summarizing Health Studies by Composing Segments",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "We introduce Nutri-bullets, a multi-document summarization task for health and nutrition. First, we present two datasets of food and health summaries from multiple scientific studies. Furthermore, we propose a novel extract-compose model to solve the problem in the regime of limited parallel data. We explicitly select key spans from several abstracts using a policy network, followed by composing the selected spans to present a summary via a task specific language model. Compared to state-of-the-art methods, our approach leads to more faithful, relevant and diverse summarization – properties imperative to this application. For instance, on the BreastCancer dataset our approach gets a more than 50% improvement on relevance and faithfulness.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17679/17486",
  "title": "Simpson's Bias in NLP Training",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "In most machine learning tasks, we evaluate a modelM on a given data population S by measuring a population-level metric F(S;M). Examples of such evaluation metric F include precision/recall for (binary) recognition, the F1 score for multi-class classification, and the BLEU metric for language generation. On the other hand, the modelM is trained by optimizing a sample-level loss G(St;M) at each learning step t, where St is a subset of S (a.k.a. the mini-batch). Popular choices of G include cross-entropy loss, the Dice loss, and sentence-level BLEU scores. A fundamental assumption behind this paradigm is that the mean value of the samplelevel loss G, if averaged over all possible samples, should effectively represent the population-level metric F of the task, such as, that E[G(St;M)] ≈ F(S;M). In this paper, we systematically investigate the above assumption in several NLP tasks. We show, both theoretically and experimentally, that some popular designs of the sample-level loss G may be inconsistent with the true population-level metric F of the task, so that models trained to optimize the former can be substantially sub-optimal to the latter, a phenomenon we call it, Simpson’s bias, due to its deep connections with the classic paradox known as Simpson’s reversal paradox in statistics and social sciences.",
  "stance": -1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17681/17488",
  "title": "What's the Best Place for an AI Conference, Vancouver or _______: Why Completing Comparative Questions is Difficult",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Although large neural language models (LMs) like BERT can be finetuned to yield state-of-the-art results on many NLP tasks, it is often unclear what these models actually learn. Here we study using such LMs to fill in entities in humanauthored comparative questions, like “Which country is older, India or ?”—i.e., we study the ability of neural LMs to ask (not answer) reasonable questions. We show that accuracy in this fill-in-the-blank task is well-correlated with human judgements of whether a question is reasonable, and that these models can be trained to achieve nearly human-level performance in completing comparative questions in three different subdomains. However, analysis shows that what they learn fails to model any sort of broad notion of which entities are semantically comparable or similar—instead the trained models are very domain-specific, and performance is highly correlated with co-occurrences between specific entities observed in the training set. This is true both for models that are pretrained on general text corpora, as well as models trained on a large corpus of comparison questions. Our study thus reinforces recent results on the difficulty of making claims about a deep model’s world knowledge or linguistic competence based on performance on specific benchmark problems. We make our evaluation datasets publicly available to foster future research on complex understanding and reasoning in such models at standards of human interaction.",
  "stance": -0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17693/17500",
  "title": "TaLNet: Voice Reconstruction from Tongue and Lip Articulation with Transfer Learning from Text-to-Speech Synthesis",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "This paper presents TaLNet, a model for voice reconstruction with ultrasound tongue and optical lip videos as inputs. TaLNet is based on an encoder-decoder architecture. Separate encoders are dedicated to processing the tongue and lip data streams respectively. The decoder predicts acoustic features conditioned on encoder outputs and speaker codes. To mitigate for having only relatively small amounts of dual articulatory-acoustic data available for training, and since our task here shares with text-to-speech (TTS) the common goal of speech generation, we propose a novel transfer learning strategy to exploit the much larger amounts of acoustic-only data available to train TTS models. For this, a Tacotron 2 TTS model is first trained, and then the parameters of its decoder are transferred to the TaLNet decoder. We have evaluated our approach on an unconstrained multi-speaker voice recovery task. Our results show the effectiveness of both the proposed model and the transfer learning strategy. Speech reconstructed using our proposed method significantly outperformed all baselines (DNN, BLSTM and without transfer learning) in terms of both naturalness and intelligibility. When using an ASR model decoding the recovery speech, the WER of our proposed method shows a relative reduction of over 30% compared to baselines.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17697/17504",
  "title": "Semantics-Aware Inferential Network for Natural Language Understanding",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "For natural language understanding tasks, either machine reading comprehension or natural language inference, both semantics-aware and inference are favorable features of the concerned modeling for better understanding performance. Thus we propose a Semantics-Aware Inferential Network (SAIN) to meet such a motivation. Taking explicit contextualized semantics as a complementary input, the inferential module of SAIN enables a series of reasoning steps over semantic clues through an attention mechanism. By stringing these steps, the inferential network effectively learns to perform iterative reasoning which incorporates both explicit semantics and contextualized representations. In terms of well pre-trained language models as front-end encoder, our model achieves significant improvement on 11 tasks including machine reading comprehension and natural language inference.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P86-1017",
  "title": "Encoding and Acquiring Meanings for Figurative Phrases",
  "year": 1986,
  "venue": "ACL",
  "abstract": "Here we address the problem of mapping phrase meanings into their conceptual representations. Figurative phrases are pervasive in human communication, yet they are difficult to explain theoretically. In fact, the ability to handle idiosyncratic behavior of phrases should be a criterion for any theory of lexical representation. Due to the huge number of such phrases in the English language, phrase representation must be amenable to parsing, generation, and also to learning. In this paper we demonstrate a semantic representation which facilitates, for a wide variety of phrases, both learning and parsing.",
  "stance": 0.4
 },
 {
  "url": "https://aclanthology.org/P86-1033",
  "title": "Linguistic Coherence: A Plan-Based Alternative",
  "year": 1986,
  "venue": "ACL",
  "abstract": "To fully understand a sequence of utterances, one must be able to infer implicit relationships between the utterances. Although the identification of sets of utterance relationships forms the basis for many theories of discourse, the formalization and recognition of such relationships has proven to be an extremely difficult computational task. This paper presents a plan-based approach to the representation and recognition of implicit relationships between utterances. Relationships are formulated as discourse plans, which allows their representation in terms of planning operators and their computation via a plan recognition process. By incorporating complex inferential processes relating utterances into a plan-based framework, a formalization and computability not available in the earlier works is provided.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/P87-1013",
  "title": "A Logical Version of Functional Grammar",
  "year": 1987,
  "venue": "ACL",
  "abstract": "Kay's functional-unification grammar notation [5] is a way of expressing grammars which relies on very few primitive notions. The primary syntactic structure is the feature structure, which can be visualised as a directed graph with arcs labeled by at t r ibutes of a constituent, and the primary structure-building operation is unification. In this paper we propose a mathematical formulation of FUG, using logic to give a precise account of the strings and the structures defined by any grammar written in this notation.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/P88-1017",
  "title": "Parsing Japanese Honorifics in Unification-Based Grammar",
  "year": 1988,
  "venue": "ACL",
  "abstract": "This paper presents a unification-based approach to Japanese honorifics based on a version of HPSG (Head-driven Phrase Structure Grammar)ll]121. Utterance parsing is based on lexical specifications of each lexical item, including honorifics, and a few general PSG rules using a parser capable of unifying cyclic feature structures. It is shown that the possible word orders of Japanese honori f ic predicate constituents can be automatically deduced in the proposed f ramework w i thou t independent ly specifying them. Discourse Information Change Rules (DICRs) that a l low resolving a class of anaphors in honorific contexts are also formulated.",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/P89-1024",
  "title": "A Hybrid Approach to Representation in the Janus Natural Language Processor",
  "year": 1989,
  "venue": "ACL",
  "abstract": "In BBN's natural language understanding and generation system (Janus), we have used a hybrid approach to representation, employing an intensional logic for the representation of the semantics of utterances and a taxonomic language with formal semantics for specification of descriptive constants and axioms relating them. Remarkably, 99.9% of 7,000 vocabulary items in our natural language applications could be adequately axiomatlzed in the taxonomic language.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/P89-1026",
  "title": "Two Constraints on Speech Act Ambiguity",
  "year": 1989,
  "venue": "ACL",
  "abstract": "Existing plan-based theories of speech act interpretation do not account for the conventional aspect of speech acts. We use patterns of linguistic features (e.g. mood, verb form, sentence adverbials, thematic roles) to suggest a range of speech act interpretations for the utterance. These are filtered using plan-bused conversational implicatures to eliminate inappropriate ones. Extended plan reasoning is available but not necessary for familiar forms. Taking speech act ambiguity seriously, with these two constraints, explains how \"Can you pass the salt?\" is a typical indirect request while \"Are you able to pass the salt?\" is not.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/P89-1027",
  "title": "Treatment of Long Distance Dependencies in LFG and TAG: Functional Uncertainty in LFG Is a Corollary in TAG",
  "year": 1989,
  "venue": "ACL",
  "abstract": "In this paper the functional uncertainty machinery in LFG is compared with the treatment of long distance dependencies in TAG. It is shown that the functional uncertainty machinery is redundant in TAG, i.e.,what functional uncertainty accomplishes for LFG follows from the TAG formalism itself and some aspects of the linguistic theory instantiated in TAG. It is also shown that the analyses provided by the functional uncertainty machinery can be obtained without requiring power beyond mildly context-sensitive grammars. Some linguistic and computational aspects of these results have been briefly discussed also.",
  "stance": -0.1
 },
 {
  "url": "https://aclanthology.org/P89-1031",
  "title": "Evaluating Discourse Processing Algorithms",
  "year": 1989,
  "venue": "ACL",
  "abstract": "In order to take steps towards establishing a methodology for evaluating Natural Language systems, we conducted a case study. We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues. We present the quantitative results of handsimulating these algorithms, but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general. We illustrate the general difficulties encountered with quantitative evaluation. These are problems with: (a) allowing for underlying assumptions, (b) determining how to handle underspecifications, and (c) evaluating the contribution of false positives and error chaining.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/P90-1006",
  "title": "Memory Capacity and Sentence Processing",
  "year": 1990,
  "venue": "ACL",
  "abstract": "The limited capacity of working memory is intrinsic to human sentence processing, and therefore must be addressed by any theory of human sentence processing. This paper gives a theory of garden-path effects and processing overload that is based on simple assumptions about human short term memory capacity.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/P91-1019",
  "title": "Subject-Dependent Co-Occurrence and Word Sense Disambiguation",
  "year": 1991,
  "venue": "ACL",
  "abstract": "We describe a method for obtaining subject-dependent word sets relative to some (subjecO domain. Using the subject classifications given in the machine-readable version of Longman's Dictionary of Contemporary English, we established subject-dependent cooccurrence links between words of the defining vocabulary to construct these \"neighborhoods\". Here, we describe the application of these neighborhoods to information retrieval, and present a method of word sense disambiguation based on these co-occurrences, an extension of previous work.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P91-1038",
  "title": "A Preference-first Language Processor: Integrating the Unification Grammar and Markov Language Model for Speech Recognition Applications",
  "year": 1991,
  "venue": "ACL",
  "abstract": "A language processor is to find out a most promising sentence hypothesis for a given word lattice obtained from acoustic signal recognition. In this paper a new language processor is proposed, in which unification granunar and Markov language model are integrated in a word lattice parsing algorithm based on an augmented chart, and the island-driven parsing concept is combined with various preference-first parsing strategies defined by different construction principles and decision rules. Test results\"show that significant improvements in both correct rate of recognition and computation speed can be achieved .",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P92-1017",
  "title": "Inside-Outside Reestimation From Partially Bracketed Corpora",
  "year": 1992,
  "venue": "ACL",
  "abstract": "The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information (constituent bracketing) in a partially parsed corpus. Experiments on formal and natural language parsed corpora show that the new algorithm can achieve faster convergence and better modeling of hierarchical structure than the original one. In particular, over 90% test set bracketing accuracy was achieved for grammars inferred by our algorithm from a training set of hand-parsed part-of-speech strings for sentences in the Air Travel Information System spoken language corpus. Finally, the new algorithm has better time complexity than the original one when sufficient bracketing is provided.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P92-1023",
  "title": "GPSM: A Generalized Probabilistic Semantic Model for Ambiguity Resolution",
  "year": 1992,
  "venue": "ACL",
  "abstract": "In natural language processing, ambiguity resolution is a central issue, and can be regarded as a preference assignment problem. In this paper, a Generalized Probabilistic Semantic Model (GPSM) is proposed for preference computation. An effective semantic tagging procedure is proposed for tagging semantic features. A semantic score function is derived based on a score function, which integrates lexical, syntactic and semantic preference under a uniform formulation. The semantic score measure shows substantial improvement in structural disambiguation over a syntax-based approach.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P92-1032",
  "title": "Estimating Upper and Lower Bounds on the Performance of Word-Sense Disambiguation Programs",
  "year": 1992,
  "venue": "ACL",
  "abstract": "We have recently reported on two new word-sense disambiguation systems, one trained on bilingual material (the Canadian Hansards) and the other trained on monolingual material (Roget's Thesaurus and Grolier's Encyclopedia). After using both the monolingual and bilingual classifiers for a few months, we have convinced ourselves that the performance is remarkably good. Nevertheless, we would really like to be able to make a stronger statement, and therefore, we decided to try to develop some more objective evaluation measures. Although there has been a fair amount of literature on sense-disambiguation, the literature does not offer much guidance in how we might establish the success or failure of a proposed solution such as the two systems mentioned in the previous paragraph. Many papers avoid quantitative evaluations altogether, because it is so difficult to come up with credible estimates of performance. This paper will attempt to establish upper and lower bounds on the level of performance that can be expected in an evaluation. An estimate of the lower bound of 75% (averaged over ambiguous types) is obtained by measuring the performance produced by a baseline system that ignores context and simply assigns the most likely sense in all cases. An estimate of the upper bound is obtained by assuming that our ability to measure performance is largely limited by our ability obtain reliable judgments from human informants. Not surprisingly, the upper bound is very dependent on the instructions given to the judges. Jorgensen, for example, suspected that lexicographers tend to depend too much on judgments by a single informant and found considerable variation over judgments (only 68% agreement), as she had suspected. In our own experiments, we have set out to find word-sense disambiguation tasks where the judges can agree often enough so that we could show that they were outperforming the baseline system. Under quite different conditions, we have found 96.8% agreement over judges.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P92-1039",
  "title": "Right Association Revisited",
  "year": 1992,
  "venue": "ACL",
  "abstract": "Consideration of when Right Association works and when it fails lead to a restatement of this parsing principle in terms of the notion of heaviness. A computational investigation of a syntactically annotated corpus provides evidence for this proposal and suggest circumstances when RA is likely to make correct attachment predictions.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P92-1041",
  "title": "Incremental Dependency Parsing",
  "year": 1992,
  "venue": "ACL",
  "abstract": "The paper introduces a dependency-based grammar and the associated parser and focusses on the problem of determinism in parsing and recovery from errors. First, it is shown how dependency-based parsing can be afforded, by taking into account the suggestions coming from other approaches, and the preference criteria for parsing are briefly addressed. Second, the issues of the interconnection between the syntactic analysis and the semantic interpretation in incremental processing are discussed and the adoption of a TMS for the recovery of the processing errors is suggested. ",
  "stance": 0.4
 },
 {
  "url": "https://aclanthology.org/P93-1007",
  "title": "A Speech-First Model for Repair Detection and Correction",
  "year": 1993,
  "venue": "ACL",
  "abstract": "Interpreting fully natural speech is an important goal for spoken language understanding systems. However, while corpus studies have shown that about 10% of spontaneous utterances contain self-corrections, or REPAIRS, little is known about the extent to which cues in the speech signal may facilitate repair processing. We identify several cues based on acoustic and prosodic analysis of repairs in a corpus of spontaneous speech, and propose methods for exploiting these cues to detect and correct repairs. We test our acoustic-prosodic cues with other lexical cues to repair identification and find that precision rates of 89-93% and recall of 78-83% can be achieved, depending upon the cues employed, from a prosodically labeled corpus.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/P93-1025",
  "title": "Transfers of Meaning",
  "year": 1993,
  "venue": "ACL",
  "abstract": "In one form or another, the phenomena associated with \"meaning transfer\" have become central issues in a lot of recent work on semantics. Speaking very roughly, we can partition approaches to the phenomenon along two dimensions, which yield four basic points of departure. In the first two, people have considered transfer in basically semantic or linguistic terms. Some have concentrated on what we might call the paradigmatic aspects of transfer, focusing on the productive lexical processes that map semantic features into features for example, the \"grinding\" rule that applies to turn the names of animals into mass terms denoting their meat or fur. This the approach that's involved in most recent work on \"regular polysemy,\" \"systematic polysemy,\" and the like, for example by Apresjan, Ostler and Atkins, Briscoe and Copestake, Nunberg and Zaenen, Wilensky, Kilgarriff and a number of other people. Other people have emphasized the syncategorematic aspects of transfer; that is, the ways meaning shifts and specifications are coerced in the course of semantic composition. This is an approach that hass been developed in particular by James Pustejovsky and his collaborators, building on earlier work on type shifting. As opposed to these, there are conceptual and pragmatic approaches to transfer, which focus on the extralinguistic circumstances that license transfers of various types. Here again there are both paradigmatic and syncategorematic approaches, loosely speaking. The first is exemplified in a lot of recent work on metaphor by people associated with the \"cognitive linguistics\" school, which has focused chiefly on the relations between domains of experience that metaphor variously exploits and imputes. The second is represented by work on indirect speech within Gricean pragmatics, Relevance Theory, and the like, which has been chiefly concerned with specifying the conversational conditions that give rise to metaphor, irony, and analogous phenomena. Of course this categorization is somewhat factitious. The borders between these approaches are highly porous, and most work on transfer overlaps several of them. This is entirely appropriate, since these are in no sense competing theories or accounts of the phenomena. Transfer is clearly a linguistic process, and in many of its most important forms a lexical one. But it just as clearly has its basis in very general cognitive and communicative principles. And while it's reasonable that people should choose to focus on one or another of these considerations relative to their immediate interests, it is also useful to keep the Big Picture in mind, lest we inadvertently ascribe to one domain of explanation a responsibility that more properly belongs to another. This is the picture I want to sketch out in this talk. A comprehensive account of transfer has to make appeal to three different kinds of regularities or rules. The first are nonlinguistic: the correspondences between domains, real or imputed, that transfer invokes, and the communicative interests that may make these invocations useful or instructive they enable us to identify one thing in virtue of its relation to another, explain an abstract domain by reference to a concrete one, and so forth. Second, there is the repertory of general linguistic processes of transfer that exploit these correspondences and principles. By these I have in mind not traditional categories like metaphor, synecdoche, and m e t o n y m y distinctions that have basically to do with the kinds of domain correspondences that transfer exploits but the various types of operations that make possible typeshifting and sortal reassignment of expressions, syntactic recategorizations, and deferred indexical reference. These processes may cross-cut the types of domain correspondences that they exploit, and I'll show that we often find a single type of domain correspondence underlying two or more distinct semantic processes of transfer. Third, there are the language-specific instantiations of these operations, for example in the form of constructions or lexical rules that license particular types or",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/P94-1005",
  "title": "From Strings to Trees to Strings to Trees ... (Abstract)",
  "year": 1994,
  "venue": "ACL",
  "abstract": "Sentences are not just strings of words (or are they ?), they have some (hierarchical) structure. This much is accepted by all grammar formalisms. But how much structure is needed? The more the sentences are like strings the less the need for structure. A certain amount of structure is necessary simply because a clause may embed another clause, or one clause may attach to another clause or parts of it. Leaving this need of structure aside, the question then is how much structure should a (minimal) clause have? Grammar formalisms can differ significantly on this issue. Minimal clauses can be just strings, or words linked by dependencies (dependency trees), or with rich phrase structure trees, or with flat (one level) phrase structure trees (almost strings) and so on. How much hierarchical structure is needed for a minimal clause is still an open question, that is being debated heatedly. How are clauses put together? Are these operations more like string manipulations (concatenation, insertion, or wrapping, for example) or are they more like tree transformations (generalized transformations of the early transformational grammars, for example)? Curiously, the early transformational grammars, although clearly using tree transformations, actually formulated the transformations as pseudo string-like operations! More recent non-transformational grammars differ significantly with respect to their use of string rewriting or tree rewriting operations. Grammar formalisms differ with respect to their stringiness or treeness. Also during their evolution, they have gone back and forth between string-like and tree-like representations, often combining them in different ways. These swings are a reflection of the complex interplay between aspects of language structure such as constituency, dependency, dominance, locality of predicates and their arguments, adjacency, order, and discontinuity. We will discuss these issues in an informal manner, in the context of a range of formalisms.",
  "stance": -0.1
 },
 {
  "url": "https://aclanthology.org/P94-1034",
  "title": "An Automatic Treebank Conversion Algorithm for Corpus Sharing",
  "year": 1994,
  "venue": "ACL",
  "abstract": "An automatic treebank conversion method is proposed in this paper to convert a treebank into another treebank. A new treebank associated with a different grammar can be generated automatically from the old one such that the information in the original treebank can be transformed to the new one and be shared among different research communities. The simple algorithm achieves conversion accuracy of 96.4% when tested on 8,867 sentences between two major grammar revisions of a large MT system.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/P94-1038",
  "title": "Similarity-Based Estimation of Word Cooccurrence Probabilities",
  "year": 1994,
  "venue": "ACL",
  "abstract": "In many applications of natural language processing it is necessary to determine the likelihood of a given word combination. For example, a speech recognizer may need to determine which of the two word combinations \"eat a peach\" and \"eat a beach\" is more likely. Statistical NLP methods determine the likelihood of a word combination according to its frequency in a training corpus. However, the nature of language is such that many word combinations are infrequent and do not occur in a given corpus. In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on \"most similar\" words. We describe a probabilistic word association model based on distributional word similarity, and apply it to improving probability estimates for unseen word bigrams in a variant of Katz 's back-off model. The similarity-based method yields a 20% perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P95-1012",
  "title": "Compiling HPSG type constraints into definite clause programs",
  "year": 1995,
  "venue": "ACL",
  "abstract": "We present a new approach to HPSG processing: compiling HPSG grammars expressed as type constraints into definite clause programs. This provides a clear and computationally useful correspondence between linguistic theories and their implementation. The compiler performs offline constraint inheritance and code optimization. As a result, we are able to efficiently process with HPSG grammars without haviog to hand-translate them into definite clause or phrase structure based systems.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/P95-1020",
  "title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances",
  "year": 1995,
  "venue": "ACL",
  "abstract": "Drawing appropriate defeasible inferences has been proven to be one of the most pervasive puzzles of natural language processing and a recurrent problem in pragmatics. This paper provides a theoretical framework, called stratified logic, that can accommodate defeasible pragmatic inferences. The framework yields an algorithm that computes the conversational, conventional, scalar, clausal, and normal state implicatures; and the presuppositions that are associated with utterances. The algorithm applies equally to simple and complex utterances and sequences of utterances.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/P95-1040",
  "title": "The Effect of Pitch Accenting on Pronoun Referent Resolution",
  "year": 1995,
  "venue": "ACL",
  "abstract": "By strictest interpretation, theories of both centering and intonational meaning fail to predict the existence of pitch accented pronominals. Yet they occur felicitously in spoken discourse. To explain this, I emphasize the dual functions served by pitch accents, as markers of both propositional (semantic/pragmatic) and attentional salience. This distinction underlies my proposals about the attentional consequences of pitch accents when applied to pronominals, in particular, that while most pitch accents may weaken or reinforce a cospecifier's status as the center of attention, a contrastively stressed pronominal may force a shift, even when contraindicated by textual features.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/P95-1046",
  "title": "Knowledge-based Automatic Topic Identification",
  "year": 1995,
  "venue": "ACL",
  "abstract": "As the first step in an automated text summarization algorithm, this work presents a new method for automatically identifying the central ideas in a text based on a knowledge-based concept counting paradigm. To represent and generalize concepts, we use the hierarchical concept taxonomy WordNet. By setting appropriate cutoff values for such parameters as concept generality and child-to-parent frequency ratio, we control the amount and level of generality of concepts extracted from the text.",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/P96-1006",
  "title": "Integrating Multiple Knowledge Sources to Disambiguate Word Sense: An Exemplar-Based Approach",
  "year": 1996,
  "venue": "ACL",
  "abstract": "In this paper, we present a new approach for word sense disambiguation (WSD) using an exemplar-based learning algorithm. This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech of neighboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation. We tested our WSD program, named LEXAS, on both a common data set used in previous work, as well as on a large sense-tagged corpus that we separately constructed. LEXAS achieves a higher accuracy on the common data set, and performs better than the most frequent heuristic on the highly ambiguous words in the large corpus tagged with the refined senses of WoRDNET.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P96-1052",
  "title": "Towards Testing the Syntax of Punctuation",
  "year": 1996,
  "venue": "ACL",
  "abstract": "Little work has been done in NLP on the subject of punctuation, owing mainly to a lack of a good theory on which computational treatments could be based. This paper described early work in progress to try to construct such a theory. Two approaches to finding the syntactic function of punctuation marks are discussed, and procedures are described by which the results from these approaches can be tested and evaluated both against each other as well as against other work. Suggestions are made for the use of these results, and for future work.",
  "stance": -0.3
 },
 {
  "url": "https://aclanthology.org/P97-1002",
  "title": "Fast Context-Free Parsing Requires Fast Boolean Matrix Multiplication",
  "year": 1997,
  "venue": "ACL",
  "abstract": "Valiant showed that Boolean matrix multiplication (BMM) can be used for CFG parsing. We prove a dual result: CFG parsers running in time O([Gl[w[ 3-e) on a grammar G and a string w can be used to multiply m x m Boolean matrices in time O(m3-e/3). In the process we also provide a formal definition of parsing motivated by an informal notion due to Lang. Our result establishes one of the first limitations on general CFG parsing: a fast, practical CFG parser would yield a fast, practical BMM algorithm, which is not believed to exist.",
  "stance": -0.0
 },
 {
  "url": "https://aclanthology.org/P97-1003",
  "title": "Three Generative, Lexicalised Models for Statistical Parsing",
  "year": 1997,
  "venue": "ACL",
  "abstract": "In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P97-1011",
  "title": "Learning Features that Predict Cue Usage",
  "year": 1997,
  "venue": "ACL",
  "abstract": "Our goal is to identify the features that predict the occurrence and placement of discourse cues in tutorial explanations in order to aid in the automatic generation of explanations. Previous attempts to devise rules for text generation were based on intuition or small numbers of constructed examples. We apply a machine learning program, C4.5, to induce decision trees for cue occurrence and placement from a corpus of data coded for a variety of features previously thought to affect cue usage. Our experiments enable us to identify the features with most predictive power, and show that machine learning can be used to induce decision trees useful for text generation.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/P97-1037",
  "title": "A DP-based Search Using Monotone Alignments in Statistical Translation",
  "year": 1997,
  "venue": "ACL",
  "abstract": "In this paper, we describe a Dynamic Programming (DP) based search algorithm for statistical translation and present experimental results. The statistical translation uses two sources of information: a translation model and a language model. The language model used is a standard bigram model. For the translation lnodel, the alignment probabilities are made dependent on the differences in the alignment positions rather than on the absolute positions. Thus, the approach amounts to a first-order Hidden Markov model (HMM) as they are used successfully in speech recognition for the time alignment problem. Under the assumption that the alignment is monotone with respect to the word order in both languages, an efficient search strategy for translation can be formulated. The details of the search algorithm are described. Experiments on the EuTrans corpus produced a word error rate of 5.1%.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/P97-1061",
  "title": "Retrieving Collocations by Co-Occurrences and Word Order Constraints",
  "year": 1997,
  "venue": "ACL",
  "abstract": "In this paper, we describe a method for automatically retrieving collocations from large text corpora. This method retrieve collocations in the following stages: 1) extracting strings of characters as units of collocations 2) extracting recurrent combinations of strings in accordance with their word order in a corpus as collocations. Through the method, various range of collocations, especially domain specific collocations, are retrieved. The method is practical because it uses plain texts without any information dependent on a language such as lexical knowledge and parts of speech.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/P99-1008",
  "title": "Finding Parts in Very Large Corpora",
  "year": 1999,
  "venue": "ACL",
  "abstract": "We present a method for extracting parts of objects from wholes (e.g. \"speedometer\" from \"car\"). Given a very large corpus our method finds part words with 55% accuracy for the top 50 words as ranked by the system. The part list could be scanned by an end-user and added to an existing ontology (such as WordNet), or used as a part of a rough semantic lexicon.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/P99-1014",
  "title": "Inducing a Semantically Annotated Lexicon via EM-Based Clustering",
  "year": 1999,
  "venue": "ACL",
  "abstract": "We present a technique for automatic induction of slot annotations for subcategorization frames, based on induction of hidden classes in the EM framework of statistical estimation. The models are empirically evalutated by a general decision test. Induction of slot labeling for subcategorization frames is accomplished by a further application of EM, and applied experimentally on frame observations derived from parsing large corpora. We outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries.",
  "stance": 0.4
 },
 {
  "url": "https://aclanthology.org/P99-1043",
  "title": "Mixed Language Query Disambiguation",
  "year": 1999,
  "venue": "ACL",
  "abstract": "We propose a mixed language query disambiguation approach by using co-occurrence information from monolingual data only. A mixed language query consists of words in a primary language and a secondary language. Our method translates the query into monolingual queries in either language. Two novel features for disambiguation, namely contextual word voting and 1-best contextual word, are introduced and compared to a baseline feature, the nearest neighbor. Average query translation accuracy for the two features are 81.37% and 83.72%, compared to the baseline accuracy 75.50%.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/P99-1047",
  "title": "A Decision-Based Approach to Rhetorical Parsing",
  "year": 1999,
  "venue": "ACL",
  "abstract": "We present a shift-reduce rhetorical parsing algorithm that learns to construct rhetorical structures of texts from a corpus of discourse-parse action sequences. The algorithm exploits robust lexical, syntactic, and semantic knowledge sources.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/P99-1053",
  "title": "A Syntactic Framework for Speech Repairs and Other Disruptions",
  "year": 1999,
  "venue": "ACL",
  "abstract": "This paper presents a grammatical and processing framework for handling the repairs, hesitations, and other interruptions in natural human dialog. The proposed framework has proved adequate for a collection of human-human task-oriented dialogs, both in a full manual examination of the corpus, and in tests with a parser capable of parsing some of that corpus. This parser can also correct a pre-parser speech repair identifier resulting in a 4.8% increase in recall.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P99-1057",
  "title": "Learning to Recognize Tables in Free Text",
  "year": 1999,
  "venue": "ACL",
  "abstract": "Many real-world texts contain tables. In order to process these texts correctly and extract the information contained within the tables, it is important to identify the presence and structure of tables. In this paper, we present a new approach that learns to recognize tables in free text, including the boundary, rows and columns of tables. When tested on Wall Street Journal news documents, our learning approach outperforms a deterministic table recognition algorithm that identifies tables based on a fixed set of conditions. Our learning approach is also more flexible and easily adaptable to texts in different domains with different table characteristics.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P99-1063",
  "title": "Lexical Semantics to Disambiguate Polysemous Phenomena of Japanese Adnominal Constituents",
  "year": 1999,
  "venue": "ACL",
  "abstract": "We exploit and extend the Generative Lexicon Theory to develop a formal description of adnominal constituents in a lexicon which can deal with linguistic phenomena found in Japanese adnominal constituents. We classify the problematic behavior into \"static disambiguation\" and \"dynamic disambiguation\" tasks. Static disambiguation can be done using lexical information in a dictionary, whereas dynamic disambiguation requires inferences at the knowledge representation level.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/P00-1012",
  "title": "The Order of Prenominal Adjectives in Natural Language Generation",
  "year": 2000,
  "venue": "ACL",
  "abstract": "The order of prenominal adjectival modifiers in English is governed by complex and difficult to describe constraints which straddle the boundary between competence and performance. This paper describes and compares a number of statistical and machine learning techniques for ordering sequences of adjectives in the context of a natural language generation system.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P00-1017",
  "title": "Using Existing Systems to Supplement Small Amounts of Annotated Grammatical Relations Training Data",
  "year": 2000,
  "venue": "ACL",
  "abstract": "Grammatical relationships (GRs) form an important level of natural language processing, but di erent sets of GRs are useful for di erent purposes. Therefore, one may often only have time to obtain a small training corpus with the desired GR annotations. To boost the performance from using such a small training corpus on a transformation rule learner, we use existing systems that nd related types of annotations.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P00-1052",
  "title": "The Role of Centering Theory's Rough-Shift in the Teaching and Evaluation of Writing Skills",
  "year": 2000,
  "venue": "ACL",
  "abstract": "Existing software systems for automated essay scoring can provide NLP researchers with opportunities to test certain theoretical hypotheses, including some derived from Centering Theory. In this study we employ ETS's e-rater essay scoring system to examine whether local discourse coherence, as de ned by a measure of Rough-Shift transitions, might be a signi cant contributor to the evaluation of essays. Our positive results indicate that Rough-Shifts do indeed capture a source of incoherence, one that has not been closely examined in the Centering literature. These results not only justify Rough-Shifts as a valid transition type, but they also support the original formulation of Centering as a measure of discourse continuity even in pronominal-free text.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P00-1077",
  "title": "Panel: Computational Linguistics in India: An Overview",
  "year": 2000,
  "venue": "ACL",
  "abstract": "In the anusaaraka systems, the load between the human reader and the machine is divided as follows: language-based analysis of the text is carried out by the machine, and knowledge-based analysis or interpretation is left to the reader. The machine uses a dictionary and grammar rules, to produce the output. Most importantly, it does not use world knowledge to interpret (or disambiguate), as it is an error prone task and involves guessing or inferring based on knowledge other than the text. Anusaaraka aims for perfect \"information preservation\". We relax the requirement that the output be grammatical. In fact, anusaaraka output follows the grammar of the source language (where the grammar rules differ, and cannot be applied with 100 percent confidence). This requires that the reader undergo a short training to read and understand the output.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P03-2014",
  "title": "ACL-03 Interactive Posters and Demonstrations",
  "year": 2003,
  "venue": "ACL",
  "abstract": "On behalf of the program committee for ACL-2003 Interactive Posters/Demonstrations Sessions, it is my great pleasure to present you with this proceedings. Though ACL occasionally have had demonstration sessions, this will be the first attempt for ACL to have this kind of sessions, which aim to provide researchers or developers of natural language technologies with a generous environment for presentation and discussion of their works. Each paper receives an opportuity to give a five-minute oral preview presentatin and a poster or poster with demo presentation for two and half hours. To this attempt, there were initially 62 submissions, one paper was withdrawn during the review process, and we could accept only 28 papers. One paper was withdrawn after the selection and we finally have 27 papers, which you find in this volume.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P05-1027",
  "title": "Question Answering as Question-Biased Term Extraction: A New Approach toward Multilingual QA",
  "year": 2005,
  "venue": "ACL",
  "abstract": "This paper regards Question Answering (QA) as Question-Biased Term Extraction (QBTE). This new QBTE approach liberates QA systems from the heavy burden imposed by question types (or answer types). In conventional approaches, a QA system analyzes a given question and determines the question type, and then it selects answers from among answer candidates that match the question type. Consequently, the output of a QA system is restricted by the design of the question types. The QBTE directly extracts answers as terms biased by the question. To confirm the feasibility of our QBTE approach, we conducted experiments on the CRL QA Data based on 10-fold cross validation, using Maximum Entropy Models (MEMs) as an ML technique. Experimental results showed that the trained system achieved 0.36 in MRR and 0.47 in Top5 accuracy.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/P05-1048",
  "title": "Word Sense Disambiguation vs. Statistical Machine Translation",
  "year": 2005,
  "venue": "ACL",
  "abstract": "We directly investigate a subject of much recent debate: do word sense disambigation models help statistical machine translation quality? We present empirical results casting doubt on this common, but unproved, assumption. Using a state-ofthe-art Chinese word sense disambiguation model to choose translation candidates for a typical IBM statistical MT system, we find that word sense disambiguation does not yield significantly better translation quality than the statistical machine translation system alone. Error analysis suggests several key factors behind this surprising finding, including inherent limitations of current statistical MT architectures.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P05-1067",
  "title": "Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars",
  "year": 2005,
  "venue": "ACL",
  "abstract": "Syntax-based statistical machine translation (MT) aims at applying statistical models to structured data. In this paper, we present a syntax-based statistical machine translation system based on a probabilistic synchronous dependency insertion grammar. Synchronous dependency insertion grammars are a version of synchronous grammars defined on dependency trees. We first introduce our approach to inducing such a grammar from parallel corpora. Second, we describe the graphical model for the machine translation task, which can also be viewed as a stochastic tree-to-tree transducer. We introduce a polynomial time decoding algorithm for the model. We evaluate the outputs of our MT system using the NIST and Bleu automatic MT evaluation software. The result shows that our system outperforms the baseline system based on the IBM models in both translation speed and quality.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P06-1007",
  "title": "A Finite-State Model of Human Sentence Processing",
  "year": 2006,
  "venue": "ACL",
  "abstract": "It has previously been assumed in the psycholinguistic literature that finite-state models of language are crucially limited in their explanatory power by the locality of the probability distribution and the narrow scope of information used by the model. We show that a simple computational model (a bigram part-of-speech tagger based on the design used by Corley and Crocker (2000)) makes correct predictions on processing difficulty observed in a wide range of empirical sentence processing data. We use two modes of evaluation: one that relies on comparison with a control sentence, paralleling practice in human studies; another that measures probability drop in the disambiguating region of the sentence. Both are surprisingly good indicators of the processing difficulty of garden-path sentences. The sentences tested are drawn from published sources and systematically explore five different types of ambiguity: previous studies have been narrower in scope and smaller in scale. We do not deny the limitations of finite-state models, but argue that our results show that their usefulness has been underestimated.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/P06-1098",
  "title": "Left-to-Right Target Generation for Hierarchical Phrase-Based Translation",
  "year": 2006,
  "venue": "ACL",
  "abstract": "We present a hierarchical phrase-based statistical machine translation in which a target sentence is efficiently generated in left-to-right order. The model is a class of synchronous-CFG with a Greibach Normal Form-like structure for the projected production rule: The paired target-side of a production rule takes a phrase prefixed form. The decoder for the targetnormalized form is based on an Earlystyle top down parser on the source side. The target-normalized form coupled with our top down parser implies a left-toright generation of translations which enables us a straightforward integration with ngram language models. Our model was experimented on a Japanese-to-English newswire translation task, and showed statistically significant performance improvements against a phrase-based translation system.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P06-1123",
  "title": "Empirical Lower Bounds on the Complexity of Translational Equivalence",
  "year": 2006,
  "venue": "ACL",
  "abstract": "This paper describes a study of the patterns of translational equivalence exhibited by a variety of bitexts. The study found that the complexity of these patterns in every bitext was higher than suggested in the literature. These findings shed new light on why “syntactic” constraints have not helped to improve statistical translation models, including finitestate phrase-based models, tree-to-string models, and tree-to-tree models. The paper also presents evidence that inversion transduction grammars cannot generate some translational equivalence relations, even in relatively simple real bitexts in syntactically similar languages with rigid word order. Instructions for replicating our experiments are at http://nlp.cs.nyu.edu/GenPar/ACL06",
  "stance": -0.5
 },
 {
  "url": "https://aclanthology.org/P06-1135",
  "title": "Improving QA Accuracy by Question Inversion",
  "year": 2006,
  "venue": "ACL",
  "abstract": "This paper demonstrates a conceptually simple but effective method of increasing the accuracy of QA systems on factoid-style questions. We define the notion of an inverted question, and show that by requiring that the answers to the original and inverted questions be mutually consistent, incorrect answers get demoted in confidence and correct ones promoted. Additionally, we show that lack of validation can be used to assert no-answer (nil) conditions. We demonstrate increases of performance on TREC and other question-sets, and discuss the kinds of future activities that can be particularly beneficial to approaches such as ours.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/P07-1012",
  "title": "Vocabulary Decomposition for Estonian Open Vocabulary Speech Recognition",
  "year": 2007,
  "venue": "ACL",
  "abstract": "Speech recognition in many morphologically rich languages suffers from a very high out-of-vocabulary (OOV) ratio. Earlier work has shown that vocabulary decomposition methods can practically solve this problem for a subset of these languages. This paper compares various vocabulary decomposition approaches to open vocabulary speech recognition, using Estonian speech recognition as a benchmark. Comparisons are performed utilizing large models of 60000 lexical items and smaller vocabularies of 5000 items. A large vocabulary model based on a manually constructed morphological tagger is shown to give the lowest word error rate, while the unsupervised morphology discovery method Morfessor Baseline gives marginally weaker results. Only the Morfessor-based approach is shown to adequately scale to smaller vocabulary sizes.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P07-1034",
  "title": "Instance Weighting for Domain Adaptation in NLP",
  "year": 2007,
  "venue": "ACL",
  "abstract": "Domain adaptation is an important problem in natural language processing (NLP) due to the lack of labeled data in novel domains. In this paper, we study the domain adaptation problem from the instance weighting perspective. We formally analyze and characterize the domain adaptation problem from a distributional view, and show that there are two distinct needs for adaptation, corresponding to the different distributions of instances and classification functions in the source and the target domains. We then propose a general instance weighting framework for domain adaptation. Our empirical results on three NLP tasks show that incorporating and exploiting more information from the target domain through instance weighting is effective.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/P07-1058",
  "title": "Instance-based Evaluation of Entailment Rule Acquisition",
  "year": 2007,
  "venue": "ACL",
  "abstract": "Obtaining large volumes of inference knowledge, such as entailment rules, has become a major factor in achieving robust semantic processing. While there has been substantial research on learning algorithms for such knowledge, their evaluation methodology has been problematic, hindering further research. We propose a novel evaluation methodology for entailment rules which explicitly addresses their semantic properties and yields satisfactory human agreement levels. The methodology is used to compare two state of the art learning algorithms, exposing critical issues for future progress.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/P07-1111",
  "title": "A Re-examination of Machine Learning Approaches for Sentence-Level MT Evaluation",
  "year": 2007,
  "venue": "ACL",
  "abstract": "Recent studies suggest that machine learning can be applied to develop good automatic evaluation metrics for machine translated sentences. This paper further analyzes aspects of learning that impact performance. We argue that previously proposed approaches of training a HumanLikeness classifier is not as well correlated with human judgments of translation quality, but that regression-based learning produces more reliable metrics. We demonstrate the feasibility of regression-based metrics through empirical analysis of learning curves and generalization studies and show that they can achieve higher correlations with human judgments than standard automatic metrics.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/P07-1117",
  "title": "Using Mazurkiewicz Trace Languages for Partition-Based Morphology",
  "year": 2007,
  "venue": "ACL",
  "abstract": "Partition-based morphology is an approach of finite-state morphology where a grammar describes a special kind of regular relations, which split all the strings of a given tuple into the same number of substrings. They are compiled in finite-state machines. In this paper, we address the question of merging grammars using different partitionings into a single finite-state machine. A morphological description may then be obtained by parallel or sequential application of constraints expressed on different partition notions (e.g. morpheme, phoneme, grapheme). The theory of Mazurkiewicz Trace Languages, a well known semantics of parallel systems, provides a way of representing and compiling such a description.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P07-1127",
  "title": "User Requirements Analysis for Meeting Information Retrieval Based on Query Elicitation",
  "year": 2007,
  "venue": "ACL",
  "abstract": "We present a user requirements study for Question Answering on meeting records that assesses the difficulty of users questions in terms of what type of knowledge is required in order to provide the correct answer. We grounded our work on the empirical analysis of elicited user queries. We found that the majority of elicited queries (around 60%) pertain to argumentative processes and outcomes. Our analysis also suggests that standard keyword-based Information Retrieval can only deal successfully with less than 20% of the queries, and that it must be complemented with other types of metadata and inference.",
  "stance": -0.1
 },
 {
  "url": "https://aclanthology.org/P08-1024",
  "title": "A Discriminative Latent Variable Model for Statistical Machine Translation",
  "year": 2008,
  "venue": "ACL",
  "abstract": "Large-scale discriminative machine translation promises to further the state-of-the-art, but has failed to deliver convincing gains over current heuristic frequency count systems. We argue that a principle reason for this failure is not dealing with multiple, equivalent translations. We present a translation model which models derivations as a latent variable, in both training and decoding, and is fully discriminative and globally optimised. Results show that accounting for multiple derivations does indeed improve performance. Additionally, we show that regularisation is essential for maximum conditional likelihood models in order to avoid degenerate solutions.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/P08-1038",
  "title": "A Logical Basis for the D Combinator and Normal Form in CCG",
  "year": 2008,
  "venue": "ACL",
  "abstract": "The standard set of rules defined in Combinatory Categorial Grammar (CCG) fails to provide satisfactory analyses for a number of syntactic structures found in natural languages. These structures can be analyzed elegantly by augmenting CCG with a class of rules based on the combinator D (Curry and Feys, 1958). We show two ways to derive the D rules: one based on unary composition and the other based on a logical characterization of CCG’s rule base (Baldridge, 2002). We also show how Eisner’s (1996) normal form constraints follow from this logic, ensuring that the D rules do not lead to spurious ambiguities.",
  "stance": -0.3
 },
 {
  "url": "https://aclanthology.org/P08-1039",
  "title": "Parsing Noun Phrase Structure with CCG",
  "year": 2008,
  "venue": "ACL",
  "abstract": "Statistical parsing of noun phrase (NP) structure has been hampered by a lack of goldstandard data. This is a significant problem for CCGbank, where binary branching NP derivations are often incorrect, a result of the automatic conversion from the Penn Treebank. We correct these errors in CCGbank using a gold-standard corpus of NP structure, resulting in a much more accurate corpus. We also implement novel NER features that generalise the lexical information needed to parse NPs and provide important semantic information. Finally, evaluating against DepBank demonstrates the effectiveness of our modified corpus and novel features, with an increase in parser performance of 1.51%.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/P08-1063",
  "title": "Robustness and Generalization of Role Sets: PropBank vs. VerbNet",
  "year": 2008,
  "venue": "ACL",
  "abstract": "This paper presents an empirical study on the robustness and generalization of two alternative role sets for semantic role labeling: PropBank numbered roles and VerbNet thematic roles. By testing a state–of–the–art SRL system with the two alternative role annotations, we show that the PropBank role set is more robust to the lack of verb–specific semantic information and generalizes better to infrequent and unseen predicates. Keeping in mind that thematic roles are better for application needs, we also tested the best way to generate VerbNet annotation. We conclude that tagging first PropBank roles and mapping into VerbNet roles is as effective as training and tagging directly on VerbNet, and more robust for domain shifts.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P08-1067",
  "title": "Forest Reranking: Discriminative Parsing with Non-Local Features",
  "year": 2008,
  "venue": "ACL",
  "abstract": "Conventional n-best reranking techniques often suffer from the limited scope of the nbest list, which rules out many potentially good alternatives. We instead propose forest reranking, a method that reranks a packed forest of exponentially many parses. Since exact inference is intractable with non-local features, we present an approximate algorithm inspired by forest rescoring that makes discriminative training practical over the whole Treebank. Our final result, an F-score of 91.7, outperforms both 50-best and 100-best reranking baselines, and is better than any previously reported systems trained on the Treebank.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P08-1071",
  "title": "Assessing Dialog System User Simulation Evaluation Measures Using Human Judges",
  "year": 2008,
  "venue": "ACL",
  "abstract": "Previous studies evaluate simulated dialog corpora using evaluation measures which can be automatically extracted from the dialog systems’ logs. However, the validity of these automatic measures has not been fully proven. In this study, we first recruit human judges to assess the quality of three simulated dialog corpora and then use human judgments as the gold standard to validate the conclusions drawn from the automatic measures. We observe that it is hard for the human judges to reach good agreement when asked to rate the quality of the dialogs from given perspectives. However, the human ratings give consistent ranking of the quality of simulated corpora generated by different simulation models. When building prediction models of human judgments using previously proposed automatic measures, we find that we cannot reliably predict human ratings using a regression model, but we can predict human rankings by a ranking model.",
  "stance": -0.1
 },
 {
  "url": "https://aclanthology.org/P08-1087",
  "title": "Enriching Morphologically Poor Languages for Statistical Machine Translation",
  "year": 2008,
  "venue": "ACL",
  "abstract": "We address the problem of translating from morphologically poor to morphologically rich languages by adding per-word linguistic information to the source language. We use the syntax of the source sentence to extract information for noun cases and verb persons and annotate the corresponding words accordingly. In experiments, we show improved performance for translating from English into Greek and Czech. For English–Greek, we reduce the error on the verb conjugation from 19% to 5.4% and noun case agreement from 9% to 6%.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P08-1100",
  "title": "Analyzing the Errors of Unsupervised Learning",
  "year": 2008,
  "venue": "ACL",
  "abstract": "We identify four types of errors that unsupervised induction systems make and study each one in turn. Our contributions include (1) using a meta-model to analyze the incorrect biases of a model in a systematic way, (2) providing an efficient and robust method of measuring distance between two parameter settings of a model, and (3) showing that local optima issues which typically plague EM can be somewhat alleviated by increasing the number of training examples. We conduct our analyses on three models: the HMM, the PCFG, and a simple dependency model.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/P08-1112",
  "title": "Better Alignments = Better Translations?",
  "year": 2008,
  "venue": "ACL",
  "abstract": "Automatic word alignment is a key step in training statistical machine translation systems. Despite much recent work on word alignment methods, alignment accuracy increases often produce little or no improvements in machine translation quality. In this work we analyze a recently proposed agreement-constrained EM algorithm for unsupervised alignment models. We attempt to tease apart the effects that this simple but effective modification has on alignment precision and recall trade-offs, and how rare and common words are affected across several language pairs. We propose and extensively evaluate a simple method for using alignment models to produce alignments better-suited for phrase-based MT systems, and show significant gains (as measured by BLEU score) in end-to-end translation systems for six languages pairs used in recent MT competitions.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P08-1118",
  "title": "Finding Contradictions in Text",
  "year": 2008,
  "venue": "ACL",
  "abstract": "Detecting conflicting statements is a foundational text understanding task with applications in information analysis. We propose an appropriate definition of contradiction for NLP tasks and develop available corpora, from which we construct a typology of contradictions. We demonstrate that a system for contradiction needs to make more fine-grained distinctions than the common systems for entailment. In particular, we argue for the centrality of event coreference and therefore incorporate such a component based on topicality. We present the first detailed breakdown of performance on this task. Detecting some types of contradiction requires deeper inferential paths than our system is capable of, but we achieve good performance on types arising from negation and antonymy.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/P08-2004",
  "title": "Dimensions of Subjectivity in Natural Language",
  "year": 2008,
  "venue": "ACL",
  "abstract": "Current research in automatic subjectivity analysis deals with various kinds of subjective statements involving human attitudes and emotions. While all of them are related to subjectivity, these statements usually touch on multiple dimensions such as non-objectivity1, uncertainty, vagueness, non-objective measurability, imprecision, and ambiguity, which are inherently different. This paper discusses the differences and relations of six dimensions of subjectivity. Conceptual and linguistic characteristics of each dimension will be demonstrated under different contexts.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P08-2020",
  "title": "Recent Improvements in the CMU Large Scale Chinese-English SMT System",
  "year": 2008,
  "venue": "ACL",
  "abstract": "In this paper we describe recent improvements to components and methods used in our statistical machine translation system for ChineseEnglish used in the January 2008 GALE evaluation. Main improvements are results of consistent data processing, larger statistical models and a POS-based word reordering approach.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P08-2029",
  "title": "Kernels on Linguistic Structures for Answer Extraction",
  "year": 2008,
  "venue": "ACL",
  "abstract": "Natural Language Processing (NLP) for Information Retrieval has always been an interesting and challenging research area. Despite the high expectations, most of the results indicate that successfully using NLP is very complex. In this paper, we show how Support Vector Machines along with kernel functions can effectively represent syntax and semantics. Our experiments on question/answer classification show that the above models highly improve on bag-of-words on a TREC dataset.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/P08-2045",
  "title": "Learning Semantic Links from a Corpus of Parallel Temporal and Causal Relations",
  "year": 2008,
  "venue": "ACL",
  "abstract": "Finding temporal and causal relations is crucial to understanding the semantic structure of a text. Since existing corpora provide no parallel temporal and causal annotations, we annotated 1000 conjoined event pairs, achieving inter-annotator agreement of 81.2% on temporal relations and 77.8% on causal relations. We trained machine learning models using features derived from WordNet and the Google N-gram corpus, and they outperformed a variety of baselines, achieving an F-measure of 49.0 for temporals and 52.4 for causals. Analysis of these models suggests that additional data will improve performance, and that temporal information is crucial to causal relation identification.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P08-2050",
  "title": "Intrinsic vs. Extrinsic Evaluation Measures for Referring Expression Generation",
  "year": 2008,
  "venue": "ACL",
  "abstract": "In this paper we present research in which we apply (i) the kind of intrinsic evaluation metrics that are characteristic of current comparative HLT evaluation, and (ii) extrinsic, human task-performance evaluations more in keeping with NLG traditions, to 15 systems implementing a language generation task. We analyse the evaluation results and find that there are no significant correlations between intrinsic and extrinsic evaluation measures for this task.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/P08-2051",
  "title": "Correlation between ROUGE and Human Evaluation of Extractive Meeting Summaries",
  "year": 2008,
  "venue": "ACL",
  "abstract": "Automatic summarization evaluation is critical to the development of summarization systems. While ROUGE has been shown to correlate well with human evaluation for content match in text summarization, there are many characteristics in multiparty meeting domain, which may pose potential problems to ROUGE. In this paper, we carefully examine how well the ROUGE scores correlate with human evaluation for extractive meeting summarization. Our experiments show that generally the correlation is rather low, but a significantly better correlation can be obtained by accounting for several unique meeting characteristics, such as disfluencies and speaker information, especially when evaluating system-generated summaries.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/P08-2053",
  "title": "Construct State Modification in the Arabic Treebank",
  "year": 2008,
  "venue": "ACL",
  "abstract": "Earlier work in parsing Arabic has speculated that attachment to construct state constructions decreases parsing performance. We make this speculation precise and define the problem of attachment to construct state constructions in the Arabic Treebank. We present the first statistics that quantify the problem. We provide a baseline and the results from a first attempt at a discriminative learning procedure for this task, achieving 80% accuracy.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/P09-1027",
  "title": "Co-Training for Cross-Lingual Sentiment Classification",
  "year": 2009,
  "venue": "ACL",
  "abstract": "The lack of Chinese sentiment corpora limits the research progress on Chinese sentiment classification. However, there are many freely available English sentiment corpora on the Web. This paper focuses on the problem of cross-lingual sentiment classification, which leverages an available English corpus for Chinese sentiment classification by using the English corpus as training data. Machine translation services are used for eliminating the language gap between the training set and test set, and English features and Chinese features are considered as two independent views of the classification problem. We propose a cotraining approach to making use of unlabeled Chinese data. Experimental results show the effectiveness of the proposed approach, which can outperform the standard inductive classifiers and the transductive classifiers.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P09-1035",
  "title": "The Contribution of Linguistic Features to Automatic Machine Translation Evaluation",
  "year": 2009,
  "venue": "ACL",
  "abstract": "A number of approaches to Automatic MT Evaluation based on deep linguistic knowledge have been suggested. However, n-gram based metrics are still today the dominant approach. The main reason is that the advantages of employing deeper linguistic information have not been clarified yet. In this work, we propose a novel approach for meta-evaluation of MT evaluation metrics, since correlation cofficient against human judges do not reveal details about the advantages and disadvantages of particular metrics. We then use this approach to investigate the benefits of introducing linguistic features into evaluation metrics. Overall, our experiments show that (i) both lexical and linguistic metrics present complementary advantages and (ii) combining both kinds of metrics yields the most robust metaevaluation performance.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/P09-1042",
  "title": "Dependency Grammar Induction via Bitext Projection Constraints",
  "year": 2009,
  "venue": "ACL",
  "abstract": "Broad-coverage annotated treebanks necessary to train parsers do not exist for many resource-poor languages. The wide availability of parallel text and accurate parsers in English has opened up the possibility of grammar induction through partial transfer across bitext. We consider generative and discriminative models for dependency grammar induction that use word-level alignments and a source language parser (English) to constrain the space of possible target trees. Unlike previous approaches, our framework does not require full projected parses, allowing partial, approximate transfer through linear expectation constraints on the space of distributions over trees. We consider several types of constraints that range from generic dependency conservation to language-specific annotation rules for auxiliary verb analysis. We evaluate our approach on Bulgarian and Spanish CoNLL shared task data and show that we consistently outperform unsupervised methods and can outperform supervised learning for limited training data.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/P09-1043",
  "title": "Cross-Domain Dependency Parsing Using a Deep Linguistic Grammar",
  "year": 2009,
  "venue": "ACL",
  "abstract": "Pure statistical parsing systems achieves high in-domain accuracy but performs poorly out-domain. In this paper, we propose two different approaches to produce syntactic dependency structures using a large-scale hand-crafted HPSG grammar. The dependency backbone of an HPSG analysis is used to provide general linguistic insights which, when combined with state-of-the-art statistical dependency parsing models, achieves performance improvements on out-domain tests.†",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/P09-1048",
  "title": "Who, What, When, Where, Why? Comparing Multiple Approaches to the Cross-Lingual 5W Task",
  "year": 2009,
  "venue": "ACL",
  "abstract": "Cross-lingual tasks are especially difficult due to the compounding effect of errors in language processing and errors in machine translation (MT). In this paper, we present an error analysis of a new cross-lingual task: the 5W task, a sentence-level understanding task which seeks to return the English 5W's (Who, What, When, Where and Why) corresponding to a Chinese sentence. We analyze systems that we developed, identifying specific problems in language processing and MT that cause errors. The best cross-lingual 5W system was still 19% worse than the best monolingual 5W system, which shows that MT significantly degrades sentence-level understanding. Neither source-language nor targetlanguage analysis was able to circumvent problems in MT, although each approach had advantages relative to the other. A detailed error analysis across multiple systems suggests directions for future research on the problem.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P09-1059",
  "title": "Automatic Adaptation of Annotation Standards: Chinese Word Segmentation and POS Tagging - A Case Study",
  "year": 2009,
  "venue": "ACL",
  "abstract": "Manually annotated corpora are valuable but scarce resources, yet for many annotation tasks such as treebanking and sequence labeling there exist multiple corpora with different and incompatible annotation guidelines or standards. This seems to be a great waste of human efforts, and it would be nice to automatically adapt one annotation standard to another. We present a simple yet effective strategy that transfers knowledge from a differently annotated corpus to the corpus with desired annotation. We test the efficacy of this method in the context of Chinese word segmentation and part-of-speech tagging, where no segmentation and POS tagging standards are widely accepted due to the lack of morphology in Chinese. Experiments show that adaptation from the much larger People’s Daily corpus to the smaller but more popular Penn Chinese Treebank results in significant improvements in both segmentation and tagging accuracies (with error reductions of 30.2% and 14%, respectively), which in turn helps improve Chinese parsing accuracy.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/P09-1063",
  "title": "Improving Tree-to-Tree Translation with Packed Forests",
  "year": 2009,
  "venue": "ACL",
  "abstract": "Current tree-to-tree models suffer from parsing errors as they usually use only 1best parses for rule extraction and decoding. We instead propose a forest-based tree-to-tree model that uses packed forests. The model is based on a probabilistic synchronous tree substitution grammar (STSG), which can be learned from aligned forest pairs automatically. The decoder finds ways of decomposing trees in the source forest into elementary trees using the source projection of STSG while building target forest in parallel. Comparable to the state-of-the-art phrase-based system Moses, using packed forests in tree-to-tree translation results in a significant absolute improvement of 3.6 BLEU points over using 1-best trees.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/P09-2018",
  "title": "Directional Distributional Similarity for Lexical Expansion",
  "year": 2009,
  "venue": "ACL",
  "abstract": "Distributional word similarity is most commonly perceived as a symmetric relation. Yet, one of its major applications is lexical expansion, which is generally asymmetric. This paper investigates the nature of directional (asymmetric) similarity measures, which aim to quantify distributional feature inclusion. We identify desired properties of such measures, specify a particular one based on averaged precision, and demonstrate the empirical benefit of directional measures for expansion.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/P09-2025",
  "title": "Correlating Human and Automatic Evaluation of a German Surface Realiser",
  "year": 2009,
  "venue": "ACL",
  "abstract": "We examine correlations between native speaker judgements on automatically generated German text against automatic evaluation metrics. We look at a number of metrics from the MT and Summarisation communities and find that for a relative ranking task, most automatic metrics perform equally well and have fairly strong correlations to the human judgements. In contrast, on a naturalness judgement task, the General Text Matcher (GTM) tool correlates best overall, although in general, correlation between the human judgements and the automatic metrics was quite weak.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P09-2027",
  "title": "Query-Focused Summaries or Query-Biased Summaries?",
  "year": 2009,
  "venue": "ACL",
  "abstract": "In the context of the Document Understanding Conferences, the task of Query-Focused Multi-Document Summarization is intended to improve agreement in content among humangenerated model summaries. Query-focus also aids the automated summarizers in directing the summary at specific topics, which may result in better agreement with these model summaries. However, while query focus correlates with performance, we show that highperforming automatic systems produce summaries with disproportionally higher query term density than human summarizers do. Experimental evidence suggests that automatic systems heavily rely on query term occurrence and repetition to achieve good performance.",
  "stance": -0.4
 },
 {
  "url": "https://aclanthology.org/P09-2066",
  "title": "From Extractive to Abstractive Meeting Summaries: Can It Be Done by Sentence Compression?",
  "year": 2009,
  "venue": "ACL",
  "abstract": "Most previous studies on meeting summarization have focused on extractive summarization. In this paper, we investigate if we can apply sentence compression to extractive summaries to generate abstractive summaries. We use different compression algorithms, including integer linear programming with an additional step of filler phrase detection, a noisychannel approach using Markovization formulation of grammar rules, as well as human compressed sentences. Our experiments on the ICSI meeting corpus show that when compared to the abstractive summaries, using sentence compression on the extractive summaries improves their ROUGE scores; however, the best performance is still quite low, suggesting the need of language generation for abstractive summarization.",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/P10-1035",
  "title": "Accurate Context-Free Parsing with Combinatory Categorial Grammar",
  "year": 2010,
  "venue": "ACL",
  "abstract": "The definition of combinatory categorial grammar (CCG) in the literature varies quite a bit from author to author. However, the differences between the definitions are important in terms of the language classes of each CCG. We prove that a wide range of CCGs are strongly context-free, including the CCG of CCGbank and of the parser of Clark and Curran (2007). In light of these new results, we train the PCFG parser of Petrov and Klein (2007) on CCGbank and achieve state of the art results in supertagging accuracy, PARSEVAL measures and dependency accuracy.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/P10-1053",
  "title": "On the Computational Complexity of Dominance Links in Grammatical Formalisms",
  "year": 2010,
  "venue": "ACL",
  "abstract": "Dominance links were introduced in grammars to model long distance scrambling phenomena, motivating the definition of multiset-valued linear indexed grammars (MLIGs) by Rambow (1994b), and inspiring quite a few recent formalisms. It turns out that MLIGs have since been rediscovered and reused in a variety of contexts, and that the complexity of their emptiness problem has become the key to several open questions in computer science. We survey complexity results and open issues on MLIGs and related formalisms, and provide new complexity bounds for some linguistically motivated restrictions.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/P10-1111",
  "title": "Hard Constraints for Grammatical Function Labelling",
  "year": 2010,
  "venue": "ACL",
  "abstract": "For languages with (semi-) free word order (such as German), labelling grammatical functions on top of phrase-structural constituent analyses is crucial for making them interpretable. Unfortunately, most statistical classifiers consider only local information for function labelling and fail to capture important restrictions on the distribution of core argument functions such as subject, object etc., namely that there is at most one subject (etc.) per clause. We augment a statistical classifier with an integer linear program imposing hard linguistic constraints on the solution space output by the classifier, capturing global distributional restrictions. We show that this improves labelling quality, in particular for argument grammatical functions, in an intrinsic evaluation, and, importantly, grammar coverage for treebankbased (Lexical-Functional) grammar acquisition and parsing, in an extrinsic evaluation.",
  "stance": 0.1
 },
 {
  "url": "https://aclanthology.org/P10-1127",
  "title": "Generating Image Descriptions Using Dependency Relational Patterns",
  "year": 2010,
  "venue": "ACL",
  "abstract": "This paper presents a novel approach to automatic captioning of geo-tagged images by summarizing multiple webdocuments that contain information related to an image’s location. The summarizer is biased by dependency pattern models towards sentences which contain features typically provided for different scene types such as those of churches, bridges, etc. Our results show that summaries biased by dependency pattern models lead to significantly higher ROUGE scores than both n-gram language models reported in previous work and also Wikipedia baseline summaries. Summaries generated using dependency patterns also lead to more readable summaries than those generated without dependency patterns.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P10-1135",
  "title": "On Learning Subtypes of the Part-Whole Relation: Do Not Mix Your Seeds",
  "year": 2010,
  "venue": "ACL",
  "abstract": "An important relation in information extraction is the part-whole relation. Ontological studies mention several types of this relation. In this paper, we show that the traditional practice of initializing minimally-supervised algorithms with a single set that mixes seeds of different types fails to capture the wide variety of part-whole patterns and tuples. The results obtained with mixed seeds ultimately converge to one of the part-whole relation types. We also demonstrate that all the different types of part-whole relations can still be discovered, regardless of the type characterized by the initializing seeds. We performed our experiments with a state-ofthe-art information extraction algorithm.",
  "stance": -0.9
 },
 {
  "url": "https://aclanthology.org/P10-1144",
  "title": "Coreference Resolution across Corpora: Languages, Coding Schemes, and Preprocessing Information",
  "year": 2010,
  "venue": "ACL",
  "abstract": "This paper explores the effect that different corpus configurations have on the performance of a coreference resolution system, as measured by MUC, B3, and CEAF. By varying separately three parameters (language, annotation scheme, and preprocessing information) and applying the same coreference resolution system, the strong bonds between system and corpus are demonstrated. The experiments reveal problems in coreference resolution evaluation relating to task definition, coding schemes, and features. They also expose systematic biases in the coreference evaluation metrics. We show that system comparison is only possible when corpus parameters are in exact agreement.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/P10-1153",
  "title": "A Generalized-Zero-Preserving Method for Compact Encoding of Concept Lattices",
  "year": 2010,
  "venue": "ACL",
  "abstract": "Constructing an encoding of a concept lattice using short bit vectors allows for efficient computation of join operations on the lattice. Join is the central operation any unification-based parser must support. We extend the traditional bit vector encoding, which represents join failure using the zero vector, to count any vector with less than a fixed number of one bits as failure. This allows non-joinable elements to share bits, resulting in a smaller vector size. A constraint solver is used to construct the encoding, and a variety of techniques are employed to find near-optimal solutions and handle timeouts. An evaluation is provided comparing the extended representation of failure with traditional bit vector",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P10-2012",
  "title": "Cognitively Plausible Models of Human Language Processing",
  "year": 2010,
  "venue": "ACL",
  "abstract": "We pose the development of cognitively plausible models of human language processing as a challenge for computational linguistics. Existing models can only deal with isolated phenomena (e.g., garden paths) on small, specifically selected data sets. The challenge is to build models that integrate multiple aspects of human language processing at the syntactic, semantic, and discourse level. Like human language processing, these models should be incremental, predictive, broad coverage, and robust to noise. This challenge can only be met if standardized data sets and evaluation measures are developed.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/P10-2016",
  "title": "Tackling Sparse Data Issue in Machine Translation Evaluation",
  "year": 2010,
  "venue": "ACL",
  "abstract": "We illustrate and explain problems of n-grams-based machine translation (MT) metrics (e.g. BLEU) when applied to morphologically rich languages such as Czech. A novel metric SemPOS based on the deep-syntactic representation of the sentence tackles the issue and retains the performance for translation to English as well.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/P10-2017",
  "title": "Exemplar-Based Models for Word Meaning in Context",
  "year": 2010,
  "venue": "ACL",
  "abstract": "This paper describes ongoing work on distributional models for word meaning in context. We abandon the usual one-vectorper-word paradigm in favor of an exemplar model that activates only relevant occurrences. On a paraphrasing task, we find that a simple exemplar model outperforms more complex state-of-the-art models.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P10-2024",
  "title": "Complexity Assumptions in Ontology Verbalisation",
  "year": 2010,
  "venue": "ACL",
  "abstract": "We describe the strategy currently pursued for verbalising OWL ontologies by sentences in Controlled Natural Language (i.e., combining generic rules for realising logical patterns with ontology-specific lexicons for realising atomic terms for individuals, classes, and properties) and argue that its success depends on assumptions about the complexity of terms and axioms in the ontology. We then show, through analysis of a corpus of ontologies, that although these assumptions could in principle be violated, they are overwhelmingly respected in practice by ontology develop-",
  "stance": -0.1
 },
 {
  "url": "https://aclanthology.org/P10-2029",
  "title": "Coreference Resolution with Reconcile",
  "year": 2010,
  "venue": "ACL",
  "abstract": "Despite the existence of several noun phrase coreference resolution data sets as well as several formal evaluations on the task, it remains frustratingly difficult to compare results across different coreference resolution systems. This is due to the high cost of implementing a complete end-to-end coreference resolution system, which often forces researchers to substitute available gold-standard information in lieu of implementing a module that would compute that information. Unfortunately, this leads to inconsistent and often unrealistic evaluation scenarios. With the aim to facilitate consistent and realistic experimental evaluations in coreference resolution, we present Reconcile, an infrastructure for the development of learning-based noun phrase (NP) coreference resolution systems. Reconcile is designed to facilitate the rapid creation of coreference resolution systems, easy implementation of new feature sets and approaches to coreference resolution, and empirical evaluation of coreference resolvers across a variety of benchmark data sets and standard scoring metrics. We describe Reconcile and present experimental results showing that Reconcile can be used to create a coreference resolver that achieves performance comparable to state-ofthe-art systems on six benchmark data sets.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/P10-2046",
  "title": "Don't 'Have a Clue'? Unsupervised Co-Learning of Downward-Entailing Operators.",
  "year": 2010,
  "venue": "ACL",
  "abstract": "Researchers in textual entailment have begun to consider inferences involving downward-entailing operators, an interesting and important class of lexical items that change the way inferences are made. Recent work proposed a method for learning English downward-entailing operators that requires access to a high-quality collection of negative polarity items (NPIs). However, English is one of the very few languages for which such a list exists. We propose the first approach that can be applied to the many languages for which there is no pre-existing high-precision database of NPIs. As a case study, we apply our method to Romanian and show that our method yields good results. Also, we perform a cross-linguistic analysis that suggests interesting connections to some findings in linguistic typology.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/P10-2058",
  "title": "Using Speech to Reply to SMS Messages While Driving: An In-Car Simulator User Study",
  "year": 2010,
  "venue": "ACL",
  "abstract": "Speech recognition affords automobile drivers a hands-free, eyes-free method of replying to Short Message Service (SMS) text messages. Although a voice search approach based on template matching has been shown to be more robust to the challenging acoustic environment of automobiles than using dictation, users may have difficulties verifying whether SMS response templates match their intended meaning, especially while driving. Using a high-fidelity driving simulator, we compared dictation for SMS replies versus voice search in increasingly difficult driving conditions. Although the two approaches did not differ in terms of driving performance measures, users made about six times more errors on average using dictation than voice search.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P11-2075",
  "title": "Is Machine Translation Ripe for Cross-Lingual Sentiment Classification?",
  "year": 2011,
  "venue": "ACL",
  "abstract": "Recent advances in Machine Translation (MT) have brought forth a new paradigm for building NLP applications in low-resource scenarios. To build a sentiment classifier for a language with no labeled resources, one can translate labeled data from another language, then train a classifier on the translated text. This can be viewed as a domain adaptation problem, where labeled translations and test data have some mismatch. Various prior work have achieved positive results using this approach. In this opinion piece, we take a step back and make some general statements about crosslingual adaptation problems. First, we claim that domain mismatch is not caused by MT errors, and accuracy degradation will occur even in the case of perfect MT. Second, we argue that the cross-lingual adaptation problem is qualitatively different from other (monolingual) adaptation problems in NLP; thus new adaptation algorithms ought to be considered. This paper will describe a series of carefullydesigned experiments that led us to these conclusions.",
  "stance": -0.4
 },
 {
  "url": "https://aclanthology.org/P11-2107",
  "title": "Predicting Relative Prominence in Noun-Noun Compounds",
  "year": 2011,
  "venue": "ACL",
  "abstract": "There are several theories regarding what influences prominence assignment in English noun-noun compounds. We have developed corpus-driven models for automatically predicting prominence assignment in noun-noun compounds using feature sets based on two such theories: the informativeness theory and the semantic composition theory. The evaluation of the prediction models indicate that though both of these theories are relevant, they account for different types of variability in prominence assignment.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P12-1024",
  "title": "Spectral Learning of Latent-Variable PCFGs",
  "year": 2012,
  "venue": "ACL",
  "abstract": "Jeju, Republic of Korea, 8-14 July 2012. c ©2012 Association for Computational Linguistics Spectral Learning of Latent-Variable PCFGs Shay B. Cohen, Karl Stratos, Michael Collins, Dean P. Foster, and Lyle Ungar Dept. of Computer Science, Columbia University Dept. of Statistics/Dept. of Computer and Information Science, University of Pennsylvania {scohen,stratos,mcollins}@cs.columbia.edu, foster@wharton.upenn.edu, ungar@cis.upenn.edu Abstract",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P12-1070",
  "title": "MIX Is Not a Tree-Adjoining Language",
  "year": 2012,
  "venue": "ACL",
  "abstract": "The language MIX consists of all strings over the three-letter alphabet {a, b, c} that contain an equal number of occurrences of each letter. We prove Joshi’s (1985) conjecture that MIX is not a tree-adjoining language.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P12-1110",
  "title": "Incremental Joint Approach to Word Segmentation, POS Tagging, and Dependency Parsing in Chinese",
  "year": 2012,
  "venue": "ACL",
  "abstract": "We propose the first joint model for word segmentation, POS tagging, and dependency parsing for Chinese. Based on an extension of the incremental joint model for POS tagging and dependency parsing (Hatori et al., 2011), we propose an efficient character-based decoding method that can combine features from state-of-the-art segmentation, POS tagging, and dependency parsing models. We also describe our method to align comparable states in the beam, and how we can combine features of different characteristics in our incremental framework. In experiments using the Chinese Treebank (CTB), we show that the accuracies of the three tasks can be improved significantly over the baseline models, particularly by 0.6% for POS tagging and 2.4% for dependency parsing. We also perform comparison experiments with the partially joint models.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P12-2070",
  "title": "Assessing the Effect of Inconsistent Assessors on Summarization Evaluation",
  "year": 2012,
  "venue": "ACL",
  "abstract": "We investigate the consistency of human assessors involved in summarization evaluation to understand its effect on system ranking and automatic evaluation techniques. Using Text Analysis Conference data, we measure annotator consistency based on human scoring of summaries for Responsiveness, Readability, and Pyramid scoring. We identify inconsistencies in the data and measure to what extent these inconsistencies affect the ranking of automatic summarization systems. Finally, we examine the stability of automatic metrics (ROUGE and CLASSY) with respect to the inconsistent assessments.",
  "stance": -0.6
 },
 {
  "url": "https://aclanthology.org/P13-1036",
  "title": "Scalable Decipherment for Machine Translation via Hash Sampling",
  "year": 2013,
  "venue": "ACL",
  "abstract": "In this paper, we propose a new Bayesian inference method to train statistical machine translation systems using only nonparallel corpora. Following a probabilistic decipherment approach, we first introduce a new framework for decipherment training that is flexible enough to incorporate any number/type of features (besides simple bag-of-words) as side-information used for estimating translation models. In order to perform fast, efficient Bayesian inference in this framework, we then derive a hash sampling strategy that is inspired by the work of Ahmed et al. (2012). The new translation hash sampler enables us to scale elegantly to complex models (for the first time) and large vocabulary/corpora sizes. We show empirical results on the OPUS data—our method yields the best BLEU scores compared to existing approaches, while achieving significant computational speedups (several orders faster). We also report for the first time—BLEU score results for a largescale MT task using only non-parallel data (EMEA corpus).",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P13-1059",
  "title": "Name-aware Machine Translation",
  "year": 2013,
  "venue": "ACL",
  "abstract": "We propose a Name-aware Machine Translation (MT) approach which can tightly integrate name processing into MT model, by jointly annotating parallel corpora, extracting name-aware translation grammar and rules, adding name phrase table and name translation driven decoding. Additionally, we also propose a new MT metric to appropriately evaluate the translation quality of informative words, by assigning different weights to different words according to their importance values in a document. Experiments on Chinese-English translation demonstrated the effectiveness of our approach on enhancing the quality of overall translation, name translation and word alignment over a high-quality MT baseline1.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P13-1071",
  "title": "The Impact of Topic Bias on Quality Flaw Prediction in Wikipedia",
  "year": 2013,
  "venue": "ACL",
  "abstract": "With the increasing amount of user generated reference texts in the web, automatic quality assessment has become a key challenge. However, only a small amount of annotated data is available for training quality assessment systems. Wikipedia contains a large amount of texts annotated with cleanup templates which identify quality flaws. We show that the distribution of these labels is topically biased, since they cannot be applied freely to any arbitrary article. We argue that it is necessary to consider the topical restrictions of each label in order to avoid a sampling bias that results in a skewed classifier and overly optimistic evaluation results. We factor out the topic bias by extracting reliable training instances from the revision history which have a topic distribution similar to the labeled articles. This approach better reflects the situation a classifier would face in a real-life application.",
  "stance": -0.3
 },
 {
  "url": "https://aclanthology.org/P13-1118",
  "title": "DErivBase: Inducing and Evaluating a Derivational Morphology Resource for German",
  "year": 2013,
  "venue": "ACL",
  "abstract": "Derivational models are still an underresearched area in computational morphology. Even for German, a rather resourcerich language, there is a lack of largecoverage derivational knowledge. This paper describes a rule-based framework for inducing derivational families (i.e., clusters of lemmas in derivational relationships) and its application to create a highcoverage German resource, DERIVBASE, mapping over 280k lemmas into more than 17k non-singleton clusters. We focus on the rule component and a qualitative and quantitative evaluation. Our approach achieves up to 93% precision and 71% recall. We attribute the high precision to the fact that our rules are based on information from grammar books.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/P13-1121",
  "title": "Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain",
  "year": 2013,
  "venue": "ACL",
  "abstract": "In automatic summarization, centrality is the notion that a summary should contain the core parts of the source text. Current systems use centrality, along with redundancy avoidance and some sentence compression, to produce mostly extractive summaries. In this paper, we investigate how summarization can advance past this paradigm towards robust abstraction by making greater use of the domain of the source text. We conduct a series of studies comparing human-written model summaries to system summaries at the semantic level of caseframes. We show that model summaries (1) are more abstractive and make use of more sentence aggregation, (2) do not contain as many topical caseframes as system summaries, and (3) cannot be reconstructed solely from the source text, but can be if texts from in-domain documents are added. These results suggest that substantial improvements are unlikely to result from better optimizing centrality-based criteria, but rather more domain knowledge is needed.",
  "stance": -0.4
 },
 {
  "url": "https://aclanthology.org/P13-1139",
  "title": "Models of Translation Competitions",
  "year": 2013,
  "venue": "ACL",
  "abstract": "What do we want to learn from a translation competition and how do we learn it with confidence? We argue that a disproportionate focus on ranking competition participants has led to lots of different rankings, but little insight about which rankings we should trust. In response, we provide the first framework that allows an empirical comparison of different analyses of competition results. We then use this framework to compare several analytical models on data from the Workshop on Machine Translation (WMT).",
  "stance": 0.4
 },
 {
  "url": "https://aclanthology.org/P13-1166",
  "title": "Offspring from Reproduction Problems: What Replication Failure Teaches Us",
  "year": 2013,
  "venue": "ACL",
  "abstract": "Repeating experiments is an important instrument in the scientific toolbox to validate previous work and build upon existing work. We present two concrete use cases involving key techniques in the NLP domain for which we show that reproducing results is still difficult. We show that the deviation that can be found in reproduction efforts leads to questions about how our results should be interpreted. Moreover, investigating these deviations provides new insights and a deeper understanding of the examined techniques. We identify five aspects that can influence the outcomes of experiments that are typically not addressed in research papers. Our use cases show that these aspects may change the answer to research questions leading us to conclude that more care should be taken in interpreting our results and more research involving systematic testing of methods is required in our field.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P13-1167",
  "title": "Evaluating Text Segmentation using Boundary Edit Distance",
  "year": 2013,
  "venue": "ACL",
  "abstract": "This work proposes a new segmentation evaluation metric, named boundary similarity (B), an inter-coder agreement coefficient adaptation, and a confusion-matrix for segmentation that are all based upon an adaptation of the boundary edit distance in Fournier and Inkpen (2012). Existing segmentation metrics such as Pk, WindowDiff, and Segmentation Similarity (S) are all able to award partial credit for near misses between boundaries, but are biased towards segmentations containing few or tightly clustered boundaries. Despite S’s improvements, its normalization also produces cosmetically high values that overestimate agreement & performance, leading this work to propose a solution.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/P13-1168",
  "title": "Crowd Prefers the Middle Path: A New IAA Metric for Crowdsourcing Reveals Turker Biases in Query Segmentation",
  "year": 2013,
  "venue": "ACL",
  "abstract": "Query segmentation, like text chunking, is the first step towards query understanding. In this study, we explore the effectiveness of crowdsourcing for this task. Through carefully designed control experiments and Inter Annotator Agreement metrics for analysis of experimental data, we show that crowdsourcing may not be a suitable approach for query segmentation because the crowd seems to have a very strong bias towards dividing the query into roughly equal (often only two) parts. Similarly, in the case of hierarchical or nested segmentation, turkers have a strong preference towards balanced binary trees.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/P13-2068",
  "title": "Bilingual Lexical Cohesion Trigger Model for Document-Level Machine Translation",
  "year": 2013,
  "venue": "ACL",
  "abstract": "In this paper, we propose a bilingual lexical cohesion trigger model to capture lexical cohesion for document-level machine translation. We integrate the model into hierarchical phrase-based machine translation and achieve an absolute improvement of 0.85 BLEU points on average over the baseline on NIST Chinese-English test sets.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P13-2151",
  "title": "Does Korean defeat phonotactic word segmentation?",
  "year": 2013,
  "venue": "ACL",
  "abstract": "Computational models of infant word segmentation have not been tested on a wide range of languages. This paper applies a phonotactic segmentation model to Korean. In contrast to the undersegmentation pattern previously found in English and Russian, the model exhibited more oversegmentation errors and more errors overall. Despite the high error rate, analysis suggested that lexical acquisition might not be problematic, provided that infants attend only to frequently segmented items.",
  "stance": -0.4
 },
 {
  "url": "https://aclanthology.org/P14-1008",
  "title": "Logical Inference on Dependency-based Compositional Semantics",
  "year": 2014,
  "venue": "ACL",
  "abstract": "Dependency-based Compositional Semantics (DCS) is a framework of natural language semantics with easy-to-process structures as well as strict semantics. In this paper, we equip the DCS framework with logical inference, by defining abstract denotations as an abstraction of the computing process of denotations in original DCS. An inference engine is built to achieve inference on abstract denotations. Furthermore, we propose a way to generate on-the-fly knowledge in logical inference, by combining our framework with the idea of tree transformation. Experiments on FraCaS and PASCAL RTE datasets show promising results.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/P14-1039",
  "title": "That's Not What I Meant! Using Parsers to Avoid Structural Ambiguities in Generated Text",
  "year": 2014,
  "venue": "ACL",
  "abstract": "We investigate whether parsers can be used for self-monitoring in surface realization in order to avoid egregious errors involving “vicious” ambiguities, namely those where the intended interpretation fails to be considerably more likely than alternative ones. Using parse accuracy in a simple reranking strategy for selfmonitoring, we find that with a stateof-the-art averaged perceptron realization ranking model, BLEU scores cannot be improved with any of the well-known Treebank parsers we tested, since these parsers too often make errors that human readers would be unlikely to make. However, by using an SVM ranker to combine the realizer’s model score together with features from multiple parsers, including ones designed to make the ranker more robust to parsing mistakes, we show that significant increases in BLEU scores can be achieved. Moreover, via a targeted manual analysis, we demonstrate that the SVM reranker frequently manages to avoid vicious ambiguities, while its ranking errors tend to affect fluency much more often than adequacy.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/P14-1047",
  "title": "Single-Agent vs. Multi-Agent Techniques for Concurrent Reinforcement Learning of Negotiation Dialogue Policies",
  "year": 2014,
  "venue": "ACL",
  "abstract": "We use single-agent and multi-agent Reinforcement Learning (RL) for learning dialogue policies in a resource allocation negotiation scenario. Two agents learn concurrently by interacting with each other without any need for simulated users (SUs) to train against or corpora to learn from. In particular, we compare the Qlearning, Policy Hill-Climbing (PHC) and Win or Learn Fast Policy Hill-Climbing (PHC-WoLF) algorithms, varying the scenario complexity (state space size), the number of training episodes, the learning rate, and the exploration rate. Our results show that generally Q-learning fails to converge whereas PHC and PHC-WoLF always converge and perform similarly. We also show that very high gradually decreasing exploration rates are required for convergence. We conclude that multiagent RL of dialogue policies is a promising alternative to using single-agent RL and SUs or learning directly from corpora.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P14-1049",
  "title": "Negation Focus Identification with Contextual Discourse Information",
  "year": 2014,
  "venue": "ACL",
  "abstract": "Negative expressions are common in natural language text and play a critical role in information extraction. However, the performances of current systems are far from satisfaction, largely due to its focus on intrasentence information and its failure to consider inter-sentence information. In this paper, we propose a graph model to enrich intrasentence features with inter-sentence features from both lexical and topic perspectives. Evaluation on the *SEM 2012 shared task corpus indicates the usefulness of contextual discourse information in negation focus identification and justifies the effectiveness of our graph model in capturing such global information. *",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P14-1119",
  "title": "Automatic Keyphrase Extraction: A Survey of the State of the Art",
  "year": 2014,
  "venue": "ACL",
  "abstract": "While automatic keyphrase extraction has been examined extensively, state-of-theart performance on this task is still much lower than that on many core natural language processing tasks. We present a survey of the state of the art in automatic keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead.",
  "stance": -0.6
 },
 {
  "url": "https://aclanthology.org/P14-1140",
  "title": "A Recursive Recurrent Neural Network for Statistical Machine Translation",
  "year": 2014,
  "venue": "ACL",
  "abstract": "In this paper, we propose a novel recursive recurrent neural network (R2NN) to model the end-to-end decoding process for statistical machine translation. R2NN is a combination of recursive neural network and recurrent neural network, and in turn integrates their respective capabilities: (1) new information can be used to generate the next hidden state, like recurrent neural networks, so that language model and translation model can be integrated naturally; (2) a tree structure can be built, as recursive neural networks, so as to generate the translation candidates in a bottom up manner. A semi-supervised training approach is proposed to train the parameters, and the phrase pair embedding is explored to model translation confidence directly. Experiments on a Chinese to English translation task show that our proposed R2NN can outperform the stateof-the-art baseline by about 1.5 points in BLEU.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P14-2045",
  "title": "Improving the Recognizability of Syntactic Relations Using Contextualized Examples",
  "year": 2014,
  "venue": "ACL",
  "abstract": "A common task in qualitative data analysis is to characterize the usage of a linguistic entity by issuing queries over syntactic relations between words. Previous interfaces for searching over syntactic structures require programming-style queries. User interface research suggests that it is easier to recognize a pattern than to compose it from scratch; therefore, interfaces for non-experts should show previews of syntactic relations. What these previews should look like is an open question that we explored with a 400-participant Mechanical Turk experiment. We found that syntactic relations are recognized with 34% higher accuracy when contextual examples are shown than a baseline of naming the relations alone. This suggests that user interfaces should display contextual examples of syntactic relations to help users choose between different relations.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/P14-2065",
  "title": "The VerbCorner Project: Findings from Phase 1 of crowd-sourcing a semantic decomposition of verbs",
  "year": 2014,
  "venue": "ACL",
  "abstract": "Any given verb can appear in some syntactic frames (Sally broke the vase, The vase broke) but not others (*Sally broke at the vase, *Sally broke the vase to John). There is now considerable evidence that the syntactic behaviors of some verbs can be predicted by their meanings, and many current theories posit that this is true for most if not all verbs. If true, this fact would have striking implications for theories and models of language acquisition, as well as numerous applications in natural language processing. However, empirical investigations to date have focused on a small number of verbs. We report on early results from VerbCorner, a crowd-sourced project extending this work to a large, representative sample of English verbs.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/P14-2068",
  "title": "Modeling Factuality Judgments in Social Media Text",
  "year": 2014,
  "venue": "ACL",
  "abstract": "How do journalists mark quoted content as certain or uncertain, and how do readers interpret these signals? Predicates such as thinks, claims, and admits offer a range of options for framing quoted content according to the author’s own perceptions of its credibility. We gather a new dataset of direct and indirect quotes from Twitter, and obtain annotations of the perceived certainty of the quoted statements. We then compare the ability of linguistic and extra-linguistic features to predict readers’ assessment of the certainty of quoted content. We see that readers are indeed influenced by such framing devices — and we find no evidence that they consider other factors, such as the source, journalist, or the content itself. In addition, we examine the impact of specific framing devices on perceptions of credibility.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/P14-2083",
  "title": "Linguistically debatable or just plain wrong?",
  "year": 2014,
  "venue": "ACL",
  "abstract": "In linguistic annotation projects, we typically develop annotation guidelines to minimize disagreement. However, in this position paper we question whether we should actually limit the disagreements between annotators, rather than embracing them. We present an empirical analysis of part-of-speech annotated data sets that suggests that disagreements are systematic across domains and to a certain extend also across languages. This points to an underlying ambiguity rather than random errors. Moreover, a quantitative analysis of tag confusions reveals that the majority of disagreements are due to linguistically debatable cases rather than annotation errors. Specifically, we show that even in the absence of annotation guidelines only 2% of annotator choices are linguistically unmotivated.",
  "stance": -0.6
 },
 {
  "url": "https://aclanthology.org/P14-2109",
  "title": "Parser Evaluation Using Derivation Trees: A Complement to evalb",
  "year": 2014,
  "venue": "ACL",
  "abstract": "This paper introduces a new technique for phrase-structure parser analysis, categorizing possible treebank structures by integrating regular expressions into derivation trees. We analyze the performance of the Berkeley parser on OntoNotes WSJ and the English Web Treebank. This provides some insight into the evalb scores, and the problem of domain adaptation with the web data. We also analyze a “test-ontrain” dataset, showing a wide variance in how the parser is generalizing from different structures in the training material.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/P14-2133",
  "title": "How much do word embeddings encode about syntax?",
  "year": 2014,
  "venue": "ACL",
  "abstract": "Do continuous word embeddings encode any useful information for constituency parsing? We isolate three ways in which word embeddings might augment a stateof-the-art statistical parser: by connecting out-of-vocabulary words to known ones, by encouraging common behavior among related in-vocabulary words, and by directly providing features for the lexicon. We test each of these hypotheses with a targeted change to a state-of-the-art baseline. Despite small gains on extremely small supervised training sets, we find that extra information from embeddings appears to make little or no difference to a parser with adequate training data. Our results support an overall hypothesis that word embeddings import syntactic information that is ultimately redundant with distinctions learned from treebanks in other ways.",
  "stance": -0.3
 },
 {
  "url": "https://aclanthology.org/P14-2138",
  "title": "Does the Phonology of L1 Show Up in L2 Texts?",
  "year": 2014,
  "venue": "ACL",
  "abstract": "The relative frequencies of character bigrams appear to contain much information for predicting the first language (L1) of the writer of a text in another language (L2). Tsur and Rappoport (2007) interpret this fact as evidence that word choice is dictated by the phonology of L1. In order to test their hypothesis, we design an algorithm to identify the most discriminative words and the corresponding character bigrams, and perform two experiments to quantify their impact on the L1 identification task. The results strongly suggest an alternative explanation of the effectiveness of character bigrams in identifying the native language of a writer.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/P15-1011",
  "title": "Revisiting Word Embedding for Contrasting Meaning",
  "year": 2015,
  "venue": "ACL",
  "abstract": "Contrasting meaning is a basic aspect of semantics. Recent word-embedding models based on distributional semantics hypothesis are known to be weak for modeling lexical contrast. We present in this paper the embedding models that achieve an F-score of 92% on the widely-used, publicly available dataset, the GRE “most contrasting word” questions (Mohammad et al., 2008). This is the highest performance seen so far on this dataset. Surprisingly at the first glance, unlike what was suggested in most previous work, where relatedness statistics learned from corpora is claimed to yield extra gains over lexicon-based models, we obtained our best result relying solely on lexical resources (Roget’s and WordNet)—corpora statistics did not lead to further improvement. However, this should not be simply taken as that distributional statistics is not useful. We examine several basic concerns in modeling contrasting meaning to provide detailed analysis, with the aim to shed some light on the future directions for this basic semantics modeling problem.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P15-1024",
  "title": "Learning Answer-Entailing Structures for Machine Comprehension",
  "year": 2015,
  "venue": "ACL",
  "abstract": "Understanding open-domain text is one of the primary challenges in NLP. Machine comprehension evaluates the system’s ability to understand text through a series of question-answering tasks on short pieces of text such that the correct answer can be found only in the given text. For this task, we posit that there is a hidden (latent) structure that explains the relation between the question, correct answer, and text. We call this the answer-entailing structure; given the structure, the correctness of the answer is evident. Since the structure is latent, it must be inferred. We present a unified max-margin framework that learns to find these hidden structures (given a corpus of question-answer pairs), and uses what it learns to answer machine comprehension questions on novel texts. We extend this framework to incorporate multi-task learning on the different subtasks that are required to perform machine comprehension. Evaluation on a publicly available dataset shows that our framework outperforms various IR and neuralnetwork baselines, achieving an overall accuracy of 67.8% (vs. 59.9%, the best previously-published result.)",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P15-1074",
  "title": "Vector-space calculation of semantic surprisal for predicting word pronunciation duration",
  "year": 2015,
  "venue": "ACL",
  "abstract": "In order to build psycholinguistic models of processing difficulty and evaluate these models against human data, we need highly accurate language models. Here we specifically consider surprisal, a word’s predictability in context. Existing approaches have mostly used n-gram models or more sophisticated syntax-based parsing models; this largely does not account for effects specific to semantics. We build on the work by Mitchell et al. (2010) and show that the semantic prediction model suggested there can successfully predict spoken word durations in naturalistic conversational data. An interesting finding is that the training data for the semantic model also plays a strong role: the model trained on indomain data, even though a better language model for our data, is not able to predict word durations, while the out-ofdomain trained language model does predict word durations. We argue that this at first counter-intuitive result is due to the out-of-domain model better matching the “language models” of the speakers in our data.",
  "stance": -0.1
 },
 {
  "url": "https://aclanthology.org/P15-1104",
  "title": "Learning Word Representations from Scarce and Noisy Data with Embedding Subspaces",
  "year": 2015,
  "venue": "ACL",
  "abstract": "We investigate a technique to adapt unsupervised word embeddings to specific applications, when only small and noisy labeled datasets are available. Current methods use pre-trained embeddings to initialize model parameters, and then use the labeled data to tailor them for the intended task. However, this approach is prone to overfitting when the training is performed with scarce and noisy data. To overcome this issue, we use the supervised data to find an embedding subspace that fits the task complexity. All the word representations are adapted through a projection into this task-specific subspace, even if they do not occur on the labeled dataset. This approach was recently used in the SemEval 2015 Twitter sentiment analysis challenge, attaining state-of-the-art results. Here we show results improving those of the challenge, as well as additional experiments in a Twitter Part-Of-Speech tagging task.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/P15-1108",
  "title": "Joint Dependency Parsing and Multiword Expression Tokenization",
  "year": 2015,
  "venue": "ACL",
  "abstract": "Complex conjunctions and determiners are often considered as pretokenized units in parsing. This is not always realistic, since they can be ambiguous. We propose a model for joint dependency parsing and multiword expressions identification, in which complex function words are represented as individual tokens linked with morphological dependencies. Our graphbased parser includes standard secondorder features and verbal subcategorization features derived from a syntactic lexicon.We train it on a modified version of the French Treebank enriched with morphological dependencies. It recognizes 81.79% of ADV+que conjunctions with 91.57% precision, and 82.74% of de+DET determiners with 86.70% precision.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P15-1111",
  "title": "Identifying Cascading Errors using Constraints in Dependency Parsing",
  "year": 2015,
  "venue": "ACL",
  "abstract": "Dependency parsers are usually evaluated on attachment accuracy. Whilst easily interpreted, the metric does not illustrate the cascading impact of errors, where the parser chooses an incorrect arc, and is subsequently forced to choose further incorrect arcs elsewhere in the parse. We apply arc-level constraints to MSTparser and ZPar, enforcing the correct analysis of specific error classes, whilst otherwise continuing with decoding. We investigate the direct and indirect impact of applying constraints to the parser. Erroneous NP and punctuation attachments cause the most cascading errors, while incorrect PP and coordination attachments are frequent but less influential. Punctuation is especially challenging, as it has long been ignored in parsing, and serves a variety of disparate syntactic roles.",
  "stance": -0.6
 },
 {
  "url": "https://aclanthology.org/P15-1116",
  "title": "Discontinuous Incremental Shift-reduce Parsing",
  "year": 2015,
  "venue": "ACL",
  "abstract": "We present an extension to incremental shift-reduce parsing that handles discontinuous constituents, using a linear classifier and beam search. We achieve very high parsing speeds (up to 640 sent./sec.) and accurate results (up to 79.52 F1 on TiGer).",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P15-2001",
  "title": "A Framework for the Construction of Monolingual and Cross-lingual Word Similarity Datasets",
  "year": 2015,
  "venue": "ACL",
  "abstract": "Despite being one of the most popular tasks in lexical semantics, word similarity has often been limited to the English language. Other languages, even those that are widely spoken such as Spanish, do not have a reliable word similarity evaluation framework. We put forward robust methodologies for the extension of existing English datasets to other languages, both at monolingual and cross-lingual levels. We propose an automatic standardization for the construction of cross-lingual similarity datasets, and provide an evaluation, demonstrating its reliability and robustness. Based on our procedure and taking the RG-65 word similarity dataset as a reference, we release two high-quality Spanish and Farsi (Persian) monolingual datasets, and fifteen cross-lingual datasets for six languages: English, Spanish, French, German, Portuguese, and Farsi.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P15-2024",
  "title": "Evaluating Machine Translation Systems with Second Language Proficiency Tests",
  "year": 2015,
  "venue": "ACL",
  "abstract": "A lightweight, human-in-the-loop evaluation scheme for machine translation (MT) systems is proposed. It extrinsically evaluates MT systems using human subjects’ scores on second language ability test problems that are machine-translated to the subjects’ native language. A largescale experiment involving 320 subjects revealed that the context-unawareness of the current MT systems severely damages human performance when solving the test problems, while one of the evaluated MT systems performed as good as a human translation produced in a context-unaware condition. An analysis of the experimental results showed that the extrinsic evaluation captured a different dimension of translation quality than that captured by manual and automatic intrinsic evaluation.",
  "stance": 0.4
 },
 {
  "url": "https://aclanthology.org/P15-2026",
  "title": "Exploring the Planet of the APEs: a Comparative Study of State-of-the-art Methods for MT Automatic Post-Editing",
  "year": 2015,
  "venue": "ACL",
  "abstract": "Downstream processing of machine translation (MT) output promises to be a solution to improve translation quality, especially when the MT system’s internal decoding process is not accessible. Both rule-based and statistical automatic postediting (APE) methods have been proposed over the years, but with contrasting results. A missing aspect in previous evaluations is the assessment of different methods: i) under comparable conditions, and ii) on different language pairs featuring variable levels of MT quality. Focusing on statistical APE methods (more portable across languages), we propose the first systematic analysis of two approaches. To understand their potential, we compare them in the same conditions over six language pairs having English as source. Our results evidence consistent improvements on all language pairs, a relation between the extent of the gain and MT output quality, slight but statistically significant performance differences between the two methods, and their possible complementarity.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/P15-2057",
  "title": "Learning Topic Hierarchies for Wikipedia Categories",
  "year": 2015,
  "venue": "ACL",
  "abstract": "Existing studies have utilized Wikipedia for various knowledge acquisition tasks. However, no attempts have been made to explore multi-level topic knowledge contained in Wikipedia articles’",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/P15-2078",
  "title": "Dependency length minimisation effects in short spans: a large-scale analysis of adjective placement in complex noun phrases",
  "year": 2015,
  "venue": "ACL",
  "abstract": "It has been extensively observed that languages minimise the distance between two related words. Dependency length minimisation effects are explained as a means to reduce memory load and for effective communication. In this paper, we ask whether they hold in typically short spans, such as noun phrases, which could be thought of being less subject to efficiency pressure. We demonstrate that minimisation does occur in short spans, but also that it is a complex effect: it is not only the length of the dependency that is at stake, but also the effect of the surrounding dependencies.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P15-2079",
  "title": "Tagging Performance Correlates with Author Age",
  "year": 2015,
  "venue": "ACL",
  "abstract": "Many NLP tools for English and German are based on manually annotated articles from the Wall Street Journal and Frankfurter Rundschau. The average readers of these two newspapers are middle-aged (55 and 47 years old, respectively), and the annotated articles are more than 20 years old by now. This leads us to speculate whether tools induced from these resources (such as part-of-speech taggers) put older language users at an advantage. We show that this is actually the case in both languages, and that the cause goes beyond simple vocabulary differences. In our experiments, we control for gender and region.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/P15-2090",
  "title": "UNRAVEL-A Decipherment Toolkit",
  "year": 2015,
  "venue": "ACL",
  "abstract": "In this paper we present the UNRAVEL toolkit: It implements many of the recently published works on decipherment, including decipherment for deterministic ciphers like e.g. the ZODIAC-408 cipher and Part two of the BEALE ciphers, as well as decipherment of probabilistic ciphers and unsupervised training for machine translation. It also includes data and example configuration files so that the previously published experiments are easy to reproduce.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/P15-2092",
  "title": "What's in a Domain? Analyzing Genre and Topic Differences in Statistical Machine Translation",
  "year": 2015,
  "venue": "ACL",
  "abstract": "Domain adaptation is an active field of research in statistical machine translation (SMT), but so far most work has ignored the distinction between the topic and genre of documents. In this paper we quantify and disentangle the impact of genre and topic differences on translation quality by introducing a new data set that has controlled topic and genre distributions. In addition, we perform a detailed analysis showing that differences across topics only explain to a limited degree translation performance differences across genres, and that genre-specific errors are more attributable to model coverage than to suboptimal scoring of translation candidates.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/P15-2097",
  "title": "Ground Truth for Grammatical Error Correction Metrics",
  "year": 2015,
  "venue": "ACL",
  "abstract": "How do we know which grammatical error correction (GEC) system is best? A number of metrics have been proposed over the years, each motivated by weaknesses of previous metrics; however, the metrics themselves have not been compared to an empirical gold standard grounded in human judgments. We conducted the first human evaluation of GEC system outputs, and show that the rankings produced by metrics such as MaxMatch and I-measure do not correlate well with this ground truth. As a step towards better metrics, we also propose GLEU, a simple variant of BLEU, modified to account for both the source and the reference, and show that it hews much more closely to human judgments.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/P15-2105",
  "title": "Automatic Keyword Extraction on Twitter",
  "year": 2015,
  "venue": "ACL",
  "abstract": "In this paper, we build a corpus of tweets from Twitter annotated with keywords using crowdsourcing methods. We identify key differences between this domain and the work performed on other domains, such as news, which makes existing approaches for automatic keyword extraction not generalize well on Twitter datasets. These datasets include the small amount of content in each tweet, the frequent usage of lexical variants and the high variance of the cardinality of keywords present in each tweet. We propose methods for addressing these issues, which leads to solid improvements on this dataset for this task.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P15-2119",
  "title": "How Well Do Distributional Models Capture Different Types of Semantic Knowledge?",
  "year": 2015,
  "venue": "ACL",
  "abstract": "In recent years, distributional models (DMs) have shown great success in representing lexical semantics. In this work we show that the extent to which DMs represent semantic knowledge is highly dependent on the type of knowledge. We pose the task of predicting properties of concrete nouns in a supervised setting, and compare between learning taxonomic properties (e.g., animacy) and attributive properties (e.g., size, color). We employ four state-of-the-art DMs as sources of feature representation for this task, and show that they all yield poor results when tested on attributive properties, achieving no more than an average F-score of 0.37 in the binary property prediction task, compared to 0.73 on taxonomic properties. Our results suggest that the distributional hypothesis may not be equally applicable to all types of semantic information.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/P16-1060",
  "title": "Which Coreference Evaluation Metric Do You Trust? A Proposal for a Link-based Entity Aware Metric",
  "year": 2016,
  "venue": "ACL",
  "abstract": "Interpretability and discriminative power are the two most basic requirements for an evaluation metric. In this paper, we report the mention identification effect in the B3, CEAF, and BLANC coreference evaluation metrics that makes it impossible to interpret their results properly. The only metric which is insensitive to this flaw is MUC, which, however, is known to be the least discriminative metric. It is a known fact that none of the current metrics are reliable. The common practice for ranking coreference resolvers is to use the average of three different metrics. However, one cannot expect to obtain a reliable score by averaging three unreliable metrics. We propose LEA, a Link-based Entity-Aware evaluation metric that is designed to overcome the shortcomings of the current evaluation metrics. LEA is available as branch LEA-scorer in the reference implementation of the official CoNLL scorer.",
  "stance": -0.4
 },
 {
  "url": "https://aclanthology.org/P16-1080",
  "title": "Analyzing Biases in Human Perception of User Age and Gender from Text",
  "year": 2016,
  "venue": "ACL",
  "abstract": "User traits disclosed through written text, such as age and gender, can be used to personalize applications such as recommender systems or conversational agents. However, human perception of these traits is not perfectly aligned with reality. In this paper, we conduct a large-scale crowdsourcing experiment on guessing age and gender from tweets. We systematically analyze the quality and possible biases of these predictions. We identify the textual cues which lead to miss-assessments of traits or make annotators more or less confident in their choice. Our study demonstrates that differences between real and perceived traits are noteworthy and elucidates inaccurately used stereotypes in human perception.",
  "stance": -0.6
 },
 {
  "url": "https://aclanthology.org/P16-2013",
  "title": "Reference Bias in Monolingual Machine Translation Evaluation",
  "year": 2016,
  "venue": "ACL",
  "abstract": "In the translation industry, human translations are assessed by comparison with the source texts. In the Machine Translation (MT) research community, however, it is a common practice to perform quality assessment using a reference translation instead of the source text. In this paper we show that this practice has a serious issue – annotators are strongly biased by the reference translation provided, and this can have a negative impact on the assessment of MT quality.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P17-1008",
  "title": "The State of the Art in Semantic Representation",
  "year": 2017,
  "venue": "ACL",
  "abstract": "Semantic representation is receiving growing attention in NLP in the past few years, and many proposals for semantic schemes (e.g., AMR, UCCA, GMB, UDS) have been put forth. Yet, little has been done to assess the achievements and the shortcomings of these new contenders, compare them with syntactic schemes, and clarify the general goals of research on semantic representation. We address these gaps by critically surveying the state of the art in the field.",
  "stance": -0.7
 },
 {
  "url": "https://aclanthology.org/P17-1014",
  "title": "Neural AMR: Sequence-to-Sequence Models for Parsing and Generation",
  "year": 2017,
  "venue": "ACL",
  "abstract": "Sequence-to-sequence models have shown strong performance across a broad range of applications. However, their application to parsing and generating text using Abstract Meaning Representation (AMR) has been limited, due to the relatively limited amount of labeled data and the non-sequential nature of the AMR graphs. We present a novel training procedure that can lift this limitation using millions of unlabeled sentences and careful preprocessing of the AMR graphs. For AMR parsing, our model achieves competitive results of 62.1 SMATCH, the current best score reported without significant use of external semantic resources. For AMR generation, our model establishes a new state-of-the-art performance of BLEU 33.8. We present extensive ablative and qualitative analysis including strong evidence that sequence-based AMR models are robust against ordering variations of graph-to-sequence conversions.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P17-1024",
  "title": "FOIL it! Find One mismatch between Image and Language caption",
  "year": 2017,
  "venue": "ACL",
  "abstract": "In this paper, we aim to understand whether current language and vision (LaVi) models truly grasp the interaction between the two modalities. To this end, we propose an extension of the MS-COCO dataset, FOIL-COCO, which associates images with both correct and 'foil' captions, that is, descriptions of the image that are highly similar to the original ones, but contain one single mistake ('foil word'). We show that current LaVi models fall into the traps of this data and perform badly on three tasks: a) caption classification (correct vs. foil); b) foil word detection; c) foil word correction. Humans, in contrast, have near-perfect performance on those tasks. We demonstrate that merely utilising language cues is not enough to model FOIL-COCO and that it challenges the state-of-the-art by requiring a fine-grained understanding of the relation between text and image.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P17-1138",
  "title": "Bandit Structured Prediction for Neural Sequence-to-Sequence Learning",
  "year": 2017,
  "venue": "ACL",
  "abstract": "Bandit structured prediction describes a stochastic optimization framework where learning is performed from partial feedback. This feedback is received in the form of a task loss evaluation to a predicted output structure, without having access to gold standard structures. We advance this framework by lifting linear bandit learning to neural sequence-to-sequence learning problems using attention-based recurrent neural networks. Furthermore, we show how to incorporate control variates into our learning algorithms for variance reduction and improved generalization. We present an evaluation on a neural machine translation task that shows improvements of up to 5.89 BLEU points for domain adaptation from simulated bandit feedback.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/P17-1149",
  "title": "Bridge Text and Knowledge by Learning Multi-Prototype Entity Mention Embedding",
  "year": 2017,
  "venue": "ACL",
  "abstract": "Integrating text and knowledge into a unified semantic space has attracted significant research interests recently. However, the ambiguity in the common space remains a challenge, namely that the same mention phrase usually refers to various entities. In this paper, to deal with the ambiguity of entity mentions, we propose a novel Multi-Prototype Mention Embedding model, which learns multiple sense embeddings for each mention by jointly modeling words from textual contexts and entities derived from a knowledge base. In addition, we further design an efficient language model based approach to disambiguate each mention to a specific sense. In experiments, both qualitative and quantitative analysis demonstrate the high quality of the word, entity and multi-prototype mention embeddings. Using entity linking as a study case, we apply our disambiguation method as well as the multi-prototype mention embeddings on the benchmark dataset, and achieve state-of-the-art performance.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P17-1163",
  "title": "Neural Belief Tracker: Data-Driven Dialogue State Tracking",
  "year": 2017,
  "venue": "ACL",
  "abstract": "One of the core components of modern spoken dialogue systems is the belief tracker, which estimates the user's goal at every step of the dialogue. However, most current approaches have difficulty scaling to larger, more complex dialogue domains. This is due to their dependency on either: a) Spoken Language Understanding models that require large amounts of annotated training data; or b) hand-crafted lexicons for capturing some of the linguistic variation in users' language. We propose a novel Neural Belief Tracking (NBT) framework which overcomes these problems by building on recent advances in representation learning. NBT models reason over pre-trained word vectors, learning to compose them into distributed representations of user utterances and dialogue context. Our evaluation on two datasets shows that this approach surpasses past limitations, matching the performance of state-of-the-art models which rely on hand-crafted semantic lexicons and outperforming them when such lexicons are not provided.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P17-1184",
  "title": "From Characters to Words to in Between: Do We Capture Morphology?",
  "year": 2017,
  "venue": "ACL",
  "abstract": "Words can be represented by composing the representations of subword units such as word segments, characters, and/or character n-grams. While such representations are effective and may capture the morphological regularities of words, they have not been systematically compared, and it is not understood how they interact with different morphological typologies. On a language modeling task, we present experiments that systematically vary (1) the basic unit of representation, (2) the composition of these representations, and (3) the morphological typology of the language modeled. Our results extend previous findings that character representations are effective across typologies, and we find that a previously unstudied combination of character trigram representations composed with bi-LSTMs outperforms most others. But we also find room for improvement: none of the character-level models match the predictive accuracy of a model with access to true morphological analyses, even when learned from an order of magnitude more data.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P17-2011",
  "title": "An Analysis of Action Recognition Datasets for Language and Vision Tasks",
  "year": 2017,
  "venue": "ACL",
  "abstract": "A large amount of recent research has focused on tasks that combine language and vision, resulting in a proliferation of datasets and methods. One such task is action recognition, whose applications include image annotation, scene understanding and image retrieval. In this survey, we categorize the existing approaches based on how they conceptualize this problem and provide a detailed review of existing datasets, highlighting their diversity as well as advantages and disadvantages. We focus on recently developed datasets which link visual information with linguistic resources and provide a fine-grained syntactic and semantic analysis of actions in images.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P17-2012",
  "title": "Learning to Parse and Translate Improves Neural Machine Translation",
  "year": 2017,
  "venue": "ACL",
  "abstract": "There has been relatively little attention to incorporating linguistic prior to neural machine translation. Much of the previous work was further constrained to considering linguistic prior on the source side. In this paper, we propose a hybrid model, called NMT+RNNG, that learns to parse and translate by combining the recurrent neural network grammar into the attention-based neural machine translation. Our approach encourages the neural machine translation model to incorporate linguistic prior during training, and lets it translate on its own afterward. Extensive experiments with four language pairs show the effectiveness of the proposed NMT+RNNG.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P17-2034",
  "title": "A Corpus of Natural Language for Visual Reasoning",
  "year": 2017,
  "venue": "ACL",
  "abstract": "We present a new visual reasoning language dataset, containing 92,244 pairs of examples of natural statements grounded in synthetic images with 3,962 unique sentences. We describe a method of crowdsourcing linguistically-diverse data, and present an analysis of our data. The data demonstrates a broad set of linguistic phenomena, requiring visual and set-theoretic reasoning. We experiment with various models, and show the data presents a strong challenge for future research.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P17-2045",
  "title": "A Neural Architecture for Generating Natural Language Descriptions from Source Code Changes",
  "year": 2017,
  "venue": "ACL",
  "abstract": "We propose a model to automatically describe changes introduced in the source code of a program using natural language. Our method receives as input a set of code commits, which contains both the modifications and message introduced by an user. These two modalities are used to train an encoder-decoder architecture. We evaluated our approach on twelve real world open source projects from four different programming languages. Quantitative and qualitative results showed that the proposed approach can generate feasible and semantically sound descriptions not only in standard in-project settings, but also in a cross-project setting.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P18-1026",
  "title": "Graph-to-Sequence Learning using Gated Graph Neural Networks",
  "year": 2018,
  "venue": "ACL",
  "abstract": "Many NLP applications can be framed as a graph-to-sequence learning problem. Previous work proposing neural architectures on graph-to-sequence obtained promising results compared to grammar-based approaches but still rely on linearisation heuristics and/or standard recurrent networks to achieve the best performance. In this work propose a new model that encodes the full structural information contained in the graph. Our architecture couples the recently proposed Gated Graph Neural Networks with an input transformation that allows nodes and edges to have their own hidden representations, while tackling the parameter explosion problem present in previous work. Experimental results shows that our model outperforms strong baselines in generation from AMR graphs and syntax-based neural machine translation.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P18-1036",
  "title": "Character-Level Models versus Morphology in Semantic Role Labeling",
  "year": 2018,
  "venue": "ACL",
  "abstract": "Character-level models have become a popular approach specially for their accessibility and ability to handle unseen data. However, little is known on their ability to reveal the underlying morphological structure of a word, which is a crucial skill for high-level semantic analysis tasks, such as semantic role labeling (SRL). In this work, we train various types of SRL models that use word, character and morphology level information and analyze how performance of characters compare to words and morphology for several languages. We conduct an in-depth error analysis for each morphological typology and analyze the strengths and limitations of character-level models that relate to out-of-domain data, training data size, long range dependencies and model complexity. Our exhaustive analyses shed light on important characteristics of character-level models and their semantic capability.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P18-1072",
  "title": "On the Limitations of Unsupervised Bilingual Dictionary Induction",
  "year": 2018,
  "venue": "ACL",
  "abstract": "Unsupervised machine translation - i.e., not assuming any cross-lingual supervision signal, whether a dictionary, translations, or comparable corpora - seems impossible, but nevertheless, Lample et al. (2017) recently proposed a fully unsupervised machine translation (MT) model. The model relies heavily on an adversarial, unsupervised cross-lingual word embedding technique for bilingual dictionary induction (Conneau et al., 2017), which we examine here. Our results identify the limitations of current unsupervised MT: unsupervised bilingual dictionary induction performs much worse on morphologically rich languages that are not dependent marking, when monolingual corpora from different domains or different embedding algorithms are used. We show that a simple trick, exploiting a weak supervision signal from identical words, enables more robust induction and establish a near-perfect correlation between unsupervised bilingual dictionary induction performance and a previously unexplored graph similarity metric.",
  "stance": -0.6
 },
 {
  "url": "https://aclanthology.org/P18-1083",
  "title": "No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling",
  "year": 2018,
  "venue": "ACL",
  "abstract": "Though impressive results have been achieved in visual captioning, the task of generating abstract stories from photo streams is still a little-tapped problem. Different from captions, stories have more expressive language styles and contain many imaginary concepts that do not appear in the images. Thus it poses challenges to behavioral cloning algorithms. Furthermore, due to the limitations of automatic metrics on evaluating story quality, reinforcement learning methods with hand-crafted rewards also face difficulties in gaining an overall performance boost. Therefore, we propose an Adversarial REward Learning (AREL) framework to learn an implicit reward function from human demonstrations, and then optimize policy search with the learned reward function. Though automatic evaluation indicates slight performance boost over state-of-the-art (SOTA) methods in cloning expert behaviors, human evaluation shows that our approach achieves significant improvement in generating more human-like stories than SOTA systems.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P18-1098",
  "title": "A Neural Architecture for Automated ICD Coding",
  "year": 2018,
  "venue": "ACL",
  "abstract": "The International Classification of Diseases (ICD) provides a hierarchy of diagnostic codes for classifying diseases. Medical coding - which assigns a subset of ICD codes to a patient visit - is a mandatory process that is crucial for patient care and billing. Manual coding is time-consuming, expensive, and error prone. In this paper, we build a neural architecture for automated coding. It takes the diagnosis descriptions (DDs) of a patient as inputs and selects the most relevant ICD codes. This architecture contains four major ingredients: (1) tree-of-sequences LSTM encoding of code descriptions (CDs), (2) adversarial learning for reconciling the different writing styles of DDs and CDs, (3) isotonic constraints for incorporating the importance order among the assigned codes, and (4) attentional matching for performing many-to-one and one-to-many mappings from DDs to CDs. We demonstrate the effectiveness of the proposed methods on a clinical datasets with 59K patient visits.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P18-1133",
  "title": "Sequicity: Simplifying Task-oriented Dialogue Systems with Single Sequence-to-Sequence Architectures",
  "year": 2018,
  "venue": "ACL",
  "abstract": "Existing solutions to task-oriented dialogue systems follow pipeline designs which introduces architectural complexity and fragility. We propose a novel, holistic, extendable framework based on a single sequence-to-sequence (seq2seq) model which can be optimized with supervised or reinforcement learning. A key contribution is that we design text spans named belief spans to track dialogue believes, allowing task-oriented dialogue systems to be modeled in a seq2seq way. Based on this, we propose a simplistic Two Stage CopyNet instantiation which emonstrates good scalability: significantly reducing model complexity in terms of number of parameters and training time by a magnitude. It significantly outperforms state-of-the-art pipeline-based methods on large datasets and retains a satisfactory entity match rate on out-of-vocabulary (OOV) cases where pipeline-designed competitors totally fail.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P18-1156",
  "title": "DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension",
  "year": 2018,
  "venue": "ACL",
  "abstract": "We propose DuoRC, a novel dataset for Reading Comprehension (RC) that motivates several new challenges for neural approaches in language understanding beyond those offered by existing RC datasets. DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie - one from Wikipedia and the other from IMDb - written by two different authors. We asked crowdsourced workers to create questions from one version of the plot and a different set of workers to extract or synthesize answers from the other version. This unique characteristic of DuoRC where questions and answers are created from different versions of a document narrating the same underlying story, ensures by design, that there is very little lexical overlap between the questions created from one version and the segments containing the answer in the other version. Further, since the two versions have different levels of plot detail, narration style, vocabulary, etc., answering questions from the second version requires deeper language understanding and incorporating external background knowledge. Additionally, the narrative style of passages arising from movie plots (as opposed to typical descriptive passages in existing datasets) exhibits the need to perform complex reasoning over events across multiple sentences. Indeed, we observe that state-of-the-art neural RC models which have achieved near human performance on the SQuAD dataset, even when coupled with traditional NLP techniques to address the challenges presented in DuoRC exhibit very poor performance (F1 score of 37.42% on DuoRC v/s 86% on SQuAD dataset). This opens up several interesting research avenues wherein DuoRC could complement other RC datasets to explore novel neural approaches for studying language understanding.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P18-1176",
  "title": "Did the Model Understand the Question?",
  "year": 2018,
  "venue": "ACL",
  "abstract": "We analyze state-of-the-art deep learning models for three tasks: question answering on (1) images, (2) tables, and (3) passages of text. Using the notion of \"attribution\" (word importance), we find that these deep networks often ignore important question terms. Leveraging such behavior, we perturb questions to craft a variety of adversarial examples. Our strongest attacks drop the accuracy of a visual question answering model from 61.1% to 19%, and that of a tabular question answering model from 33.5% to 3.3%. Additionally, we show how attributions can strengthen attacks proposed by Jia and Liang (2017) on paragraph comprehension models. Our results demonstrate that attributions can augment standard measures of accuracy and empower investigation of model performance. When a model is accurate but for the wrong reasons, attributions can surface erroneous logic in the model that indicates inadequacies in the test data.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P18-2036",
  "title": "Characterizing Departures from Linearity in Word Translation",
  "year": 2018,
  "venue": "ACL",
  "abstract": "We investigate the behavior of maps learned by machine translation methods. The maps translate words by projecting between word embedding spaces of different languages. We locally approximate these maps using linear maps, and find that they vary across the word embedding space. This demonstrates that the underlying maps are non-linear. Importantly, we show that the locally linear maps vary by an amount that is tightly correlated with the distance between the neighborhoods on which they are trained. Our results can be used to test non-linear methods, and to drive the design of more accurate maps for word translation.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P18-2074",
  "title": "Do Neural Network Cross-Modal Mappings Really Bridge Modalities?",
  "year": 2018,
  "venue": "ACL",
  "abstract": "Feed-forward networks are widely used in cross-modal applications to bridge modalities by mapping distributed vectors of one modality to the other, or to a shared space. The predicted vectors are then used to perform e.g., retrieval or labeling. Thus, the success of the whole system relies on the ability of the mapping to make the neighborhood structure (i.e., the pairwise similarities) of the predicted vectors akin to that of the target vectors. However, whether this is achieved has not been investigated yet. Here, we propose a new similarity measure and two ad hoc experiments to shed light on this issue. In three cross-modal benchmarks we learn a large number of language-to-vision and vision-to-language neural network mappings (up to five layers) using a rich diversity of image and text features and loss functions. Our results reveal that, surprisingly, the neighborhood structure of the predicted vectors consistently resembles more that of the input vectors than that of the target vectors. In a second experiment, we further show that untrained nets do not significantly disrupt the neighborhood (i.e., semantic) structure of the input vectors.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/P18-2108",
  "title": "Enhancing Drug-Drug Interaction Extraction from Texts by Molecular Structure Information",
  "year": 2018,
  "venue": "ACL",
  "abstract": "We propose a novel neural method to extract drug-drug interactions (DDIs) from texts using external drug molecular structure information. We encode textual drug pairs with convolutional neural networks and their molecular pairs with graph convolutional networks (GCNs), and then we concatenate the outputs of these two networks. In the experiments, we show that GCNs can predict DDIs from the molecular structures of drugs in high accuracy and the molecular information can enhance text-based DDI extraction by 2.39 percent points in the F-score on the DDIExtraction 2013 shared task data set.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P19-1008",
  "title": "Semantic Expressive Capacity with Bounded Memory",
  "year": 2019,
  "venue": "ACL",
  "abstract": "We investigate the capacity of mechanisms for compositional semantic parsing to describe relations between sentences and semantic representations. We prove that in order to represent certain relations, mechanisms which are syntactically projective must be able to remember an unbounded number of locations in the semantic representations, where nonprojective mechanisms need not. This is the first result of this kind, and has consequences both for grammar-based and for neural systems.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P19-1070",
  "title": "How to (Properly) Evaluate Cross-Lingual Word Embeddings: On Strong Baselines, Comparative Analyses, and Some Misconceptions",
  "year": 2019,
  "venue": "ACL",
  "abstract": "Cross-lingual word embeddings (CLEs) facilitate cross-lingual transfer of NLP models. Despite their ubiquitous downstream usage, increasingly popular projection-based CLE models are almost exclusively evaluated on bilingual lexicon induction (BLI). Even the BLI evaluations vary greatly, hindering our ability to correctly interpret performance and properties of different CLE models. In this work, we take the first step towards a comprehensive evaluation of CLE models: we thoroughly evaluate both supervised and unsupervised CLE models, for a large number of language pairs, on BLI and three downstream tasks, providing new insights concerning the ability of cutting-edge CLE models to support cross-lingual NLP. We empirically demonstrate that the performance of CLE models largely depends on the task at hand and that optimizing CLE models for BLI may hurt downstream performance. We indicate the most robust supervised and unsupervised CLE models and emphasize the need to reassess simple baselines, which still display competitive performance across the board. We hope our work catalyzes further research on CLE evaluation and model analysis.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P19-1075",
  "title": "ChID: A Large-scale Chinese IDiom Dataset for Cloze Test",
  "year": 2019,
  "venue": "ACL",
  "abstract": "Cloze-style reading comprehension in Chinese is still limited due to the lack of various corpora. In this paper we propose a large-scale Chinese cloze test dataset ChID, which studies the comprehension of idiom, a unique language phenomenon in Chinese. In this corpus, the idioms in a passage are replaced by blank symbols and the correct answer needs to be chosen from well-designed candidate idioms. We carefully study how the design of candidate idioms and the representation of idioms affect the performance of state-of-the-art models. Results show that the machine accuracy is substantially worse than that of human, indicating a large space for further research.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P19-1246",
  "title": "You Write like You Eat: Stylistic Variation as a Predictor of Social Stratification",
  "year": 2019,
  "venue": "ACL",
  "abstract": "Inspired by Labov's seminal work on stylisticvariation as a function of social stratification,we develop and compare neural models thatpredict a person's presumed socio-economicstatus, obtained through distant supervision,from their writing style on social media. Thefocus of our work is on identifying the mostimportant stylistic parameters to predict socio-economic group. In particular, we show theeffectiveness of morpho-syntactic features aspredictors of style, in contrast to lexical fea-tures, which are good predictors of topic",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P19-1267",
  "title": "We Need to Talk about Standard Splits",
  "year": 2019,
  "venue": "ACL",
  "abstract": "It is standard practice in speech & language technology to rank systems according to their performance on a test set held out for evaluation. However, few researchers apply statistical tests to determine whether differences in performance are likely to arise by chance, and few examine the stability of system ranking across multiple training-testing splits. We conduct replication and reproduction experiments with nine part-of-speech taggers published between 2000 and 2018, each of which claimed state-of-the-art performance on a widely-used \"standard split\". While we replicate results on the standard split, we fail to reliably reproduce some rankings when we repeat this analysis with randomly generated training-testing splits. We argue that randomly generated splits should be used in system evaluation.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P19-1286",
  "title": "Domain Adaptation of Neural Machine Translation by Lexicon Induction",
  "year": 2019,
  "venue": "ACL",
  "abstract": "It has been previously noted that neural machine translation (NMT) is very sensitive to domain shift. In this paper, we argue that this is a dual effect of the highly lexicalized nature of NMT, resulting in failure for sentences with large numbers of unknown words, and lack of supervision for domain-specific words. To remedy this problem, we propose an unsupervised adaptation method which fine-tunes a pre-trained out-of-domain NMT model using a pseudo-in-domain corpus. Specifically, we perform lexicon induction to extract an in-domain lexicon, and construct a pseudo-parallel in-domain corpus by performing word-for-word back-translation of monolingual in-domain target sentences. In five domains over twenty pairwise adaptation settings and two model architectures, our method achieves consistent improvements without using any in-domain parallel sentences, improving up to 14 BLEU over unadapted models, and up to 2 BLEU over strong back-translation baselines.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P19-1334",
  "title": "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference",
  "year": 2019,
  "venue": "ACL",
  "abstract": "A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P19-1376",
  "title": "Are we there yet? Encoder-decoder neural networks as cognitive models of English past tense inflection",
  "year": 2019,
  "venue": "ACL",
  "abstract": "The cognitive mechanisms needed to account for the English past tense have long been a subject of debate in linguistics and cognitive science. Neural network models were proposed early on, but were shown to have clear flaws. Recently, however, Kirov and Cotterell (2018) showed that modern encoder-decoder (ED) models overcome many of these flaws. They also presented evidence that ED models demonstrate humanlike performance in a nonce-word task. Here, we look more closely at the behaviour of their model in this task. We find that (1) the model exhibits instability across multiple simulations in terms of its correlation with human data, and (2) even when results are aggregated across simulations (treating each simulation as an individual human participant), the fit to the human data is not strong-worse than an older rule-based model. These findings hold up through several alternative training regimes and evaluation measures. Although other neural architectures might do better, we conclude that there is still insufficient evidence to claim that neural nets are a good cognitive model for this task.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P19-1380",
  "title": "Miss Tools and Mr Fruit: Emergent Communication in Agents Learning about Object Affordances",
  "year": 2019,
  "venue": "ACL",
  "abstract": "Recent research studies communication emergence in communities of deep network agents assigned a joint task, hoping to gain insights on human language evolution. We propose here a new task capturing crucial aspects of the human environment, such as natural object affordances, and of human conversation, such as full symmetry among the participants. By conducting a thorough pragmatic and semantic analysis of the emergent protocol, we show that the agents solve the shared task through genuine bilateral, referential communication. However, the agents develop multiple idiolects, which makes us conclude that full symmetry is not a sufficient condition for a common language to emerge.",
  "stance": -0.4
 },
 {
  "url": "https://aclanthology.org/P19-1381",
  "title": "CNNs found to jump around more skillfully than RNNs: Compositional Generalization in Seq2seq Convolutional Networks",
  "year": 2019,
  "venue": "ACL",
  "abstract": "Lake and Baroni (2018) introduced the SCAN dataset probing the ability of seq2seq models to capture compositional generalizations, such as inferring the meaning of \"jump around\" 0-shot from the component words. Recurrent networks (RNNs) were found to completely fail the most challenging generalization cases. We test here a convolutional network (CNN) on these tasks, reporting hugely improved performance with respect to RNNs. Despite the big improvement, the CNN has however not induced systematic rules, suggesting that the difference between compositional and non-compositional behaviour is not clear-cut.",
  "stance": -0.7
 },
 {
  "url": "https://aclanthology.org/P19-1412",
  "title": "Do You Know That Florence Is Packed with Visitors? Evaluating State-of-the-art Models of Speaker Commitment",
  "year": 2019,
  "venue": "ACL",
  "abstract": "When a speaker, Mary, asks \"Do you know that Florence is packed with visitors?\", we take her to believe that Florence is packed with visitors, but not if she asks \"Do you think that Florence is packed with visitors?\". Inferring speaker commitment (aka event factuality) is crucial for information extraction and question answering. Here, we explore the hypothesis that linguistic deficits drive the error patterns of existing speaker commitment models by analyzing the linguistic correlates of model error on a challenging naturalistic dataset. We evaluate two state-of-the-art speaker commitment models on the CommitmentBank, an English dataset of naturally occurring discourses. The CommitmentBank is annotated with speaker commitment towards the content of the complement (\"Florence is packed with visitors\" in our example) of clause-embedding verbs (\"know\", \"think\") under four entailment-canceling environments (negation, modal, question, conditional). A breakdown of items by linguistic features reveals asymmetrical error patterns: while the models achieve good performance on some classes (e.g., negation), they fail to generalize to the diverse linguistic constructions (e.g., conditionals) in natural language, highlighting directions for improvement.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/P19-1459",
  "title": "Probing Neural Network Comprehension of Natural Language Arguments",
  "year": 2019,
  "venue": "ACL",
  "abstract": "We are surprised to find that BERT's peak performance of 77% on the Argument Reasoning Comprehension Task reaches just three points below the average untrained human baseline. However, we show that this result is entirely accounted for by exploitation of spurious statistical cues in the dataset. We analyze the nature of these cues and demonstrate that a range of models all exploit them. This analysis informs the construction of an adversarial dataset on which all models achieve random accuracy. Our adversarial dataset provides a more robust assessment of argument comprehension and should be adopted as the standard in future work.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P19-1472",
  "title": "HellaSwag: Can a Machine Really Finish Your Sentence?",
  "year": 2019,
  "venue": "ACL",
  "abstract": "Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as \"A woman sits at a piano,\" a machine must select the most likely followup: \"She sets her fingers on the keys.\" With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference? In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans (>95% accuracy), state-of-the-art models struggle (<48%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical 'Goldilocks' zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models. Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/P19-1492",
  "title": "Analyzing the Limitations of Cross-lingual Word Embedding Mappings",
  "year": 2019,
  "venue": "ACL",
  "abstract": "Recent research in cross-lingual word embeddings has almost exclusively focused on offline methods, which independently train word embeddings in different languages and map them to a shared space through linear transformations. While several authors have questioned the underlying isomorphism assumption, which states that word embeddings in different languages have approximately the same structure, it is not clear whether this is an inherent limitation of mapping approaches or a more general issue when learning cross-lingual embeddings. So as to answer this question, we experiment with parallel corpora, which allows us to compare offline mapping to an extension of skip-gram that jointly learns both embedding spaces. We observe that, under these ideal conditions, joint learning yields to more isomorphic embeddings, is less sensitive to hubness, and obtains stronger results in bilingual lexicon induction. We thus conclude that current mapping methods do have strong limitations, calling for further research to jointly learn cross-lingual embeddings with a weaker cross-lingual signal.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P19-1554",
  "title": "Misleading Failures of Partial-input Baselines",
  "year": 2019,
  "venue": "ACL",
  "abstract": "Recent work establishes dataset difficulty and removes annotation artifacts via partial-input baselines (e.g., hypothesis-only model for SNLI or question-only model for VQA). A successful partial-input baseline indicates that the dataset is cheatable. But the converse is not necessarily true: failures of partial-input baselines do not mean the dataset is free of artifacts. We first design artificial datasets to illustrate how the trivial patterns that are only visible in the full input can evade any partial-input baseline. Next, we identify such artifacts in the SNLI dataset-a hypothesis-only model augmented with trivial patterns in the premise can solve 15% of previously-thought \"hard\" examples. Our work provides a caveat for the use and creation of partial-input baselines for datasets.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P19-1652",
  "title": "Bridging by Word: Image Grounded Vocabulary Construction for Visual Captioning",
  "year": 2019,
  "venue": "ACL",
  "abstract": "Image Captioning aims at generating a short description for an image. Existing research usually employs the architecture of CNN-RNN that views the generation as a sequential decision-making process and the entire dataset vocabulary is used as decoding space. They suffer from generating high frequent n-gram with irrelevant words. To tackle this problem, we propose to construct an image-grounded vocabulary, based on which, captions are generated with limitation and guidance. In specific, a novel hierarchical structure is proposed to construct the vocabulary incorporating both visual information and relations among words. For generation, we propose a word-aware RNN cell incorporating vocabulary information into the decoding process directly. Reinforce algorithm is employed to train the generator using constraint vocabulary as action space. Experimental results on MS COCO and Flickr30k show the effectiveness of our framework compared to some state-of-the-art models.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.118",
  "title": "iSarcasm: A Dataset of Intended Sarcasm",
  "year": 2020,
  "venue": "ACL",
  "abstract": "We consider the distinction between intended and perceived sarcasm in the context of textual sarcasm detection. The former occurs when an utterance is sarcastic from the perspective of its author, while the latter occurs when the utterance is interpreted as sarcastic by the audience. We show the limitations of previous labelling methods in capturing intended sarcasm and introduce the iSarcasm dataset of tweets labeled for sarcasm directly by their authors. Examining the state-of-the-art sarcasm detection models on our dataset showed low performance compared to previously studied datasets, which indicates that these datasets might be biased or obvious and sarcasm could be a phenomenon under-studied computationally thus far. By providing the iSarcasm dataset, we aim to encourage future NLP research to develop methods for detecting sarcasm in text as intended by the authors of the text, not as labeled under assumptions that we demonstrate to be sub-optimal.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.158",
  "title": "A Systematic Assessment of Syntactic Generalization in Neural Language Models",
  "year": 2020,
  "venue": "ACL",
  "abstract": "While state-of-the-art neural network models continue to achieve lower perplexity scores on language modeling benchmarks, it remains unknown whether optimizing for broad-coverage predictive performance leads to human-like syntactic knowledge. Furthermore, existing work has not provided a clear picture about the model properties required to produce proper syntactic generalizations. We present a systematic evaluation of the syntactic knowledge of neural language models, testing 20 combinations of model types and data sizes on a set of 34 English-language syntactic test suites. We find substantial differences in syntactic generalization performance by model architecture, with sequential models underperforming other architectures. Factorially manipulating model architecture and training dataset size (1M-40M words), we find that variability in syntactic generalization performance is substantially greater by architecture than by dataset size for the corpora tested in our experiments. Our results also reveal a dissociation between perplexity and syntactic generalization performance.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.159",
  "title": "Inflecting When There's No Majority: Limitations of Encoder-Decoder Neural Networks as Cognitive Models for German Plurals",
  "year": 2020,
  "venue": "ACL",
  "abstract": "Can artificial neural networks learn to represent inflectional morphology and generalize to new words as human speakers do? Kirov and Cotterell (2018) argue that the answer is yes: modern Encoder-Decoder (ED) architectures learn human-like behavior when inflecting English verbs, such as extending the regular past tense form /-(e)d/ to novel words. However, their work does not address the criticism raised by Marcus et al. (1995): that neural models may learn to extend not the regular, but the most frequent class - and thus fail on tasks like German number inflection, where infrequent suffixes like /-s/ can still be productively generalized. To investigate this question, we first collect a new dataset from German speakers (production and ratings of plural forms for novel nouns) that is designed to avoid sources of information unavailable to the ED model. The speaker data show high variability, and two suffixes evince 'regular' behavior, appearing more often with phonologically atypical inputs. Encoder-decoder models do generalize the most frequently produced plural class, but do not show human-like variability or 'regular' extension of these other plural markers. We conclude that modern neural models may still struggle with minority-class generalization.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.179",
  "title": "Recurrent Neural Network Language Models Always Learn English-Like Relative Clause Attachment",
  "year": 2020,
  "venue": "ACL",
  "abstract": "A standard approach to evaluating language models analyzes how models assign probabilities to valid versus invalid syntactic constructions (i.e. is a grammatical sentence more probable than an ungrammatical sentence). Our work uses ambiguous relative clause attachment to extend such evaluations to cases of multiple simultaneous valid interpretations, where stark grammaticality differences are absent. We compare model performance in English and Spanish to show that non-linguistic biases in RNN LMs advantageously overlap with syntactic structure in English but not Spanish. Thus, English models may appear to acquire human-like syntactic preferences, while models trained on Spanish fail to acquire comparable human-like preferences. We conclude by relating these results to broader concerns about the relationship between comprehension (i.e. typical language model use cases) and production (which generates the training data for language models), suggesting that necessary linguistic biases are not present in the training signal at all.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.196",
  "title": "On Importance Sampling-Based Evaluation of Latent Language Models",
  "year": 2020,
  "venue": "ACL",
  "abstract": "Language models that use additional latent structures (e.g., syntax trees, coreference chains, knowledge graph links) provide several advantages over traditional language models. However, likelihood-based evaluation of these models is often intractable as it requires marginalizing over the latent space. Existing works avoid this issue by using importance sampling. Although this approach has asymptotic guarantees, analysis is rarely conducted on the effect of decisions such as sample size and choice of proposal distribution on the reported estimates. In this paper, we carry out this analysis for three models: RNNG, EntityNLM, and KGLM. In addition, we elucidate subtle differences in how importance sampling is applied in these works that can have substantial effects on the final estimates, as well as provide theoretical results which reinforce the validity of this technique.",
  "stance": -0.1
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.234",
  "title": "What is Learned in Visually Grounded Neural Syntax Acquisition",
  "year": 2020,
  "venue": "ACL",
  "abstract": "Visual features are a promising signal for learning bootstrap textual models. However, blackbox learning models make it difficult to isolate the specific contribution of visual components. In this analysis, we consider the case study of the Visually Grounded Neural Syntax Learner (Shi et al., 2019), a recent approach for learning syntax from a visual training signal. By constructing simplified versions of the model, we isolate the core factors that yield the model's strong performance. Contrary to what the model might be capable of learning, we find significantly less expressive versions produce similar predictions and perform just as well, or even better. We also find that a simple lexical signal of noun concreteness plays the main role in the model's predictions as opposed to more complex syntactic reasoning.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.337",
  "title": "Investigating Word-Class Distributions in Word Vector Spaces",
  "year": 2020,
  "venue": "ACL",
  "abstract": "This paper presents an investigation on the distribution of word vectors belonging to a certain word class in a pre-trained word vector space. To this end, we made several assumptions about the distribution, modeled the distribution accordingly, and validated each assumption by comparing the goodness of each model. Specifically, we considered two types of word classes - the semantic class of direct objects of a verb and the semantic class in a thesaurus - and tried to build models that properly estimate how likely it is that a word in the vector space is a member of a given word class. Our results on selectional preference and WordNet datasets show that the centroid-based model will fail to achieve good enough performance, the geometry of the distribution and the existence of subgroups will have limited impact, and also the negative instances need to be considered for adequate modeling of the distribution. We further investigated the relationship between the scores calculated by each model and the degree of membership and found that discriminative learning-based models are best in finding the boundaries of a class, while models based on the offset between positive and negative instances perform best in determining the degree of membership.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.36",
  "title": "Jointly Masked Sequence-to-Sequence Model for Non-Autoregressive Neural Machine Translation",
  "year": 2020,
  "venue": "ACL",
  "abstract": "The masked language model has received remarkable attention due to its effectiveness on various natural language processing tasks. However, few works have adopted this technique in the sequence-to-sequence models. In this work, we introduce a jointly masked sequence-to-sequence model and explore its application on non-autoregressive neural machine translation~(NAT). Specifically, we first empirically study the functionalities of the encoder and the decoder in NAT models, and find that the encoder takes a more important role than the decoder regarding the translation quality. Therefore, we propose to train the encoder more rigorously by masking the encoder input while training. As for the decoder, we propose to train it based on the consecutive masking of the decoder input with an $n$-gram loss function to alleviate the problem of translating duplicate words. The two types of masks are applied to the model jointly at the training stage. We conduct experiments on five benchmark machine translation tasks, and our model can achieve 27.69/32.24 BLEU scores on WMT14 English-German/German-English tasks with $5+$ times speed up compared with an autoregressive model.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.375",
  "title": "Do Neural Language Models Show Preferences for Syntactic Formalisms?",
  "year": 2020,
  "venue": "ACL",
  "abstract": "Recent work on the interpretability of deep neural language models has concluded that many properties of natural language syntax are encoded in their representational spaces. However, such studies often suffer from limited scope by focusing on a single language and a single linguistic formalism. In this study, we aim to investigate the extent to which the semblance of syntactic structure captured by language models adheres to a surface-syntactic or deep syntactic style of analysis, and whether the patterns are consistent across different languages. We apply a probe for extracting directed dependency trees to BERT and ELMo models trained on 13 different languages, probing for two different syntactic annotation styles: Universal Dependencies (UD), prioritizing deep syntactic relations, and Surface-Syntactic Universal Dependencies (SUD), focusing on surface structure. We find that both models exhibit a preference for UD over SUD - with interesting variations across languages and layers - and that the strength of this preference is correlated with differences in tree shape.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.384",
  "title": "Probing for Referential Information in Language Models",
  "year": 2020,
  "venue": "ACL",
  "abstract": "Language models keep track of complex information about the preceding context - including, e.g., syntactic relations in a sentence. We investigate whether they also capture information beneficial for resolving pronominal anaphora in English. We analyze two state of the art models with LSTM and Transformer architectures, via probe tasks and analysis on a coreference annotated corpus. The Transformer outperforms the LSTM in all analyses. Our results suggest that language models are more successful at learning grammatical constraints than they are at learning truly referential information, in the sense of capturing the fact that we use language to refer to entities in the world. However, we find traces of the latter aspect, too.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.408",
  "title": "ERASER: A Benchmark to Evaluate Rationalized NLP Models",
  "year": 2020,
  "venue": "ACL",
  "abstract": "State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more interpretable deep models for NLP that reveal the 'reasoning' behind model outputs. But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. We propose the Evaluating Rationales And Simple English Reasoning (ERASER a benchmark to advance research on interpretable models in NLP. This benchmark comprises multiple datasets and tasks for which human annotations of \"rationales\" (supporting evidence) have been collected. We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how faithful these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions). Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems. The benchmark, code, and documentation are available at https://www.eraserbenchmark.com/",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.424",
  "title": "ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations",
  "year": 2020,
  "venue": "ACL",
  "abstract": "In order to simplify a sentence, human editors perform multiple rewriting transformations: they split it into several shorter sentences, paraphrase words (i.e. replacing complex words or phrases by simpler synonyms), reorder components, and/or delete information deemed unnecessary. Despite these varied range of possible text alterations, current models for automatic sentence simplification are evaluated using datasets that are focused on a single transformation, such as lexical paraphrasing or splitting. This makes it impossible to understand the ability of simplification models in more realistic settings. To alleviate this limitation, this paper introduces ASSET, a new dataset for assessing sentence simplification in English. ASSET is a crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations. Through quantitative and qualitative experiments, we show that simplifications in ASSET are better at capturing characteristics of simplicity when compared to other standard evaluation datasets for the task. Furthermore, we motivate the need for developing better methods for automatic evaluation using ASSET, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.431",
  "title": "Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings",
  "year": 2020,
  "venue": "ACL",
  "abstract": "Contextualized representations (e.g. ELMo, BERT) have become the default pretrained representations for downstream NLP applications. In some settings, this transition has rendered their static embedding predecessors (e.g. Word2Vec, GloVe) obsolete. As a side-effect, we observe that older interpretability methods for static embeddings - while more diverse and mature than those available for their dynamic counterparts - are underutilized in studying newer contextualized representations. Consequently, we introduce simple and fully general methods for converting from contextualized representations to static lookup-table embeddings which we apply to 5 popular pretrained models and 9 sets of pretrained weights. Our analysis of the resulting static embeddings notably reveals that pooling over many contexts significantly improves representational quality under intrinsic evaluation. Complementary to analyzing representational quality, we consider social biases encoded in pretrained representations with respect to gender, race/ethnicity, and religion and find that bias is encoded disparately across pretrained models and internal layers even for models with the same training data. Concerningly, we find dramatic inconsistencies between social bias estimators for word embeddings.",
  "stance": -0.9
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.434",
  "title": "Spying on Your Neighbors: Fine-grained Probing of Contextual Embeddings for Information about Surrounding Words",
  "year": 2020,
  "venue": "ACL",
  "abstract": "Although models using contextual word embeddings have achieved state-of-the-art results on a host of NLP tasks, little is known about exactly what information these embeddings encode about the context words that they are understood to reflect. To address this question, we introduce a suite of probing tasks that enable fine-grained testing of contextual embeddings for encoding of information about surrounding words. We apply these tasks to examine the popular BERT, ELMo and GPT contextual encoders, and find that each of our tested information types is indeed encoded as contextual information across tokens, often with near-perfect recoverability-but the encoders vary in which features they distribute to which tokens, how nuanced their distributions are, and how robust the encoding of each feature is to distance. We discuss implications of these results for how different types of models break down and prioritize word-level context information when constructing token embeddings.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.441",
  "title": "Adversarial NLI: A New Benchmark for Natural Language Understanding",
  "year": 2020,
  "venue": "ACL",
  "abstract": "We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate.",
  "stance": -0.5
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.442",
  "title": "Beyond Accuracy: Behavioral Testing of NLP Models with CheckList",
  "year": 2020,
  "venue": "ACL",
  "abstract": "Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.465",
  "title": "How Can We Accelerate Progress Towards Human-like Linguistic Generalization?",
  "year": 2020,
  "venue": "ACL",
  "abstract": "This position paper describes and critiques the Pretraining-Agnostic Identically Distributed (PAID) evaluation paradigm, which has become a central tool for measuring progress in natural language understanding. This paradigm consists of three stages: (1) pre-training of a word prediction model on a corpus of arbitrary size; (2) fine-tuning (transfer learning) on a training set representing a classification task; (3) evaluation on a test set drawn from the same distribution as that training set. This paradigm favors simple, low-bias architectures, which, first, can be scaled to process vast amounts of data, and second, can capture the fine-grained statistical properties of a particular data set, regardless of whether those properties are likely to generalize to examples of the task outside the data set. This contrasts with humans, who learn language from several orders of magnitude less data than the systems favored by this evaluation paradigm, and generalize to new tasks in a consistent way. We advocate for supplementing or replacing PAID with paradigms that reward architectures that generalize as quickly and robustly as humans.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.467",
  "title": "Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?",
  "year": 2020,
  "venue": "ACL",
  "abstract": "While pretrained models such as BERT have shown large gains across natural language understanding tasks, their performance can be improved by further training the model on a data-rich intermediate task, before fine-tuning it on a target task. However, it is still poorly understood when and why intermediate-task training is beneficial for a given target task. To investigate this, we perform a large-scale study on the pretrained RoBERTa model with 110 intermediate-target task combinations. We further evaluate all trained models with 25 probing tasks meant to reveal the specific skills that drive transfer. We observe that intermediate tasks requiring high-level inference and reasoning abilities tend to work best. We also observe that target task performance is strongly correlated with higher-level abilities such as coreference resolution. However, we fail to observe more granular correlations between probing and target task performance, highlighting the need for further work on broad-coverage probing benchmarks. We also observe evidence that the forgetting of knowledge learned during pretraining may limit our analysis, highlighting the need for further work on transfer learning methods in these settings.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.485",
  "title": "Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP",
  "year": 2020,
  "venue": "ACL",
  "abstract": "We survey 146 papers analyzing \"bias\" in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing \"bias\" is an inherently normative process. We further find that these papers' proposed quantitative techniques for measuring or mitigating \"bias\" are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing \"bias\" in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of \"bias\"---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements-and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.486",
  "title": "Social Bias Frames: Reasoning about Social and Power Implications of Language",
  "year": 2020,
  "venue": "ACL",
  "abstract": "To address the limitation, in this paper, we propose a novel framework of initiative defense to degrade the performance of facial manipulation models controlled by malicious users. The basic idea is to actively inject imperceptible venom into target facial data before manipulation. To this end, we first imitate the target manipulation model with a surrogate model, and then devise a poison perturbation generator to obtain the desired venom. An alternating training strategy are further leveraged to train both the surrogate model and the pertur- bation generator. Two typical facial manipulation tasks: face attribute editing and face reenactment, are considered in our initiative defense framework. Extensive experiments demon- strate the effectiveness and robustness of our framework in different settings. Finally, we hope this work can shed some light on initiative countermeasures against more adversarial scenarios.",
  "stance": -0.3
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.495",
  "title": "Obtaining Faithful Interpretations from Compositional Neural Networks",
  "year": 2020,
  "venue": "ACL",
  "abstract": "Neural module networks (NMNs) are a popular approach for modeling compositionality: they achieve high accuracy when applied to problems in language and vision, while reflecting the compositional structure of the problem in the network architecture. However, prior work implicitly assumed that the structure of the network modules, describing the abstract reasoning process, provides a faithful explanation of the model's reasoning; that is, that all modules perform their intended behaviour. In this work, we propose and conduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2 and DROP, two datasets which require composing multiple reasoning steps. We find that the intermediate outputs differ from the expected output, illustrating that the network structure does not provide a faithful explanation of model behaviour. To remedy that, we train the model with auxiliary supervision and propose particular choices for module architecture that yield much better faithfulness, at a minimal cost to accuracy.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.586",
  "title": "Words Aren't Enough, Their Order Matters: On the Robustness of Grounding Visual Referring Expressions",
  "year": 2020,
  "venue": "ACL",
  "abstract": "Visual referring expression recognition is a challenging task that requires natural language understanding in the context of an image. We critically examine RefCOCOg, a standard benchmark for this task, using a human study and show that 83.7% of test instances do not require reasoning on linguistic structure, i.e., words are enough to identify the target object, the word order doesn't matter. To measure the true progress of existing models, we split the test set into two sets, one which requires reasoning on linguistic structure and the other which doesn't. Additionally, we create an out-of-distribution dataset Ref-Adv by asking crowdworkers to perturb in-domain examples such that the target object changes. Using these datasets, we empirically show that existing methods fail to exploit linguistic structure and are 12% to 23% lower in performance than the established progress for this task. We also propose two methods, one based on contrastive learning and the other based on multi-task learning, to increase the robustness of ViLBERT, the current state-of-the-art model for this task. Our datasets are publicly available at https://github.com/aws/aws-refcocog-adv.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.608",
  "title": "Unsupervised Dual Paraphrasing for Two-stage Semantic Parsing",
  "year": 2020,
  "venue": "ACL",
  "abstract": "One daunting problem for semantic parsing is the scarcity of annotation. Aiming to reduce nontrivial human labor, we propose a two-stage semantic parsing framework, where the first stage utilizes an unsupervised paraphrase model to convert an unlabeled natural language utterance into the canonical utterance. The downstream naive semantic parser accepts the intermediate output and returns the target logical form. Furthermore, the entire training process is split into two phases: pre-training and cycle learning. Three tailored self-supervised tasks are introduced throughout training to activate the unsupervised paraphrase model. Experimental results on benchmarks Overnight and GeoGranno demonstrate that our framework is effective and compatible with supervised training.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.633",
  "title": "Out of the Echo Chamber: Detecting Countering Debate Speeches",
  "year": 2020,
  "venue": "ACL",
  "abstract": "An educated and informed consumption of media content has become a challenge in modern times. With the shift from traditional news outlets to social media and similar venues, a major concern is that readers are becoming encapsulated in \"echo chambers\" and may fall prey to fake news and disinformation, lacking easy access to dissenting views. We suggest a novel task aiming to alleviate some of these concerns - that of detecting articles that most effectively counter the arguments - and not just the stance - made in a given text. We study this problem in the context of debate speeches. Given such a speech, we aim to identify, from among a set of speeches on the same topic and with an opposing stance, the ones that directly counter it. We provide a large dataset of 3,685 such speeches (in English), annotated for this relation, which hopefully would be of general interest to the NLP community. We explore several algorithms addressing this task, and while some are successful, all fall short of expert human performance, suggesting room for further research. All data collected during this work is freely available for research.",
  "stance": -0.5
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.635",
  "title": "KdConv: A Chinese Multi-domain Dialogue Dataset Towards Multi-turn Knowledge-driven Conversation",
  "year": 2020,
  "venue": "ACL",
  "abstract": "The research of knowledge-driven conversational systems is largely limited due to the lack of dialog data which consists of multi-turn conversations on multiple topics and with knowledge annotations. In this paper, we propose a Chinese multi-domain knowledge-driven conversation dataset, KdConv, which grounds the topics in multi-turn conversations to knowledge graphs. Our corpus contains 4.5K conversations from three domains (film, music, and travel), and 86K utterances with an average turn number of 19.0. These conversations contain in-depth discussions on related topics and natural transition between multiple topics. To facilitate the following research on this corpus, we provide several benchmark models. Comparative results show that the models can be enhanced by introducing background knowledge, yet there is still a large space for leveraging knowledge to model multi-turn conversations for further research. Results also show that there are obvious performance differences between different domains, indicating that it is worth further explore transfer learning and domain adaptation. The corpus and benchmark models are publicly available.",
  "stance": -0.5
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.650",
  "title": "Semi-supervised Contextual Historical Text Normalization",
  "year": 2020,
  "venue": "ACL",
  "abstract": "Historical text normalization, the task of mapping historical word forms to their modern counterparts, has recently attracted a lot of interest (Bollmann, 2019; Tang et al., 2018; Lusetti et al., 2018; Bollmann et al., 2018;Robertson and Goldwater, 2018; Bollmannet al., 2017; Korchagina, 2017). Yet, virtually all approaches suffer from the two limitations: 1) They consider a fully supervised setup, often with impractically large manually normalized datasets; 2) Normalization happens on words in isolation. By utilizing a simple generative normalization model and obtaining powerful contextualization from the target-side language model, we train accurate models with unlabeled historical data. In realistic training scenarios, our approach often leads to reduction in manually normalized data at the same accuracy levels.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.658",
  "title": "A Call for More Rigor in Unsupervised Cross-lingual Learning",
  "year": 2020,
  "venue": "ACL",
  "abstract": "We review motivations, definition, approaches, and methodology for unsupervised cross-lingual learning and call for a more rigorous position in each of them. An existing rationale for such research is based on the lack of parallel data for many of the world's languages. However, we argue that a scenario without any parallel data and abundant monolingual data is unrealistic in practice. We also discuss different training signals that have been used in previous work, which depart from the pure unsupervised setting. We then describe common methodological issues in tuning and evaluation of unsupervised cross-lingual models and present best practices. Finally, we provide a unified outlook for different types of research in this area (i.e., cross-lingual word embeddings, deep multilingual pretraining, and unsupervised machine translation) and argue for comparable evaluation of these models.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.671",
  "title": "Contrastive Self-Supervised Learning for Commonsense Reasoning",
  "year": 2020,
  "venue": "ACL",
  "abstract": "We propose a self-supervised method to solve Pronoun Disambiguation and Winograd Schema Challenge problems. Our approach exploits the characteristic structure of training corpora related to so-called \"trigger\" words, which are responsible for flipping the answer in pronoun disambiguation. We achieve such commonsense reasoning by constructing pair-wise contrastive auxiliary predictions. To this end, we leverage a mutual exclusive loss regularized by a contrastive margin. Our architecture is based on the recently introduced transformer networks, BERT, that exhibits strong performance on many NLP benchmarks. Empirical results show that our method alleviates the limitation of current supervised approaches for commonsense reasoning. This study opens up avenues for exploiting inexpensive self-supervision to achieve performance gain in commonsense reasoning tasks.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.698",
  "title": "Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly",
  "year": 2020,
  "venue": "ACL",
  "abstract": "Building on Petroni et al. 2019, we propose two new probing tasks analyzing factual knowledge stored in Pretrained Language Models (PLMs). (1) Negation. We find that PLMs do not distinguish between negated (''Birds cannot [MASK]\") and non-negated (''Birds can [MASK]\") cloze questions. (2) Mispriming. Inspired by priming methods in human psychology, we add \"misprimes\" to cloze questions (''Talk? Birds can [MASK]\"). We find that PLMs are easily distracted by misprimes. These results suggest that PLMs still have a long way to go to adequately learn human-like factual knowledge.",
  "stance": -0.9
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.730",
  "title": "TVQA+: Spatio-Temporal Grounding for Video Question Answering",
  "year": 2020,
  "venue": "ACL",
  "abstract": "We present the task of Spatio-Temporal Video Question Answering, which requires intelligent systems to simultaneously retrieve relevant moments and detect referenced visual concepts (people and objects) to answer natural language questions about videos. We first augment the TVQA dataset with 310.8K bounding boxes, linking depicted objects to visual concepts in questions and answers. We name this augmented version as TVQA+. We then propose Spatio-Temporal Answerer with Grounded Evidence (STAGE), a unified framework that grounds evidence in both spatial and temporal domains to answer questions about videos. Comprehensive experiments and analyses demonstrate the effectiveness of our framework and how the rich annotations in our TVQA+ dataset can contribute to the question answering task. Moreover, by performing this joint task, our model is able to produce insightful and interpretable spatio-temporal attention visualizations.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.766",
  "title": "Should All Cross-Lingual Embeddings Speak English?",
  "year": 2020,
  "venue": "ACL",
  "abstract": "Most of recent work in cross-lingual word embeddings is severely Anglocentric. The vast majority of lexicon induction evaluation dictionaries are between English and another language, and the English embedding space is selected by default as the hub when learning in a multilingual setting. With this work, however, we challenge these practices. First, we show that the choice of hub language can significantly impact downstream lexicon induction zero-shot POS tagging performance. Second, we both expand a standard English-centered evaluation dictionary collection to include all language pairs using triangulation, and create new dictionaries for under-represented languages. Evaluating established methods over all these language pairs sheds light into their suitability for aligning embeddings from distant languages and presents new challenges for the field. Finally, in our analysis we identify general guidelines for strong cross-lingual embedding baselines, that extend to language pairs that do not include English.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.768",
  "title": "Are Natural Language Inference Models IMPPRESsive? Learning IMPlicature and PRESupposition",
  "year": 2020,
  "venue": "ACL",
  "abstract": "Natural language inference (NLI) is an increasingly important task for natural language understanding, which requires one to infer whether a sentence entails another. However, the ability of NLI models to make pragmatic inferences remains understudied. We create an IMPlicature and PRESupposition diagnostic dataset (IMPPRES), consisting of 32K semi-automatically generated sentence pairs illustrating well-studied pragmatic inference types. We use IMPPRES to evaluate whether BERT, InferSent, and BOW NLI models trained on MultiNLI (Williams et al., 2018) learn to make pragmatic inferences. Although MultiNLI appears to contain very few pairs illustrating these inference types, we find that BERT learns to draw pragmatic inferences. It reliably treats scalar implicatures triggered by \"some\" as entailments. For some presupposition triggers like \"only\", BERT reliably recognizes the presupposition as an entailment, even when the trigger is embedded under an entailment canceling operator like negation. BOW and InferSent show weaker evidence of pragmatic reasoning. We conclude that NLI training encourages models to learn some, but not all, pragmatic inferences.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/J90-3001",
  "title": "Resolving Quasi Logical Forms",
  "year": 1990,
  "venue": "CL",
  "abstract": "The paper describes intermediate and resolved logical form representations of sentences involving referring expressions and a reference resolution process for mapping between these representations. The intermediate representation, Quasi Logical Form (or QLF), may contain unresolved terms corresponding to anaphoric noun phrases covering bound variable anaphora, reflexives, and definite descriptions. Implict relations arising in constructs such as compound nominals appear in QLF as unresolved formulae. The QLF representation is also neutral with respect to ambiguities corresponding to quantifier scope and the collective/distributive distinction, the latter being treated as quantifier resolution. Reference candidates are proposed according to an ordered set of \"reference resolution rules\" producing possible resolved logical forms to which linguistic and pragmatic constraints are then applied.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/J91-1003",
  "title": "met*: A Method for Discriminating Metonymy and Metaphor by Computer",
  "year": 1991,
  "venue": "CL",
  "abstract": "The met* method distinguishes selected examples of metonymy from metaphor and from literalness and anomaly in short English sentences. In the met* method, literalness is distinguished because it satisfies contextual constraints that the nonliteral others all violate. Metonymy is discriminated from metaphor and anomaly in a way that [1] supports Lakoff and Johnson's (1980) view that in metonymy one entity stands for another whereas in metaphor one entity is viewed as another, [2] permits chains of metonymies (Reddy 1979), and [3] allows metonymies to co-occur with instances of either literalness, metaphor, or anomaly. Metaphor is distinguished from anomaly because the former contains a relevant analogy, unlike the latter. The met* method is part of Collative Semantics, a semantics for natural language processing, and has been implemented in a computer program called meta5. Some examples of meta5's analysis of metaphor and metonymy are given. The met* method is compared with approaches from artificial intelligence, linguistics, philosophy, and psychology.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J91-3002",
  "title": "Chinese Number-Names, Tree Adjoining Languages, and Mild Context-Sensitivity",
  "year": 1991,
  "venue": "CL",
  "abstract": "The Tree Adjoining Grammar formalism, both its singleas well as multiple-component versions, has recently received attention as a basis for the description and explication of natural language. We show in this paper that the number-name system of Chinese is generated neither by this formalism nor by any other equivalent or weaker ones, suggesting that such a task might require the use of the more powerful Indexed Grammar formalism. Given that our formal results apply only to a proper subset of Chinese, we extensively discuss the issue of whether they have any implications for the whole of that natural language. We conclude that our results bear directly either on the syntax of Chinese or on the interface between Chinese and the cognitive component responsible for arithmetic reasoning. Consequently, either Tree Adjoining Grammars, as currently defined, fail to generate the class of natural languages in a way that discriminates between linguistically warranted sublanguages, or formalisms with generative power equivalent to Tree Adjoining Grammar cannot serve as a basis for the interface between the human linguistic and mathematical faculties.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/J94-1002",
  "title": "A Hierarchical Stochastic Model for Automatic Prediction of Prosodic Boundary Location",
  "year": 1994,
  "venue": "CL",
  "abstract": "Prosodic phrase structure provides important information for the understanding and naturalness of synthetic speech, and a good model of prosodic phrases has applications in both speech synthesis and speech understanding. This work describes a statistical model of an embedded hierarchy of prosodic phrase structure, motivated by results in linguistic theory. Each level of the hierarchy is modeled as a sequence of subunits at the next level, with the lowest level of the hierarchy representing factors such as syntactic branching and prosodic constituent length using a binary tree classification. A maximum likelihood solution for parameter estimation is presented, allowing automatic training of different speaking styles. For predicting prosodic phrase breaks from text, a dynamic programming algorithm is given for finding the maximum probability prosodic parse. Experimental results on a corpus of radio news demonstrate a high rate of success for predicting major and minor phrase boundaries from text without syntactic information (81% correct prediction with 4% false prediction).",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/J95-2002",
  "title": "An Efficient Probabilistic Context-Free Parsing Algorithm that Computes Prefix Probabilities",
  "year": 1995,
  "venue": "CL",
  "abstract": "We describe an extension of Earley's parser for stochastic context-free grammars that computes the following quantities given a stochastic context-free grammar and an input string: a) probabilities of successive prefixes being generated by the grammar; b) probabilities of substrings being generated by the nonterminals, including the entire string being generated by the grammar; c) most likely (Viterbi) parse of the string; d) posterior expected number of applications of each grammar production, as required for reestimating rule probabilities. Probabilities (a) and (b) are computed incrementally in a single left-to-right pass over the input. Our algorithm compares favorably to standard bottom-up parsing methods for SCFGs in that it works efficiently on sparse grammars by making use of Earley's top-down control structure. It can process any context-free rule format without conversion to some normal form, and combines computations for (a) through (d) in a single algorithm. Finally, the algorithm has simple extensions for processing partially bracketed inputs, and for finding partial parses and their likelihoods on ungrammatical inputs.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/J96-1004",
  "title": "From Conceptual Time to Linguistic Time",
  "year": 1996,
  "venue": "CL",
  "abstract": "In this paper, we present a method for generating French texts conveying temporal information that integrates Discourse Representation Theory (DRT) and Systemic Grammar Theory. DRT is used to represent temporal information and an intermediate semantic level for the temporal localization expressed by temporal adverbial phrases and verb phrases. This representation is then translated into a syntactic form using Systemic Grammar Theory. We have implemented this method in a working prototype called Prdtexte.",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/J96-2004",
  "title": "Assessing Agreement on Classification Tasks: The Kappa Statistic",
  "year": 1996,
  "venue": "CL",
  "abstract": "Currently, computational linguists and cognitive scientists working in the area of discourse and dialogue argue that their subjective judgments are reliable using several different statistics, none of which are easily interpretable or comparable to each other. Meanwhile, researchers in content analysis have already experienced the same difficulties and come up with a solution in the kappa statistic. We discuss what is wrong with reliability measures as they are currently used for discourse and dialogue work in computational linguistics and cognitive science, and argue that we would be better off as afield adopting techniques from content analysis.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/J97-3006",
  "title": "Current theories of centering for pronoun interpretation: a critical evaluation",
  "year": 1997,
  "venue": "CL",
  "abstract": "We review the fundamental concepts of centering theory and discuss some facets of the pronoun interpretation problem that motivate a centering-style analysis. We then demonstrate some problems with a popular centering-based approach with respect to these motivations.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J98-1002",
  "title": "Similarity-based Word Sense Disambiguation",
  "year": 1998,
  "venue": "CL",
  "abstract": "We describe a method for automatic word sense disambiguation using a text corpus and a machinereadable dictionary (MRD). The method is based on word similarity and context similarity measures. Words are considered similar if they appear in similar contexts; contexts are similar if they contain similar words. The circularity of this definition is resolved by an iterative, converging process, in which the system learns from the corpus a set of typical usages for each of the senses of the polysemous word listed in the MRD. A new instance of a polysemous word is assigned the sense associated with the typical usage most similar to its context. Experiments show that this method can learn even from very sparse training data, achieving over 92% correct disambiguation performance.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/J98-1004",
  "title": "Automatic Word Sense Discrimination",
  "year": 1998,
  "venue": "CL",
  "abstract": "This paper presents context-group discrimination, a disambiguation algorithm based on clustering. Senses are interpreted as groups (or clusters) of similar contexts of the ambiguous word. Words, contexts, and senses are represented in Word Space, a high-dimensional, real-valued space in which closeness corresponds to semantic similarity. Similarity in Word Space is based on second-order co-occurrence: two tokens (or contexts) of the ambiguous word are assigned to the same sense cluster if the words they co-occur with in turn occur with similar words in a training corpus. The algorithm is automatic and unsupervised in both training and application: senses are induced from a corpus without labeled training instances or other external knowledge sources. The paper demonstrates good performance of context-group discrimination for a sample of natural and artificial ambiguous words.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/J98-2002",
  "title": "Generalizing Case Frames Using a Thesaurus and the MDL Principle",
  "year": 1998,
  "venue": "CL",
  "abstract": "A new method for automatically acquiring case frame patterns from large corpora is proposed. In particular, the problem of generalizing values of a case frame slot for a verb is viewed as that of estimating a conditional probability distribution over a partition of words, and a new generalization method based on the Minimum Description Length (MDL) principle is proposed. In order to assist with efficiency, the proposed method makes use of an existing thesaurus and restricts its attention to those partitions that are present as \"cuts\" in the thesaurus tree, thus reducing the generalization problem to that of estimating a \"tree cut model\" of the thesaurus tree. An efficient algorithm is given, which provably obtains the optimal tree cut model for the given frequency data of a case slot, in the sense of MDL. Case frame patterns obtained by the method were used to resolve PP-attachment ambiguity. Experimental results indicate that the proposed method improves upon or is at least comparable with existing methods.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/J98-4002",
  "title": "Selective Sampling for Example-based Word Sense Disambiguation",
  "year": 1998,
  "venue": "CL",
  "abstract": "This paper proposes an efficient example sampling method for example-based word sense disambiguation systems. To construct a database of practical size, a considerable overhead for manual sense disambiguation (overhead for supervision) is required. In addition, the time complexity of searching a large-sized database poses a considerable problem (overhead for search). To counter these problems, our method selectively samples a smaller-sized effective subset from a given example set for use in word sense disambiguation. Our method is characterized by the reliance on the notion of training utility: the degree to which each example is informative for future example sampling when used for the training of the system. The system progressively collects examples by selecting those with greatest utility. The paper reports the effectiveness of our method through experiments on about one thousand sentences. Compared to experiments with other example sampling methods, our method reduced both the overhead for supervision and the overhead for search, without the degeneration of the performance of the system.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/J98-4005",
  "title": "Letter to the Editor: Clues from the Depth Hypothesis: A Reply to Geoffrey Sampson's Review",
  "year": 1998,
  "venue": "CL",
  "abstract": "In linguistics it has not been possible to use the standard criteria and assumptions of science because the ancients placed our discipline not in the physical domain but in the logical domain where concepts and theories do not represent parts of the natural world. Many of the problems facing linguistics follow inevitably, for example the difficulties that linguistics experiences in agreeing on grammatical theory. One symptom is the long-standing difficulty in testing the depth hypothesis, which came out of early MT research. Sampson (1997) attempted recently to test the depth hypothesis by a computer analysis of a grammatically annotated corpus of English. It is shown that this attempted test and his attempt at defending the testability of the dept h hypothesis are invalid. But clues from the depth hypothesis have led to new foundations for general linguistics put forth in the book (Yngve 1996) that Sampson (1998) reviewed. This work reconstitutes linguistics in the physical domain where the criteria and assumptions of science can be applied. Sampson's review of this book contains a number of serious errors and inaccuracies.",
  "stance": -0.5
 },
 {
  "url": "https://aclanthology.org/J99-4006",
  "title": "Conceptions of limited attention and discourse focus",
  "year": 1999,
  "venue": "CL",
  "abstract": "Walker (1996) presents a cache model of the operation of attention in the processing of discourse as an alternative to the focus space stack that was proposed previously by Grosz and Sidner (Grosz 1977a; Grosz and Sidner 1986). In this squib, we present a critical analysis of the cache model and of Walker's supporting evidence from anaphora in discourses with interruptions and from informationally redundant utterances. We argue that the cache model is underdetermined in several ways that are crucial to a comparison of the two models and conclude that Walker has not established the superiority of the cache model. We also argue that psycholinguistic evidence does not support the cache model over the focus stack model.",
  "stance": -0.9
 },
 {
  "url": "https://aclanthology.org/J00-3001",
  "title": "Extracting the lowest-frequency words: pitfalls and possibilities",
  "year": 2000,
  "venue": "CL",
  "abstract": "In a medical information extraction system, we use common word association techniques to extract side-effect-related terms. Many of these terms have afrequency of less than five. Standard word-association-based applications disregard the lowest-frequency words, and hence disregard useful information. We therefore devised an extraction system for the full word frequency range. This system computes the significance of association by the log-likelihood ratio and Fisher's exact test. The output of the system shows a recurrent, corpus-independent pattern in both recall and the number of significant words. We will explain these patterns by the statistical behavior of the lowest-frequency words. We used Dutch verb-particle combinations as a second and independent collocation extraction application to illustrate the generality of the observed phenomena. We will conclude that a) word-association-based extraction systems can be enhanced by also considering the lowest-frequency words, b) significance levels should not befixed but adjusted for the optimal window size, c) hapax legomena, words occurring only once, should be disregarded a priori in the statistical analysis, and d) the distribution of the targets to extract should be considered in combination with the extraction method.",
  "stance": 0.1
 },
 {
  "url": "https://aclanthology.org/J01-3001",
  "title": "The Interaction of Knowledge Sources in Word Sense Disambiguation",
  "year": 2001,
  "venue": "CL",
  "abstract": "Word sense disambiguation (WSD) is a computational linguistics task likely to benefit from the tradition of combining different knowledge sources in artificial intelligence research. An important step in the exploration of this hypothesis is to determine which linguistic knowledge sources are most useful and whether their combination leads to improved results. We present a sense tagger which uses several knowledge sources. Tested accuracy exceeds 94% on our evaluation corpus. Our system attempts to disambiguate all content words in running text rather than limiting itself to treating a restricted vocabulary of words. It is argued that this approach is more likely to assist the creation of practical systems.",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/J07-1001",
  "title": "Letter to the Editor",
  "year": 2007,
  "venue": "CL",
  "abstract": "In her review of our book Memory-based Language Processing, which appeared in Computational Linguistics, Issue 32(4), Sandra Kübler comments on the illegibility of the phonetic font. Unfortunately, she had an early copy; the book was reprinted as soon as this problem was noticed, and Cambridge University Press has offered to replace any remaining misprinted copies with reprints upon request. This applies to copies with incorrectly overlapping phonetic symbols on pages 28, 59–63, and 108. They may be sent to Helen Barton, 3rd Floor, The Edinburgh Building, Cambridge University Press, Shaftesbury Road, Cambridge, UK, CB2 2RU. Postage will also be reimbursed if a receipt for the cost is provided.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J08-4010",
  "title": "Erratum: Dependency Parsing of Turkish",
  "year": 2008,
  "venue": "CL",
  "abstract": "In Section 5 of the article “Dependency Parsing of Turkish” by Gülşen Eryiğit, Joakim Nivre, and Kemal Oflazer (September 2008, Vol. 34, No. 3: 357–389), some abbreviations were misinterpreted during the copyediting process. The third sentence of Section 5.2 should be as follows: “We use an unlexicalized feature model where the parser uses only the minor part-of-speech category (as POS) and dependency type of tokens (as DEP) and compare the results with the probabilistic parser.” The first sentence of the second paragraph of Section 5.2.1 should start as follows: “We take the minor part-of-speech category. . . .” The “POS” abbreviation used on page 20 should be read as “minor part-ofspeech,” and the “POS” abbreviations on pages 21, 26, 27, and 28 should be read as “part-of-speech.”",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/J09-1005",
  "title": "Unsupervised Type and Token Identification of Idiomatic Expressions",
  "year": 2009,
  "venue": "CL",
  "abstract": "Idiomatic expressions are plentiful in everyday language, yet they remain mysterious, as it is not clear exactly how people learn and understand them. They are of special interest to linguists, psycholinguists, and lexicographers, mainly because of their syntactic and semantic idiosyncrasies as well as their unclear lexical status. Despite a great deal of research on the properties of idioms in the linguistics literature, there is not much agreement on which properties are characteristic of these expressions. Because of their peculiarities, idiomatic expressions have mostly been overlooked by researchers in computational linguistics. In this article, we look into the usefulness of some of the identified linguistic properties of idioms for their automatic recognition. Specifically, we develop statistical measures that each model a specific property of idiomatic expressions by looking at their actual usage patterns in text. We use these statistical measures in a type-based classification task where we automatically separate idiomatic expressions (expressions with a possible idiomatic interpretation) from similar-on-the-surface literal phrases (for which no idiomatic interpretation is possible). In addition, we use some of the measures in a token identification task where we distinguish idiomatic and literal usages of potentially idiomatic expressions in context.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/J09-4008",
  "title": "An Investigation into the Validity of Some Metrics for Automatically Evaluating Natural Language Generation Systems",
  "year": 2009,
  "venue": "CL",
  "abstract": "There is growing interest in using automatically computed corpus-based evaluation metrics to evaluate Natural Language Generation (NLG) systems, because these are often considerably cheaper than the human-based evaluations which have traditionally been used in NLG. We review previous work on NLG evaluation and on validation of automatic metrics in NLP, and then present the results of two studies of how well some metrics which are popular in other areas of NLP (notably BLEU and ROUGE) correlate with human judgments in the domain of computer-generated weather forecasts. Our results suggest that, at least in this domain, metrics may provide a useful measure of language quality, although the evidence for this is not as strong as we would ideally like to see; however, they do not provide a useful measure of content quality. We also discuss a number of caveats which must be kept in mind when interpreting this and other validation studies.",
  "stance": -0.5
 },
 {
  "url": "https://aclanthology.org/J12-1006",
  "title": "Computational Generation of Referring Expressions: A Survey",
  "year": 2012,
  "venue": "CL",
  "abstract": "This article offers a survey of computational research on referring expression generation (REG). It introduces the REG problem and describes early work in this area, discussing what basic assumptions lie behind it, and showing how its remit has widened in recent years. We discuss computational frameworks underlying REG, and demonstrate a recent trend that seeks to link REG algorithms with well-established Knowledge Representation techniques. Considerable attention is given to recent efforts at evaluating REG algorithms and the lessons that they allow us to learn. The article concludes with a discussion of the way forward in REG, focusing on references in larger and more realistic settings.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J12-2001",
  "title": "Modality and Negation: An Introduction to the Special Issue",
  "year": 2012,
  "venue": "CL",
  "abstract": "objects can be assertions, beliefs, facts, or eventualities. Discourse connectives and their arguments are assigned attribution-related features (Prasad et al. 2006) such as SOURCE (writer, other, arbitrary), TYPE (reflecting the nature of the relation between the agent and the abstract object), SCOPAL POLARITY of attribution, and DETERMINACY (indicating the presence of contexts canceling the entailment of attribution). The text spans signaling the attribution are also marked. Prasad et al. (2006) report that 34% of the discourse relations have some non-writer agent. SCOPAL POLARITY is annotated to identify cases when verbs of attribution (say, think, ...) are negated syntactically",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J12-2003",
  "title": "Did It Happen? The Pragmatic Complexity of Veridicality Assessment",
  "year": 2012,
  "venue": "CL",
  "abstract": "Natural language understanding depends heavily on assessing veridicality—whether events mentioned in a text are viewed as happening or not—but little consideration is given to this property in current relation and event extraction systems. Furthermore, the work that has been done has generally assumed that veridicality can be captured by lexical semantic properties whereas we show that context and world knowledge play a significant role in shaping veridicality. We extend the FactBank corpus, which contains semantically driven veridicality annotations, with pragmatically informed ones. Our annotations are more complex than the lexical assumption predicts but systematic enough to be included in computational work on textual understanding. They also indicate that veridicality judgments are not always categorical, and should therefore be modeled as distributions. We build a classifier to automatically assign event veridicality distributions based on our new annotations. The classifier relies not only on lexical features like hedges or negations, but also on structural features and approximations of world knowledge, thereby providing a nuanced picture of the diverse factors that shape veridicality.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/J12-2006",
  "title": "Modality and Negation in SIMT Use of Modality and Negation in Semantically-Informed Syntactic MT",
  "year": 2012,
  "venue": "CL",
  "abstract": "∗ U.S. Department of Defense, 9800 Savage Rd., Suite 6811, Fort Meade, MD 20755. E-mail: kathrynlb@gmail.com. ∗∗ Center for Advanced Study of Language, University of Maryland, 7005 52nd Avenue, College Park, MD 20742. E-mail: meb@umd.edu. † Department of Computer Science and UMIACS, University of Maryland, AV Williams Building 3153, College Park, MD 20742. E-mail: bonnie@umiacs.umd.edu. ‡ Center for Language and Speech Processing, Johns Hopkins University, 3400 N. Charles Street, Hackerman Hall 320, Baltimore MD 21218. E-mail: {ccb,nwf}@cs.jhu.edu. § Applied Physics Laboratory, Johns Hopkins University, 11000 Johns Hopkins Rd., Laurel, MD 20723. E-mail: christine.piatko@jhuapl.edu. || Carnegie Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213. E-mail: lsl@cs.cmu.edu. # BNN Technologies, 10 Moulton Street, Cambridge, MA 02138. E-mail: smiller@bbn.com.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J12-3006",
  "title": "Tree-Adjoining Grammars Are Not Closed Under Strong Lexicalization",
  "year": 2012,
  "venue": "CL",
  "abstract": "A lexicalized tree-adjoining grammar is a tree-adjoining grammar where each elementary tree contains some overt lexical item. Such grammars are being used to give lexical accounts of syntactic phenomena, where an elementary tree defines the domain of locality of the syntactic and semantic dependencies of its lexical items. It has been claimed in the literature that for every tree-adjoining grammar, one can construct a strongly equivalent lexicalized version. We show that such a procedure does not exist: Tree-adjoining grammars are not closed under strong lexicalization.",
  "stance": -0.1
 },
 {
  "url": "https://aclanthology.org/J12-3007",
  "title": "A Scalable Distributed Syntactic, Semantic, and Lexical Language Model",
  "year": 2012,
  "venue": "CL",
  "abstract": "This paper presents an attempt at building a large scale distributed composite language model that is formed by seamlessly integrating an n-gram model, a structured language model, and probabilistic latent semantic analysis under a directed Markov random field paradigm to simultaneously account for local word lexical information, mid-range sentence syntactic structure, and long-span document semantic content. The composite language model has been trained by performing a convergent N-best list approximate EM algorithm and a follow-up EM algorithm to improve word prediction power on corpora with up to a billion tokens and stored on a supercomputer. The large scale distributed composite language model gives drastic perplexity reduction over n-grams and achieves significantly better translation quality measured by the Bleu score and “readability” of translations when applied to the task of re-ranking the N-best list from a state-of-the-art parsing-based machine translation system.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/J14-3007",
  "title": "Improved Estimation of Entropy for Evaluation of Word Sense Induction",
  "year": 2014,
  "venue": "CL",
  "abstract": "Information-theoretic measures are among the most standard techniques for evaluation of clustering methods including word sense induction (WSI) systems. Such measures rely on sample-based estimates of the entropy. However, the standard maximum likelihood estimates of the entropy are heavily biased with the bias dependent on, among other things, the number of clusters and the sample size. This makes the measures unreliable and unfair when the number of clusters produced by different systems vary and the sample size is not exceedingly large. This corresponds exactly to the setting of WSI evaluation where a ground-truth cluster sense number arguably does not exist and the standard evaluation scenarios use a small number of instances of each word to compute the score. We describe more accurate entropy estimators and analyze their performance both in simulations and on evaluation of WSI systems.",
  "stance": 0.1
 },
 {
  "url": "https://aclanthology.org/J15-4002",
  "title": "Design and Evaluation of Metaphor Processing Systems",
  "year": 2015,
  "venue": "CL",
  "abstract": "ness–concreteness algorithm. However, the evaluation was done on only five",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/J16-2001",
  "title": "Surveys: A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena",
  "year": 2016,
  "venue": "CL",
  "abstract": "Word reordering is one of the most difficult aspects of statistical machine translation (SMT), and an important factor of its quality and efficiency. Despite the vast amount of research published to date, the interest of the community in this problem has not decreased, and no single method appears to be strongly dominant across language pairs. Instead, the choice of the optimal approach for a new translation task still seems to be mostly driven by empirical trials. To orient the reader in this vast and complex research area, we present a comprehensive survey of word reordering viewed as a statistical modeling challenge and as a natural language phenomenon. The survey describes in detail how word reordering is modeled within different string-based and tree-based SMT frameworks and as a stand-alone task, including systematic overviews of the literature in advanced reordering modeling. We then question why some approaches are more successful than others in different language pairs. We argue that besides measuring the amount of reordering, it is important to understand which kinds of reordering occur in a given language pair. To this end, we conduct a qualitative analysis of word reordering phenomena in a diverse sample of language pairs, based on a large collection of linguistic knowledge. Empirical results in the SMT literature are shown to support the hypothesis that a few linguistic facts can be very useful to anticipate the reordering characteristics of a language pair and to select the SMT framework that best suits them.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J16-4013",
  "title": "Reviewers for Volume 42",
  "year": 2016,
  "venue": "CL",
  "abstract": "This journal has a knowledgeable and hard-working editorial board, listed on the journal’s Web site, but for most submissions we also enlist the aid of specialist reviewers outside the editorial board. The Editor of Computational Linguistics would like to express her gratitude to the external reviewers listed here, who anonymously reviewed papers for the journal during the preparation of this volume (Volume 42). Their generosity, judicious judgment, and prompt response substantially helped us to publish a journal that both is timely and maintains exacting scientific standards; it is a genuine pleasure to thank them collectively for their dedicated service.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J18-3002",
  "title": "A Structured Review of the Validity of BLEU",
  "year": 2018,
  "venue": "CL",
  "abstract": "The BLEU metric has been widely used in NLP for over 15 years to evaluate NLP systems, especially in machine translation and natural language generation. I present a structured review of the evidence on whether BLEU is a valid evaluation technique-in other words, whether BLEU scores correlate with real-world utility and user-satisfaction of NLP systems; this review covers 284 correlations reported in 34 papers. Overall, the evidence supports using BLEU for diagnostic evaluation of MT systems (which is what it was originally proposed for), but does not support using BLEU outside of MT, for evaluation of individual texts, or for scientific hypothesis testing.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/2020.cl-2.1",
  "title": "Multilingual and Interlingual Semantic Representations for Natural Language Processing: A Brief Introduction",
  "year": 2020,
  "venue": "CL",
  "abstract": "We introduce the Computational Linguistics special issue on Multilingual and Interlingual Semantic Representations for Natural Language Processing. We situate the special issue's five articles in the context of our fast-changing field, explaining our motivation for this project. We offer a brief summary of the work in the issue, which includes developments on lexical and sentential semantic representations, from symbolic and neural perspectives.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P84-1054",
  "title": "On Parsing Preferences",
  "year": 1984,
  "venue": "COLING",
  "abstract": "It is argued that syntactic preference principles such as Right Association and Minimal Attachment are unsatisfactory as usually formulated. Among the difficulties are: (I) dependence on ill-specified or implausible principles of parser operation; (2) dependence on questionable assumptions about syntax; (3) lack Of provision, even in principle, for integration with semantic and pragmatic preference principles; and (4) apparent counterexamples, even when discounting (I)-(3). A possible approach to a solution is sketched.",
  "stance": -0.1
 },
 {
  "url": "https://aclanthology.org/P84-1071",
  "title": "What Not to Say",
  "year": 1984,
  "venue": "COLING",
  "abstract": "A problem with most text production and language generation systems is that they tend to become rather verbose. This may be due to negleetion of the pragmatic factors involved in communication. In this paper, a text production system, COMMENTATOR, is described and taken as a starting point for a more general discussion of some problems in Computational Pragmatics. A new line of research is suggested, based on the concept of unification.",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/C86-1067",
  "title": "A Kana-Kanji Translation System for Non-Segmented Input Sentences Based on Syntactic and Semantic Analysis",
  "year": 1986,
  "venue": "COLING",
  "abstract": "This paper presents a disambiguation approach for t ransla t ing non-segmented-Kana into Kanji. The method consists of two steps. In the first step, an input sentence is analyzed morphologically and ambiguous morphemes are stored in a network form. In the second step, the best path, which is a string of morphemes, is selected by syntactic and semantic analysis based on case grammar. In order to avoid the combinatorial explosion of possible paths, the following heuristic search method is adopted. First, a path that contains the smallest number of weighted-morphemes is chosen as the quasi-best path by a best-first-search technique. Next, the restricted range of morphemes near the quasi-best path is extracted from the morpheme network to construct preferential paths. An experimental system incorporating large dictionaries has been developed and evaluated. A translat ion accracy of 90.5% was obtained. This can be improved to about 95% by optimizing the dictionaries.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/C86-1128",
  "title": "A PROLOG Implementation of Government-Binding Theory",
  "year": 1986,
  "venue": "COLING",
  "abstract": "A parser which is founded on Chomsky's Government-Binding Theory and implemented in PROLOG is described. By focussing on systems of constraints as proposed by this theory, the system is capable of parsing without an elaborate rule set and subcategorization features on lexical items. In addition to the parse, theta, binding, and control relations are determined simultaneously.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/C88-1011",
  "title": "A System for Creating and Manipulating Generalized Wordclass Transition Matrices From Large Labelled Text-Corpora",
  "year": 1988,
  "venue": "COLING",
  "abstract": "This paper deals with the training phase of a Markov-type linguistic model that is based on transition probabilities between pairs and triplets of syntactic categories. To determine the optimal level of detail for a set of syntactic classes we developed a system that uses a set-theoretical formalism to defiue such sets mid has some measures to compare and optimize them individually. In section two we describe the optimization problem (hi terms of piediction, infoimation and economy requilements) and our approach to its solution. Section three introduces the system dlat will assist a linguist in handling the prediction and economy criteria and in the last section we present some sample results that can be achieved with it.",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/C88-1040",
  "title": "Robust parsing of severely corrupted spoken utterances",
  "year": 1988,
  "venue": "COLING",
  "abstract": "This paper describes a technique for enabling a speech understanding system to deal with sentences for which some monosyllabic words are not recognized. Such words are supposed to act as mere syntactic markers within the system linguistic domain. This result is achieved by combining a modified caseframe approach to linguistic knowledge representation with a parsing strategy able to integra te expectat ions from the language model and predictions from words. Experimental results show that the proposed technique permits to greatly increase the quota of corrupted sentences correctly understandable without sensibly decreasing parsing efficiency.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C88-1049",
  "title": "Chart Parsing According to the Slot and Filler Principle",
  "year": 1988,
  "venue": "COLING",
  "abstract": "A parser is an algorithm that assigns a structural description to a string according to a grammar. It follows from this definition that there are three general issues in parser design: the structure to be assigned, the type of grammar, the recognition algorithm. Common parsers employ phrase structure descriptions, rule-based grammars, and derivation or transition oriented recognition. The following choices result in a new parser: The structure to be assigned to the input is a dependency tree with lexical, morpho-syntactic and functional-syntactic information associated with each node and coded by complex categories which are subject to unification. The grammar is lexicalized, i.e. the syntactical relationships are stated as part of the lexical descriptions of the elements of the language. The algorithm relies on the slot and filler principle in order to draw up complex structures. It utilizes a well-formed substring table (chart) which allows for discontinuous segments.",
  "stance": 0.1
 },
 {
  "url": "https://aclanthology.org/C88-1070",
  "title": "Schema Method: A Framework for Correcting Grammatically Ill-formed Input",
  "year": 1988,
  "venue": "COLING",
  "abstract": "The schema method is a framework for correcting grammatically ill-formed input. In a natural language processing system ill-formed input cannot be overlooked. A computer assisted instruction (CAI) system, in particular, needs to show the user's errors. This framework diagnoses ill-formed input, corrects it and explains the error, if an input is ill-formed. The framework recognizes a sentence at two steps: first parses weak grammar, and then strongly filters the parsed sentence. When it is known what sentences are passed by the filter, it can be used even if it is imperfect. As the strong filter, a new method is used: an interpretation schema and an interpretation rule. An interpretation schema collects input information schemata and then an interpretat ion rule judges whether the collected schemata are correct or incorrect. This approach overcomes the problem of relaxation control, the major drawback of the previous syntactically-oriented methods, and is also more efficient.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C88-1082",
  "title": "Linguistic Processing Using a Dependency Structure Grammar for Speech Recognition and Understanding",
  "year": 1988,
  "venue": "COLING",
  "abstract": "This paper proposes an efficient linguistic processing strategy for speech recognition and understanding using a dependency structure grammar. The strategy includes parsing and phrase prediction algorithms. After speech processing and phrase recognition based on phoneme recognition, the parser extracts the sentence with the best likelihood taking account of the phonetic likelihood of phrase candidates and the linguistic likelihood of the semantic inter-phrase dependency relationships. A fast parsing algorithm using breadth-first search is also proposed. The predictor pre-selects the p}~.ase candidates using transition rules combined with a dependency structure to reduce the amount of phonetic processing. The proposed linguistic processor has been tested through speech recognition experiments. The experimental results show that it greatly increases the accuracy of speech recognitions, and the breadth-first parsing algorithm and predictor increase processing speed.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C90-2026",
  "title": "GPSG Parsing, Bidirectional Charts, and Connection Graphs",
  "year": 1990,
  "venue": "COLING",
  "abstract": "This paper describes a tractable method for parsing GPSG grammars without altering the modularity and expressiveness of this formalism. The proposed method is based on a constraint propagation mechanism which reduces the number of unnecessary structures built at parse time through the early detection of inadmissible local trees. The propagation of constraints is rendered efficient by indexing constraints and categories in a connection graph and by using a bidirectional chart parser together with a bottomup strategy centered around head constituents. ",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/C90-2061",
  "title": "A Type-theoretical Analysis of Complex Verb Generation",
  "year": 1990,
  "venue": "COLING",
  "abstract": "Tense and aspect, together with mood and modality, usually form the entangled structure of a complex verb. They are often hard to translate by machines, because of both syntactic and semantic differences between languages. This problem seriously affects upon the generation process because those verb components in interlingua are hardly rearranged correctly in the target language. We propose here a method in which each verb element is defined as a mathematical function according to its type of type theory. This formalism gives each element its legal position in the complex verb in the target language and certifies so-called partial translation. In addition, the generation algorithm is totally free from the stepwise calculation, and is available on parallel architecture.",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/C90-3019",
  "title": "Organizing linguistic knowledge for multilingual generation",
  "year": 1990,
  "venue": "COLING",
  "abstract": "We propose an architecture for the organisation of linguistic knowledge which allows to (1) separately formulate generalizations for different types of linguistic information, and (2) state interrelations between partial information belonging to different levels of description. We use typed feature structures for encoding linguistic knowledge. We show the application of this representational device for the architecture of linguistic knowledge sources for multilingual generation. As an example, we describe the use of interacting collocational and syntactic constraints in the generation of French and German sentences.",
  "stance": 0.4
 },
 {
  "url": "https://aclanthology.org/C90-3035",
  "title": "Expressive Power of Grammatical Formalisms",
  "year": 1990,
  "venue": "COLING",
  "abstract": "We propose formalisms and concepts which allow to make precise the m'gmnents in controversies over the adequacy of competing models of language, and over their formal equivalence.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/C90-3087",
  "title": "Corpus Work With Pc Beta",
  "year": 1990,
  "venue": "COLING",
  "abstract": "PC Beta is a PC oriented tool for corpus work in this term's broadest possible sense. With PC Beta one can prepare texts for corpus work, e.g. standardize texts in different ways (very important when texts from different sources together will constitute a corpus), one can process texts, and one can analyze texts. Making ordinary concordances and similar things with PC Beta is, of course, very simple, and, in fact, PC Beta gives \"concordance making\" a new dimension. One can perform morphological analyses, one can use PC Beta as a \"tagger\", i.e. provide the words with different kinds of tags. In all, PC Beta is a versatile program, and it is in many cases the only program needed (together with functions belonging to the MS/PC-DOS operative system) for pursuing a complete corpus project. The program's main distinctive feature is simplicity: it is rule controlled, and the rules adhere to a format that any linguist can learn to understand very quickly. But beware, in spite of its innocent appearence the program is a little tiger.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/C92-1009",
  "title": "Feature Structure Based Semantic Head Driven Generation",
  "year": 1992,
  "venue": "COLING",
  "abstract": "This paper proposes a generation method for feature structure based unification grammars. As compared with fixed arity term notation, feature structure notation is more flexible for representing knowledge needed to generate idiomatic structures as well as general constructions. The method enables feature structure retrieval via multiple indices. The indexing mechanism, when used with a semantic head driven generation algorithm, attains efficient generation even when a large amount of generation knowledge must be considered. Our method can produce all possible structures in parallel, using structure sharing among ambiguous substructures. ",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/C92-1032",
  "title": "Left-Corner Parsing and Psychological Plausibility",
  "year": 1992,
  "venue": "COLING",
  "abstract": "It is well known that even extremely limited centerembedding causes people to have difficulty in comprehension, but that leftand right-branching constractions produce no such effect. If the difficulty in comprehension is taken to be a result of processing load, as is widely assumed, then measuring the processing load induced by a parsing strategy on these constructions may help determine its plausibility as a psychological model. On this basis, it has been argued [AJ91, JL83] that by identifying processing load with space utilization, we can rule out both top-down and bottom-up parsing as viable candidates for the human sentence processing mechanism, attd that left-corner parsing represents a plausible alternative. Examining their arguments in detail, we find difficulties with each presentation. In this paper we revise the argument and validate its central claim. In so doing, we discover that the key distinction between the parsing methods is not the form of prediction (top-down vs. bottom-up vs. leftcorner), but rather the ability to instantiate the operation of composition.",
  "stance": -0.6
 },
 {
  "url": "https://aclanthology.org/C92-1033",
  "title": "TTP: A Fast and Robust Parser for Natural Language",
  "year": 1992,
  "venue": "COLING",
  "abstract": "In this paper we describe TTP , a fast and robust natural language parser which can analyze written text and generate regularized parse structures for sentences and phrases at the speed of approximately 0.5 sec/sentence, or 44 word per second. The parser is based on a wide coverage grammar for English, developed by the New York University's Linguistic String Project, and it uses the machine-readable version of the Oxford Advanced Learner's Dictionary as a source of its basic vocabulary. The parser operates on stochastically tagged text, and contains a powerful skip-and-fit recovery mechanism that allows it to deal with extra-grammatical input and to operate effectively under a severe time pressure. Empirical experiments, testing parser's speed and accuracy, were performed on several collections: a collection of technical abstracts (CACM-3204), a corpus of news messages (MUC-3), a selection from ACM Computer Library database, and a collection of Wall Street Journal articles, approximately 50 million words in total.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/C92-1047",
  "title": "Une ontologie du temps pour le langage naturel",
  "year": 1992,
  "venue": "COLING",
  "abstract": "We propose a new ontology for the time in natural language which provides the following features : - it renders an account of most of the temporal phenomena of language (dates, duration, events, states) ; it offers capacities for the comprehension of the narratives ; - in the contrary to the traditional systems, it needs no hard \"types\" for dealing with the different classes used by the terminology : thus an event may date others \"when John died, there were many demonstrations in the world\", a date may be the beginning of an event \"monthly, Paul was leaving for a six months tour\" ; our ontology endures the fluidity of natural language which does not make rigid, in the narratives, the signification of the temporal entities ; - it allows some multiplicity of points of viev, (partially inconsistent) about a single event : in the sentence \"the travel of Christophe Colomb, which endured a long time, has been the beginning of a rich period of exploration\", \"the voyage of CC\" is seen as a simple event in the main proposition and as a complex event in the subordinate one. With respect to these issues, the ontology uses the following frameworks : - a KLONE-like network aiming at a quick detection of incoherences : it contains taxonomic inferences (including the whole terminology) plus the facts ; a bulk of rules (assertional device) embodying contingent properties ; - non-monotonic reasoning is needed to revise simplistic conclusions obtained from superficial descriptions : the rules and the links of the network may be default rules ; - the VaDe (Variable Depth) system supports our ontology ; it is implemented with an ATMS-like truth maintenance system. The flexible ontology we present offers an interesting frame for further researches in computational linguistics, in particular in the domain of the interpretation of the narratives.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C92-2071",
  "title": "Un Systeme Inferentiel Oriente Objet Pour Des Applications En Langues Naturelles",
  "year": 1992,
  "venue": "COLING",
  "abstract": "Up to now, there is still no specific model for solving the problem of natured language representation and reasoning. In this paper, we propose an object oriented formalism for supporting knowledge representation, extraction and exploitation in the context of natural language processing. In the natural language analysis, this system is situated after the morpho-syntax and the linguistic semantics. It represents two classes of concepts: objects of discourse and action schemata, the former resulting from nominal syntagms and the latter from the 'processes'. We are concerned here just by the  representation of objects. In the natural language discourse, manipulated objects ,are complex objects and the reasoning is by nature first inferential and then deductive. To take into account this kind of reasoning we need a suitable representation: a model of inferential objects. The theoretical foundations of the proposed model are Lesniewski's logical systems: tile Calculus of Names and the Mereology. The former is based on a primitive functor called \"epsilon\" interpreted as is-a, the latter is based on a part-of relation which is called the \"ingredience\". The whole system is supported by these two primitives and theirs derived functions. The concepts of our model result from a collaboration between linguists and computer scientists. The main concepts are the intensional and extensional universes, notions and types. The possible inferential reasoning can be of different types : it can concern the status, the denominations, the structures or the \"fonctifs\" of the objects.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/C92-2087",
  "title": "Logical Form of Hierarchical Relation on Verbs and Extracting it from Definition Sentences in a Japanese Dictionary",
  "year": 1992,
  "venue": "COLING",
  "abstract": "We are studying how to extract hierarchical relation on verbs from definition sentences in a Japanese dictionary. The hierarchical relation on verbs has been dealt with as a binary relation on verbs, but it should be dealt with as logical relation on predicates. We will define the logical form of the hierarchical relation on verbs and then discuss which part of the syntactic structure of the definition sentence represents that relation. We will call the main predicate verb in this part the definition verb. Furthermore we will describe how to semiautomatically select the proper meaning of the definition verb and the proper correspondence between cases of an entry verb and the definition verb in order to extract the hierarchical relation as logical relation.",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/C92-2100",
  "title": "A Three-level Revision Model for Improving Japanese Bad-styled Expressions",
  "year": 1992,
  "venue": "COLING",
  "abstract": "This paper proposes a three-level revision model for improving badly-styled Japanese expressions, especially in the field of technical communication. The model is a mixture of the regeneration-based model and tile rewriting-based model. The first level divides long sentences, while the second level improves several badly-styled expressions with iterative partial rewriting operations. The last level performs regeneration, in which word ordering and punctuation to reduce the reading ambiguity are currently involved. Experimental results show that our model is effective in realizing practical revision support systems.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/C92-2121",
  "title": "Semantic Network Array Processor as a Massively Parallel Computing Platform for High Performance and Large-Scale Natural Language Processing",
  "year": 1992,
  "venue": "COLING",
  "abstract": "This paper demonstrates the utility of the Semantic Network Array Processor (SNAP) as a massively parallel platform for high performance and large-scale natural language processing systems. SNAP is an experimental massively parallel machine which is dedicated to, but not limited to, the natural language processing using semantic networks. In designing the SNAP, we have investigated various natural language processing systems and theories to determine the scope of the hardware support and a set of micro-coded instructions to be provided. As a result, SNAP employs an extended markerpassing model and a dynamically modifiable network model. A set of primitive instructions is micro-coded to directly support a parallel marker-passing, bit-operations, numeric operations, network modifications, and other essential functions for natural language processing. This paper demonstrates the utility of SNAP for various paradigms of natural language processing. We have discovered that the SNAP provides milliseconds or microseconds performance on several important applications such as the memory-based parsing and translation, classification-based parsing, and VLKB search. Also, we argue that there are numerous opportunities in the NLP community to take advantages of the comlmtational power of the SNAP.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/C92-3150",
  "title": "Surface Grammatical Analysis for the Extraction of Terminological Noun Phrases",
  "year": 1992,
  "venue": "COLING",
  "abstract": "LEXTER is a software package for extracting terminology. A corpus of French language texts on any subject field is fed in, and LEXTER produces a list of likely terminological units to be submitted to an expert to be validated. To identify the terminological units, LEXTER takes their form into account and proceeds in two main stages : analysis, parsing. In the first stage, LEXTER uses a base of rules designed to identify frontier markers in view to analysing the texts and extracting maximal length noun phrases. In the second stage, LEXTER parses these maximal-length noun phrases to extract subgroups which by virtue of their grammatical structure and their place in the maximal-length noun phrases are likely to be terminological units. In this article, the type of analysis used (surface grammatical analysis) is highlighted, as the methodological approach adopted to adapt the rules (experimental approach).",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/C92-3161",
  "title": "Shalt2- a Symmetric Machine Translation System with Conceptual Transfer",
  "year": 1992,
  "venue": "COLING",
  "abstract": "Shal l2 is a knowledge-based machine translation system with a symmetric architecture. The grammar rules, mapping rules between syntactic and conceptual (semantic) representations, and transfer rules for conceptual paraphrasing are all bi-directional knowledge sources used by both a parser and a generator.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/C92-4206",
  "title": "Multimodal Database Query",
  "year": 1992,
  "venue": "COLING",
  "abstract": "The paper proposes a mult imodal interface for a real sales database application. We show how natural language processing may be integrated with a visual, direct manipulat ion method of database query, to produce a user interface which supports a flexible form of query specification, provides implicit guidance about the coverage of the linguistic component, and allows more focused discourse reference.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/C94-1023",
  "title": "AUTOMATIC MODEL REFINEMENT - with an application to tagging",
  "year": 1994,
  "venue": "COLING",
  "abstract": "Statistical NLP models usually only consider coarse information and very restricted context to make the estimation of parameters feasible. To reduce the modeling error introduced by a simplified probabilistic model, the Classitication and Regression Tree (CART) method was adopted in this paper to select more discriminative features for automatic model refinement. Because the features are adopted dependently during splitting the classification tree in CART, the number of training data in each terminal node is small, which makes the labeling process of terminal nodes not robust. This over-tuning phenomenon cannot be completely removed by crossvalidation process (i.e., pruning process). A probabilistic classification model based on the selected discriminative features is thus proposed to use the training data more efficiently. In tagging the Brown Corpus, our probabilistic classification model reduces the error rate of the top 10 error dominant words from 5.71% to 4.35%, which shows 23.82% improvement over the unrefined model.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/C94-1032",
  "title": "A Stochastic Japanese Morphological Analyzer Using a Forward-DP Backward-A* N-Best Search Algorithm",
  "year": 1994,
  "venue": "COLING",
  "abstract": "We present a novel method for segmenting the input sentence into words and assigning parts of speech to the words. It consists of a statistical language model and an efficient two-pass N-best search algorithm. The algorithm does not require delimiters between words. Thus it is suitable for written Japanese. The proposed Japanese morphological analyzer achieved 95.1% recall and 94.6% precision for open text when it was trained and tested on the ATR Corpus.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C94-1075",
  "title": "A Modular Architecture for Constraint-Based Parsing",
  "year": 1994,
  "venue": "COLING",
  "abstract": "This paper presents a framework and a system for implementing, comparing and analyzing parsers for some classes of Constraint-Based Grammars. The framework consists in a uniform theoretic description of parsing algorithms, and provides the structure for decomposing the system into logical components, with possibly several interchangeable implementations. Many parsing algorithms can be obtained by composition of the modules of our system. Modularity is also, a way of achieving code sharing for the common parts of these various algorithms. Furthermore, the design helps reusing the existing modules when implementing other algorithms. The system uses the flexible modularity provided by the programming languages Alcool-90, based on a type system that ensures the safety of module composition.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/C94-1079",
  "title": "PRINCIPAR - An Efficient, Broad-coverage, Principle-based Parser",
  "year": 1994,
  "venue": "COLING",
  "abstract": "We present an efI]cient, broad-coverage, principle-based parser for English. The parser has been implemented in C++ and runs on SUN Sparcstations with X-windows. It conrains a lexicon with over 90,000 entries, constructed automatically by applying a set of extraction and conversion rules to entries from machine readable dictionaries.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/C94-2106",
  "title": "A System of Verbal Semantic Attributes Focused on the Syntactic Correspondence between Japanese and English",
  "year": 1994,
  "venue": "COLING",
  "abstract": "This paper proposes a system of 97 verbal semantic attributes for Japanese verbs which considers both dynamic characteristics and the relationship of verbs to cases. These attribute values are used to disambiguate the meanings of all Japanese and English pattern pairs in a Japanese to English transfer pattern dictionary consisting of 15,000 pairs of Japanese valence patterns and equivalent English syntactic structures.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/C94-2138",
  "title": "A Reestimation Algorithm for Probabilistic ttecursive Transition Network",
  "year": 1994,
  "venue": "COLING",
  "abstract": "Probabilistic Recursive Transition Network (PRTN) is an elevated version of RTN to model and process languages in stochastic parameters. The representation is a direct derivation from the RTN and keeps much the spirit of Hidden Markov Model at the same time. We present a reestimation algorithm for PRTN that is a variation of InsideOutside algorithm that computes the values of the probabilistic parameters from sample sentences (parsed or unparsed). ",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/C94-2149",
  "title": "XTAG System - A Wide Coverage Grammar for English",
  "year": 1994,
  "venue": "COLING",
  "abstract": "This paper presents the XTAG system, a grammar development tool based on the Tree Adjoining Grammar (TAG) formalism that includes a wide-coverage syntactic grammar for English. The various components of the system are discussed and preliminary evaluation results from the parsing of various corpora are given. Results from the comparison of X3AG against the IBM statistical parser and the Alvey Natural Language Tool parser are also given.",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/C94-2153",
  "title": "An Efficient Syntactic Tagging Tool for Corpora",
  "year": 1994,
  "venue": "COLING",
  "abstract": "The tree bank is an important resources for MT and linguistics researches, but it requires that large number of sentences be annotated with syntactic information. It is time consuming and troublesome, and difficult to keep consistency, if annotation is done manually. In this paper, we presented a new technique for the semi-automatic tagging of Chinese text. The system takes as input Chinese text, and outputs the syntactically tagged sentence(dependency tree). We use dependency grammar and employ a stack based shift/reduce context-dependent parser as the tagging mechanism. The system works in human-machine cooperative way, in which the machine can acquire tagging rules from human intervention. The automation level can be improved step by step by accumulating rules during annotation. In addition, good consistency of tagging is guaranteed.",
  "stance": 0.4
 },
 {
  "url": "https://aclanthology.org/C94-2177",
  "title": "Reverse Queries in DATR",
  "year": 1994,
  "venue": "COLING",
  "abstract": "DATR is a declarative representation language for lexical information and as such, in principle, neutral with respect to particular processing strategies. Previous DATR compiler/interpreter systems support only one access strategy that closely resembles the set, of inference rules of the procedural semantics of DATR (Evans Gazdar 1989a). In this paper we present an alternative access strategy (reverse query strategy) for a nontrivial subset of DATR.",
  "stance": 0.4
 },
 {
  "url": "https://aclanthology.org/C94-2212",
  "title": "NL Understanding with a Grammar of Constructions",
  "year": 1994,
  "venue": "COLING",
  "abstract": "We present an approach to natural language understanding based on a computable grammar of constructions. A construction consists of a set of features of form and a description of meaning in a context. A grammar is a set of constructions. This kind of grammar is the key element of MINCAL, an implemented natural language speech-enabled interface to an online calendar system. The architecture has two key aspects: (a) the use of constructions, integrating descriptions of form, meaning and context into one whole; and (b) the separation of domain knowledge (about calendars) from application knowledge (about the particular on-line calendar).",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/C96-1006",
  "title": "Extracting Word Correspondences from Bilingual Corpora Based on Word Co-occurrence Information",
  "year": 1996,
  "venue": "COLING",
  "abstract": "A new method has been developed for extracting word correspondences from a bilingual corpus. First, the co-occurrence information for each word in both languages is extracted from the corpus. Then, the correlations between the co-occurrence features of the words are calculated pairwisely with the assistance of a basic word bilingual dictionary. Finally, the pairs of words with the highest correlations are output selectively. This method is applicable to rather small, unaligned corpora; it can extract correspondences between compound words as well as simple words. An experiment using bilingual patent-specification corpora achieved 28% recall and 76% precision; this demonstrates that the method effectively reduces the cost of bilingual dictionary augmentation.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C96-1069",
  "title": "An Automatic Clustering of Articles Using Dictionary Definitions",
  "year": 1996,
  "venue": "COLING",
  "abstract": "In this paper, we propose a statistical approach for clustering of artMes using on-line dictionary definitions. One of the characteristics of our approach is that every sense of word in ar tMes is automatically disambiguated using dictionary definitions. The other is that in order to cope with the problem of a phrasal lexicon, linking which links words with their semantically similar words in articles is introduced in our method. The results of experiments demonstrate the effectiveness of the proposed method.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/C96-1086",
  "title": "Inherited Feature-based Similarity Measure Based on Large Semantic Hierarchy and Large Text Corpus",
  "year": 1996,
  "venue": "COLING",
  "abstract": "We describe a similarity calculation model called IFSM (Inherited Feature Similarity Measure) between objects (words/concepts) based on their common and distinctive features. We propose an implementation method for obtaining features based on abstracted triples extracted fi'om a large text eorpus utilizing taxonomical knowledge. This model represents an integration of traditional methods, i.e,. relation based simlarity measure and distribution based similarity measure. An experiment, using our new concept abstraction method which we call the flat probability grouping method, over 80,000 surface triples, shows that the abstraction level of 3000 is a good basis for feature description.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/C96-1087",
  "title": "A Probabilistic Approach to Compound Noun Indexing in Korean Texts",
  "year": 1996,
  "venue": "COLING",
  "abstract": "In this paper we address the problem of compound noun indexing that is about segmenting or decomposing compound nouns into promising index terms. Compound nouns as index terms that usually subscribe to specific notions tend to increase the precision of retrieval performance. The use of the component nouns of a compound noun as index terms, on the other hand, may improve the recall performance, but can decrease the precision. Our proposed method to handle compound nouns with a goal to increase the recall while preserving the precision computes the relevance of the component nouns of a compound noun to the document content by comparing the document sets that are supported by the component nouns and the terms of the document. The operational content of a term is represented as the probabilistic distribution of the term over the document set. Experiments with a set of 1,000 documents show that our method gains 33% increase of retrieval performance compared to the indexing method without compound noun analysis, and is as good as manual decomposition by human experts.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/C96-1089",
  "title": "Learning Bilingual Collocations by Word-Level Sorting",
  "year": 1996,
  "venue": "COLING",
  "abstract": "This paper proposes a new method for learning bilingual collocations from sentence aligned parallel corpora. Our method comprises two steps: (1) extracting useful word chunks (n-grams) by word-level sorting and (2) constructing bilingual collocations by combining the word chunks acquired in stage (1). We apply the method to a very challenging text pair: a stock market bulletin in Japanese and its abstract in English. Domain specific collocations are well captured even if they were not contained in the dictionaries of economic terms. ",
  "stance": 0.4
 },
 {
  "url": "https://aclanthology.org/C96-2098",
  "title": "Extraction of Lexical Translations from Non-Aligned Corpora",
  "year": 1996,
  "venue": "COLING",
  "abstract": "A method for extracting lexical translations from non-aligned corpora is proposed to cope with the unavailability of large aligned corpus. The assumption that \"translations of two co-occurring words in a source language also co-occur in the target language\" is adopted and represented in the stochastic matrix formulation. The translation matrix provides the co-occurring information translated from the source into the target. This translated co-occurring information should resemble that of the original in the target when the ambiguity of the translational relation is resolved. An algorithm to obtain the best translation matrix is introduced. Some experiments were performed to evaluate the effectiveness of the ambiguity resolution and the refinement of the dictionary.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/C96-2102",
  "title": "Towards a Syntactic Account of Punctuation",
  "year": 1996,
  "venue": "COLING",
  "abstract": "Little notice has been taken of punctuation in the field of natural language processing, chiefly due to the lack of any coherent theory on which to base implementations. Some work has been carried out concerning punctuation and parsing, but much of it seems to have been rather ad-hoc and performance-motivated. This paper describes the first step towards the construction of a theoretically-motivated account of punctuation. Parsed corpora are processed to extract punctuation patterns, which are then checked and generalised to a small set of General Punctuation Rules. Their usage is discussed, and suggestions are made for possible methods of including punctuation information in grammars.",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/C96-2110",
  "title": "Identifying the Coding System and Language of On-line Documents on the Internet",
  "year": 1996,
  "venue": "COLING",
  "abstract": "This paper proposes a new algorithm that simultaneously identifies the coding system and language of a code string fetched from the Internet, especially World-Wide Web. The algorithm uses statistic language models to select the correctly decoded string as well as to determine the language. The proposed algorithm covers 9 languages and 11 coding systems used in Eastern Asia and Western Europe. Experimental results show that the level of accuracy of our algorithm is over 95% for 640 on-line documents.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C96-2114",
  "title": "Linguistic Indeterminacy as a Source of Errors in Tagging",
  "year": 1996,
  "venue": "COLING",
  "abstract": "Most evaluations of part-of-speech tagging compare the utput of an automatic tagger to some established standard, define the differences as tagging errors and try to remedy them by, e.g., more training of the tagger. The present article is based on a manual analysis of a large number of tagging errors. Some clear patterns among the errors can be discerned, and the sources of the errors as well as possible alternative methods of remedy are presented and discussed. In particular are the problems with undecidable cases treated.",
  "stance": 0.1
 },
 {
  "url": "https://aclanthology.org/C96-2144",
  "title": "A Constraint-based Case Frame Lexicon",
  "year": 1996,
  "venue": "COLING",
  "abstract": "We present a constraint based case frame lexicon architecture for bidirectional mapping between a syntactic case frame and a semantic frame. The lexicon uses a semantic sense as the basic unit and employs a multitiered constraint structure for the resolution of syntactic information into the appropriate senses and/or idiomatic usage. Valency changing transformations such as morphologically marked passivized or causativized forms are handled via lexical rules that manipulate case frames templates. The system has been implemented in a typed-feature system and applied to Turkish. ",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/C96-2145",
  "title": "Error-tolerant Tree Matching",
  "year": 1996,
  "venue": "COLING",
  "abstract": "This paper presents an efficient algorithm for retrieving from a database of trees, all trees that match a given query tree approximately, that is, within a certain error tolerance. It has natural language processing applications in searching for matches in example-based translation systems, and retrieval from lexical databases containing entries of complex feature structures. The algorithm has been implemented on SparcStations, and for large randomly generated synthetic tree databases (some having tens of thousands of trees) it can associatively search for trees with a small error, in a matter of tenths of a second to few seconds.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/C98-1021",
  "title": "Spoken Dialogue Interpretation with the DOP Model",
  "year": 1998,
  "venue": "COLING",
  "abstract": "We show how the DOP model can be used for fast and robust processing of spoken input in a practical spoken dialogue system called OVIS. OVIS, Openbaar Vervoer Informatie Systeem (\"Public Transport Information System\"), is a Dutch spoken language information system which operates over ordinary telephone lines. The prototype system is the immediate goal of the NWO 1 Priority Programme \"Language and Speech Technology\". In this paper, we extend the original DOP model to context-sensit ive interpretation of spoken input. The system we describe uses the OVIS corpus (10,000 trees enriched with compositional semantics) to compute from an input word-graph the best utterance together with its meaning. Dialogue context is taken into account by dividing up the OVIS corpus into context-dependent subcorpora. Each system question triggers a subcorpus by which the user answer is analyzed and interpreted. Our experiments indicate that the context-sensitive DOP model obtains better accuracy than the original model, allowing for fast and robust processing of spoken input.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C98-1048",
  "title": "Experiments with Learning Parsing Heuristics",
  "year": 1998,
  "venue": "COLING",
  "abstract": "Any large language processing software relies in its operation on heuristic decisions concerning the strategy of processing. These decisions are usually \"hard-wired\" into the software in the form of handcrafted heuristic rules, independent of the nature of the processed texts. We propose an alternative, adaptive approach in which machine learning techniques learn the rules from examples of sentences in each class. We have experimented with a variety of learning techniques on a representative instance of this problem within the realm of parsing. Our approach lead to the discovery of new heuristics that perform significantly better than the current hand-crafted heuristic. We discuss the entire cycle of application of machine learning and suggest a methodology for the use of machine learning as a technique for the adaptive optimisation of language-processing software.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C98-1087",
  "title": "Long Distance Pronominalisation and Global Focus",
  "year": 1998,
  "venue": "COLING",
  "abstract": "1) Out corpus of descriptive text contains a significant number of long-distance pronominal references (8.4% of the total). In order to account for how these pronouns are interpreted, we re-examine Grosz and Sidner's theory of the attentional state, and in particular the use of the global focus to supplement centering theory. Our corpus evidence concerning these long-distance pronominal references, as well as studies of the use of descriptions, proper names and ambiguous uses of pronouns, lead us to conclude that a discourse focus stack mechanism of the type proposed by Sidner is essential to account for the use of these referring expressions. We suggest revising the Grosz & Sidner fl'amework by allowing for the possibility that an entity in a focus space may have special status.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/C98-1106",
  "title": "Term-list Translation using Mono-lingual Word Co-occurrence Vectors",
  "year": 1998,
  "venue": "COLING",
  "abstract": "A term-list is a list of content words that characterize a consistent text or a concept. This paper presents a new method for translating a term-list by using a corpus in the target language. The method first retrieves alternative translations for each input word from a bilingual dictionary. It then determines the most 'coherent' combination of alternative translations, where the coherence of a set of words is defined as the proximity among multi-dimensional vectors produced from the words on the basis of co-occurrence statistics. The method was applied to term-lists extracted from newspaper articles and achieved 81% translation accuracy for ambiguous words (i.e., words with multiple translations).",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/C98-2119",
  "title": "Word Clustering and Disambiguation Based on Co-occurrence Data",
  "year": 1998,
  "venue": "COLING",
  "abstract": "We address the problem of clustering words (or constructing a thesaurus) based on co-occurrence data, and using the acquired word classes to improve the accuracy of syntactic disambiguation. We view this problem as that of estimating a joint probability distribution specifying the joint probabilities of word pairs, such as noun verb pairs. We propose an efficient algorithm based on the Minimum Description Length (MDL) principle for estimating such a probability distribution. Our method is a natural extension of those proposed in (Brown et al., 1992) and (Li and Abe, 1996), and overcomes their drawbacks while retaining their advantages. We then combined this clustering method with the disambiguation method of (Li and Abe, 1995) to derive a disambiguation method that makes use of both automatically constructed thesauruses and a hand-made thesaurus. The overall disambiguation accuracy achieved by our method is 85.2%, which compares favorably against the accuracy (82.4%) obtained by the state-of-the-art disambiguation method of (Brill and Resnik, 1994).",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C98-2120",
  "title": "Identifying Syntactic Role of Antecedent in Korean Relative Clause Using Corpus and Thesaurus Information",
  "year": 1998,
  "venue": "COLING",
  "abstract": "This paper describes an approach to identifying the syntactic role of an antecedent in a Korean relative clause, which is essential to structural disambiguation and semantic analysis. In a learning phase, linguistic knowledge such as conceptual co-occurrence patterns and syntactic role distribution of antecedents is extracted from a large-scale corpus. Then, in an application phase, the extracted knowledge is applied in determining the correct syntactic role of an antecedent in relative clauses. Unlike previous research based on co-occurrence patterns at the lexical level, we represent co-occurrence patterns with concept types in a thesaurus. In an experiment, the proposed method showed a high accuracy rate of 90.4% in resolving ambiguities of syntactic role determination of antecedents.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/C98-2145",
  "title": "An Estimate of Referent of Noun Phrases in Japanese Sentences",
  "year": 1998,
  "venue": "COLING",
  "abstract": "In machine translation and man-machine dialogue, it is important to clarify referents of noun phrases. We present a method for determining the referents of noun phrases in Japanese sentences by using the referential properties, modifiers, and possessors 1 of noun phrases. Since the Japanese language has no articles, it is difficult to decide whether a noun phrase has an antecedent or not. We had previously estimated the referential properties of noun phrases that correspond to articles by using clue words in the sentences (Murata and Nagao 1993). By using these referential properties, our system determined the referents of noun phrases ill Japanese sentences. Furthermore we used the modifiers and possessors of noun phrases in determining the referents of noun phrases. As a result, on training sentences we obtained a precision rate of 82% and a recall rate of 85% in the determination of the referents of noun phrases that have antecedents. On test sentences, we obtained a precision rate of 79% and a recall rate of 77%.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/C98-2147",
  "title": "Japanese OCR Error Correction using Character Shape Similarity and Statistical Language Model",
  "year": 1998,
  "venue": "COLING",
  "abstract": "We present a novel OCR error correction method for languages without word delimiters that have a large character set, such as Japanese and Chinese. It consists of a statistical OCR model, an approximate word matching method using character shape similarity, and a word segmentation algorithm using a statistical language model. By using a statistical OCR model and character shape similarity, the proposed error corrector outperforms the previously published method. When the baseline character recognition accuracy is 90%, it achieves 97.4% character recognition accuracy.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/C98-2154",
  "title": "An Efficient Parallel Substrate for Typed Feature Structures on Shared Memory Parallel Machines",
  "year": 1998,
  "venue": "COLING",
  "abstract": "This paper describes an efficient parallel system for processing Typed Feature Structures (TFSs) on shared-memory parallel machines. We call the system Parallel Substrate for TFS (PSTFS). PSTFS is designed for parallel computing environments where a large number of agents are working and communicating with each other. Such agents use PSTFS as their low-level module for solving constraints on TFSs and sending/receiving TFSs to/from other agents in an efficient manner. From a programmers point of view, PSTFS provides a simple and unified mechanism for building high-level parallel NLP systems. The performance and the flexibility of our PSTFS are shown through the experiments on two different types of parallel HPSG parsers. The speed-up was more than 10 times on both parsers.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/C98-2158",
  "title": "Recognition of the Coherence Relation between Te-linked Clauses",
  "year": 1998,
  "venue": "COLING",
  "abstract": "This paper describes a method for recognizing coherence relations between clauses which are linked by te in Japanese a translational equivalent of English and. We consider that the coherence relations are categories each of which has a prototype structure as well as the relationships among them. By utilizing this organization of the relations, we can infer an appropriate relation from the semantic structures of the clauses between which that relation holds. We carried out an experiment and obtained the correct recognition ratio of 82% for the 280 sentences.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/C98-2159",
  "title": "On the Evaluation and Comparison of Taggers: the Effect of Noise in Testing Corpora.",
  "year": 1998,
  "venue": "COLING",
  "abstract": "This paper addresses the issue of Pos tagger evaluation. Such evaluation is usually performed by comparing the tagger output with a reference test corpus, which is resumed to be error-free. Currently used corpora contain noise which causes the obtained performance to be a distortion of the real value. We analyze to what extent this distortion may invalidate the comparison between taggers or the measure of the improvement given by a new system. The main conclusion is that a more rigorous testing experimentation setting/designing is needed to reliably evaluate and compare tagger accuracies.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/C98-2162",
  "title": "Machine Aided Error-Correction Environment for Korean Morphological Analysis and Part-of-Speech Tagging",
  "year": 1998,
  "venue": "COLING",
  "abstract": "Statistical methods require very large corpus with high quality. But building large and faultless annota ted corpus is a very difficult job. This paper proposes an efficient method to construct part-of-speech tagged corpus. A rulebased error correction method is proposed to find and correct errors semi-automatical ly by user-defined rules. We also make use of user's correction log to reflect feedback. Experiments were carried out to show the efficiency of error correction process of this workbench. The result shows that about 63.2 % of tagging errors can be corrected.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C98-2176",
  "title": "Building Accurate Semantic Taxonomies Monolingual MRDs",
  "year": 1998,
  "venue": "COLING",
  "abstract": "This paper presents a method that combines a set of unsupervised algorithms in order to accurately build large taxonomies from any machine-readable dictionary (MRD). Our aim is to profit from conventional MRDs, with no explicit semantic coding. We propose a system that 1) performs fully automatic extraction of taxonomic links from MRD entries and 2) ranks the extracted relations in a way that selective manual refinement is allowed. Tested accuracy can reach around 100% depending on the degree of coverage selected, showing that taxonomy building is not limited to structured dictionaries such as LDOCE.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/C98-2202",
  "title": "Keyword Extraction using Term-Domain Interdependence for Dictation of Radio News",
  "year": 1998,
  "venue": "COLING",
  "abstract": "In this paper, we propose keyword extraction method for dictation of radio news which consists of several domains. In our method, newspaper articles which are automatically classified into suitable domains are used in order to calculate feature vectors. The feature vectors shows term-domain interdependence and are used for selecting a suitable domain of each part of radio news.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/C98-2216",
  "title": "Modeling with Structures in Statistical Machine Translation",
  "year": 1998,
  "venue": "COLING",
  "abstract": "Most statistical machine translation systems employ a word-based alignment model. In this paper we demonstrate that word-based align: ment is a major cause of translation errors. We propose a new alignment model based on shallow phrase structures, and the structures can be automatically acquired from parallel corpus. This new model achieved over 10% error reduction for our spoken language translation task.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C98-2223",
  "title": "Word Sense Disambiguation using Optimised Combinations of Knowledge Sources",
  "year": 1998,
  "venue": "COLING",
  "abstract": "Word sense disambiguation algorithms, with few exceptions, have made use of only one lexical knowledge source. We describe a system which performs word sense disambiguation on all content words in free text by combining different knowledge sources: semantic preferences, dictionary definitions and subject/domain codes along with part-of-speech tags, optimised by means of a learning algorithm. We also describe the creation of a new sense tagged corpus by combining existing resources. Tested accuracy of our approach on this corpus exceeds 92%, demonstrating the viability of all-word disambiguation rather than restricting oneself to a small sample.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/P98-1004",
  "title": "A Simple Hybrid Aligner for Generating Lexical Correspondences in Parallel Texts",
  "year": 1998,
  "venue": "COLING",
  "abstract": "We present an algorithm for bilingual word alignment that extends previous work by treating multi-word candidates on a par with single words, and combining some simple assumptions about the translation process to capture alignments for low frequency words. As most other alignment algorithms it uses cooccurrence statistics as a basis, but differs in the assumptions it makes about the translation process. The algorithm has been implemented in a modular system that allows the user to experiment with different combinations and variants of these assumptions. We give performance results from two evaluations, which compare well with results reported in the literature.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P98-1018",
  "title": "Consonant Spreading in Arabic Stems",
  "year": 1998,
  "venue": "COLING",
  "abstract": "This paper examines the phenomenon of consonant spreading in Arabic stems. Each spreading involves a local surface copying of an underlying consonant, and, in certain phonological contexts, spreading alternates productively with consonant lengthening (or gemination). The morphophonemic triggers of spreading lie in the patterns or even in the roots themselves, and the combination of a spreading root and a spreading pattern causes a consonant to be copied multiple times. The interdigitation of Arabic stems and the realization of consonant spreading are formalized using finite-state morphotactics and variation rules, and this approach has been successfully implemented in a large-scale Arabic morphological analyzer which is available for testing on the Internet.",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/P98-1022",
  "title": "A Probabilistic Corpus-Driven Model for Lexical-Functional Analysis",
  "year": 1998,
  "venue": "COLING",
  "abstract": "We develop a Data-Oriented Parsing (DOP) model based on the syntactic representations of Lexical-Functional Grammar (LFG). We start by summarizing the original DOP model for tree representations and then show how it can be extended with corresponding functional structures. The resulting LFG-DOP model triggers a new, corpus-based notion of grammaticality, and its probability models exhibit interesting behavior with respect to specificity and the interpretation of ill-formed strings.",
  "stance": 0.1
 },
 {
  "url": "https://aclanthology.org/P98-1040",
  "title": "Dialogue Management in Vector-Based Call Routing",
  "year": 1998,
  "venue": "COLING",
  "abstract": "This paper describes a domain independent, automatically trained call router which directs customer calls based on their response to an open-ended \"How may I direct your call?\" query. Routing behavior is trained from a corpus of transcribed and hand-routed calls and then carried out using vector-based information retrieval techniques. Based on the statistical discriminating power of the n-gram terms extracted from the caller's request, the caller is 1) routed to the appropriate destination, 2) transferred to a human operator, or 3) asked a disambiguation question. In the last case, the system dynamically generates queries tailored to the caller's request and the destinations with which it is consistent. Our approach is domain independent and the training process is fully automatic. Evaluations over a financial services call center handling hundreds of activities with dozens of destinations demonstrate a substantial improvement on existing systems by correctly routing 93.8% of the calls after punting 10.2% of the calls to a human operator.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P98-1061",
  "title": "A Structure-sharing Parser for Lexicalized Grammars",
  "year": 1998,
  "venue": "COLING",
  "abstract": "In wide-coverage lexicalized grammars many of the elementary structures have substructures in common. This means that in conventional parsing algorithms some of the computation associated with different structures is duplicated. In this paper we describe a precompilation technique for such grammars which allows some of this computation to be shared. In our approach the elementary structures of the grammar are transformed into finite state automata which can be merged and minimised using standard algorithms, and then parsed using an automatonbased parser. We present algorithms for constructing automata from elementary structures, merging and minimising them, and string recognition and parse recovery with the resulting grammar.",
  "stance": 0.4
 },
 {
  "url": "https://aclanthology.org/P98-1111",
  "title": "Unlimited Vocabulary Grapheme to Phoneme Conversion for Korean TTS",
  "year": 1998,
  "venue": "COLING",
  "abstract": "This paper describes a grapheme-to-phoneme conversion method using phoneme connectivity and CCV conversion rules. The method consists of mainly four modules including morpheme normalization, phrase-break detection, morpheme to phoneme conversion and phoneme connectivity check. The morpheme normalization is to replace non-Korean symbols into standard Korean graphemes. The phrase-break detector assigns phrase breaks using part-of-speech (POS) information. In the morpheme-to-phoneme conversion module, each morpheme in the phrase is converted into phonetic patterns by looking up the morpheme phonetic pattern dictionary which contains candidate phonological changes in boundaries of the morphemes. Graphemes within a morpheme are grouped into CCV patterns and converted into phonemes by the CCV conversion rules. The phoneme connectivity table supports grammaticality checking of the adjacent two phonetic morphemes. In the experiments with a corpus of 4,973 sentences, we achieved 99.9% of the graphemeto-phoneme conversion performance and 97.5% of the sentence conversion performance. The full Korean TTS system is now being implemented using this conversion method.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P98-1113",
  "title": "A Flexible Example-Based Parser Based on the SSTC",
  "year": 1998,
  "venue": "COLING",
  "abstract": "In this paper we sketch an approach for Natural Language parsing. Our approach is an example-based approach, which relies mainly on examples that already parsed to their representation structure, and on the knowledge that we can get from these examples the required information to parse a new input sentence. In our approach, examples are annotated with the Structured String Tree Correspondence (SSTC) annotation schema where each SSTC describes a sentence, a representation tree as well as the correspondence between substrings in the sentence and subtrees in the representation tree. In the process of parsing, we first try to build subtrees for phrases in the input sentence which have been successfully found in the example-base a bottom up approach. These subtrees will then be combined together to form a single rooted representation tree based on an example with similar representation structure a top down approach.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P98-2145",
  "title": "Text Segmentation with Multiple Surface Linguistic Cues",
  "year": 1998,
  "venue": "COLING",
  "abstract": "In general, a certain range of sentences in a text, is widely assumed to form a coherent unit which is called a discourse segment. Identifying the segment boundaries is a first step to recognize the structure of a text. In this paper, we describe a method for identifying segment boundaries of a Japanese text with the aid of multiple surface linguistic cues, though our experiments might be small-scale. We also present a method of training the weights for multiple linguistic cues automatically without the overfitting problem.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/P98-2148",
  "title": "A Stochastic Language Model using Dependency and its Improvement by Word Clustering",
  "year": 1998,
  "venue": "COLING",
  "abstract": "In this paper, we present a stochastic language model for Japanese using dependency. The prediction unit in this model is all attribute of \"bunsetsu\". This is represented by the product of the head of content words and that of function words. The relation between the attributes of \"bunsetsu\" is ruled by a context-free grammar. The word sequences axe predicted from the attribute using word n-gram model. The spell of Unknow word is predicted using character n-grain model. This model is robust in that it can compute the probability of an arbitrary string and is complete in that it models from unknown word to dependency at the same time.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P98-2187",
  "title": "A Generative Lexicon Perspective for Adjectival Modification",
  "year": 1998,
  "venue": "COLING",
  "abstract": "This paper presents a semantic interpretation of adjectival modification in terms of the Generative Lexicon. It highlights the elements which can be borrowed from the GL and develops limitations and extensions. We show how elements of the Qualia structure can be incorporated into semantic composition rules to make explicit the semantics of the combination adjective + noun.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/P98-2226",
  "title": "Translating Idioms",
  "year": 1998,
  "venue": "COLING",
  "abstract": "This paper discusses the treatment of fixed word expressions developed for our ITS-2 French-English translation system. This treatment makes a clear distinction between compounds i.e. multiword expressions of X°-level in which the chunks are adjacent and idiomatic phrases i.e. multiword expressions of phrasal categories, where the chunks are not necessarily adjacent. In our system, compounds are handled during the lexical analysis, while idioms are treated in the syntax, where they are treated as \"specialized lexemes\". Once recognized, an idiom can be transfered according to the specifications of the bilingual dictionary. We will show several cases of transfer to corresponding idioms in the target language, or to simple lexemes. The complete system, including several hundreds of compounds and idioms can be consulted on the Internet (http ://latl.unige.ch/itsweb.html).",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/P98-2247",
  "title": "Detecting Verbal Participation in Diathesis Alternations",
  "year": 1998,
  "venue": "COLING",
  "abstract": "We present a method for automatically identifying verbal participation in diathesis alternations. Automatically acquired subcategorization frames are compared to a hand-crafted classification for selecting candidate verbs. The minimum description length principle is then used to produce a model and cost for storing the head noun instances from a training corpus at the relevant argument slots. Alternating subcategorization frames are identified where the data from corresponding argument slots in the respective frames can be combined to produce a cheaper model than that produced if the data is encoded separately. I.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/C16-1052",
  "title": "Product Classification in E-Commerce using Distributional Semantics",
  "year": 2016,
  "venue": "COLING",
  "abstract": "Product classification is the task of automatically predicting a taxonomy path for a product in a predefined taxonomy hierarchy given a textual product description or title. For efficient product classification we require a suitable representation for a document (the textual description of a product) feature vector and efficient and fast algorithms for prediction.To address the above challenges, we propose a new distributional semantics representation for document vector formation. We also develop a new two-level ensemble approach utilising (with respect to the taxonomy tree) path-wise, node-wise and depth-wise classifiers to reduce error in the final product classification task. Our experiments show the effectiveness of the distributional representation and the ensemble approach on data sets from a leading e-commerce platform and achieve improved results on various evaluation metrics compared to earlier approaches.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C16-1104",
  "title": "A Hybrid Approach to Generation of Missing Abstracts in Biomedical Literature",
  "year": 2016,
  "venue": "COLING",
  "abstract": "Readers usually rely on abstracts to identify relevant medical information from scientific articles. Abstracts are also essential to advanced information retrieval methods. More than 50 thousand scientific publications in PubMed lack author-generated abstracts, and the relevancy judgements for these papers have to be based on their titles alone. In this paper, we propose a hybrid summarization technique that aims to select the most pertinent sentences from articles to generate an extractive summary in lieu of a missing abstract. We combine i) health outcome detection, ii) keyphrase extraction, and iii) textual entailment recognition between sentences. We evaluate our hybrid approach and analyze the improvements of multi-factor summarization over techniques that rely on a single method, using a collection of 295 manually generated reference summaries. The obtained results show that the hybrid approach outperforms the baseline techniques with an improvement of 13% in recall and 4% in F1 score.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/C16-1171",
  "title": "A Distribution-based Model to Learn Bilingual Word Embeddings",
  "year": 2016,
  "venue": "COLING",
  "abstract": "We introduce a distribution based model to learn bilingual word embeddings from monolingual data. It is simple, effective and does not require any parallel data or any seed lexicon. We take advantage of the fact that word embeddings are usually in form of dense real-valued low-dimensional vector and therefore the distribution of them can be accurately estimated. A novel cross-lingual learning objective is proposed which directly matches the distributions of word embeddings in one language with that in the other language. During the joint learning process, we dynamically estimate the distributions of word embeddings in two languages respectively and minimize the dissimilarity between them through standard back propagation algorithm. Our learned bilingual word embeddings allow to group each word and its translations together in the shared vector space. We demonstrate the utility of the learned embeddings on the task of finding word-to-word translations from monolingual corpora. Our model achieved encouraging performance on data in both related languages and substantially different languages.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C16-1182",
  "title": "Representation and Learning of Temporal Relations",
  "year": 2016,
  "venue": "COLING",
  "abstract": "Determining the relative order of events and times described in text is an important problem in natural language processing. It is also a difficult one: general state-of-the-art performance has been stuck at a relatively low ceiling for years. We investigate the representation of temporal relations, and empirically evaluate the effect that various temporal relation representations have on machine learning performance. While machine learning performance decreases with increased representational expressiveness, not all representation simplifications have equal impact.",
  "stance": -0.6
 },
 {
  "url": "https://aclanthology.org/C16-1277",
  "title": "Keyphrase Annotation with Graph Co-Ranking",
  "year": 2016,
  "venue": "COLING",
  "abstract": "Keyphrase annotation is the task of identifying textual units that represent the main content of a document. Keyphrase annotation is either carried out by extracting the most important phrases from a document, keyphrase extraction, or by assigning entries from a controlled domain-specific vocabulary, keyphrase assignment. Assignment methods are generally more reliable. They provide better-formed keyphrases, as well as keyphrases that do not occur in the document. But they are often silent on the contrary of extraction methods that do not depend on manually built resources. This paper proposes a new method to perform both keyphrase extraction and keyphrase assignment in an integrated and mutual reinforcing manner. Experiments have been carried out on datasets covering different domains of humanities and social sciences. They show statistically significant improvements compared to both keyphrase extraction and keyphrase assignment state-of-the art methods.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C18-1116",
  "title": "Deep Neural Networks at the Service of Multilingual Parallel Sentence Extraction",
  "year": 2018,
  "venue": "COLING",
  "abstract": "Wikipedia provides an invaluable source of parallel multilingual data, which are in high demand for various sorts of linguistic inquiry, including both theoretical and practical studies. We introduce a novel end-to-end neural model for large-scale parallel data harvesting from Wikipedia. Our model is language-independent, robust, and highly scalable. We use our system for collecting parallel German-English, French-English and Persian-English sentences. Human evaluations at the end show the strong performance of this model in collecting high-quality parallel data. We also propose a statistical framework which extends the results of our human evaluation to other language pairs. Our model also obtained a state-of-the-art result on the German-English dataset of BUCC 2017 shared task on parallel sentence extraction from comparable corpora.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C18-1138",
  "title": "Relation Induction in Word Embeddings Revisited",
  "year": 2018,
  "venue": "COLING",
  "abstract": "Given a set of instances of some relation, the relation induction task is to predict which other word pairs are likely to be related in the same way. While it is natural to use word embeddings for this task, standard approaches based on vector translations turn out to perform poorly. To address this issue, we propose two probabilistic relation induction models. The first model is based on translations, but uses Gaussians to explicitly model the variability of these translations and to encode soft constraints on the source and target words that may be chosen. In the second model, we use Bayesian linear regression to encode the assumption that there is a linear relationship between the vector representations of related words, which is considerably weaker than the assumption underlying translation based models.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/C18-1150",
  "title": "A Reinforcement Learning Framework for Natural Question Generation using Bi-discriminators",
  "year": 2018,
  "venue": "COLING",
  "abstract": "Visual Question Generation (VQG) aims to ask natural questions about an image automatically. Existing research focus on training model to fit the annotated data set that makes it indifferent from other language generation tasks. We argue that natural questions need to have two specific attributes from the perspectives of content and linguistic respectively, namely, natural and human-written. Inspired by the setting of discriminator in adversarial learning, we propose two discriminators, one for each attribute, to enhance the training. We then use the reinforcement learning framework to incorporate scores from the two discriminators as the reward to guide the training of the question generator. Experimental results on a benchmark VQG dataset show the effectiveness and robustness of our model compared to some state-of-the-art models in terms of both automatic and human evaluation metrics.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/C18-1157",
  "title": "Recognizing Humour using Word Associations and Humour Anchor Extraction",
  "year": 2018,
  "venue": "COLING",
  "abstract": "This paper attempts to marry the interpretability of statistical machine learning approaches with the more robust models of joke structure and joke semantics capable of being learned by neural models. Specifically, we explore the use of semantic relatedness features based on word associations, rather than the more common Word2Vec similarity, on a binary humour identification task and identify several factors that make word associations a better fit for humour. We also explore the effects of using joke structure, in the form of humour anchors (Yang et al., 2015), for improving the performance of semantic features and show that, while an intriguing idea, humour anchors contain several pitfalls that can hurt performance.",
  "stance": -0.3
 },
 {
  "url": "https://aclanthology.org/C18-1192",
  "title": "Exploratory Neural Relation Classification for Domain Knowledge Acquisition",
  "year": 2018,
  "venue": "COLING",
  "abstract": "The state-of-the-art methods for relation classification are primarily based on deep neural net- works. This kind of supervised learning method suffers from not only limited training data, but also the large number of low-frequency relations in specific domains. In this paper, we propose the task of exploratory relation classification for domain knowledge harvesting. The goal is to learn a classifier on pre-defined relations and discover new relations expressed in texts. A dynamically structured neural network is introduced to classify entity pairs to a continuously expanded relation set. We further propose the similarity sensitive Chinese restaurant process to discover new relations. Experiments conducted on a large corpus show the effectiveness of our neural network, while new relations are discovered with high precision and recall.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/C18-1217",
  "title": "Urdu Word Segmentation using Conditional Random Fields (CRFs)",
  "year": 2018,
  "venue": "COLING",
  "abstract": "State-of-the-art Natural Language Processing algorithms rely heavily on efficient word segmentation. Urdu is amongst languages for which word segmentation is a complex task as it exhibits space omission as well as space insertion issues. This is partly due to the Arabic script which although cursive in nature, consists of characters that have inherent joining and non-joining attributes regardless of word boundary. This paper presents a word segmentation system for Urdu which uses a Conditional Random Field sequence modeler with orthographic, linguistic and morphological features. Our proposed model automatically learns to predict white space as word boundary as well as Zero Width Non-Joiner (ZWNJ) as sub-word boundary. Using a manually annotated corpus, our model achieves F1 score of 0.97 for word boundary identification and 0.85 for sub-word boundary identification tasks. We have made our code and corpus publicly available to make our results reproducible.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.coling-main.117",
  "title": "What Can We Learn from Noun Substitutions in Revision Histories?",
  "year": 2020,
  "venue": "COLING",
  "abstract": "In community-edited resources such as wikiHow, sentences are subject to revisions on a daily basis. Recent work has shown that resulting improvements over time can be modelled computationally, assuming that each revision contributes to the improvement. We take a closer look at a subset of such revisions, for which we attempt to improve a computational model and validate in how far the assumption that 'revised means better' actually holds. The subset of revisions considered here are noun substitutions, which often involve interesting semantic relations, including synonymy, antonymy and hypernymy. Despite the high semantic relatedness, we find that a supervised classifier can distinguish the revised version of a sentence from an original version with an accuracy close to 70%, when taking context into account. In a human annotation study, we observe that annotators identify the revised sentence as the 'better version' with similar performance. Our analysis reveals a fair agreement among annotators when a revision improves fluency. In contrast, noun substitutions that involve other lexical-semantic relationships are often perceived as being equally good or tend to cause disagreements. While these findings are also reflected in classification scores, a comparison of results shows that our model fails in cases where humans can resort to factual knowledge or intuitions about the required level of specificity.",
  "stance": 0.1
 },
 {
  "url": "https://aclanthology.org/2020.coling-main.501",
  "title": "Metrics also Disagree in the Low Scoring Range: Revisiting Summarization Evaluation Metrics",
  "year": 2020,
  "venue": "COLING",
  "abstract": "In text summarization, evaluating the efficacy of automatic metrics without human judgments has become recently popular. One exemplar work (Peyrard, 2019) concludes that automatic metrics strongly disagree when ranking high-scoring summaries. In this paper, we revisit their experiments and find that their observations stem from the fact that metrics disagree in ranking summaries from any narrow scoring range. We hypothesize that this may be because summaries are similar to each other in a narrow scoring range and are thus, difficult to rank. Apart from the width of the scoring range of summaries, we analyze three other properties that impact inter-metric agreement - Ease of Summarization, Abstractiveness, and Coverage.",
  "stance": -0.5
 },
 {
  "url": "https://aclanthology.org/2020.semeval-1.175",
  "title": "NITS-Hinglish-SentiMix at SemEval-2020 Task 9: Sentiment Analysis for Code-Mixed Social Media Text Using an Ensemble Model",
  "year": 2020,
  "venue": "COLING",
  "abstract": "Sentiment Analysis refers to the process of interpreting what a sentence emotes and classifying them as positive, negative, or neutral. The widespread popularity of social media has led to the generation of a lot of text data and specifically, in the Indian social media scenario, the code-mixed Hinglish text i.e, the words of Hindi language, written in the Roman script along with other English words is a common sight. The ability to effectively understand the sentiments in these texts is much needed. This paper proposes a system titled NITS-Hinglish to effectively carry out the sentiment analysis of such code-mixed Hinglish text. The system has fared well with a final F-Score of 0.617 on the test data.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.semeval-1.198",
  "title": "AdelaideCyC at SemEval-2020 Task 12: Ensemble of Classifiers for Offensive Language Detection in Social Media",
  "year": 2020,
  "venue": "COLING",
  "abstract": "This paper describes the systems our team (AdelaideCyC) has developed for SemEval Task 12 (OffensEval 2020) to detect offensive language in social media. The challenge focuses on three subtasks - offensive language identification (subtask A), offense type identification (subtask B), and offense target identification (subtask C). Our team has participated in all the three subtasks. We have developed machine learning and deep learning-based ensembles of models. We have achieved F1-scores of 0.906, 0.552, and 0.623 in subtask A, B, and C respectively. While our performance scores are promising for subtask A, the results demonstrate that subtask B and C still remain challenging to classify.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/2020.semeval-1.218",
  "title": "LAST at SemEval-2020 Task 10: Finding Tokens to Emphasise in Short Written Texts with Precomputed Embedding Models and LightGBM",
  "year": 2020,
  "venue": "COLING",
  "abstract": "To select tokens to be emphasised in short texts, a system mainly based on precomputed embedding models, such as BERT and ELMo, and LightGBM is proposed. Its performance is low. Additional analyzes suggest that its effectiveness is poor at predicting the highest emphasis scores while they are the most important for the challenge and that it is very sensitive to the specific instances provided during learning.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/2020.semeval-1.266",
  "title": "JCT at SemEval-2020 Task 12: Offensive Language Detection in Tweets Using Preprocessing Methods, Character and Word N-grams",
  "year": 2020,
  "venue": "COLING",
  "abstract": "In this paper, we describe our submissions to SemEval-2020 contest. We tackled subtask 12 - \"Multilingual Offensive Language Identification in Social Media\". We developed different models for four languages: Arabic, Danish, Greek, and Turkish. We applied three supervised machine learning methods using various combinations of character and word n-gram features. In addition, we applied various combinations of basic preprocessing methods. Our best submission was a model we built for offensive language identification in Danish using Random Forest. This model was ranked at the 6 position out of 39 submissions. Our result is lower by only 0.0025 than the result of the team that won the 4 place using entirely non-neural methods. Our experiments indicate that char ngram features are more helpful than word ngram features. This phenomenon probably occurs because tweets are more characterized by characters than by words, tweets are short, and contain various special sequences of characters, e.g., hashtags, shortcuts, slang words, and typos.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/W98-1211",
  "title": "Linguistic Theory in Statistical Language Learning",
  "year": 1998,
  "venue": "CoNLL",
  "abstract": "This article attempts to determine what elements of linguistic theory are used in statistical language learning, and why the extracted language models look like they do. The study indicates that some linguistic elements, such as the notion of a word, are simply too useful to be ignored. The second most important factor seems to be features inherited from the original task for which the technique was used, for example using hidden Markov models for partof-speech tagging, rather than speech recognition. The two remaining important factors are properties of the runtime processing scheme employing the extracted language model, and the properties of the available corpus resources to which the statistical learning techniques are applied. Deliberate attempts to include linguistic theory seem to end up in a fifth place.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/W98-1227",
  "title": "A Method of Incorporating Bigram Constraints into an LR Table and Its Effectiveness in Natural Language Processing",
  "year": 1998,
  "venue": "CoNLL",
  "abstract": "In this paper, we propose a method for constructing bigram LR tables by way of incorporating bigram constraints into an LR table. Using a bigram LR table, it is possible for a GLR parser to make use of both big'ram and CFG constraints in natural language processing. Applying bigram LR tables to our GLR method has the following advantages: (1) Language models utilizing bigzam LR tables have lower perplexity than simple bigram language models, since local constraints (higram) and global constraints (CFG) are combined in a single bigram LR table. (2) Bigram constraints are easily acquired from a given corpus. Therefore data sparseness is not likely to arise. (3) Separation of local and global constraints keeps down the number of CFG rules. The first advantage leads to a reduction in complexity, and as the result, better performance in GLR parsing. Our experiments demonstrate the effectiveness of our method.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/W99-0706",
  "title": "Learning Transformation Rules to Find Grammatical Relations",
  "year": 1999,
  "venue": "CoNLL",
  "abstract": "Grammatical relationships are an important level of natural language processing. We present a trainable approach to find these relationships through transformation sequences and-error-driven learning. Our approach finds grammatical relationships between core syntax groups and bypasses much of the parsing phase. On our training and test set, our procedure achieves 63.6% recall and 77.3% precision (f-score = 69.8).",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/W00-0716",
  "title": "Generating Synthetic Speech Prosody with Lazy Learning in Tree Structures",
  "year": 2000,
  "venue": "CoNLL",
  "abstract": "We present ongoing work on prosody prediction for speech synthesis. This approach considers sentences as tree structures and infers the prosody from a corpus of such structures using machine learning techniques. The prediction is achieved from the prosody of the closest sentence of the corpus through tree similarity measurements, using either the nearest neighbour algorithm or an analogy-based approach. We introduce two different tree structure representations, the tree similarity metrics considered, and then we discuss the different prediction methods. Experiments are currently under process to qualify this approach.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/W00-0729",
  "title": "Chunking with Maximum Entropy Models",
  "year": 2000,
  "venue": "CoNLL",
  "abstract": "Maximum Entropy (MaxEnt) models (Jaynes, 1957) are exponential models that implement the intuition that if there is no evidence to favour one alternative solution above another, both alternatives should be equally likely. In order to accomplish this, as much information as possible about the process you want to model must be collected. This information consists of frequencies of events relevant to the process. The frequencies of relevant events are considered to be properties of the process. When building a model we have to constrain our attention to models with these properties. In most cases the process is only partially described. The MaxEnt framework now demands that from all the models that satisfy these constraints, we choose the model with the flattest probability distribution. This is the model with the highest entropy (given the fact that the constraints are met). When we are looking for a conditional model P(w]h), the MaxEnt solution has the form:",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/W00-0730",
  "title": "Use of Support Vector Learning for Chunk Identification",
  "year": 2000,
  "venue": "CoNLL",
  "abstract": "Support Vector Machines (SVMs), first introduced by Vapnik (Cortes and Vapnik, 1995; Vapnik, 1995), are relatively new learning approaches for solving two-class pat tern recognition problems. SVMs are well-known for their good generalization performance, and have been applied to many pat tern recognition problems. In the field of natural language processing, SVMs are applied to text categorization, and are reported to have achieved high accuracy without falling into over-fitting even with a large number of words taken as the features (Joachims, 1998; Taira and Haruno, 1999) First of all, let us define the training data which belongs to either positive or negative class as follows:",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/W00-0735",
  "title": "Single-Classifier Memory-Based Phrase Chunking",
  "year": 2000,
  "venue": "CoNLL",
  "abstract": "In the shared task for CoNLL-2000, words and tags form the basic multi-valued features for predicting a rich phrase segmentation code. While the tag features, containing WSJ paxt-ofspeech tags (Marcus et al., 1993), have about 45 values, the word features have more than 10,000 values. In our study we have looked at how memory-based learning, as implemented in the TiMBL software system (Daelemans et al., 2000), can handle such features. We have limited our search to single classifiers, thereby explicitly ignoring the possibility to build a metalearning classifier architecture that could be expected to improve accuracy. Given this restriction we have explored the following:",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/W01-0712",
  "title": "Learning Computational Grammars",
  "year": 2001,
  "venue": "CoNLL",
  "abstract": "This paper reports on the LEARNING COMPUTATIONAL GRAMMARS (LCG) project, a postdoc network devoted to studying the application of machine learning techniques to grammars suitable for computational use. We were interested in a more systematic survey to understand the relevance of many factors to the success of learning, esp. the availability of annotated data, the kind of dependencies in the data, and the availability of knowledge bases (grammars). We focused on syntax, esp. noun phrase (NP) syntax.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/W05-0623",
  "title": "A Joint Model for Semantic Role Labeling",
  "year": 2005,
  "venue": "CoNLL",
  "abstract": "We present a semantic role labeling system submitted to the closed track of the CoNLL-2005 shared task. The system, introduced in (Toutanova et al., 2005), implements a joint model that captures dependencies among arguments of a predicate using log-linear models in a discriminative re-ranking framework. We also describe experiments aimed at increasing the robustness of the system in the presence of syntactic parse errors. Our final system achieves F1-Measures of 76.68 and 78.45 on the development and the WSJ portion of the test set, respectively.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/W06-2901",
  "title": "A Mission for Computational Natural Language Learning",
  "year": 2006,
  "venue": "CoNLL",
  "abstract": "In this presentation, I will look back at 10 years of CoNLL conferences and the state of the art of machine learning of language that is evident from this decade of research. My conclusion, intended to provoke discussion, will be that we currently lack a clear motivation or “mission” to survive as a discipline. I will suggest that a new mission for the field could be found in a renewed interest for theoretical work (which learning algorithms have a bias that matches the properties of language?, what is the psycholinguistic relevance of learner design issues?), in more sophisticated comparative methodology, and in solving the problem of transfer, reusability, and adaptation of learned knowledge.",
  "stance": -0.3
 },
 {
  "url": "https://aclanthology.org/W06-2907",
  "title": "Investigating Lexical Substitution Scoring for Subtitle Generation",
  "year": 2006,
  "venue": "CoNLL",
  "abstract": "This paper investigates an isolated setting of the lexical substitution task of replacing words with their synonyms. In particular, we examine this problem in the setting of subtitle generation and evaluate state of the art scoring methods that predict the validity of a given substitution. The paper evaluates two context independent models and two contextual models. The major findings suggest that distributional similarity provides a useful complementary estimate for the likelihood that two Wordnet synonyms are indeed substitutable, while proper modeling of contextual constraints is still a challenging task for future research.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/W06-2923",
  "title": "LingPars, a Linguistically Inspired, Language-Independent Machine Learner for Dependency Treebanks",
  "year": 2006,
  "venue": "CoNLL",
  "abstract": "This paper presents a Constraint Grammarinspired machine learner and parser, Ling­ Pars, that assigns dependencies to morpho­ logically annotated treebanks in a functioncentred way. The system not only bases at­ tachment probabilities for PoS, case, mood, lemma on those features' function probabili­ ties, but also uses topological features like function/PoS n-grams, barrier tags and daughter-sequences. In the CoNLL shared task, performance was below average on at­ tachment scores, but a relatively higher score for function tags/deprels in isolation suggests that the system's strengths were not fully exploited in the current architecture.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D07-1008",
  "title": "Large Margin Synchronous Generation and its Application to Sentence Compression",
  "year": 2007,
  "venue": "CoNLL",
  "abstract": "This paper presents a tree-to-tree transduction method for text rewriting. Our model is based on synchronous tree substitution grammar, a formalism that allows local distortion of the tree topology and can thus naturally capture structural mismatches. We describe an algorithm for decoding in this framework and show how the model can be trained discriminatively within a large margin framework. Experimental results on sentence compression bring significant improvements over a state-of-the-art model.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D07-1011",
  "title": "Incremental Generation of Plural Descriptions: Similarity and Partitioning",
  "year": 2007,
  "venue": "CoNLL",
  "abstract": "Approaches to plural reference generation emphasise descriptive brevity, but often lack empirical backing. This paper describes a corpus-based study of plural descriptions, and proposes a psycholinguisticallymotivated algorithm for plural reference generation. The descriptive strategy is based on partitioning and incorporates corpusderived heuristics. An exhaustive evaluation shows that the output closely matches human data.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/D07-1030",
  "title": "Using RBMT Systems to Produce Bilingual Corpus for SMT",
  "year": 2007,
  "venue": "CoNLL",
  "abstract": "This paper proposes a method using the existing Rule-based Machine Translation (RBMT) system as a black box to produce synthetic bilingual corpus, which will be used as training data for the Statistical Machine Translation (SMT) system. We use the existing RBMT system to translate the monolingual corpus into synthetic bilingual corpus. With the synthetic bilingual corpus, we can build an SMT system even if there is no real bilingual corpus. In our experiments using BLEU as a metric, the system achieves a relative improvement of 11.7% over the best RBMT system that is used to produce the synthetic bilingual corpora. We also interpolate the model trained on a real bilingual corpus and the models trained on the synthetic bilingual corpora. The interpolated model achieves an absolute improvement of 0.0245 BLEU score (13.1% relative) as compared with the individual model trained on the real bilingual corpus.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D07-1091",
  "title": "Factored Translation Models",
  "year": 2007,
  "venue": "CoNLL",
  "abstract": "We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level — may it be linguistic markup or automatically generated word classes. In a number of experiments we show that factored translation models lead to better translation performance, both in terms of automatic scores, as well as more grammatical coherence.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D07-1102",
  "title": "Log-Linear Models of Non-Projective Trees, $k$-best MST Parsing and Tree-Ranking",
  "year": 2007,
  "venue": "CoNLL",
  "abstract": "We present our system used in the CoNLL 2007 shared task on multilingual parsing. The system is composed of three components: a k-best maximum spanning tree (MST) parser, a tree labeler, and a reranker that orders the k-best labeled trees. We present two techniques for training the MST parser: tree-normalized and graphnormalized conditional training. The treebased reranking model allows us to explicitly model global syntactic phenomena. We describe the reranker features which include non-projective edge attributes. We provide an analysis of the errors made by our system and suggest changes to the models and features that might rectify the current system.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D07-1112",
  "title": "Frustratingly Hard Domain Adaptation for Dependency Parsing",
  "year": 2007,
  "venue": "CoNLL",
  "abstract": "We describe some challenges of adaptation in the 2007 CoNLL Shared Task on Domain Adaptation. Our error analysis for this task suggests that a primary source of error is differences in annotation guidelines between treebanks. Our suspicions are supported by the observation that no team was able to improve target domain performance substantially over a state of the art baseline.",
  "stance": -0.1
 },
 {
  "url": "https://aclanthology.org/W08-2108",
  "title": "Fast Mapping in Word Learning: What Probabilities Tell Us",
  "year": 2008,
  "venue": "CoNLL",
  "abstract": "Children can determine the meaning of a new word from hearing it used in a familiar context—an ability often referred to as fast mapping. In this paper, we study fast mapping in the context of a general probabilistic model of word learning. We use our model to simulate fast mapping experiments on children, such as referent selection and retention. The word learning model can perform these tasks through an inductive interpretation of the acquired probabilities. Our results suggest that fast mapping occurs as a natural consequence of learning more words, and provides explanations for the (occasionally contradictory) child experimental data.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/W08-2125",
  "title": "Collective Semantic Role Labelling with Markov Logic",
  "year": 2008,
  "venue": "CoNLL",
  "abstract": "This paper presents our system for the Open Track of the CoNLL 2008 Shared Task (Surdeanu et al., 2008) in Joint Dependency Parsing1 and Semantic Role Labelling. We use Markov Logic to define a joint SRL model and achieve a semantic F-score of 74.59%, the second best in the Open Track.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/W08-2130",
  "title": "Discriminative Learning of Syntactic and Semantic Dependencies",
  "year": 2008,
  "venue": "CoNLL",
  "abstract": "A Maximum Entropy Model based system for discriminative learning of syntactic and semantic dependencies submitted to the CoNLL-2008 shared task (Surdeanu, et al., 2008) is presented in this paper. The system converts the dependency learning task to classification issues and reconstructs the dependent relations based on classification results. Finally F1 scores of 86.69, 69.95 and 78.35 are obtained for syntactic dependencies, semantic dependencies and the whole system respectively in closed challenge. For open challenge the corresponding F1 scores are 86.69, 68.99 and 77.84.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/W08-2140",
  "title": "Applying Sentence Simplification to the CoNLL-2008 Shared Task",
  "year": 2008,
  "venue": "CoNLL",
  "abstract": "Our submission to the CoNLL-2008 shared task (Surdeanu et al., 2008) focused on applying a novel method for semantic role labeling to the shared task. Our system first simplifies each sentence to be labeled using a set of hand-constructed rules; the weights of the system are trained on semantic role labeling data to generate simplifications which are as useful as possible for semantic role labeling. Our system is only a semantic role labeling system, and thus did not receive a score for Syntactic Dependencies (or, by extension, a score for the complete problem). Unlike most systems in the shared task, our system took constituency parses as input. On the subtask of semantic dependencies, our system obtained an F1 score of 76.17, the highest in the open task. In this paper we give a high-level overview of the sentence simplification system, and discuss and analyze the modifications to this system required for the CoNLL-2008 shared task.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/W09-1101",
  "title": "Joint Inference for Natural Language Processing",
  "year": 2009,
  "venue": "CoNLL",
  "abstract": "of the Invited Talk In recent decades, researchers in natural language processing have made great progress on welldefined subproblems such as part-of-speech tagging, phrase chunking, syntactic parsing, named-entity recognition, coreference and semantic-role labeling. Better models, features, and learning algorithms have allowed systems to perform many of these tasks with 90% accuracy or better. However, success in integrated, end-to-end natural language understanding",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/W09-1110",
  "title": "Interactive Feature Space Construction using Semantic Information",
  "year": 2009,
  "venue": "CoNLL",
  "abstract": "Specifying an appropriate feature space is an important aspect of achieving good performance when designing systems based upon learned classifiers. Effectively incorporating information regarding semantically related words into the feature space is known to produce robust, accurate classifiers and is one apparent motivation for efforts to automatically generate such resources. However, naive incorporation of this semantic information may result in poor performance due to increased ambiguity. To overcome this limitation, we introduce the interactive feature space construction protocol, where the learner identifies inadequate regions of the feature space and in coordination with a domain expert adds descriptiveness through existing semantic resources. We demonstrate effectiveness on an entity and relation extraction system including both performance improvements and robustness to reductions in annotated data.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/W09-1115",
  "title": "Investigating Automatic Alignment Methods for Slide Generation from Academic Papers",
  "year": 2009,
  "venue": "CoNLL",
  "abstract": "In this paper we investigate the task of automatic generation of slide presentations from academic papers, focusing initially on slide to paper alignment. We compare and evaluate four different alignment systems which utilize various combinations of methods used widely in other alignment and question answering approaches, such as TF-IDF term weighting and query expansion. Our best aligner achieves an accuracy of 75% and our findings show that for this application, average TF-IDF scoring performs more poorly than a simpler method based on the number of matched terms, and query expansion degrades aligner performance.",
  "stance": -0.4
 },
 {
  "url": "https://aclanthology.org/W09-1122",
  "title": "Lexical Patterns or Dependency Patterns: Which Is Better for Hypernym Extraction?",
  "year": 2009,
  "venue": "CoNLL",
  "abstract": "We compare two different types of extraction patterns for automatically deriving semantic information from text: lexical patterns, built from words and word class information, and dependency patterns with syntactic information obtained from a full parser. We are particularly interested in whether the richer linguistic information provided by a parser allows for a better performance of subsequent information extraction work. We evaluate automatic extraction of hypernym information from text and conclude that the application of dependency patterns does not lead to substantially higher precision and recall scores than using lexical patterns.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/W10-2909",
  "title": "Improved Unsupervised POS Induction Using Intrinsic Clustering Quality and a Zipfian Constraint",
  "year": 2010,
  "venue": "CoNLL",
  "abstract": "Modern unsupervised POS taggers usually apply an optimization procedure to a nonconvex function, and tend to converge to local maxima that are sensitive to starting conditions. The quality of the tagging induced by such algorithms is thus highly variable, and researchers report average results over several random initializations. Consequently, applications are not guaranteed to use an induced tagging of the quality reported for the algorithm. In this paper we address this issue using an unsupervised test for intrinsic clustering quality. We run a base tagger with different random initializations, and select the best tagging using the quality test. As a base tagger, we modify a leading unsupervised POS tagger (Clark, 2003) to constrain the distributions of word types across clusters to be Zipfian, allowing us to utilize a perplexity-based quality test. We show that the correlation between our quality test and gold standard-based tagging quality measures is high. Our results are better in most evaluation measures than all results reported in the literature for this task, and are always better than the Clark average results.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/W10-2927",
  "title": "Inspecting the Structural Biases of Dependency Parsing Algorithms",
  "year": 2010,
  "venue": "CoNLL",
  "abstract": "We propose the notion of a structural bias inherent in a parsing system with respect to the language it is aiming to parse. This structural bias characterizes the behaviour of a parsing system in terms of structures it tends to underand overproduce. We propose a Boosting-based method for uncovering some of the structural bias inherent in parsing systems. We then apply our method to four English dependency parsers (an Arc-Eager and Arc-Standard transition-based parsers, and firstand second-order graph-based parsers). We show that all four parsers are biased with respect to the kind of annotation they are trained to parse. We present a detailed analysis of the biases that highlights specific differences and commonalities between the parsing systems, and improves our understanding of their strengths and weaknesses.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/W11-0310",
  "title": "Gender Attribution: Tracing Stylometric Evidence Beyond Topic and Genre",
  "year": 2011,
  "venue": "CoNLL",
  "abstract": "Sociolinguistic theories (e.g., Lakoff (1973)) postulate that women’s language styles differ from that of men. In this paper, we explore statistical techniques that can learn to identify the gender of authors in modern English text, such as web blogs and scientific papers. Although recent work has shown the efficacy of statistical approaches to gender attribution, we conjecture that the reported performance might be overly optimistic due to non-stylistic factors such as topic bias in gender that can make the gender detection task easier. Our work is the first that consciously avoids gender bias in topics, thereby providing stronger evidence to gender-specific styles in language beyond topic. In addition, our comparative study provides new insights into robustness of various stylometric techniques across topic and genre.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D12-1043",
  "title": "An \"AI readability\" Formula for French as a Foreign Language",
  "year": 2012,
  "venue": "CoNLL",
  "abstract": "This paper present a new readability formula for French as a foreign language (FFL), which relies on 46 textual features representative of the lexical, syntactic, and semantic levels as well as some of the specificities of the FFL context. We report comparisons between several techniques for feature selection and various learning algorithms. Our best model, based on support vector machines (SVM), significantly outperforms previous FFL formulas. We also found that semantic features behave poorly in our case, in contrast with some previous readability studies on English as a first language.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D12-1116",
  "title": "Entity based Q&A Retrieval",
  "year": 2012,
  "venue": "CoNLL",
  "abstract": "Bridging the lexical gap between the user’s question and the question-answer pairs in the Q&A archives has been a major challenge for Q&A retrieval. State-of-the-art approaches address this issue by implicitly expanding the queries with additional words using statistical translation models. While useful, the effectiveness of these models is highly dependant on the availability of quality corpus in the absence of which they are troubled by noise issues. Moreover these models perform word based expansion in a context agnostic manner resulting in translation that might be mixed and fairly general. This results in degraded retrieval performance. In this work we address the above issues by extending the lexical word based translation model to incorporate semantic concepts (entities). We explore strategies to learn the translation probabilities between words and the concepts using the Q&A archives and a popular entity catalog. Experiments conducted on a large scale real data show that the proposed techniques are promising.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/W13-3502",
  "title": "Analysis of Stopping Active Learning based on Stabilizing Predictions",
  "year": 2013,
  "venue": "CoNLL",
  "abstract": "Within the natural language processing (NLP) community, active learning has been widely investigated and applied in order to alleviate the annotation bottleneck faced by developers of new NLP systems and technologies. This paper presents the first theoretical analysis of stopping active learning based on stabilizing predictions (SP). The analysis has revealed three elements that are central to the success of the SP method: (1) bounds on Cohen’s Kappa agreement between successively trained models impose bounds on differences in F-measure performance of the models; (2) since the stop set does not have to be labeled, it can be made large in practice, helping to guarantee that the results transfer to previously unseen streams of examples at test/application time; and (3) good (low variance) sample estimates of Kappa between successive models can be obtained. Proofs of relationships between the level of Kappa agreement and the difference in performance between consecutive models are presented. Specifically, if the Kappa agreement between two models exceeds a threshold T (where T > 0), then the difference in F-measure performance between those models is bounded above by 4(1−T ) T in all cases. If precision of the positive conjunction of the models is assumed to be p, then the bound can be tightened to 4(1−T ) (p+1)T .",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/W13-3511",
  "title": "Hidden Markov tree models for semantic class induction",
  "year": 2013,
  "venue": "CoNLL",
  "abstract": "In this paper, we propose a new method for semantic class induction. First, we introduce a generative model of sentences, based on dependency trees and which takes into account homonymy. Our model can thus be seen as a generalization of Brown clustering. Second, we describe an efficient algorithm to perform inference and learning in this model. Third, we apply our proposed method on two large datasets (108 tokens, 105 words types), and demonstrate that classes induced by our algorithm improve performance over Brown clustering on the task of semisupervised supersense tagging and named entity recognition.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/W14-1601",
  "title": "What's in a p-value in NLP?",
  "year": 2014,
  "venue": "CoNLL",
  "abstract": "In NLP, we need to document that our proposed methods perform significantly better with respect to standard metrics than previous approaches, typically by reporting p-values obtained by rankor randomization-based tests. We show that significance results following current research standards are unreliable and, in addition, very sensitive to sample size, covariates such as sentence length, as well as to the existence of multiple metrics. We estimate that under the assumption of perfect metrics and unbiased data, we need a significance cut-off at ⇠0.0025 to reduce the risk of false positive results to <5%. Since in practice we often have considerable selection bias and poor metrics, this, however, will not do alone.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/K15-1033",
  "title": "Do dependency parsing metrics correlate with human judgments?",
  "year": 2015,
  "venue": "CoNLL",
  "abstract": "Using automatic measures such as labeled and unlabeled attachment scores is common practice in dependency parser evaluation. In this paper, we examine whether these measures correlate with human judgments of overall parse quality. We ask linguists with experience in dependency annotation to judge system outputs. We measure the correlation between their judgments and a range of parse evaluation metrics across five languages. The humanmetric correlation is lower for dependency parsing than for other NLP tasks. Also, inter-annotator agreement is sometimes higher than the agreement between judgments and metrics, indicating that the standard metrics fail to capture certain aspects of parse quality, such as the relevance of root attachment or the relative importance of the different parts of speech.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/K15-1037",
  "title": "One Million Sense-Tagged Instances for Word Sense Disambiguation and Induction",
  "year": 2015,
  "venue": "CoNLL",
  "abstract": "Supervised word sense disambiguation (WSD) systems are usually the best performing systems when evaluated on standard benchmarks. However, these systems need annotated training data to function properly. While there are some publicly available open source WSD systems, very few large annotated datasets are available to the research community. The two main goals of this paper are to extract and annotate a large number of samples and release them for public use, and also to evaluate this dataset against some word sense disambiguation and induction tasks. We show that the open source IMS WSD system trained on our dataset achieves stateof-the-art results in standard disambiguation tasks and a recent word sense induction task, outperforming several task submissions and strong baselines.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/K16-1025",
  "title": "Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation",
  "year": 2016,
  "venue": "CoNLL",
  "abstract": "Named Entity Disambiguation (NED) refers to the task of resolving multiple named entity mentions in a document to their correct references in a knowledge base (KB) (e.g., Wikipedia). In this paper, we propose a novel embedding method specifically designed for NED. The proposed method jointly maps words and entities into the same continuous vector space. We extend the skip-gram model by using two models. The KB graph model learns the relatedness of entities using the link structure of the KB, whereas the anchor context model aims to align vectors such that similar words and entities occur close to one another in the vector space by leveraging KB anchors and their context words. By combining contexts based on the proposed embedding with standard NED features, we achieved state-of-theart accuracy of 93.1% on the standard CoNLL dataset and 85.2% on the TAC 2010 dataset.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/K17-1023",
  "title": "Robust Coreference Resolution and Entity Linking on Dialogues: Character Identification on TV Show Transcripts",
  "year": 2017,
  "venue": "CoNLL",
  "abstract": "This paper presents a novel approach to character identification, that is an entity linking task that maps mentions to characters in dialogues from TV show transcripts. We first augment and correct several cases of annotation errors in an existing corpus so the corpus is clearer and cleaner for statistical learning. We also introduce the agglomerative convolutional neural network that takes groups of features and learns mention and mention-pair embeddings for coreference resolution. We then propose another neural model that employs the embeddings learned and creates cluster embeddings for entity linking. Our coreference resolution model shows comparable results to other state-of-the-art systems. Our entity linking model significantly outperforms the previous work, showing the F1 score of 86.76% and the accuracy of 95.30% for character identification.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/K18-1038",
  "title": "Vectorial Semantic Spaces Do Not Encode Human Judgments of Intervention Similarity",
  "year": 2018,
  "venue": "CoNLL",
  "abstract": "Despite their practical success and impressive performances, neural-network-based and distributed semantics techniques have often been criticized as they remain fundamentally opaque and difficult to interpret. In a vein similar to recent pieces of work investigating the linguistic abilities of these representations, we study another core, defining property of language: the property of long-distance dependencies. Human languages exhibit the ability to interpret discontinuous elements distant from each other in the string as if they were adjacent. This ability is blocked if a similar, but extraneous, element intervenes between the discontinuous components. We present results that show, under exhaustive and precise conditions, that one kind of word embeddings and the similarity spaces they define do not encode the properties of intervention similarity in long-distance dependencies, and that therefore they fail to represent this core linguistic notion.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/K19-1062",
  "title": "Deep Structured Neural Network for Event Temporal Relation Extraction",
  "year": 2019,
  "venue": "CoNLL",
  "abstract": "We propose a novel deep structured learning framework for event temporal relation extraction. The model consists of 1) a recurrent neural network (RNN) to learn scoring functions for pair-wise relations, and 2) a structured support vector machine (SSVM) to make joint predictions. The neural network automatically learns representations that account for long-term contexts to provide robust features for the structured model, while the SSVM incorporates domain knowledge such as transitive closure of temporal relations as constraints to make better globally consistent decisions. By jointly training the two components, our model combines the benefits of both data-driven learning and knowledge exploitation. Experimental results on three high-quality event temporal relation datasets (TCR, MATRES, and TB-Dense) demonstrate that incorporated with pre-trained contextualized embeddings, the proposed model achieves significantly better performances than the state-of-the-art methods on all three datasets. We also provide thorough ablation studies to investigate our model.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.conll-1.18",
  "title": "Processing effort is a poor predictor of cross-linguistic word order frequency",
  "year": 2020,
  "venue": "CoNLL",
  "abstract": "Some have argued that word orders which are more difficult to process should be rarer cross-linguistically. Our current study fails to replicate the results of Maurits, Navarro, and Perfors (2010), who used an entropy-based Uniform Information Density (UID) measure to moderately predict the Greenbergian typology of transitive word orders. We additionally report an inability of three measures of processing difficulty - entropy-based UID, surprisal-based UID, and pointwise mutual information - to correctly predict the correct typological distribution, using transitive constructions from 20 languages in the Universal Dependencies project (version 2.5). However, our conclusions are limited by data sparsity.",
  "stance": -0.7
 },
 {
  "url": "https://aclanthology.org/2020.conll-1.26",
  "title": "\"LazImpa\": Lazy and Impatient neural agents learn to communicate efficiently",
  "year": 2020,
  "venue": "CoNLL",
  "abstract": "Previous work has shown that artificial neural agents naturally develop surprisingly non-efficient codes. This is illustrated by the fact that in a referential game involving a speaker and a listener neural networks optimizing accurate transmission over a discrete channel, the emergent messages fail to achieve an optimal length. Furthermore, frequent messages tend to be longer than infrequent ones, a pattern contrary to the Zipf Law of Abbreviation (ZLA) observed in all natural languages. Here, we show that near-optimal and ZLA-compatible messages can emerge, but only if both the speaker and the listener are modified. We hence introduce a new communication system, \"LazImpa\", where the speaker is made increasingly lazy, i.e., avoids long messages, and the listener impatient, i.e., seeks to guess the intended content as soon as possible.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.conll-1.37",
  "title": "On the Computational Power of Transformers and Its Implications in Sequence Modeling",
  "year": 2020,
  "venue": "CoNLL",
  "abstract": "Transformers are being used extensively across several sequence modeling tasks. Significant research effort has been devoted to experimentally probe the inner workings of Transformers. However, our conceptual and theoretical understanding of their power and inherent limitations is still nascent. In particular, the roles of various components in Transformers such as positional encodings, attention heads, residual connections, and feedforward networks, are not clear. In this paper, we take a step towards answering these questions. We analyze the computational power as captured by Turing-completeness. We first provide an alternate and simpler proof to show that vanilla Transformers are Turing-complete and then we prove that Transformers with only positional masking and without any positional encoding are also Turing-complete. We further analyze the necessity of each component for the Turing-completeness of the network; interestingly, we find that a particular type of residual connection is necessary. We demonstrate the practical implications of our results via experiments on machine translation and synthetic tasks.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/2020.conll-1.39",
  "title": "Filler-gaps that neural networks fail to generalize",
  "year": 2020,
  "venue": "CoNLL",
  "abstract": "It can be difficult to separate abstract linguistic knowledge in recurrent neural networks (RNNs) from surface heuristics. In this work, we probe for highly abstract syntactic constraints that have been claimed to govern the behavior of filler-gap dependencies across different surface constructions. For models to generalize abstract patterns in expected ways to unseen data, they must share representational features in predictable ways. We use cumulative priming to test for representational overlap between disparate filler-gap constructions in English and find evidence that the models learn a general representation for the existence of filler-gap dependencies. However, we find no evidence that the models learn any of the shared underlying grammatical constraints we tested. Our work raises questions about the degree to which RNN language models learn abstract linguistic representations.",
  "stance": -0.9
 },
 {
  "url": "https://aclanthology.org/W97-0321",
  "title": "Word Sense Disambiguation Based on Structured Semantic Space",
  "year": 1997,
  "venue": "EMNLP",
  "abstract": "In this paper, we propose a framework, structured semantic space, as a foundation for word sense disarnbiguation tasks, and present a strategy to identify the correct sense of a word in some context based on the space. The semantic space is a set of multidimensional real-valued vectors, which formally describe the contexts of words. Instead of locating all word senses in the space, we only make use of mono-sense words to outline it. We design a merging procedure to establish the dendrogram structure of the space and give an heuristic algorithm to find the nodes (sense clusters) corresponding with sets of similar senses in the dendrogram. Given a word in a particular context' the context would activate some clusters in the dendrogram, based on its similarity with the contexts of the words in the clusters, then the correct sense of the word could be determined by comparing its definitions with those of the words in the clusters.",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/W98-1503",
  "title": "Aligning Clattses in Parallel Texts",
  "year": 1998,
  "venue": "EMNLP",
  "abstract": "This paper describes a method for the automatic alignment of parallel texts at clause level. The method features statistical techniques coupled with shallow linguistic processing. It presupposes a parallel bilingual corpus and identifies alignments between the clauses of the source and target language sides of the corpus. Parallel texts are first statistically aligned at sentence level and then tagged with their part-of-speech categories. Regular grammars functioning on tags, recognize clauses on both sides of the parallel text. A probabilistic model is applied next, operating on the basis of word occurrence and co-occurrence probabilities and character lengths. Depending on sentence size, possible alignments arc fed into a dynamic progranuning framework or a simulated annealing system in order to find or approximate the best alignment. 1he method has been tested on a Small English-Greek corpus consisting of texts relevant to software systems and has produced promising results in terms of correctly identified clause alignments.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/W98-1509",
  "title": "An Empirical Approach to Text Categorization Based on Term Weight Learning",
  "year": 1998,
  "venue": "EMNLP",
  "abstract": "In this paper, we propose a method for text categorization task using term weight learning. In our approach, learning is to learn true keywords from the error of clustering results. Parameters of term weighting are then estimated so as to maximize the true keywords and minimize the other words in the text. The characteristic of our approach is that the degree of context dependency is used in order to judge whether a word in a text is a true keyword or not. The experiments using Wall Street Journal corpus demonstrate the effectiveness of the method.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/W99-0615",
  "title": "HMM Specialization with Selective Lexicalization",
  "year": 1999,
  "venue": "EMNLP",
  "abstract": "We present a technique which complements Hidden Markov Models by incorporating some lexicalized states representing syntactically uncommon words. 'Our approach examines the distribution of transitions, selects the uncommon words, and makes lexicalized states for the words. We perfor'med a part-of-speech tagging experiment on the Brown corpus to evaluate the resultant language model and discovered that this technique improved the tagging accuracy by 0.21% at the 95% level of confidence.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/W99-0630",
  "title": "Automatically Merging Lexicons that have Incompatible Part-of-Speech Categories",
  "year": 1999,
  "venue": "EMNLP",
  "abstract": "We present a new method to automatically merge lexicons that employ different incompatible POS categories. Such incompatibilities have hindered efforts to combine lexicons to maximize coverage with reasonable human effort. Given an \"original lexicon\", our method is able to merge lexemes from an \"additional lexicon\" into the original lexicon, converting lexemes from the additional lexicon with about 89% precision. This level of precision is achieved with the aid of a device we introduce called an anti-lexicon, which neatly summarizes all the essential information we need about the co-occurrence of tags and lemmas. Our model is intuitive, fast, easy to implement, and does not require heavy computational resources nor training corpus.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/W01-0518",
  "title": "Probabilistic Context-Free Grammars for Syllabification and Grapheme-to-Phoneme Conversion",
  "year": 2001,
  "venue": "EMNLP",
  "abstract": "We investigated the applicability of probabilistic context-free grammars to syllabi cation and grapheme-to-phoneme conversion. The results show that the standard probability model of context-free grammars performs very well in predicting syllable boundaries. However, our results indicate that the standard probability model does not solve grapheme-to-phoneme conversion su ciently although, we varied all free parameters of the probabilistic reestimation procedure.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/W02-1025",
  "title": "A Method for Open-Vocabulary Speech-Driven Text Retrieval",
  "year": 2002,
  "venue": "EMNLP",
  "abstract": "While recent retrieval techniques do not limit the number of",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/W04-3221",
  "title": "Attribute-Based and Value-Based Clustering: An Evaluation",
  "year": 2004,
  "venue": "EMNLP",
  "abstract": "In most research on concept acquisition from corpora, concepts are modeled as vectors of relations extracted from syntactic structures. In the case of modifiers, these relations often specify values of attributes, as in (attr red); this is unlike what typically proposed in theories of knowledge representation, where concepts are typically defined in terms of their attributes (e.g., color). We compared models of concepts based on values with models based on attributes, using lexical clustering as the basis for comparison. We find that attribute-based models work better than value-based ones, and result in shorter descriptions; but that mixed models including both the best attributes and the best values work best of all.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/H05-1038",
  "title": "Using Question Series to Evaluate Question Answering System Effectiveness",
  "year": 2005,
  "venue": "EMNLP",
  "abstract": "The original motivation for using question series in the TREC 2004 question answering track was the desire to model aspects of dialogue processing in an evaluation task that included different question types. The structure introduced by the series also proved to have an important additional benefit: the series is at an appropriate level of granularity for aggregating scores for an effective evaluation. The series is small enough to be meaningful at the task level since it represents a single user interaction, yet it is large enough to avoid the highly skewed score distributions exhibited by single questions. An analysis of the reliability of the per-series evaluation shows the evaluation is stable for differences in scores seen in the track.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/H05-1051",
  "title": "Differentiating Homonymy and Polysemy in Information Retrieval",
  "year": 2005,
  "venue": "EMNLP",
  "abstract": "Recent studies into Web retrieval have shown that word sense disambiguation can increase retrieval effectiveness. However, it remains unclear as to the minimum disambiguation accuracy required and the granularity with which one must define word sense in order to maximize these benefits. This study answers these questions using a simulation of the effects of ambiguity on information retrieval. It goes beyond previous studies by differentiating between homonymy and polysemy. Results show that retrieval is more sensitive to polysemy than homonymy and that, when resolving polysemy, accuracy as low as 55% can potentially lead to increased performance.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/D08-1002",
  "title": "It's a Contradiction - no, it's not: A Case Study using Functional Relations",
  "year": 2008,
  "venue": "EMNLP",
  "abstract": "Contradiction Detection (CD) in text is a difficult NLP task. We investigate CD over functions (e.g., BornIn(Person)=Place), and present a domain-independent algorithm that automatically discovers phrases denoting functions with high precision. Previous work on CD has investigated hand-chosen sentence pairs. In contrast, we automatically harvested from the Web pairs of sentences that appear contradictory, but were surprised to find that most pairs are in fact consistent. For example, “Mozart was born in Salzburg” does not contradict “Mozart was born in Austria” despite the functional nature of the phrase “was born in”. We show that background knowledge about meronyms (e.g., Salzburg is in Austria), synonyms, functions, and more is essential for success in the CD task.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D08-1043",
  "title": "Bridging Lexical Gaps between Queries and Questions on Large Online Q&A Collections with Compact Translation Models",
  "year": 2008,
  "venue": "EMNLP",
  "abstract": "Lexical gaps between queries and questions (documents) have been a major issue in question retrieval on large online question and answer (Q&A) collections. Previous studies address the issue by implicitly expanding queries with the help of translation models pre-constructed using statistical techniques. However, since it is possible for unimportant words (e.g., non-topical words, common words) to be included in the translation models, a lack of noise control on the models can cause degradation of retrieval performance. This paper investigates a number of empirical methods for eliminating unimportant words in order to construct compact translation models for retrieval purposes. Experiments conducted on a real world Q&A collection show that substantial improvements in retrieval performance can be achieved by using compact translation models.",
  "stance": 0.4
 },
 {
  "url": "https://aclanthology.org/D08-1048",
  "title": "Automatic induction of FrameNet lexical units",
  "year": 2008,
  "venue": "EMNLP",
  "abstract": "Most attempts to integrate FrameNet in NLP systems have so far failed because of its limited coverage. In this paper, we investigate the applicability of distributional and WordNetbased models on the task of lexical unit induction, i.e. the expansion of FrameNet with new lexical units. Experimental results show that our distributional and WordNet-based models achieve good level of accuracy and coverage, especially when combined.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/D08-1050",
  "title": "Adapting a Lexicalized-Grammar Parser to Contrasting Domains",
  "year": 2008,
  "venue": "EMNLP",
  "abstract": "Most state-of-the-art wide-coverage parsers are trained on newspaper text and suffer a loss of accuracy in other domains, making parser adaptation a pressing issue. In this paper we demonstrate that a CCG parser can be adapted to two new domains, biomedical text and questions for a QA system, by using manually-annotated training data at the POS and lexical category levels only. This approach achieves parser accuracy comparable to that on newspaper data without the need for annotated parse trees in the new domain. We find that retraining at the lexical category level yields a larger performance increase for questions than for biomedical text and analyze the two datasets to investigate why different domains might behave differently for parser adaptation.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/D08-1052",
  "title": "LTAG Dependency Parsing with Bidirectional Incremental Construction",
  "year": 2008,
  "venue": "EMNLP",
  "abstract": "In this paper, we first introduce a new architecture for parsing, bidirectional incremental parsing. We propose a novel algorithm for incremental construction, which can be applied to many structure learning problems in NLP. We apply this algorithm to LTAG dependency parsing, and achieve significant improvement on accuracy over the previous best result on the same data set.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D08-1064",
  "title": "Decomposability of Translation Metrics for Improved Evaluation and Efficient Algorithms",
  "year": 2008,
  "venue": "EMNLP",
  "abstract": "BLEU is the de facto standard for evaluation and development of statistical machine translation systems. We describe three real-world situations involving comparisons between different versions of the same systems where one can obtain improvements in BLEU scores that are questionable or even absurd. These situations arise because BLEU lacks the property of decomposability, a property which is also computationally convenient for various applications. We propose a very conservative modification to BLEU and a cross between BLEU and word error rate that address these issues while improving correlation with human judgments.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/D08-1082",
  "title": "A Generative Model for Parsing Natural Language to Meaning Representations",
  "year": 2008,
  "venue": "EMNLP",
  "abstract": "In this paper, we present an algorithm for learning a generative model of natural language sentences together with their formal meaning representations with hierarchical structures. The model is applied to the task of mapping sentences to hierarchical representations of their underlying meaning. We introduce dynamic programming techniques for efficient training and decoding. In experiments, we demonstrate that the model, when coupled with a discriminative reranking technique, achieves state-of-the-art performance when tested on two publicly available corpora. The generative model degrades robustly when presented with instances that are different from those seen in training. This allows a notable improvement in recall compared to previous models.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D08-1087",
  "title": "N-gram Weighting: Reducing Training Data Mismatch in Cross-Domain Language Model Estimation",
  "year": 2008,
  "venue": "EMNLP",
  "abstract": "In domains with insufficient matched training data, language models are often constructed by interpolating component models trained from partially matched corpora. Since the ngrams from such corpora may not be of equal relevance to the target domain, we propose an n-gram weighting technique to adjust the component n-gram probabilities based on features derived from readily available segmentation and metadata information for each corpus. Using a log-linear combination of such features, the resulting model achieves up to a 1.2% absolute word error rate reduction over a linearly interpolated baseline language model on a lecture transcription task.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D08-1089",
  "title": "A Simple and Effective Hierarchical Phrase Reordering Model",
  "year": 2008,
  "venue": "EMNLP",
  "abstract": "While phrase-based statistical machine translation systems currently deliver state-of-theart performance, they remain weak on word order changes. Current phrase reordering models can properly handle swaps between adjacent phrases, but they typically lack the ability to perform the kind of long-distance reorderings possible with syntax-based systems. In this paper, we present a novel hierarchical phrase reordering model aimed at improving non-local reorderings, which seamlessly integrates with a standard phrase-based system with little loss of computational efficiency. We show that this model can successfully handle the key examples often used to motivate syntax-based systems, such as the rotation of a prepositional phrase around a noun phrase. We contrast our model with reordering models commonly used in phrase-based systems, and show that our approach provides statistically significant BLEU point gains for two language pairs: Chinese-English (+0.53 on MT05 and +0.71 on MT08) and Arabic-English (+0.55 on MT05).",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/D08-1105",
  "title": "Word Sense Disambiguation Using OntoNotes: An Empirical Study",
  "year": 2008,
  "venue": "EMNLP",
  "abstract": "The accuracy of current word sense disambiguation (WSD) systems is affected by the fine-grained sense inventory of WordNet as well as a lack of training examples. Using the WSD examples provided through OntoNotes, we conduct the first large-scale WSD evaluation involving hundreds of word types and tens of thousands of sense-tagged examples, while adopting a coarse-grained sense inventory. We show that though WSD systems trained with a large number of examples can obtain a high level of accuracy, they nevertheless suffer a substantial drop in accuracy when applied to a different domain. To address this issue, we propose combining a domain adaptation technique using feature augmentation with active learning. Our results show that this approach is effective in reducing the annotation effort required to adapt a WSD system to a new domain. Finally, we propose that one can maximize the dual benefits of reducing the annotation effort while ensuring an increase in WSD accuracy, by only performing active learning on the set of most frequently occurring word types.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/D09-1006",
  "title": "Feasibility of Human-in-the-loop Minimum Error Rate Training",
  "year": 2009,
  "venue": "EMNLP",
  "abstract": "Minimum error rate training (MERT) involves choosing parameter values for a machine translation (MT) system that maximize performance on a tuning set as measured by an automatic evaluation metric, such as BLEU. The method is best when the system will eventually be evaluated using the same metric, but in reality, most MT evaluations have a human-based component. Although performing MERT with a human-based metric seems like a daunting task, we describe a new metric, RYPT, which takes human judgments into account, but only requires human input to build a database that can be reused over and over again, hence eliminating the need for human input at tuning time. In this investigative study, we analyze the diversity (or lack thereof) of the candidates produced during MERT, we describe how this redundancy can be used to our advantage, and show that RYPT is a better predictor of translation quality than BLEU.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D09-1008",
  "title": "Effective Use of Linguistic and Contextual Information for Statistical Machine Translation",
  "year": 2009,
  "venue": "EMNLP",
  "abstract": "Current methods of using lexical features in machine translation have difficulty in scaling up to realistic MT tasks due to a prohibitively large number of parameters involved. In this paper, we propose methods of using new linguistic and contextual features that do not suffer from this problem and apply them in a state-ofthe-art hierarchical MT system. The features used in this work are non-terminal labels, non-terminal length distribution, source string context and source dependency LM scores. The effectiveness of our techniques is demonstrated by significant improvements over a strong baseline. On Arabic-to-English translation, improvements in lower-cased BLEU are 2.0 on NIST MT06 and 1.7 on MT08 newswire data on decoding output. On Chinese-to-English translation, the improvements are 1.0 on MT06 and 0.8 on MT08 newswire data.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/D09-1035",
  "title": "It's Not You, it's Me: Detecting Flirting and its Misperception in Speed-Dates",
  "year": 2009,
  "venue": "EMNLP",
  "abstract": "Automatically detecting human social intentions from spoken conversation is an important task for dialogue understanding. Since the social intentions of the speaker may differ from what is perceived by the hearer, systems that analyze human conversations need to be able to extract both the perceived and the intended social meaning. We investigate this difference between intention and perception by using a spoken corpus of speed-dates in which both the speaker and the listener rated the speaker on flirtatiousness. Our flirtationdetection system uses prosodic, dialogue, and lexical features to detect a speaker’s intent to flirt with up to 71.5% accuracy, significantly outperforming the baseline, but also outperforming the human interlocuters. Our system addresses lexical feature sparsity given the small amount of training data by using an autoencoder network to map sparse lexical feature vectors into 30 compressed features. Our analysis shows that humans are very poor perceivers of intended flirtatiousness, instead often projecting their own intended behavior onto their interlocutors.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/D09-1041",
  "title": "A Comparison of Model Free versus Model Intensive Approaches to Sentence Compression",
  "year": 2009,
  "venue": "EMNLP",
  "abstract": "This work introduces a model free approach to sentence compression, which grew out of ideas from Nomoto (2008), and examines how it compares to a state-of-art model intensive approach known as Tree-to-Tree Transducer, or T3 (Cohn and Lapata, 2008). It is found that a model free approach significantly outperforms T3 on the particular data we created from the Internet. We also discuss what might have caused T3’s poor performance.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/D09-1055",
  "title": "Mining Search Engine Clickthrough Log for Matching N-gram Features",
  "year": 2009,
  "venue": "EMNLP",
  "abstract": "User clicks on a URL in response to a query are extremely useful predictors of the URL’s relevance to that query. Exact match click features tend to suffer from severe data sparsity issues in web ranking. Such sparsity is particularly pronounced for new URLs or long queries where each distinct query-url pair will rarely occur. To remedy this, we present a set of straightforward yet informative query-url n-gram features that allows for generalization of limited user click data to large amounts of unseen query-url pairs. The method is motivated by techniques leveraged in the NLP community for dealing with unseen words. We find that there are interesting regularities across queries and their preferred destination URLs; for example, queries containing “form” tend to lead to clicks on URLs containing “pdf”. We evaluate our set of new query-url features on a web search ranking task and obtain improvements that are statistically significant at a p-value < 0.0001 level over a strong baseline with exact match clickthrough features.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/D09-1065",
  "title": "EEG responds to conceptual stimuli and corpus semantics",
  "year": 2009,
  "venue": "EMNLP",
  "abstract": "Mitchell et al. (2008) demonstrated that corpus-extracted models of semantic knowledge can predict neural activation patterns recorded using fMRI. This could be a very powerful technique for evaluating conceptual models extracted from corpora; however, fMRI is expensive and imposes strong constraints on data collection. Following on experiments that demonstrated that EEG activation patterns encode enough information to discriminate broad conceptual categories, we show that corpus-based semantic representations can predict EEG activation patterns with significant accuracy, and we evaluate the relative performance of different corpus-models on this task.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/D09-1090",
  "title": "Bilingual dictionary generation for low-resourced language pairs",
  "year": 2009,
  "venue": "EMNLP",
  "abstract": "Bilingual dictionaries are vital resources in many areas of natural language processing. Numerous methods of machine translation require bilingual dictionaries with large coverage, but less-frequent language pairs rarely have any digitalized resources. Since the need for these resources is increasing, but the human resources are scarce for less represented languages, efficient automatized methods are needed. This paper introduces a fully automated, robust pivot language based bilingual dictionary generation method that uses the WordNet of the pivot language to build a new bilingual dictionary. We propose the usage of WordNet in order to increase accuracy; we also introduce a bidirectional selection method with a flexible threshold to maximize recall. Our evaluations showed 79% accuracy and 51% weighted recall, outperforming representative pivot language based methods. A dictionary generated with this method will still need manual post-editing, but the improved recall and precision decrease the work of human correctors.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/D09-1113",
  "title": "Empirical Exploitation of Click Data for Task Specific Ranking",
  "year": 2009,
  "venue": "EMNLP",
  "abstract": "There have been increasing needs for task specific rankings in web search such as rankings for specific query segments like long queries, time-sensitive queries, navigational queries, etc; or rankings for specific domains/contents like answers, blogs, news, etc. In the spirit of ”divide-andconquer”, task specific ranking may have potential advantages over generic ranking since different tasks have task-specific features, data distributions, as well as featuregrade correlations. A critical problem for the task-specific ranking is training data insufficiency, which may be solved by using the data extracted from click log. This paper empirically studies how to appropriately exploit click data to improve rank function learning in task-specific ranking. The main contributions are 1) the exploration on the utilities of two promising approaches for click pair extraction; 2) the analysis of the role played by the noise information which inevitably appears in click data extraction; 3) the appropriate strategy for combining training data and click data; 4) the comparison of click data which are consistent and inconsistent with baseline function.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/D09-1149",
  "title": "Semi-Supervised Learning for Semantic Relation Classification using Stratified Sampling Strategy",
  "year": 2009,
  "venue": "EMNLP",
  "abstract": "This paper presents a new approach to selecting the initial seed set using stratified sampling strategy in bootstrapping-based semi-supervised learning for semantic relation classification. First, the training data is partitioned into several strata according to relation types/subtypes, then relation instances are randomly sampled from each stratum to form the initial seed set. We also investigate different augmentation strategies in iteratively adding reliable instances to the labeled set, and find that the bootstrapping procedure may stop at a reasonable point to significantly decrease the training time without degrading too much in performance. Experiments on the ACE RDC 2003 and 2004 corpora show the stratified sampling strategy contributes more than the bootstrapping procedure itself. This suggests that a proper sampling strategy is critical in semi-supervised learning.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/D09-1159",
  "title": "Phrase Dependency Parsing for Opinion Mining",
  "year": 2009,
  "venue": "EMNLP",
  "abstract": "In this paper, we present a novel approach for mining opinions from product reviews, where it converts opinion mining task to identify product features, expressions of opinions and relations between them. By taking advantage of the observation that a lot of product features are phrases, a concept of phrase dependency parsing is introduced, which extends traditional dependency parsing to phrase level. This concept is then implemented for extracting relations between product features and expressions of opinions. Experimental evaluations show that the mining task can benefit from phrase dependency parsing.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D10-1022",
  "title": "Negative Training Data Can be Harmful to Text Classification",
  "year": 2010,
  "venue": "EMNLP",
  "abstract": "This paper studies the effects of training data on binary text classification and postulates that negative training data is not needed and may even be harmful for the task. Traditional binary classification involves building a classifier using labeled positive and negative training examples. The classifier is then applied to classify test instances into positive and negative classes. A fundamental assumption is that the training and test data are identically distributed. However, this assumption may not hold in practice. In this paper, we study a particular problem where the positive data is identically distributed but the negative data may or may not be so. Many practical text classification and retrieval applications fit this model. We argue that in this setting negative training data should not be used, and that PU learning can be employed to solve the problem. Empirical evaluation has been conducted to support our claim. This result is important as it may fundamentally change the current binary classification paradigm.",
  "stance": -0.1
 },
 {
  "url": "https://aclanthology.org/D10-1024",
  "title": "Evaluating Models of Latent Document Semantics in the Presence of OCR Errors",
  "year": 2010,
  "venue": "EMNLP",
  "abstract": "Models of latent document semantics such as the mixture of multinomials model and Latent Dirichlet Allocation have received substantial attention for their ability to discover topical semantics in large collections of text. In an effort to apply such models to noisy optical character recognition (OCR) text output, we endeavor to understand the effect that character-level noise can have on unsupervised topic modeling. We show the effects both with document-level topic analysis (document clustering) and with word-level topic analysis (LDA) on both synthetic and real-world OCR data. As expected, experimental results show that performance declines as word error rates increase. Common techniques for alleviating these problems, such as filtering low-frequency words, are successful in enhancing model quality, but exhibit failure trends similar to models trained on unprocessed OCR output in the case of LDA. To our knowledge, this study is the first of its kind.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/D10-1055",
  "title": "Further Meta-Evaluation of Broad-Coverage Surface Realization",
  "year": 2010,
  "venue": "EMNLP",
  "abstract": "We present the first evaluation of the utility of automatic evaluation metrics on surface realizations of Penn Treebank data. Using outputs of the OpenCCG and XLE realizers, along with ranked WordNet synonym substitutions, we collected a corpus of generated surface realizations. These outputs were then rated and post-edited by human annotators. We evaluated the realizations using seven automatic metrics, and analyzed correlations obtained between the human judgments and the automatic scores. In contrast to previous NLG meta-evaluations, we find that several of the metrics correlate moderately well with human judgments of both adequacy and fluency, with the TER family performing best overall. We also find that all of the metrics correctly predict more than half of the significant systemlevel differences, though none are correct in all cases. We conclude with a discussion of the implications for the utility of such metrics in evaluating generation in the presence of variation. A further result of our research is a corpus of post-edited realizations, which will be made available to the research community.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/D10-1080",
  "title": "Lessons Learned in Part-of-Speech Tagging of Conversational Speech",
  "year": 2010,
  "venue": "EMNLP",
  "abstract": "This paper examines tagging models for spontaneous English speech transcripts. We analyze the performance of state-of-the-art tagging models, either generative or discriminative, left-to-right or bidirectional, with or without latent annotations, together with the use of ToBI break indexes and several methods for segmenting the speech transcripts (i.e., conversation side, speaker turn, or humanannotated sentence). Based on these studies, we observe that: (1) bidirectional models tend to achieve better accuracy levels than left-toright models, (2) generative models seem to perform somewhat better than discriminative models on this task, and (3) prosody improves tagging performance of models on conversation sides, but has much less impact on smaller segments. We conclude that, although the use of break indexes can indeed significantly improve performance over baseline models without them on conversation sides, tagging accuracy improves more by using smaller segments, for which the impact of the break indexes is marginal.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/D10-1118",
  "title": "It Depends on the Translation: Unsupervised Dependency Parsing via Word Alignment",
  "year": 2010,
  "venue": "EMNLP",
  "abstract": "We reveal a previously unnoticed connection between dependency parsing and statistical machine translation (SMT), by formulating the dependency parsing task as a problem of word alignment. Furthermore, we show that two well known models for these respective tasks (DMV and the IBM models) share common modeling assumptions. This motivates us to develop an alignment-based framework for unsupervised dependency parsing. The framework (which will be made publicly available) is flexible, modular and easy to extend. Using this framework, we implement several algorithms based on the IBM alignment models, which prove surprisingly effective on the dependency parsing task, and demonstrate the potential of the alignment-based approach.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D11-1012",
  "title": "A Joint Model for Extended Semantic Role Labeling",
  "year": 2011,
  "venue": "EMNLP",
  "abstract": "This paper presents a model that extends semantic role labeling. Existing approaches independently analyze relations expressed by verb predicates or those expressed as nominalizations. However, sentences express relations via other linguistic phenomena as well. Furthermore, these phenomena interact with each other, thus restricting the structures they articulate. In this paper, we use this intuition to define a joint inference model that captures the inter-dependencies between verb semantic role labeling and relations expressed using prepositions. The scarcity of jointly labeled data presents a crucial technical challenge for learning a joint model. The key strength of our model is that we use existing structure predictors as black boxes. By enforcing consistency constraints between their predictions, we show improvements in the performance of both tasks without retraining the individual models.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D11-1044",
  "title": "Quasi-Synchronous Phrase Dependency Grammars for Machine Translation",
  "year": 2011,
  "venue": "EMNLP",
  "abstract": "We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009). This formulation allows us to combine structural components of phrase-based and syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D11-1124",
  "title": "Summarize What You Are Interested In: An Optimization Framework for Interactive Personalized Summarization",
  "year": 2011,
  "venue": "EMNLP",
  "abstract": "Most traditional summarization methods treat their outputs as static and plain texts, which fail to capture user interests during summarization because the generated summaries are the same for different users. However, users have individual preferences on a particular source document collection and obviously a universal summary for all users might not always be satisfactory. Hence we investigate an important and challenging problem in summary generation, i.e., Interactive Personalized Summarization (IPS), which generates summaries in an interactive and personalized manner. Given the source documents, IPS captures user interests by enabling interactive clicks and incorporates personalization by modeling captured reader preference. We develop experimental systems to compare 5 rival algorithms on 4 instinctively different datasets which amount to 5197 documents. Evaluation results in ROUGE metrics indicate the comparable performance between IPS and the best competing system but IPS produces summaries with much more user satisfaction according to evaluator ratings. Besides, low ROUGE consistency among these user preferred summaries indicates the existence of personalization.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/D11-1149",
  "title": "A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions",
  "year": 2011,
  "venue": "EMNLP",
  "abstract": "This paper describes a novel probabilistic approach for generating natural language sentences from their underlying semantics in the form of typed lambda calculus. The approach is built on top of a novel reduction-based weighted synchronous context free grammar formalism, which facilitates the transformation process from typed lambda calculus into natural language sentences. Sentences can then be generated based on such grammar rules with a log-linear model. To acquire such grammar rules automatically in an unsupervised manner, we also propose a novel approach with a generative model, which maps from sub-expressions of logical forms to word sequences in natural language sentences. Experiments on benchmark datasets for both English and Chinese generation tasks yield significant improvements over results obtained by two state-of-the-art machine translation models, in terms of both automatic metrics and human evaluation.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D13-1020",
  "title": "MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text",
  "year": 2013,
  "venue": "EMNLP",
  "abstract": "We present MCTest, a freely available set of stories and associated questions intended for research on the machine comprehension of text. Previous work on machine comprehension (e.g., semantic modeling) has made great strides, but primarily focuses either on limited-domain datasets, or on solving a more restricted goal (e.g., open-domain relation extraction). In contrast, MCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension. Reading comprehension can test advanced abilities such as causal reasoning and understanding the world, yet, by being multiple-choice, still provide a clear metric. By being fictional, the answer typically can be found only in the story itself. The stories and questions are also carefully limited to those a young child would understand, reducing the world knowledge that is required for the task. We present the scalable crowd-sourcing methods that allow us to cheaply construct a dataset of 500 stories and 2000 questions. By screening workers (with grammar tests) and stories (with grading), we have ensured that the data is the same quality as another set that we manually edited, but at one tenth the editing cost. By being open-domain, yet carefully restricted, we hope MCTest will serve to encourage research and provide a clear metric for advancement on the machine comprehension of text.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/D13-1072",
  "title": "Exploiting Language Models for Visual Recognition",
  "year": 2013,
  "venue": "EMNLP",
  "abstract": "The problem of learning language models from large text corpora has been widely studied within the computational linguistic community. However, little is known about the performance of these language models when applied to the computer vision domain. In this work, we compare representative models: a window-based model, a topic model, a distributional memory and a commonsense knowledge database, ConceptNet, in two visual recognition scenarios: human action recognition and object prediction. We examine whether the knowledge extracted from texts through these models are compatible to the knowledge represented in images. We determine the usefulness of different language models in aiding the two visual recognition tasks. The study shows that the language models built from general text corpora can be used instead of expensive annotated images and even outperform the image model when testing on a big general dataset.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/D13-1079",
  "title": "Rule-Based Information Extraction is Dead! Long Live Rule-Based Information Extraction Systems!",
  "year": 2013,
  "venue": "EMNLP",
  "abstract": "The rise of “Big Data” analytics over unstructured text has led to renewed interest in information extraction (IE). We surveyed the landscape of IE technologies and identified a major disconnect between industry and academia: while rule-based IE dominates the commercial world, it is widely regarded as dead-end technology by the academia. We believe the disconnect stems from the way in which the two communities measure the benefits and costs of IE, as well as academia’s perception that rulebased IE is devoid of research challenges. We make a case for the importance of rule-based IE to industry practitioners. We then lay out a research agenda in advancing the state-of-theart in rule-based IE systems which we believe has the potential to bridge the gap between academic research and industry practice.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/D13-1202",
  "title": "Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts",
  "year": 2013,
  "venue": "EMNLP",
  "abstract": "Traditional distributional semantic models extract word meaning representations from cooccurrence patterns of words in text corpora. Recently, the distributional approach has been extended to models that record the cooccurrence of words with visual features in image collections. These image-based models should be complementary to text-based ones, providing a more cognitively plausible view of meaning grounded in visual perception. In this study, we test whether image-based models capture the semantic patterns that emerge from fMRI recordings of the neural signal. Our results indicate that, indeed, there is a significant correlation between image-based and brain-based semantic similarities, and that image-based models complement text-based ones, so that the best correlations are achieved when the two modalities are combined. Despite some unsatisfactory, but explained outcomes (in particular, failure to detect differential association of models with brain areas), the results show, on the one hand, that imagebased distributional semantic models can be a precious new tool to explore semantic representation in the brain, and, on the other, that neural data can be used as the ultimate test set to validate artificial semantic models in terms of their cognitive plausibility.",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/D14-1005",
  "title": "Learning Image Embeddings using Convolutional Neural Networks for Improved Multi-Modal Semantics",
  "year": 2014,
  "venue": "EMNLP",
  "abstract": "We construct multi-modal concept representations by concatenating a skip-gram linguistic representation vector with a visual concept representation vector computed using the feature extraction layers of a deep convolutional neural network (CNN) trained on a large labeled object recognition dataset. This transfer learning approach brings a clear performance gain over features based on the traditional bag-of-visual-word approach. Experimental results are reported on the WordSim353 and MEN semantic relatedness evaluation tasks. We use visual features computed using either ImageNet or ESP Game images.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/D14-1020",
  "title": "Testing for Significance of Increased Correlation with Human Judgment",
  "year": 2014,
  "venue": "EMNLP",
  "abstract": "Automatic metrics are widely used in machine translation as a substitute for human assessment. With the introduction of any new metric comes the question of just how well that metric mimics human assessment of translation quality. This is often measured by correlation with human judgment. Significance tests are generally not used to establish whether improvements over existing methods such as BLEU are statistically significant or have occurred simply by chance, however. In this paper, we introduce a significance test for comparing correlations of two metrics, along with an open-source implementation of the test. When applied to a range of metrics across seven language pairs, tests show that for a high proportion of metrics, there is insufficient evidence to conclude significant improvement over BLEU.",
  "stance": -0.4
 },
 {
  "url": "https://aclanthology.org/D14-1215",
  "title": "Brighter than Gold: Figurative Language in User Generated Comparisons",
  "year": 2014,
  "venue": "EMNLP",
  "abstract": "Comparisons are common linguistic devices used to indicate the likeness of two things. Often, this likeness is not meant in the literal sense—for example, “I slept like a log” does not imply that logs actually sleep. In this paper we propose a computational study of figurative comparisons, or similes. Our starting point is a new large dataset of comparisons extracted from product reviews and annotated for figurativeness. We use this dataset to characterize figurative language in naturally occurring comparisons and reveal linguistic patterns indicative of this phenomenon. We operationalize these insights and apply them to a new task with high relevance to text understanding: distinguishing between figurative and literal comparisons. Finally, we apply this framework to explore the social context in which figurative language is produced, showing that similes are more likely to accompany opinions showing extreme sentiment, and that they are uncommon in reviews deemed helpful.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/D15-1013",
  "title": "Re-evaluating Automatic Summarization with BLEU and 192 Shades of ROUGE",
  "year": 2015,
  "venue": "EMNLP",
  "abstract": "We provide an analysis of current evaluation methodologies applied to summarization metrics and identify the following areas of concern: (1) movement away from evaluation by correlation with human assessment; (2) omission of important components of human assessment from evaluations, in addition to large numbers of metric variants; (3) absence of methods of significance testing improvements over a baseline. We outline an evaluation methodology that overcomes all such challenges, providing the first method of significance testing suitable for evaluation of summarization metrics. Our evaluation reveals for the first time which metric variants significantly outperform others, optimal metric variants distinct from current recommended best variants, as well as machine translation metric BLEU to have performance on-par with ROUGE for the purpose of evaluation of summarization systems. We subsequently replicate a recent large-scale evaluation that relied on, what we now know to be, suboptimal ROUGE variants revealing distinct conclusions about the relative performance of state-of-the-art summarization systems.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/D15-1038",
  "title": "Traversing Knowledge Graphs in Vector Space",
  "year": 2015,
  "venue": "EMNLP",
  "abstract": "Path queries on a knowledge graph can be used to answer compositional questions such as “What languages are spoken by people living in Lisbon?”. However, knowledge graphs often have missing facts (edges) which disrupts path queries. Recent models for knowledge base completion impute missing facts by embedding knowledge graphs in vector spaces. We show that these models can be recursively applied to answer path queries, but that they suffer from cascading errors. This motivates a new “compositional” training objective, which dramatically improves all models’ ability to answer path queries, in some cases more than doubling accuracy. On a standard knowledge base completion task, we also demonstrate that compositional training acts as a novel form of structural regularization, reliably improving performance across all base models (reducing errors by up to 43%) and achieving new state-of-the-art results.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D15-1052",
  "title": "Human Evaluation of Grammatical Error Correction Systems",
  "year": 2015,
  "venue": "EMNLP",
  "abstract": "The paper presents the results of the first large-scale human evaluation of automatic grammatical error correction (GEC) systems. Twelve participating systems and the unchanged input of the CoNLL-2014 shared task have been reassessed in a WMT-inspired human evaluation procedure. Methods introduced for the Workshop of Machine Translation evaluation campaigns have been adapted to GEC and extended where necessary. The produced rankings are used to evaluate standard metrics for grammatical error correction in terms of correlation with human judgment.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/D15-1071",
  "title": "Detecting Risks in the Banking System by Sentiment Analysis",
  "year": 2015,
  "venue": "EMNLP",
  "abstract": "In November 2014, the European Central Bank (ECB) started to directly supervise the largest banks in the Eurozone via the Single Supervisory Mechanism (SSM). While supervisory risk assessments are usually based on quantitative data and surveys, this work explores whether sentiment analysis is capable of measuring a bank’s attitude and opinions towards risk by analyzing text data. For realizing this study, a collection consisting of more than 500 CEO letters and outlook sections extracted from bank annual reports is built up. Based on these data, two distinct experiments are conducted. The evaluations find promising opportunities, but also limitations for risk sentiment analysis in banking supervision. At the level of individual banks, predictions are relatively inaccurate. In contrast, the analysis of aggregated figures revealed strong and significant correlations between uncertainty or negativity in textual disclosures and the quantitative risk indicator’s future evolution. Risk sentiment analysis should therefore rather be used for macroprudential analyses than for assessments of individual banks.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/D15-1148",
  "title": "Detecting Content-Heavy Sentences: A Cross-Language Case Study",
  "year": 2015,
  "venue": "EMNLP",
  "abstract": "The information conveyed by some sentences would be more easily understood by a reader if it were expressed in multiple sentences. We call such sentences content heavy: these are possibly grammatical but difficult to comprehend, cumbersome sentences. In this paper we introduce the task of detecting content-heavy sentences in cross-lingual context. Specifically we develop methods to identify sentences in Chinese for which English speakers would prefer translations consisting of more than one sentence. We base our analysis and definitions on evidence from multiple human translations and reader preferences on flow and understandability. We show that machine translation quality when translating content heavy sentences is markedly worse than overall quality and that this type of sentence are fairly common in Chinese news. We demonstrate that sentence length and punctuation usage in Chinese are not sufficient clues for accurately detecting heavy sentences and present a richer classification model that accurately identifies these sentences.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D15-1243",
  "title": "Evaluation of Word Vector Representations by Subspace Alignment",
  "year": 2015,
  "venue": "EMNLP",
  "abstract": "Unsupervisedly learned word vectors have proven to provide exceptionally effective features in many NLP tasks. Most common intrinsic evaluations of vector quality measure correlation with similarity judgments. However, these often correlate poorly with how well the learned representations perform as features in downstream evaluation tasks. We present QVEC—a computationally inexpensive intrinsic evaluation measure of the quality of word embeddings based on alignment to a matrix of features extracted from manually crafted lexical resources—that obtains strong correlation with performance of the vectors in a battery of downstream semantic evaluation tasks.1",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/D15-1285",
  "title": "Topic Identification and Discovery on Text and Speech",
  "year": 2015,
  "venue": "EMNLP",
  "abstract": "We compare the multinomial i-vector framework from the speech community with LDA, SAGE, and LSA as feature learners for topic ID on multinomial speech and text data. We also compare the learned representations in their ability to discover topics, quantified by distributional similarity to gold-standard topics and by human interpretability. We find that topic ID and topic discovery are competing objectives. We argue that LSA and i-vectors should be more widely considered by the text processing community as pre-processing steps for downstream tasks, and also speculate about speech processing tasks that could benefit from more interpretable representations like SAGE.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/D16-1010",
  "title": "Comparing Computational Cognitive Models of Generalization in a Language Acquisition Task",
  "year": 2016,
  "venue": "EMNLP",
  "abstract": "Natural language acquisition relies on appropriate generalization: the ability to produce novel sentences, while learning to restrict productions to acceptable forms in the language. Psycholinguists have proposed various properties that might play a role in guiding appropriate generalizations, looking at learning of verb alternations as a testbed. Several computational cognitive models have explored aspects of this phenomenon, but their results are hard to compare given the high variability in the linguistic properties represented in their input. In this paper, we directly compare two recent approaches, a Bayesian model and a connectionist model, in their ability to replicate human judgments of appropriate generalizations. We find that the Bayesian model more accurately mimics the judgments due to its richer learning mechanism that can exploit distributional properties of the input in a manner consistent with human behaviour.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/D16-1015",
  "title": "Improving Semantic Parsing via Answer Type Inference",
  "year": 2016,
  "venue": "EMNLP",
  "abstract": "In this work, we show the possibility of inferring the answer type before solving a factoid question and leveraging the type information to improve semantic parsing. By replacing the topic entity in a question with its type, we are able to generate an abstract form of the question, whose answer corresponds to the answer type of the original question. A bidirectional LSTM model is built to train over the abstract form of questions and infer their answer types. It is also observed that if we convert a question into a statement form, our LSTM model achieves better accuracy. Using the predicted type information to rerank the logical forms returned by AgendaIL, one of the leading semantic parsers, we are able to improve the F1-score from 49.7% to 52.6% on the WEBQUESTIONS data.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D16-1183",
  "title": "Neural Shift-Reduce CCG Semantic Parsing",
  "year": 2016,
  "venue": "EMNLP",
  "abstract": "We present a shift-reduce CCG semantic parser. Our parser uses a neural network architecture that balances model capacity and computational cost. We train by transferring a model from a computationally expensive loglinear CKY parser. Our learner addresses two challenges: selecting the best parse for learning when the CKY parser generates multiple correct trees, and learning from partial derivations when the CKY parser fails to parse. We evaluate on AMR parsing. Our parser performs comparably to the CKY parser, while doing significantly fewer operations. We also present results for greedy semantic parsing with a relatively small drop in performance.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D16-1217",
  "title": "Does 'well-being' translate on Twitter?",
  "year": 2016,
  "venue": "EMNLP",
  "abstract": "We investigate whether psychological wellbeing translates across English and Spanish Twitter, by building and comparing source language and automatically translated weighted lexica in English and Spanish. We find that the source language models perform substantially better than the machine translated versions. Moreover, manually correcting translation errors does not improve model performance, suggesting that meaningful cultural information is being lost in translation. Further work is needed to clarify when automatic translation of well-being lexica is effective and how it can be improved for crosscultural analysis.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/D16-1218",
  "title": "Beyond Canonical Texts: A Computational Analysis of Fanfiction",
  "year": 2016,
  "venue": "EMNLP",
  "abstract": "While much computational work on fiction has focused on works in the literary canon, user-created fanfiction presents a unique opportunity to study an ecosystem of literary production and consumption, embodying qualities both of large-scale literary data (55 billion tokens) and also a social network (with over 2 million users). We present several empirical analyses of this data in order to illustrate the range of affordances it presents to research in NLP, computational social science and the digital humanities. We find that fanfiction deprioritizes main protagonists in comparison to canonical texts, has a statistically significant difference in attention allocated to female characters, and offers a framework for developing models of reader reactions to stories.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/D17-1123",
  "title": "A Short Survey on Taxonomy Learning from Text Corpora: Issues, Resources and Recent Advances",
  "year": 2017,
  "venue": "EMNLP",
  "abstract": "A taxonomy is a semantic hierarchy, consisting of concepts linked by is-a relations. While a large number of taxonomies have been constructed from human-compiled resources (e.g., Wikipedia), learning taxonomies from text corpora has received a growing interest and is essential for long-tailed and domain-specific knowledge acquisition. In this paper, we overview recent advances on taxonomy construction from free texts, reorganizing relevant subtasks into a complete framework. We also overview resources for evaluation and discuss challenges for future research.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/D17-1167",
  "title": "A Question Answering Approach for Emotion Cause Extraction",
  "year": 2017,
  "venue": "EMNLP",
  "abstract": "Emotion cause extraction aims to identify the reasons behind a certain emotion expressed in text. It is a much more difficult task compared to emotion classification. Inspired by recent advances in using deep memory networks for question answering (QA), we propose a new approach which considers emotion cause identification as a reading comprehension task in QA. Inspired by convolutional neural networks, we propose a new mechanism to store relevant context in different memory slots to model context information. Our proposed approach can extract both word level sequence features and lexical features. Performance evaluation shows that our method achieves the state-of-the-art performance on a recently released emotion cause dataset, outperforming a number of competitive baselines by at least 3.01% in F-measure.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D17-1215",
  "title": "Adversarial Examples for Evaluating Reading Comprehension Systems",
  "year": 2017,
  "venue": "EMNLP",
  "abstract": "Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of 75% F1 score to 36%; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7%. We hope our insights will motivate the development of new models that understand language more precisely.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/D17-1239",
  "title": "Challenges in Data-to-Document Generation",
  "year": 2017,
  "venue": "EMNLP",
  "abstract": "Recent neural models have shown significant progress on the problem of generating short descriptive texts conditioned on a small number of database records. In this work, we suggest a slightly more difficult data-to-text generation task, and investigate how effective current approaches are on this task. In particular, we introduce a new, large-scale corpus of data records paired with descriptive documents, propose a series of extractive evaluation methods for analyzing performance, and obtain baseline results using current neural generation methods. Experiments show that these models produce fluent text, but fail to convincingly approximate human-generated documents. Moreover, even templated baselines exceed the performance of these neural models on some metrics, though copy- and reconstruction-based extensions lead to noticeable improvements.",
  "stance": -0.9
 },
 {
  "url": "https://aclanthology.org/D18-1029",
  "title": "On the Relation between Linguistic Typology and (Limitations of) Multilingual Language Modeling",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "A key challenge in cross-lingual NLP is developing general language-independent architectures that are equally applicable to any language. However, this ambition is largely hampered by the variation in structural and semantic properties, i.e. the typological profiles of the world's languages. In this work, we analyse the implications of this variation on the language modeling (LM) task. We present a large-scale study of state-of-the art n-gram based and neural language models on 50 typologically diverse languages covering a wide variety of morphological systems. Operating in the full vocabulary LM setup focused on word-level prediction, we demonstrate that a coarse typology of morphological systems is predictive of absolute LM performance. Moreover, fine-grained typological features such as exponence, flexivity, fusion, and inflectional synthesis are borne out to be responsible for the proliferation of low-frequency phenomena which are organically difficult to model by statistical architectures, or for the meaning ambiguity of character n-grams. Our study strongly suggests that these features have to be taken into consideration during the construction of next-level language-agnostic LM architectures, capable of handling morphologically complex languages such as Tamil or Korean.",
  "stance": -0.4
 },
 {
  "url": "https://aclanthology.org/D18-1040",
  "title": "Back-Translation Sampling by Targeting Difficult Words in Neural Machine Translation",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "Neural Machine Translation has achieved state-of-the-art performance for several language pairs using a combination of parallel and synthetic data. Synthetic data is often generated by back-translating sentences randomly sampled from monolingual data using a reverse translation model. While back-translation has been shown to be very effective in many cases, it is not entirely clear why. In this work, we explore different aspects of back-translation, and show that words with high prediction loss during training benefit most from the addition of synthetic data. We introduce several variations of sampling strategies targeting difficult-to-predict words using prediction losses and frequencies of words. In addition, we also target the contexts of difficult words and sample sentences that are similar in context. Experimental results for the WMT news translation task show that our method improves translation quality by up to 1.7 and 1.2 Bleu points over back-translation using random sampling for German-English and English-German, respectively.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/D18-1128",
  "title": "Do explanations make VQA models more predictable to a human?",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "A rich line of research attempts to make deep neural networks more transparent by generating human-interpretable 'explanations' of their decision process, especially for interactive tasks like Visual Question Answering (VQA). In this work, we analyze if existing explanations indeed make a VQA model - its responses as well as failures - more predictable to a human. Surprisingly, we find that they do not. On the other hand, we find that human-in-the-loop approaches that treat the model as a black-box do.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/D18-1174",
  "title": "Disambiguated skip-gram model",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "We present disambiguated skip-gram: a neural-probabilistic model for learning multi-sense distributed representations of words. Disambiguated skip-gram jointly estimates a skip-gram-like context word prediction model and a word sense disambiguation model. Unlike previous probabilistic models for learning multi-sense word embeddings, disambiguated skip-gram is end-to-end differentiable and can be interpreted as a simple feed-forward neural network. We also introduce an effective pruning strategy for the embeddings learned by disambiguated skip-gram. This allows us to control the granularity of representations learned by our model. In experimental evaluation disambiguated skip-gram improves state-of-the are results in several word sense induction benchmarks.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D18-1175",
  "title": "Picking Apart Story Salads",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "During natural disasters and conflicts, information about what happened is often confusing and messy, and distributed across many sources. We would like to be able to automatically identify relevant information and assemble it into coherent narratives of what happened. To make this task accessible to neural models, we introduce Story Salads, mixtures of multiple documents that can be generated at scale. By exploiting the Wikipedia hierarchy, we can generate salads that exhibit challenging inference problems. Story salads give rise to a novel, challenging clustering task, where the objective is to group sentences from the same narratives. We demonstrate that simple bag-of-words similarity clustering falls short on this task, and that it is necessary to take into account global context and coherence.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D18-1209",
  "title": "Improved Semantic-Aware Network Embedding with Fine-Grained Word Alignment",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "Network embeddings, which learns low-dimensional representations for each vertex in a large-scale network, have received considerable attention in recent years. For a wide range of applications, vertices in a network are typically accompanied by rich textual information such as user profiles, paper abstracts, etc. In this paper, we propose to incorporate semantic features into network embeddings by matching important words between text sequences for all pairs of vertices. We introduce an word-by-word alignment framework that measures the compatibility of embeddings between word pairs, and then adaptively accumulates these alignment features with a simple yet effective aggregation function. In experiments, we evaluate the proposed framework on three real-world benchmarks for downstream tasks, including link prediction and multi-label vertex classification. The experimental results demonstrate that our model outperforms state-of-the-art network embedding methods by a large margin.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D18-1306",
  "title": "Marginal Likelihood Training of BiLSTM-CRF for Biomedical Named Entity Recognition from Disjoint Label Sets",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "Extracting typed entity mentions from text is a fundamental component to language understanding and reasoning. While there exist substantial labeled text datasets for multiple subsets of biomedical entity types-such as genes and proteins, or chemicals and diseases-it is rare to find large labeled datasets containing labels for all desired entity types together. This paper presents a method for training a single CRF extractor from multiple datasets with disjoint or partially overlapping sets of entity types. Our approach employs marginal likelihood training to insist on labels that are present in the data, while filling in \"missing labels\". This allows us to leverage all the available data within a single model. In experimental results on the Biocreative V CDR (chemicals/diseases), Biocreative VI ChemProt (chemicals/proteins) and MedMentions (19 entity types) datasets, we show that joint training on multiple datasets improves NER F1 over training in isolation, and our methods achieve state-of-the-art results.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D18-1313",
  "title": "The Lazy Encoder: A Fine-Grained Analysis of the Role of Morphology in Neural Machine Translation",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "Neural sequence-to-sequence models have proven very effective for machine translation, but at the expense of model interpretability. To shed more light into the role played by linguistic structure in the process of neural machine translation, we perform a fine-grained analysis of how various source-side morphological features are captured at different levels of the NMT encoder while varying the target language. Differently from previous work, we find no correlation between the accuracy of source morphology encoding and translation quality. We do find that morphological features are only captured in context and only to the extent that they are directly transferable to the target words.",
  "stance": -0.5
 },
 {
  "url": "https://aclanthology.org/D18-1324",
  "title": "The Importance of Generation Order in Language Modeling",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "Neural language models are a critical component of state-of-the-art systems for machine translation, summarization, audio transcription, and other tasks. These language models are almost universally autoregressive in nature, generating sentences one token at a time from left to right. This paper studies the influence of token generation order on model quality via a novel two-pass language model that produces partially-filled sentence \"templates\" and then fills in missing tokens. We compare various strategies for structuring these two passes and observe a surprisingly large variation in model quality. We find the most effective strategy generates function words in the first pass followed by content words in the second. We believe these experimental results justify a more extensive investigation of the generation order for neural language models.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/D18-1364",
  "title": "Implicational Universals in Stochastic Constraint-Based Phonology",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "This paper focuses on the most basic implicational universals in phonological theory, called T-orders after Anttila and Andrus (2006). It shows that the T-orders predicted by stochastic (and partial order) Optimality Theory coincide with those predicted by categorical OT. Analogously, the T-orders predicted by stochastic Harmonic Grammar coincide with those predicted by categorical HG. In other words, these stochastic constraint-based frameworks do not tamper with the typological structure induced by the original categorical frameworks.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/D18-1425",
  "title": "Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "We present Spider, a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 college students. It consists of 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables covering 138 different domains. We define a new complex and cross-domain semantic parsing and text-to-SQL task so that different complicated SQL queries and databases appear in train and test sets. In this way, the task requires the model to generalize well to both new SQL queries and new database schemas. Therefore, Spider is distinct from most of the previous semantic parsing tasks because they all use a single database and have the exact same program in the train set and the test set. We experiment with various state-of-the-art models and the best model achieves only 9.7% exact matching accuracy on a database split setting. This shows that Spider presents a strong challenge for future research. Our dataset and task with the most recent updates are publicly available at https://yale-lily.github.io/seq2sql/spider.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D18-1437",
  "title": "Object Hallucination in Image Captioning",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "Despite continuously improving performance, contemporary image captioning models are prone to \"hallucinating\" objects that are not actually in a scene. One problem is that standard metrics only measure similarity to ground truth captions and may not fully capture image relevance. In this work, we propose a new image relevance metric to evaluate current models with veridical visual labels and assess their rate of object hallucination. We analyze how captioning model architectures and learning objectives contribute to object hallucination, explore when hallucination is likely due to image misclassification or language priors, and assess how well current sentence metrics capture object hallucination. We investigate these questions on the standard image captioning benchmark, MSCOCO, using a diverse set of models. Our analysis yields several interesting findings, including that models which score best on standard sentence metrics do not always have lower hallucination and that models which hallucinate more tend to make errors driven by language priors.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/D18-1442",
  "title": "Iterative Document Representation Learning Towards Summarization with Polishing",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "In this paper, we introduce Iterative Text Summarization (ITS), an iteration-based model for supervised extractive text summarization, inspired by the observation that it is often necessary for a human to read an article multiple times in order to fully understand and summarize its contents. Current summarization approaches read through a document only once to generate a document representation, resulting in a sub-optimal representation. To address this issue we introduce a model which iteratively polishes the document representation on many passes through the document. As part of our model, we also introduce a selective reading mechanism that decides more accurately the extent to which each sentence in the model should be updated. Experimental results on the CNN/DailyMail and DUC2002 datasets demonstrate that our model significantly outperforms state-of-the-art extractive systems when evaluated by machines and by humans.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D18-1467",
  "title": "Making \"fetch\" happen: The influence of social and linguistic context on nonstandard word growth and decline",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "In an online community, new words come and go: today's \"haha\" may be replaced by tomorrow's \"lol.\" Changes in online writing are usually studied as a social process, with innovations diffusing through a network of individuals in a speech community. But unlike other types of innovation, language change is shaped and constrained by the grammatical system in which it takes part. To investigate the role of social and structural factors in language change, we undertake a large-scale analysis of the frequencies of non-standard words in Reddit. Dissemination across many linguistic contexts is a predictor of success: words that appear in more linguistic contexts grow faster and survive longer. Furthermore, social dissemination plays a less important role in explaining word growth and decline than previously hypothesized.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/D18-1500",
  "title": "Understanding Deep Learning Performance through an Examination of Test Set Difficulty: A Psychometric Case Study",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "Interpreting the performance of deep learning models beyond test set accuracy is challenging. Characteristics of individual data points are often not considered during evaluation, and each data point is treated equally. In this work we examine the impact of a test set question's difficulty to determine if there is a relationship between difficulty and performance. We model difficulty using well-studied psychometric methods on human response patterns. Experiments on Natural Language Inference (NLI) and Sentiment Analysis (SA) show that the likelihood of answering a question correctly is impacted by the question's difficulty. In addition, as DNNs are trained on larger datasets easy questions start to have a higher probability of being answered correctly than harder questions.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/D18-1512",
  "title": "Has Machine Translation Achieved Human Parity? A Case for Document-level Evaluation",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese-English news translation task. We empirically test this claim with alternative evaluation protocols, contrasting the evaluation of single sentences and entire documents. In a pairwise ranking experiment, human raters assessing adequacy and fluency show a stronger preference for human over machine translation when evaluating documents as compared to isolated sentences. Our findings emphasise the need to shift towards document-level evaluation as machine translation improves to the degree that errors which are hard or impossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/D18-1513",
  "title": "Automatic Reference-Based Evaluation of Pronoun Translation Misses the Point",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "We compare the performance of the APT and AutoPRF metrics for pronoun translation against a manually annotated dataset comprising human judgements as to the correctness of translations of the PROTEST test suite. Although there is some correlation with the human judgements, a range of issues limit the performance of the automated metrics. Instead, we recommend the use of semi-automatic metrics and test suites in place of fully automatic metrics.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/D19-1107",
  "title": "Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets",
  "year": 2019,
  "venue": "EMNLP",
  "abstract": "Crowdsourcing has been the prevalent paradigm for creating natural language understanding datasets in recent years. A common crowdsourcing practice is to recruit a small number of high-quality workers, and have them massively generate examples. Having only a few workers generate the majority of examples raises concerns about data diversity, especially when workers freely generate sentences. In this paper, we perform a series of experiments showing these concerns are evident in three recent NLP datasets. We show that model performance improves when training with annotator identifiers as features, and that models are able to recognize the most productive annotators. Moreover, we show that often models do not generalize well to examples from annotators that did not contribute to the training set. Our findings suggest that annotator bias should be monitored during dataset creation, and that test set annotators should be disjoint from training set annotators.",
  "stance": -0.7
 },
 {
  "url": "https://aclanthology.org/D19-1133",
  "title": "uniblock: Scoring and Filtering Corpus with Unicode Block Information",
  "year": 2019,
  "venue": "EMNLP",
  "abstract": "The preprocessing pipelines in Natural Language Processing usually involve a step of removing sentences consisted of illegal characters. The definition of illegal characters and the specific removal strategy depend on the task, language, domain, etc, which often lead to tiresome and repetitive scripting of rules. In this paper, we introduce a simple statistical method, uniblock, to overcome this problem. For each sentence, uniblock generates a fixed-size feature vector using Unicode block information of the characters. A Gaussian mixture model is then estimated on some clean corpus using variational inference. The learned model can then be used to score sentences and filter corpus. We present experimental results on Sentiment Analysis, Language Modeling and Machine Translation, and show the simplicity and effectiveness of our method.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D19-1222",
  "title": "To Annotate or Not? Predicting Performance Drop under Domain Shift",
  "year": 2019,
  "venue": "EMNLP",
  "abstract": "Performance drop due to domain-shift is an endemic problem for NLP models in production. This problem creates an urge to continuously annotate evaluation datasets to measure the expected drop in the model performance which can be prohibitively expensive and slow. In this paper, we study the problem of predicting the performance drop of modern NLP models under domain-shift, in the absence of any target domain labels. We investigate three families of methods ($H$-divergence, reverse classification accuracy and confidence measures), show how they can be used to predict the performance drop and study their robustness to adversarial domain-shifts. Our results on sentiment classification and sequence labelling show that our method is able to predict performance drops with an error rate as low as 2.15% and 0.89% for sentiment analysis and POS tagging respectively.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D19-1228",
  "title": "How well do NLI models capture verb veridicality?",
  "year": 2019,
  "venue": "EMNLP",
  "abstract": "In natural language inference (NLI), contexts are considered veridical if they allow us to infer that their underlying propositions make true claims about the real world. We investigate whether a state-of-the-art natural language inference model (BERT) learns to make correct inferences about veridicality in verb-complement constructions. We introduce an NLI dataset for veridicality evaluation consisting of 1,500 sentence pairs, covering 137 unique verbs. We find that both human and model inferences generally follow theoretical patterns, but exhibit a systematic bias towards assuming that verbs are veridical-a bias which is amplified in BERT. We further show that, encouragingly, BERT's inferences are sensitive not only to the presence of individual verb types, but also to the syntactic role of the verb, the form of the complement clause (to- vs. that-complements), and negation.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/D19-1269",
  "title": "Collaborative Policy Learning for Open Knowledge Graph Reasoning",
  "year": 2019,
  "venue": "EMNLP",
  "abstract": "In recent years, there has been a surge of interests in interpretable graph reasoning methods. However, these models often suffer from limited performance when working on sparse and incomplete graphs, due to the lack of evidential paths that can reach target entities. Here we study open knowledge graph reasoning-a task that aims to reason for missing facts over a graph augmented by a background text corpus. A key challenge of the task is to filter out \"irrelevant\" facts extracted from corpus, in order to maintain an effective search space during path inference. We propose a novel reinforcement learning framework to train two collaborative agents jointly, i.e., a multi-hop graph reasoner and a fact extractor. The fact extraction agent generates fact triples from corpora to enrich the graph on the fly; while the reasoning agent provides feedback to the fact extractor and guides it towards promoting facts that are helpful for the interpretable reasoning. Experiments on two public datasets demonstrate the effectiveness of the proposed approach.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/D19-1285",
  "title": "Is the Red Square Big? MALeViC: Modeling Adjectives Leveraging Visual Contexts",
  "year": 2019,
  "venue": "EMNLP",
  "abstract": "This work aims at modeling how the meaning of gradable adjectives of size ('big', 'small') can be learned from visually-grounded contexts. Inspired by cognitive and linguistic evidence showing that the use of these expressions relies on setting a threshold that is dependent on a specific context, we investigate the ability of multi-modal models in assessing whether an object is 'big' or 'small' in a given visual scene. In contrast with the standard computational approach that simplistically treats gradable adjectives as 'fixed' attributes, we pose the problem as relational: to be successful, a model has to consider the full visual context. By means of four main tasks, we show that state-of-the-art models (but not a relatively strong baseline) can learn the function subtending the meaning of size adjectives, though their performance is found to decrease while moving from simple to more complex tasks. Crucially, models fail in developing abstract representations of gradable adjectives that can be used compositionally.",
  "stance": -0.4
 },
 {
  "url": "https://aclanthology.org/D19-1287",
  "title": "Representation of Constituents in Neural Language Models: Coordination Phrase as a Case Study",
  "year": 2019,
  "venue": "EMNLP",
  "abstract": "Neural language models have achieved state-of-the-art performances on many NLP tasks, and recently have been shown to learn a number of hierarchically-sensitive syntactic dependencies between individual words. However, equally important for language processing is the ability to combine words into phrasal constituents, and use constituent-level features to drive downstream expectations. Here we investigate neural models' ability to represent constituent-level features, using coordinated noun phrases as a case study. We assess whether different neural language models trained on English and French represent phrase-level number and gender features, and use those features to drive downstream expectations. Our results suggest that models use a linear combination of NP constituent number to drive CoordNP/verb number agreement. This behavior is highly regular and even sensitive to local syntactic context, however it differs crucially from observed human behavior. Models have less success with gender agreement. Models trained on large corpora perform best, and there is no obvious advantage for models trained using explicit syntactic supervision.",
  "stance": -0.3
 },
 {
  "url": "https://aclanthology.org/D19-1296",
  "title": "Weakly Supervised Multilingual Causality Extraction from Wikipedia",
  "year": 2019,
  "venue": "EMNLP",
  "abstract": "We present a method for extracting causality knowledge from Wikipedia, such as Protectionism -> Trade war, where the cause and effect entities correspond to Wikipedia articles. Such causality knowledge is easy to verify by reading corresponding Wikipedia articles, to translate to multiple languages through Wikidata, and to connect to knowledge bases derived from Wikipedia. Our method exploits Wikipedia article sections that describe causality and the redundancy stemming from the multilinguality of Wikipedia. Experiments showed that our method achieved precision and recall above 98% and 64%, respectively. In particular, it could extract causalities whose cause and effect were written distantly in a Wikipedia article. We have released the code and data for further research.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D19-1310",
  "title": "Table-to-Text Generation with Effective Hierarchical Encoder on Three Dimensions (Row, Column and Time)",
  "year": 2019,
  "venue": "EMNLP",
  "abstract": "Although Seq2Seq models for table-to-text generation have achieved remarkable progress, modeling table representation in one dimension is inadequate. This is because (1) the table consists of multiple rows and columns, which means that encoding a table should not depend only on one dimensional sequence or set of records and (2) most of the tables are time series data (e.g. NBA game data, stock market data), which means that the description of the current table may be affected by its historical data. To address aforementioned problems, not only do we model each table cell considering other records in the same row, we also enrich table's representation by modeling each table cell in context of other cells in the same column or with historical (time dimension) data respectively. In addition, we develop a table cell fusion gate to combine representations from row, column and time dimension into one dense vector according to the saliency of each dimension's representation. We evaluated our methods on ROTOWIRE, a benchmark dataset of NBA basketball games. Both automatic and human evaluation results demonstrate the effectiveness of our model with improvement of 2.66 in BLEU over the strong baseline and outperformance of state-of-the-art model.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D19-1331",
  "title": "On NMT Search Errors and Model Errors: Cat Got Your Tongue?",
  "year": 2019,
  "venue": "EMNLP",
  "abstract": "We report on search errors and model errors in neural machine translation (NMT). We present an exact inference procedure for neural sequence models based on a combination of beam search and depth-first search. We use our exact search to find the global best model scores under a Transformer base model for the entire WMT15 English-German test set. Surprisingly, beam search fails to find these global best model scores in most cases, even with a very large beam size of 100. For more than 50% of the sentences, the model in fact assigns its global best score to the empty translation, revealing a massive failure of neural models in properly accounting for adequacy. We show by constraining search with a minimum translation length that at the root of the problem of empty translations lies an inherent bias towards shorter translations. We conclude that vanilla NMT in its current form requires just the right amount of beam search errors, which, from a modelling perspective, is a highly unsatisfactory conclusion indeed, as the model often prefers an empty translation.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/D19-1381",
  "title": "A Search-based Neural Model for Biomedical Nested and Overlapping Event Detection",
  "year": 2019,
  "venue": "EMNLP",
  "abstract": "We tackle the nested and overlapping event detection task and propose a novel search-based neural network (SBNN) structured prediction model that treats the task as a search problem on a relation graph of trigger-argument structures. Unlike existing structured prediction tasks such as dependency parsing, the task targets to detect DAG structures, which constitute events, from the relation graph. We define actions to construct events and use all the beams in a beam search to detect all event structures that may be overlapping and nested. The search process constructs events in a bottom-up manner while modelling the global properties for nested and overlapping structures simultaneously using neural networks. We show that the model achieves performance comparable to the state-of-the-art model Turku Event Extraction System (TEES) on the BioNLP Cancer Genetics (CG) Shared Task 2013 without the use of any syntactic and hand-engineered features. Further analyses on the development set show that our model is more computationally efficient while yielding higher F1-score performance.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/D19-1414",
  "title": "Nearly-Unsupervised Hashcode Representations for Biomedical Relation Extraction",
  "year": 2019,
  "venue": "EMNLP",
  "abstract": "Recently, kernelized locality sensitive hashcodes have been successfully employed as representations of natural language text, especially showing high relevance to biomedical relation extraction tasks. In this paper, we propose to optimize the hashcode representations in a nearly unsupervised manner, in which we only use data points, but not their class labels, for learning. The optimized hashcode representations are then fed to a supervised classifi er following the prior work. This nearly unsupervised approach allows fine-grained optimization of each hash function, which is particularly suitable for building hashcode representations generalizing from a training set to a test set. We empirically evaluate the proposed approach for biomedical relation extraction tasks, obtaining significant accuracy improvements w.r.t. state-of-the-art supervised and semi-supervised approaches.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D19-1563",
  "title": "A Knowledge Regularized Hierarchical Approach for Emotion Cause Analysis",
  "year": 2019,
  "venue": "EMNLP",
  "abstract": "Emotion cause analysis, which aims to identify the reasons behind emotions, is a key topic in sentiment analysis. A variety of neural network models have been proposed recently, however, these previous models mostly focus on the learning architecture with local textual information, ignoring the discourse and prior knowledge, which play crucial roles in human text comprehension. In this paper, we propose a new method to extract emotion cause with a hierarchical neural model and knowledge-based regularizations, which aims to incorporate discourse context information and restrain the parameters by sentiment lexicon and common knowledge. The experimental results demonstrate that our proposed method achieves the state-of-the-art performance on two public datasets in different languages (Chinese and English), outperforming a number of competitive baselines by at least 2.08% in F-measure.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D19-1607",
  "title": "Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model",
  "year": 2019,
  "venue": "EMNLP",
  "abstract": "Because it is not feasible to collect training data for every language, there is a growing interest in cross-lingual transfer learning. In this paper, we systematically explore zero-shot cross-lingual transfer learning on reading comprehension tasks with language representation model pre-trained on multi-lingual corpus. The experimental results show that with pre-trained language representation zero-shot learning is feasible, and translating the source data into the target language is not necessary and even degrades the performance. We further explore what does the model learn in zero-shot setting.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/D19-1662",
  "title": "Adversarial Removal of Demographic Attributes Revisited",
  "year": 2019,
  "venue": "EMNLP",
  "abstract": "Elazar and Goldberg (2018) showed that protected attributes can be extracted from the representations of a debiased neural network for mention detection at above-chance levels, by evaluating a diagnostic classifier on a held-out subsample of the data it was trained on. We revisit their experiments and conduct a series of follow-up experiments showing that, in fact, the diagnostic classifier generalizes poorly to both new in-domain samples and new domains, indicating that it relies on correlations specific to their particular data sample. We further show that a diagnostic classifier trained on the biased baseline neural network also does not generalize to new samples. In other words, the biases detected in Elazar and Goldberg (2018) seem restricted to their particular data sample, and would therefore not bias the decisions of the model on new samples, whether in-domain or out-of-domain. In light of this, we discuss better methodologies for detecting bias in our models.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.127",
  "title": "Double Graph Based Reasoning for Document-level Relation Extraction",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Document-level relation extraction aims to extract relations among entities within a document. Different from sentence-level relation extraction, it requires reasoning over multiple sentences across paragraphs. In this paper, we propose Graph Aggregation-and-Inference Network (GAIN), a method to recognize such relations for long paragraphs. GAIN constructs two graphs, a heterogeneous mention-level graph (MG) and an entity-level graph (EG). The former captures complex interaction among different mentions and the latter aggregates mentions underlying for the same entities. Based on the graphs we propose a novel path reasoning mechanism to infer relations between entities. Experiments on the public dataset, DocRED, show GAIN achieves a significant performance improvement (2.85 on F1) over the previous state-of-the-art. Our code is available at https://github.com/PKUnlp-icler/GAIN.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.147",
  "title": "GraphDialog: Integrating Graph Knowledge into End-to-End Task-Oriented Dialogue Systems",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "End-to-end task-oriented dialogue systems aim to generate system responses directly from plain text inputs. There are two challenges for such systems: one is how to effectively incorporate external knowledge bases (KBs) into the learning framework; the other is how to accurately capture the semantics of dialogue history. In this paper, we address these two challenges by exploiting the graph structural information in the knowledge base and in the dependency parsing tree of the dialogue. To effectively leverage the structural information in dialogue history, we propose a new recurrent cell architecture which allows representation learning on graphs. To exploit the relations between entities in KBs, the model combines multi-hop reasoning ability based on the graph structure. Experimental results show that the proposed model achieves consistent improvement over state-of-the-art models on two different task-oriented dialogue datasets.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.156",
  "title": "RNNs can generate bounded hierarchical languages with optimal memory",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Recurrent neural networks empirically generate natural language with high syntactic fidelity. However, their success is not well-understood theoretically. We provide theoretical insight into this success, proving in a finite-precision setting that RNNs can efficiently generate bounded hierarchical languages that reflect the scaffolding of natural language syntax. We introduce Dyck-$(k,m)$, the language of well-nested brackets (of $k$ types) and $m$-bounded nesting depth, reflecting the bounded memory needs and long-distance dependencies of natural language syntax. The best known results use $O(k^{\\fracm2})$ memory (hidden units) to generate these languages. We prove that an RNN with $O(m \\log k)$ hidden units suffices, an exponential reduction in memory, by an explicit construction. Finally, we show that no algorithm, even with unbounded computation, can suffice with $o(m \\log k)$ hidden units.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.26",
  "title": "Incremental Processing in the Age of Non-Incremental Encoders: An Empirical Assessment of Bidirectional Models for Incremental NLU",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "While humans process language incrementally, the best language encoders currently used in NLP do not. Both bidirectional LSTMs and Transformers assume that the sequence that is to be encoded is available in full, to be processed either forwards and backwards (BiLSTMs) or as a whole (Transformers). We investigate how they behave under incremental interfaces, when partial output must be provided based on partial input seen up to a certain time step, which may happen in interactive systems. We test five models on various NLU datasets and compare their performance using three incremental evaluation metrics. The results support the possibility of using bidirectional encoders in incremental mode while retaining most of their non-incremental quality. The \"omni-directional\" BERT model, which achieves better non-incremental performance, is impacted more by the incremental access. This can be alleviated by adapting the training regime (truncated training), or the testing procedure, by delaying the output until some right context is available or by incorporating hypothetical right contexts generated by a language model like GPT-2.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.260",
  "title": "On the weak link between importance and prunability of attention heads",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Given the success of Transformer-based models, two directions of study have emerged: interpreting role of individual attention heads and down-sizing the models for efficiency. Our work straddles these two streams: We analyse the importance of basing pruning strategies on the interpreted role of the attention heads. We evaluate this on Transformer and BERT models on multiple NLP tasks. Firstly, we find that a large fraction of the attention heads can be randomly pruned with limited effect on accuracy. Secondly, for Transformers, we find no advantage in pruning attention heads identified to be important based on existing studies that relate importance to the location of a head. On the BERT model too we find no preference for top or bottom layers, though the latter are reported to have higher importance. However, strategies that avoid pruning middle layers and consecutive layers perform better. Finally, during fine-tuning the compensation for pruned attention heads is roughly equally distributed across the un-pruned heads. Our results thus suggest that interpretation of attention heads does not strongly inform pruning.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.33",
  "title": "What Have We Achieved on Text Summarization?",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Deep learning has led to significant improvement in text summarization with various methods investigated and improved ROUGE scores reported over the years. However, gaps still exist between summaries produced by automatic summarizers and human professionals. Aiming to gain more understanding of summarization systems with respect to their strengths and limits on a fine-grained syntactic and semantic level, we consult the Multidimensional Quality Metric (MQM) and quantify 8 major sources of errors on 10 representative summarization models manually. Primarily, we find that 1) under similar settings, extractive summarizers are in general better than their abstractive counterparts thanks to strength in faithfulness and factual-consistency; 2) milestone techniques such as copy, coverage and hybrid extractive/abstractive methods do bring specific improvements but also demonstrate limitations; 3) pre-training techniques, and in particular sequence-to-sequence pre-training, are highly effective for improving text summarization, with BART giving the best results.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.349",
  "title": "PlotMachines: Outline-Conditioned Generation with Dynamic Plot State Tracking",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "We propose the task of outline-conditioned story generation: given an outline as a set of phrases that describe key characters and events to appear in a story, the task is to generate a coherent narrative that is consistent with the provided outline. This task is challenging as the input only provides a rough sketch of the plot, and thus, models need to generate a story by interweaving the key points provided in the outline. This requires the model to keep track of the dynamic states of the latent plot, conditioning on the input outline while generating the full story. We present PlotMachines, a neural narrative model that learns to transform an outline into a coherent story by tracking the dynamic plot states. In addition, we enrich PlotMachines with high-level discourse structure so that the model can learn different writing styles corresponding to different parts of the narrative. Comprehensive experiments over three fiction and non-fiction datasets demonstrate that large-scale language models, such as GPT-2 and Grover, despite their impressive generation performance, are not sufficient in generating coherent narratives for the given outline, and dynamic plot state tracking is important for composing narratives with tighter, more consistent plots.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.350",
  "title": "Do sequence-to-sequence VAEs learn global features of sentences?",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Autoregressive language models are powerful and relatively easy to train. However, these models are usually trained without explicit conditioning labels and do not offer easy ways to control global aspects such as sentiment or topic during generation. Bowman & al. 2016 adapted the Variational Autoencoder (VAE) for natural language with the sequence-to-sequence architecture and claimed that the latent vector was able to capture such global features in an unsupervised manner. We question this claim. We measure which words benefit most from the latent information by decomposing the reconstruction loss per position in the sentence. Using this method, we find that VAEs are prone to memorizing the first words and the sentence length, producing local features of limited usefulness. To alleviate this, we investigate alternative architectures based on bag-of-words assumptions and language model pretraining. These variants learn latent variables that are more global, i.e., more predictive of topic or sentiment labels. Moreover, using reconstructions, we observe that they decrease memorization: the first word and the sentence length are not recovered as accurately than with the baselines, consequently yielding more diverse reconstructions.",
  "stance": -0.5
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.362",
  "title": "Do Explicit Alignments Robustly Improve Multilingual Encoders?",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Multilingual BERT (mBERT), XLM-RoBERTa (XLMR) and other unsupervised multilingual encoders can effectively learn cross-lingual representation. Explicit alignment objectives based on bitexts like Europarl or MultiUN have been shown to further improve these representations. However, word-level alignments are often suboptimal and such bitexts are unavailable for many languages. In this paper, we propose a new contrastive alignment objective that can better utilize such signal, and examine whether these previous alignment methods can be adapted to noisier sources of aligned data: a randomly sampled 1 million pair subset of the OPUS collection. Additionally, rather than report results on a single dataset with a single model run, we report the mean and standard derivation of multiple runs with different seeds, on four datasets and tasks. Our more extensive analysis finds that, while our new objective outperforms previous work, overall these methods do not improve performance with a more robust evaluation framework. Furthermore, the gains from using a better underlying model eclipse any benefits from alignment training. These negative results dictate more care in evaluating these methods and suggest limitations in applying explicit alignment objectives.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.363",
  "title": "From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Massively multilingual transformers (MMTs) pretrained via language modeling (e.g., mBERT, XLM-R) have become a default paradigm for zero-shot language transfer in NLP, offering unmatched transfer performance. Current evaluations, however, verify their efficacy in transfers (a) to languages with sufficiently large pretraining corpora, and (b) between close languages. In this work, we analyze the limitations of downstream language transfer with MMTs, showing that, much like cross-lingual word embeddings, they are substantially less effective in resource-lean scenarios and for distant languages. Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER) and two high-level tasks (NLI, QA), empirically correlate transfer performance with linguistic proximity between source and target languages, but also with the size of target language corpora used in MMT pretraining. Most importantly, we demonstrate that the inexpensive few-shot transfer (i.e., additional fine-tuning on a few target-language instances) is surprisingly effective across the board, warranting more research efforts reaching beyond the limiting zero-shot conditions.",
  "stance": -0.3
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.394",
  "title": "An Empirical Investigation Towards Efficient Multi-Domain Language Model Pre-training",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Pre-training large language models has become a standard in the natural language processing community. Such models are pre-trained on generic data (e.g. BookCorpus and English Wikipedia) and often fine-tuned on tasks in the same domain. However, in order to achieve state-of-the-art performance on out of domain tasks such as clinical named entity recognition and relation extraction, additional in domain pre-training is required. In practice, staged multi-domain pre-training presents performance deterioration in the form of catastrophic forgetting (CF) when evaluated on a generic benchmark such as GLUE. In this paper we conduct an empirical investigation into known methods to mitigate CF. We find that elastic weight consolidation provides best overall scores yielding only a 0.33% drop in performance across seven generic tasks while remaining competitive in bio-medical tasks. Furthermore, we explore gradient and latent clustering based data selection techniques to improve coverage when using elastic weight consolidation and experience replay methods.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.40",
  "title": "Don't Use English Dev: On the Zero-Shot Cross-Lingual Evaluation of Contextual Embeddings",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Multilingual contextual embeddings have demonstrated state-of-the-art performance in zero-shot cross-lingual transfer learning, where multilingual BERT is fine-tuned on one source language and evaluated on a different target language. However, published results for mBERT zero-shot accuracy vary as much as 17 points on the MLDoc classification task across four papers. We show that the standard practice of using English dev accuracy for model selection in the zero-shot setting makes it difficult to obtain reproducible results on the MLDoc and XNLI tasks. English dev accuracy is often uncorrelated (or even anti-correlated) with target language accuracy, and zero-shot performance varies greatly at different points in the same fine-tuning run and between different fine-tuning runs. These reproducibility issues are also present for other tasks with different pre-trained embeddings (e.g., MLQA with XLM-R). We recommend providing oracle scores alongside zero-shot results: still fine-tune using English data, but choose a checkpoint with the target dev set. Reporting this upper bound makes results more consistent by avoiding arbitrarily bad checkpoints.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.429",
  "title": "Writing Strategies for Science Communication: Data and Computational Analysis",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Communicating complex scientific ideas without misleading or overwhelming the public is challenging. While science communication guides exist, they rarely offer empirical evidence for how their strategies are used in practice. Writing strategies that can be automatically recognized could greatly support science communication efforts by enabling tools to detect and suggest strategies for writers. We compile a set of writing strategies drawn from a wide range of prescriptive sources and develop an annotation scheme allowing humans to recognize them. We collect a corpus of 128k science writing documents in English and annotate a subset of this corpus. We use the annotations to train transformer-based classifiers and measure the strategies' use in the larger corpus. We find that the use of strategies, such as storytelling and emphasizing the most important findings, varies significantly across publications with different reader audiences.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.446",
  "title": "Imitation Attacks and Defenses for Black-box Machine Translation Systems",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Adversaries may look to steal or attack black-box NLP systems, either for financial gain or to exploit model errors. One setting of particular interest is machine translation (MT), where models have high commercial value and errors can be costly. We investigate possible exploitations of black-box MT systems and explore a preliminary defense against such threats. We first show that MT systems can be stolen by querying them with monolingual sentences and training models to imitate their outputs. Using simulated experiments, we demonstrate that MT model stealing is possible even when imitation models have different input data or architectures than their target models. Applying these ideas, we train imitation models that reach within 0.6 BLEU of three production MT systems on both high-resource and low-resource language pairs. We then leverage the similarity of our imitation models to transfer adversarial examples to the production systems. We use gradient-based attacks that expose inputs which lead to semantically-incorrect translations, dropped content, and vulgar model outputs. To mitigate these vulnerabilities, we propose a defense that modifies translation outputs in order to misdirect the optimization of imitation models. This defense degrades the adversary's BLEU score and attack success rate at some cost in the defender's BLEU and inference speed.",
  "stance": -0.5
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.45",
  "title": "Condolence and Empathy in Online Communities",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Offering condolence is a natural reaction to hearing someone's distress. Individuals frequently express distress in social media, where some communities can provide support. However, not all condolence is equal-trite responses offer little actual support despite their good intentions. Here, we develop computational tools to create a massive dataset of 11.4M expressions of distress and 2.8M corresponding offerings of condolence in order to examine the dynamics of condolence online. Our study reveals widespread disparity in what types of distress receive supportive condolence rather than just engagement. Building on studies from social psychology, we analyze the language of condolence and develop a new dataset for quantifying the empathy in a condolence using appraisal theory. Finally, we demonstrate that the features of condolence individuals find most helpful online differ substantially in their features from those seen in interpersonal settings.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.477",
  "title": "LAReQA: Language-Agnostic Answer Retrieval from a Multilingual Pool",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "We present LAReQA, a challenging new benchmark for language-agnostic answer retrieval from a multilingual candidate pool. Unlike previous cross-lingual tasks, LAReQA tests for \"strong\" cross-lingual alignment, requiring semantically related cross-language pairs to be closer in representation space than unrelated same-language pairs. This level of alignment is important for the practical task of cross-lingual information retrieval. Building on multilingual BERT (mBERT), we study different strategies for achieving strong alignment. We find that augmenting training data via machine translation is effective, and improves significantly over using mBERT out-of-the-box. Interestingly, model performance on zero-shot variants of our task that only target \"weak\" alignment is not predictive of performance on LAReQA. This finding underscores our claim that language-agnostic retrieval is a substantively new kind of cross-lingual evaluation, and suggests that measuring both weak and strong alignment will be important for improving cross-lingual systems going forward. We release our dataset and evaluation code at https://github.com/google-research-datasets/lareqa.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.573",
  "title": "Compositional and Lexical Semantics in RoBERTa, BERT and DistilBERT: A Case Study on CoQA",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Many NLP tasks have benefited from transferring knowledge from contextualized word embeddings, however the picture of what type of knowledge is transferred is incomplete. This paper studies the types of linguistic phenomena accounted for by language models in the context of a Conversational Question Answering (CoQA) task. We identify the problematic areas for the finetuned RoBERTa, BERT and DistilBERT models through systematic error analysis - basic arithmetic (counting phrases), compositional semantics (negation and Semantic Role Labeling), and lexical semantics (surprisal and antonymy). When enhanced with the relevant linguistic knowledge through multitask learning, the models improve in performance. Ensembles of the enhanced models yield a boost between 2.2 and 2.7 points in F1 score overall, and up to 42.1 points in F1 on the hardest question classes. The results show differences in ability to represent compositional and lexical information between RoBERTa, BERT and DistilBERT.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.576",
  "title": "On the Ability and Limitations of Transformers to Recognize Formal Languages",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Transformers have supplanted recurrent models in a large number of NLP tasks. However, the differences in their abilities to model different syntactic properties remain largely unknown. Past works suggest that LSTMs generalize very well on regular languages and have close connections with counter languages. In this work, we systematically study the ability of Transformers to model such languages as well as the role of its individual components in doing so. We first provide a construction of Transformers for a subclass of counter languages, including well-studied languages such as n-ary Boolean Expressions, Dyck-1, and its generalizations. In experiments, we find that Transformers do well on this subclass, and their learned mechanism strongly correlates with our construction. Perhaps surprisingly, in contrast to LSTMs, Transformers do well only on a subset of regular languages with degrading performance as we make languages more complex according to a well-known measure of complexity. Our analysis also provides insights on the role of self-attention mechanism in modeling certain behaviors and the influence of positional encoding schemes on the learning and generalization abilities of the model.",
  "stance": -0.3
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.616",
  "title": "Detecting Word Sense Disambiguation Biases in Machine Translation for Model-Agnostic Adversarial Attacks",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Word sense disambiguation is a well-known source of translation errors in NMT. We posit that some of the incorrect disambiguation choices are due to models' over-reliance on dataset artifacts found in training data, specifically superficial word co-occurrences, rather than a deeper understanding of the source text. We introduce a method for the prediction of disambiguation errors based on statistical data properties, demonstrating its effectiveness across several domains and model types. Moreover, we develop a simple adversarial attack strategy that minimally perturbs sentences in order to elicit disambiguation errors to further probe the robustness of translation models. Our findings indicate that disambiguation robustness varies substantially between domains and that different models trained on the same data are vulnerable to different attacks.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.693",
  "title": "Knowledge-guided Open Attribute Value Extraction with Reinforcement Learning",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Open attribute value extraction for emerging entities is an important but challenging task. A lot of previous works formulate the problem as a question-answering (QA) task. While the collections of articles from web corpus provide updated information about the emerging entities, the retrieved texts can be noisy, irrelevant, thus leading to inaccurate answers. Effectively filtering out noisy articles as well as bad answers is the key to improve extraction accuracy. Knowledge graph (KG), which contains rich, well organized information about entities, provides a good resource to address the challenge. In this work, we propose a knowledge-guided reinforcement learning (RL) framework for open attribute value extraction. Informed by relevant knowledge in KG, we trained a deep Q-network to sequentially compare extracted answers to improve extraction accuracy. The proposed framework is applicable to different information extraction system. Our experimental results show that our method outperforms the baselines by 16.5 - 27.8%.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.696",
  "title": "Improving Low Compute Language Modeling with In-Domain Embedding Initialisation",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Many NLP applications, such as biomedical data and technical support, have 10-100 million tokens of in-domain data and limited computational resources for learning from it. How should we train a language model in this scenario? Most language modeling research considers either a small dataset with a closed vocabulary (like the standard 1 million token Penn Treebank), or the whole web with byte-pair encoding. We show that for our target setting in English, initialising and freezing input embeddings using in-domain data can improve language model performance by providing a useful representation of rare words, and this pattern holds across several different domains. In the process, we show that the standard convention of tying input and output embeddings does not improve perplexity when initializing with embeddings trained on in-domain data.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/2020.findings-emnlp.117",
  "title": "Evaluating Models' Local Decision Boundaries via Contrast Sets",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Standard test sets for supervised learning evaluate in-distribution generalization. Unfortunately, when a dataset has systematic gaps (e.g., annotation artifacts), these evaluations are misleading: a model can learn simple decision rules that perform well on the test set but do not capture the abilities a dataset is intended to test. We propose a more rigorous annotation paradigm for NLP that helps to close systematic gaps in the test data. In particular, after a dataset is constructed, we recommend that the dataset authors manually perturb the test instances in small but meaningful ways that (typically) change the gold label, creating contrast sets. Contrast sets provide a local view of a model's decision boundary, which can be used to more accurately evaluate a model's true linguistic capabilities. We demonstrate the efficacy of contrast sets by creating them for 10 diverse NLP datasets (e.g., DROP reading comprehension, UD parsing, and IMDb sentiment analysis). Although our contrast sets are not explicitly adversarial, model performance is significantly lower on them than on the original test sets-up to 25% in some cases. We release our contrast sets as new evaluation benchmarks and encourage future dataset construction efforts to follow similar annotation processes.",
  "stance": -0.1
 },
 {
  "url": "https://aclanthology.org/2020.findings-emnlp.123",
  "title": "TextHide: Tackling Data Privacy in Language Understanding Tasks",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "An unsolved challenge in distributed or federated learning is to effectively mitigate privacy risks without slowing down training or reducing accuracy. In this paper, we propose TextHide aiming at addressing this challenge for natural language understanding tasks. It requires all participants to add a simple encryption step to prevent an eavesdropping attacker from recovering private text data. Such an encryption step is efficient and only affects the task performance slightly. In addition, TextHide fits well with the popular framework of fine-tuning pre-trained language models (e.g., BERT) for any sentence or sentence-pair task. We evaluate TextHide on the GLUE benchmark, and our experiments show that TextHide can effectively defend attacks on shared gradients or representations and the averaged accuracy reduction is only 1.9%. We also present an analysis of the security of TextHide using a conjecture about the computational intractability of a mathematical problem.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.findings-emnlp.27",
  "title": "Understanding tables with intermediate pre-training",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Table entailment, the binary classification task of finding if a sentence is supported or refuted by the content of a table, requires parsing language and table structure as well as numerical and discrete reasoning. While there is extensive work on textual entailment, table entailment is less well studied. We adapt TAPAS (Herzig et al., 2020), a table-based BERT model, to recognize entailment. Motivated by the benefits of data augmentation, we create a balanced dataset of millions of automatically created training examples which are learned in an intermediate step prior to fine-tuning. This new data is not only useful for table entailment, but also for SQA (Iyyer et al., 2017), a sequential table QA task. To be able to use long examples as input of BERT models, we evaluate table pruning techniques as a pre-processing step to drastically improve the training and prediction efficiency at a moderate drop in accuracy. The different methods set the new state-of-the-art on the TabFact (Chen et al., 2020) and SQA datasets.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.findings-emnlp.295",
  "title": "WER we are and WER we think we are",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Natural language processing of conversational speech requires the availability of high-quality transcripts. In this paper, we express our skepticism towards the recent reports of very low Word Error Rates (WERs) achieved by modern Automatic Speech Recognition (ASR) systems on benchmark datasets. We outline several problems with popular benchmarks and compare three state-of-the-art commercial ASR systems on an internal dataset of real-life spontaneous human conversations and HUB'05 public benchmark. We show that WERs are significantly higher than the best reported results. We formulate a set of guidelines which may aid in the creation of real-life, multi-domain datasets with high quality annotations for training and testing of robust ASR systems.",
  "stance": -0.9
 },
 {
  "url": "https://aclanthology.org/2020.findings-emnlp.319",
  "title": "Improving Target-side Lexical Transfer in Multilingual Neural Machine Translation",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "To improve the performance of Neural Machine Translation (NMT) for low-resource languages (LRL), one effective strategy is to leverage parallel data from a related high-resource language (HRL). However, multilingual data has been found more beneficial for NMT models that translate from the LRL to a target language than the ones that translate into the LRLs. In this paper, we aim to improve the effectiveness of multilingual transfer for NMT models that translate into the LRL, by designing a better decoder word embedding. Extending upon a general-purpose multilingual encoding method Soft Decoupled Encoding (Wang et al., 2019), we propose DecSDE, an efficient character n-gram based embedding specifically designed for the NMT decoder. Our experiments show that DecSDE leads to consistent gains of up to 1.8 BLEU on translation from English to four different languages.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.findings-emnlp.358",
  "title": "Effects of Naturalistic Variation in Goal-Oriented Dialog",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Existing benchmarks used to evaluate the performance of end-to-end neural dialog systems lack a key component: natural variation present in human conversations. Most datasets are constructed through crowdsourcing, where the crowd workers follow a fixed template of instructions while enacting the role of a user/agent. This results in straight-forward, somewhat routine, and mostly trouble-free conversations, as crowd workers do not think to represent the full range of actions that occur naturally with real users. In this work, we investigate the impact of naturalistic variation on two goal-oriented datasets: bAbI dialog task and Stanford Multi-Domain Dataset (SMD). We also propose new and more effective testbeds for both datasets, by introducing naturalistic variation by the user. We observe that there is a significant drop in performance (more than 60% in Ent. F1 on SMD and 85% in per-dialog accuracy on bAbI task) of recent state-of-the-art end-to-end neural methods such as BossNet and GLMP on both datasets.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/2020.findings-emnlp.389",
  "title": "What's so special about BERT's layers? A closer look at the NLP pipeline in monolingual and multilingual models",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Peeking into the inner workings of BERT has shown that its layers resemble the classical NLP pipeline, with progressively more complex tasks being concentrated in later layers. To investigate to what extent these results also hold for a language other than English, we probe a Dutch BERT-based model and the multilingual BERT model for Dutch NLP tasks. In addition, through a deeper analysis of part-of-speech tagging, we show that also within a given task, information is spread over different parts of the network and the pipeline might not be as neat as it seems. Each layer has different specialisations, so that it may be more useful to combine information from different layers, instead of selecting a single one based on the best overall performance.",
  "stance": -0.3
 },
 {
  "url": "https://arxiv.org/abs/1301.4083",
  "title": "Knowledge Matters: Importance of Prior Information for Optimization",
  "year": 2013,
  "venue": "ICLR",
  "abstract": "We explore the effect of introducing prior information into the intermediate level of deep supervised neural networks for a learning task on which all the black-box state-of-the-art machine learning algorithms tested have failed to learn. We motivate our work from the hypothesis that there is an optimization obstacle involved in the nature of such tasks, and that humans learn useful intermediate concepts from other individuals via a form of supervision or guidance using a curriculum. The experiments we have conducted provide positive evidence in favor of this hypothesis. In our experiments, a two-tiered MLP architecture is trained on a dataset for which each image input contains three sprites, and the binary target class is 1 if all three have the same shape. Black-box machine learning algorithms only got chance on this task. Standard deep supervised neural networks also failed. However, using a particular structure and guiding the learner by providing intermediate targets in the form of intermediate concepts (the presence of each object) allows to nail the task. Much better than chance but imperfect results are also obtained by exploring architecture and optimization variants, pointing towards a difficult optimization task. We hypothesize that the learning difficulty is due to the composition of two highly non-linear tasks. Our findings are also consistent with hypotheses on cultural learning inspired by the observations of effective local minima (possibly due to ill-conditioning and the training procedure not being able to escape what appears like a local minimum).",
  "stance": 0.7
 },
 {
  "url": "https://arxiv.org/abs/1312.5921",
  "title": "Group-sparse Embeddings in Collective Matrix Factorization",
  "year": 2014,
  "venue": "ICLR",
  "abstract": "CMF is a technique for simultaneously learning low-rank representations based on a collection of matrices with shared entities. A typical example is the joint modeling of useritem, item-property, and user-feature matrices in a recommender system. The key idea in CMF is that the embeddings are shared across the matrices, which enables transferring information between them. The existing solutions, however, break down when the individual matrices have low-rank structure not shared with others. In this work we present a novel CMF solution that allows each of the matrices to have a separate low-rank structure that is independent of the other matrices, as well as structures that are shared only by a subset of them. We compare MAP and variational Bayesian solutions based on alternating optimization algorithms and show that the model automatically infers the nature of each factor using group-wise sparsity. Our approach supports in a principled way continuous, binary and count observations and is efficient for sparse matrices involving missing data. We illustrate the solution on a number of examples, focusing in particular on an interesting use-case of augmented multi-view learning.",
  "stance": 0.8
 },
 {
  "url": "https://arxiv.org/abs/1312.6095",
  "title": "Multi-View Priors for Learning Detectors from Sparse Viewpoint Data",
  "year": 2014,
  "venue": "ICLR",
  "abstract": "While the majority of today’s object class models provide only 2D bounding boxes, far richer output hypotheses are desirable including viewpoint, fine-grained category, and 3D geometry estimate. However, models trained to provide richer output require larger amounts of training data, preferably well covering the relevant aspects such as viewpoint and fine-grained categories. In this paper, we address this issue from the perspective of transfer learning, and design an object class model that explicitly leverages correlations between visual features. Specifically, our model represents prior distributions over permissible multi-view detectors in a parametric way – the priors are learned once from training data of a source object class, and can later be used to facilitate the learning of a detector for a target class. As we show in our experiments, this transfer is not only beneficial for detectors based on basic-level category representations, but also enables the robust learning of detectors that represent classes at finer levels of granularity, where training data is typically even scarcer and more unbalanced. As a result, we report largely improved performance in simultaneous 2D object localization and viewpoint estimation on a recent dataset of challenging street scenes.",
  "stance": 0.9
 },
 {
  "url": "https://arxiv.org/abs/1312.6199",
  "title": "Intriguing properties of neural networks",
  "year": 2014,
  "venue": "ICLR",
  "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network’s prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
  "stance": -1.0
 },
 {
  "url": "https://arxiv.org/abs/1312.6211",
  "title": "An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks",
  "year": 2014,
  "venue": "ICLR",
  "abstract": "Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models “forget” how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting. We find that it is always best to train using the dropout algorithm– the dropout algorithm is consistently best at adapting to the new task, remembering the old task, and has the best tradeoff curve between these two extremes. We find that different tasks and relationships between tasks result in very different rankings of activation function performance. This suggests that the choice of activation function should always be cross-validated.",
  "stance": 0.2
 },
 {
  "url": "https://arxiv.org/abs/1312.6229",
  "title": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",
  "year": 2014,
  "venue": "ICLR",
  "abstract": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.",
  "stance": 1.0
 },
 {
  "url": "https://arxiv.org/abs/1412.6856",
  "title": "Object Detectors Emerge in Deep Scene CNNs",
  "year": 2015,
  "venue": "ICLR",
  "abstract": "With the success of new computational architectures for visual processing, such as convolutional neural networks (CNN) and access to image databases with millions of labeled examples (e.g., ImageNet, Places), the state of the art in computer vision is advancing rapidly. One important factor for continued progress is to understand the representations that are learned by the inner layers of these deep architectures. Here we show that object detectors emerge from training CNNs to perform scene classification. As scenes are composed of objects, the CNN for scene classification automatically discovers meaningful objects detectors, representative of the learned scene categories. With object detectors emerging as a result of learning to recognize scenes, our work demonstrates that the same network can perform both scene recognition and object localization in a single forward-pass, without ever having been explicitly taught the notion of objects.",
  "stance": 0.2
 },
 {
  "url": "https://arxiv.org/abs/1411.7676",
  "title": "Visual Representations: Defining Properties and Deep Approximations",
  "year": 2016,
  "venue": "ICLR",
  "abstract": "Visual representations are defined in terms of minimal sufficient statistics of visual data, for a class of tasks, that are also invariant to nuisance variability. Minimal sufficiency guarantees that we can store a representation in lieu of raw data with smallest complexity and no performance loss on the task at hand. Invariance guarantees that the statistic is constant with respect to uninformative transformations of the data. We derive analytical expressions for such representations and show they are related to feature descriptors commonly used in computer vision, as well as to convolutional neural networks. This link highlights the assumptions and approximations tacitly assumed by these methods and explains empirical practices such as clamping, pooling and joint normalization.",
  "stance": 0.3
 },
 {
  "url": "https://arxiv.org/abs/1511.01844",
  "title": "A note on the evaluation of generative models",
  "year": 2016,
  "venue": "ICLR",
  "abstract": "Probabilistic generative models can be used for compression, denoising, inpainting, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often difficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most commonly used criteria—average log-likelihood, Parzen window estimates, and visual fidelity of samples—are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided.",
  "stance": 0.0
 },
 {
  "url": "https://arxiv.org/abs/1511.02301",
  "title": "The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations",
  "year": 2016,
  "venue": "ICLR",
  "abstract": "We introduce a new test of how well language models capture meaning in children’s books. Unlike standard language modelling benchmarks, it distinguishes the task of predicting syntactic function words from that of predicting lowerfrequency words, which carry greater semantic content. We compare a range of state-of-the-art models, each with a different way of encoding what has been previously read. We show that models which store explicit representations of long-term contexts outperform state-of-the-art neural language models at predicting semantic content words, although this advantage is not observed for syntactic function words. Interestingly, we find that the amount of text encoded in a single memory representation is highly influential to the performance: there is a sweet-spot, not too big and not too small, between single words and full sentences that allows the most meaningful information in a text to be effectively retained and recalled. Further, the attention over such window-based memories can be trained effectively through self-supervision. We then assess the generality of this principle by applying it to the CNN QA benchmark, which involves identifying named entities in paraphrased summaries of news articles, and achieve state-of-the-art performance.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=BJ6oOfqge",
  "title": "Temporal Ensembling for Semi-Supervised Learning",
  "year": 2017,
  "venue": "ICLR",
  "abstract": "In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce self-ensembling, where we form a consensus prediction of the unknown labels using the outputs of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the (non-augmented) classification error rate from 18.44% to 7.05% in SVHN with 500 labels and from 18.63% to 16.55% in CIFAR-10 with 4000 labels, and further to 5.12% and 12.16% by enabling the standard augmentations. We additionally obtain a clear improvement in CIFAR-100 classification accuracy by using random images from the Tiny Images dataset as unlabeled extra inputs during training. Finally, we demonstrate good tolerance to incorrect labels.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=BJC_jUqxe",
  "title": "A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING",
  "year": 2017,
  "venue": "ICLR",
  "abstract": "This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=BJKYvt5lg",
  "title": "PixelVAE: A Latent Variable Model for Natural Images",
  "year": 2017,
  "venue": "ICLR",
  "abstract": "Natural image modeling is a landmark challenge of unsupervised learning. Variational Autoencoders (VAEs) learn a useful latent representation and model global structure well but have difficulty capturing small details. PixelCNN models details very well, but lacks a latent code and is difficult to scale for capturing large structures. We present PixelVAE, a VAE model with an autoregressive decoder based on PixelCNN. Our model requires very few expensive autoregressive layers compared to PixelCNN and learns latent codes that are more compressed than a standard VAE while still capturing most non-trivial structure. Finally, we extend our model to a hierarchy of latent variables at different scales. Our model achieves state-of-the-art performance on binarized MNIST, competitive performance on 64 × 64 ImageNet, and high-quality samples on the LSUN bedrooms dataset.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=H1oyRlYgg",
  "title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima",
  "year": 2017,
  "venue": "ICLR",
  "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say 32–512 data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions—and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.",
  "stance": -0.5
 },
 {
  "url": "https://openreview.net/forum?id=HJ1kmv9xx",
  "title": "LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation",
  "year": 2017,
  "venue": "ICLR",
  "abstract": "We present LR-GAN: an adversarial image generation model which takes scene structure and context into account. Unlike previous generative adversarial networks (GANs), the proposed GAN learns to generate image background and foregrounds separately and recursively, and stitch the foregrounds on the background in a contextually relevant manner to produce a complete natural image. For each foreground, the model learns to generate its appearance, shape and pose. The whole model is unsupervised, and is trained in an end-to-end manner with gradient descent methods. The experiments demonstrate that LR-GAN can generate more natural images with objects that are more human recognizable than DCGAN.",
  "stance": 0.5
 },
 {
  "url": "https://openreview.net/forum?id=HJKkY35le",
  "title": "Mode Regularized Generative Adversarial Networks",
  "year": 2017,
  "venue": "ICLR",
  "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution. We introduce several ways of regularizing the objective, which can dramatically stabilize the training of GAN models. We also show that our regularizers can help the fair distribution of probability mass across the modes of the data generating distribution, during the early phases of training and thus providing a unified solution to the missing modes problem.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=Hk3mPK5gg",
  "title": "Training Agent for First-Person Shooter Game with Actor-Critic Curriculum Learning",
  "year": 2017,
  "venue": "ICLR",
  "abstract": "In this paper, we propose a new framework for training vision-based agent for First-Person Shooter (FPS) Game, in particular Doom. Our framework combines the state-of-the-art reinforcement learning approach (Asynchronous Advantage Actor-Critic (A3C) model [Mnih et al. (2016)]) with curriculum learning. Our model is simple in design and only uses game states from the AI side, rather than using opponents’ information [Lample & Chaplot (2016)]. On a known map, our agent won 10 out of the 11 attended games and the champion of Track1 in ViZDoom AI Competition 2016 by a large margin, 35% higher score than the second place.",
  "stance": 0.7
 },
 {
  "url": "https://openreview.net/forum?id=HkYhZDqxg",
  "title": "Tree-structured decoding with doubly-recurrent neural networks",
  "year": 2017,
  "venue": "ICLR",
  "abstract": "We propose a neural network architecture for generating tree-structured objects from encoded representations. The core of the method is a doubly recurrent neural network model comprised of separate width and depth recurrences that are combined inside each cell (node) to generate an output. The topology of the tree is modeled explicitly together with the content. That is, in response to an encoded vector representation, co-evolving recurrences are used to realize the associated tree and the labels for the nodes in the tree. We test this architecture in an encoderdecoder framework, where we train a network to encode a sentence as a vector, and then generate a tree structure from it. The experimental results show the effectiveness of this architecture at recovering latent tree structure in sequences and at mapping sentences to simple functional programs.",
  "stance": 0.4
 },
 {
  "url": "https://openreview.net/forum?id=HyTqHL5xg",
  "title": "Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data",
  "year": 2017,
  "venue": "ICLR",
  "abstract": "We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning and identification of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions via variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=SJMGPrcle",
  "title": "Learning to Navigate in Complex Environments",
  "year": 2017,
  "venue": "ICLR",
  "abstract": "Learning to navigate in complex environments with dynamic elements is an important milestone in developing AI agents. In this work we formulate the navigation question as a reinforcement learning problem and show that data efficiency and task performance can be dramatically improved by relying on additional auxiliary tasks leveraging multimodal sensory inputs. In particular we consider jointly learning the goal-driven reinforcement learning problem with auxiliary depth prediction and loop closure classification tasks. This approach can learn to navigate from raw sensory input in complicated 3D mazes, approaching human-level performance even under conditions where the goal location changes frequently. We provide detailed analysis of the agent behaviour1, its ability to localise, and its network activity dynamics, showing that the agent implicitly learns key navigation abilities.",
  "stance": 0.3
 },
 {
  "url": "https://openreview.net/forum?id=SJU4ayYgl",
  "title": "Semi-Supervised Classification with Graph Convolutional Networks",
  "year": 2017,
  "venue": "ICLR",
  "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=Sy8gdB9xx",
  "title": "Understanding deep learning requires rethinking generalization",
  "year": 2017,
  "venue": "ICLR",
  "abstract": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.",
  "stance": 0.0
 },
 {
  "url": "https://openreview.net/forum?id=r10FA8Kxg",
  "title": "Do Deep Convolutional Nets Really Need to be Deep and Convolutional?",
  "year": 2017,
  "venue": "ICLR",
  "abstract": "Yes, they do. This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with methods such as distillation that allow small or shallow models of high accuracy to be trained. Although previous research showed that shallow feed-forward nets sometimes can learn the complex functions previously learned by deep nets while using the same number of parameters as the deep models they mimic, in this paper we demonstrate that the same methods cannot be used to train accurate models on CIFAR-10 unless the student models contain multiple layers of convolution. Although the student models do not have to be as deep as the teacher model they mimic, the students need multiple convolutional layers to learn functions of comparable accuracy as the deep convolutional teacher.",
  "stance": 0.0
 },
 {
  "url": "https://openreview.net/forum?id=r1fYuytex",
  "title": "Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks",
  "year": 2017,
  "venue": "ICLR",
  "abstract": "Recently deep neural networks have received considerable attention due to their ability to extract and represent high-level abstractions in data sets. Deep neural networks such as fully-connected and convolutional neural networks have shown excellent performance on a wide range of recognition and classification tasks. However, their hardware implementations currently suffer from large silicon area and high power consumption due to the their high degree of complexity. The power/energy consumption of neural networks is dominated by memory accesses, the majority of which occur in fully-connected networks. In fact, they contain most of the deep neural network parameters. In this paper, we propose sparsely-connected networks, by showing that the number of connections in fullyconnected networks can be reduced by up to 90% while improving the accuracy performance on three popular datasets (MNIST, CIFAR10 and SVHN). We then propose an efficient hardware architecture based on linear-feedback shift registers to reduce the memory requirements of the proposed sparsely-connected networks. The proposed architecture can save up to 90% of memory compared to the conventional implementations of fully-connected neural networks. Moreover, implementation results show up to 84% reduction in the energy consumption of a single neuron of the proposed sparsely-connected networks compared to a single neuron of fully-connected neural networks.",
  "stance": 0.9
 },
 {
  "url": "https://openreview.net/forum?id=B18WgG-CZ",
  "title": "Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general purpose features for words across a range of NLP problems. However, extending this success to learning representations of sequences of words, such as sentences, remains an open problem. Recent work has explored unsupervised as well as supervised learning techniques with different training objectives to learn general purpose fixed-length sentence representations. In this work, we present a simple, effective multi-task learning framework for sentence representations that combines the inductive biases of diverse training objectives in a single model. We train this model on several data sources with multiple training objectives on over 100 million sentences. Extensive experiments demonstrate that sharing a single recurrent sentence encoder across weakly related tasks leads to consistent improvements over previous methods. We present substantial improvements in the context of transfer learning and low-resource settings using our learned general-purpose representations.",
  "stance": 0.6
 },
 {
  "url": "https://openreview.net/forum?id=BJ8c3f-0b",
  "title": "Auto-Encoding Sequential Monte Carlo",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "We build on auto-encoding sequential Monte Carlo (AESMC):1 a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models. Our approach relies on the efficiency of sequential Monte Carlo (SMC) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions. We develop additional theoretical insights and experiment with a new training procedure which can improve both model and proposal learning. We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.",
  "stance": 0.9
 },
 {
  "url": "https://openreview.net/forum?id=BJehNfW0-",
  "title": "Do GANs learn the distribution? Some Theory and Empirics",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "Do GANS (Generative Adversarial Nets) actually learn the target distribution? The foundational paper of Goodfellow et al. (2014) suggested they do, if they were given “sufficiently large” deep nets, sample size, and computation time. A recent theoretical analysis in Arora et al. (2017) raised doubts whether the same holds when discriminator has bounded size. It showed that the training objective can approach its optimum value even if the generated distribution has very low support —in other words, the training objective is unable to prevent mode collapse. The current paper makes two contributions. (1) It proposes a novel test for estimating support size using the birthday paradox of discrete probability. Using this evidence is presented that well-known GANs approaches do learn distributions of fairly low support. (2) It theoretically studies encoder-decoder GANs architectures (e.g., BiGAN/ALI), which were proposed to learn more meaningful features via GANs and (consequently) to also solve the mode-collapse issue. Our result shows that such encoder-decoder training objectives also cannot guarantee learning of the full distribution because they cannot prevent serious mode collapse. More seriously, they cannot prevent learning meaningless codes for data, contrary to usual intuition.",
  "stance": -0.3
 },
 {
  "url": "https://openreview.net/forum?id=ByJHuTgA-",
  "title": "On the State of the Art of Evaluation in Neural Language Models",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=ByQpn1ZA-",
  "title": "Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players’ parameters. One useful approach for the theory of GANs is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium. Several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence. We show that this view is overly restrictive. During GAN training, the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful. We provide empirical counterexamples to the view of GAN training as divergence minimization. Specifically, we demonstrate that GANs are able to learn distributions in situations where the divergence minimization point of view predicts they would fail. We also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful. This contributes to a growing body of evidence that GAN training may be more usefully viewed as approaching Nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step.",
  "stance": -1.0
 },
 {
  "url": "https://openreview.net/forum?id=BydLzGb0Z",
  "title": "Twin Networks: Matching the Future for Sequence Generation",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "We propose a simple technique for encouraging generative RNNs to plan ahead. We train a “backward” recurrent network to generate a given sequence in reverse order, and we encourage states of the forward model to predict cotemporal states of the backward model. The backward network is used only during training, and plays no role during sampling or inference. We hypothesize that our approach eases modeling of long-term dependencies by implicitly forcing the forward states to hold information about the longer-term future (as contained in the backward states). We show empirically that our approach achieves 9% relative improvement for a speech recognition task, and achieves significant improvement on a COCO caption generation task.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=BydjJte0-",
  "title": "Towards Reverse-Engineering Black-Box Neural Networks",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "Many deployed learned models are black boxes: given input, returns output. Internal information about the model, such as the architecture, optimisation procedure, or training data, is not disclosed explicitly as it might contain proprietary information or make the system more vulnerable. This work shows that such attributes of neural networks can be exposed from a sequence of queries. This has multiple implications. On the one hand, our work exposes the vulnerability of black-box neural networks to different types of attacks – we show that the revealed internal information helps generate more effective adversarial examples against the black box model. On the other hand, this technique can be used for better protection of private content from automatic recognition models using adversarial examples. Our paper suggests that it is actually hard to draw a line between white box and black box models. The code is available at goo.gl/MbYfsv.",
  "stance": -0.2
 },
 {
  "url": "https://openreview.net/forum?id=H1Dy---0Z",
  "title": "Distributed Prioritized Experience Replay",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "We propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible. The algorithm decouples acting from learning: the actors interact with their own instances of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared experience replay memory; the learner replays samples of experience and updates the neural network. The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors. Our architecture substantially improves the state of the art on the Arcade Learning Environment, achieving better final performance in a fraction of the wall-clock training time.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=H1MczcgR-",
  "title": "Understanding Short-Horizon Bias in Stochastic Meta-Optimization",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "Careful tuning of the learning rate, or even schedules thereof, can be crucial to effective neural net training. There has been much recent interest in gradient-based meta-optimization, where one tunes hyperparameters, or even learns an optimizer, in order to minimize the expected loss when the training procedure is unrolled. But because the training procedure must be unrolled thousands of times, the metaobjective must be defined with an orders-of-magnitude shorter time horizon than is typical for neural net training. We show that such short-horizon meta-objectives cause a serious bias towards small step sizes, an effect we term short-horizon bias. We introduce a toy problem, a noisy quadratic cost function, on which we analyze short-horizon bias by deriving and comparing the optimal schedules for short and long time horizons. We then run meta-optimization experiments (both offline and online) on standard benchmark datasets, showing that meta-optimization chooses too small a learning rate by multiple orders of magnitude, even when run with a moderately long time horizon (100 steps) typical of work in the area. We believe short-horizon bias is a fundamental problem that needs to be addressed if metaoptimization is to scale to practical neural net training regimes.",
  "stance": -1.0
 },
 {
  "url": "https://openreview.net/forum?id=HJGv1Z-AW",
  "title": "Emergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "The ability of algorithms to evolve or learn (compositional) communication protocols has traditionally been studied in the language evolution literature through the use of emergent communication tasks. Here we scale up this research by using contemporary deep learning methods and by training reinforcement-learning neural network agents on referential communication games. We extend previous work, in which agents were trained in symbolic environments, by developing agents which are able to learn from raw pixel data, a more challenging and realistic input representation. We find that the degree of structure found in the input data affects the nature of the emerged protocols, and thereby corroborate the hypothesis that structured compositional language is most likely to emerge when agents perceive the world as being structured.",
  "stance": 0.4
 },
 {
  "url": "https://openreview.net/forum?id=HJIoJWZCZ",
  "title": "Adversarial Dropout Regularization",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "We present a domain adaptation method for transferring neural representations from label-rich source domains to unlabeled target domains. Recent adversarial methods proposed for this task learn to align features across domains by “fooling” a special domain classifier network. However, a drawback of this approach is that the domain classifier simply labels the generated features as in-domain or not, without considering the boundaries between classes. This means that ambiguous target features can be generated near class boundaries, reducing target classification accuracy. We propose a novel approach, Adversarial Dropout Regularization (ADR), which encourages the generator to output more discriminative features for the target domain. Our key idea is to replace the traditional domain critic with a critic that detects non-discriminative features by using dropout on the classifier network. The generator then learns to avoid these areas of the feature space and thus creates better features. We apply our ADR approach to the problem of unsupervised domain adaptation for image classification and semantic segmentation tasks, and demonstrate significant improvements over the state of the art.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=HkAClQgA-",
  "title": "A Deep Reinforced Model for Abstractive Summarization",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intraattention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). Models trained only with supervised learning often exhibit “exposure bias” – they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=HkTEFfZRb",
  "title": "Attacking Binarized Neural Networks",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "Neural networks with low-precision weights and activations offer compelling efficiency advantages over their full-precision equivalents. The two most frequently discussed benefits of quantization are reduced memory consumption, and a faster forward pass when implemented with efficient bitwise operations. We propose a third benefit of very low-precision neural networks: improved robustness against some adversarial attacks, and in the worst case, performance that is on par with full-precision models. We focus on the very low-precision case where weights and activations are both quantized to ±1, and note that stochastically quantizing weights in just one layer can sharply reduce the impact of iterative attacks. We observe that non-scaled binary neural networks exhibit a similar effect to the original defensive distillation procedure that led to gradient masking, and a false notion of security. We address this by conducting both black-box and white-box experiments with binary models that do not artificially mask gradients.1",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=Hkc-TeZ0W",
  "title": "A Hierarchical Model for Device Placement",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "We introduce a hierarchical model for efficient placement of computational graphs onto hardware devices, especially in heterogeneous environments with a mixture of CPUs, GPUs, and other computational devices. Our method learns to assign graph operations to groups and to allocate those groups to available devices. The grouping and device allocations are learned jointly. The proposed method is trained with policy gradient and requires no human intervention. Experiments with widely-used computer vision and natural language models show that our algorithm can find optimized, non-trivial placements for TensorFlow computational graphs with over 80,000 operations. In addition, our approach outperforms placements by human experts as well as a previous state-of-the-art placement method based on deep reinforcement learning. Our method achieves runtime reductions of up to 60.6% per training step when applied to models such as Neural Machine Translation.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=Hy7fDog0b",
  "title": "AmbientGAN: Generative models from lossy measurements",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "Generative models provide a way to model structure in complex distributions and have been shown to be useful for many tasks of practical interest. However, current techniques for training generative models require access to fully-observed samples. In many settings, it is expensive or even impossible to obtain fullyobserved samples, but economical to obtain partial, noisy observations. We consider the task of learning an implicit generative model given only lossy measurements of samples from the distribution of interest. We show that the true underlying distribution can be provably recovered even in the presence of per-sample information loss for a class of measurement models. Based on this, we propose a new method of training Generative Adversarial Networks (GANs) which we call AmbientGAN. On three benchmark datasets, and for various measurement models, we demonstrate substantial qualitative and quantitative improvements. Generative models trained with our method can obtain 2-4x higher inception scores than the baselines.",
  "stance": 0.4
 },
 {
  "url": "https://openreview.net/forum?id=S1nQvfgA-",
  "title": "Semantically Decomposing the Latent Spaces of Generative Adversarial Networks",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "We propose a new algorithm for training generative adversarial networks that jointly learns latent codes for both identities (e.g. individual humans) and observations (e.g. specific photographs). By fixing the identity portion of the latent codes, we can generate diverse images of the same subject, and by fixing the observation portion, we can traverse the manifold of subjects while maintaining contingent aspects such as lighting and pose. Our algorithm features a pairwise training scheme in which each sample from the generator consists of two images with a common identity code. Corresponding samples from the real dataset consist of two distinct photographs of the same subject. In order to fool the discriminator, the generator must produce pairs that are photorealistic, distinct, and appear to depict the same individual. We augment both the DCGAN and BEGAN approaches with Siamese discriminators to facilitate pairwise training. Experiments with human judges and an off-the-shelf face verification system demonstrate our algorithm’s ability to generate convincing, identity-matched photographs.",
  "stance": 0.6
 },
 {
  "url": "https://openreview.net/forum?id=S1uxsye0Z",
  "title": "Adaptive Dropout with Rademacher Complexity Regularization",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation. However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space. In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices. Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power. Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm. We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns. Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of-the-art dropout algorithms.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=Sk7KsfW0-",
  "title": "Lifelong Learning with Dynamically Expandable Networks",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efficiently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets under lifelong learning scenarios, on which it not only significantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch counterparts with substantially fewer number of parameters. Further, the obtained network fine-tuned on all tasks obtained siginficantly better performance over the batch models, which shows that it can be used to estimate the optimal network structure even when all tasks are available in the first place.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=SyX0IeWAW",
  "title": "META LEARNING SHARED HIERARCHIES",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "We develop a metalearning approach for learning hierarchically structured policies, improving sample efficiency on unseen tasks through the use of shared primitives—policies that are executed for large numbers of timesteps. Specifically, a set of primitives are shared within a distribution of tasks, and are switched between by task-specific policies. We provide a concrete metric for measuring the strength of such hierarchies, leading to an optimization problem for quickly reaching high reward on unseen tasks. We then present an algorithm to solve this problem end-to-end through the use of any off-the-shelf reinforcement learning method, by repeatedly sampling new tasks and resetting task-specific policies. We successfully discover1 meaningful motor primitives for the directional movement of four-legged robots, solely by interacting with distributions of mazes. We also demonstrate the transferability of primitives to solve long-timescale sparse-reward obstacle courses, and we enable 3D humanoid robots to robustly walk and crawl with the same policy.",
  "stance": 0.7
 },
 {
  "url": "https://openreview.net/forum?id=SyYe6k-CW",
  "title": "Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "Recent advances in deep reinforcement learning have made significant strides in performance on applications such as Go and Atari games. However, developing practical methods to balance exploration and exploitation in complex domains remains largely unsolved. Thompson Sampling and its extension to reinforcement learning provide an elegant approach to exploration that only requires access to posterior samples of the model. At the same time, advances in approximate Bayesian methods have made posterior approximation for flexible neural network models practical. Thus, it is attractive to consider approximate Bayesian neural networks in a Thompson Sampling framework. To understand the impact of using an approximate posterior on Thompson Sampling, we benchmark well-established and recently developed methods for approximate posterior sampling combined with Thompson Sampling over a series of contextual bandit problems. We found that many approaches that have been successful in the supervised learning setting underperformed in the sequential decision-making scenario. In particular, we highlight the challenge of adapting slowly converging uncertainty estimates to the online setting.",
  "stance": -0.5
 },
 {
  "url": "https://openreview.net/forum?id=r1iuQjxCZ",
  "title": "On the importance of single directions for generalization",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "Despite their ability to memorize large datasets, deep neural networks often achieve good generalization performance. However, the differences between the learned solutions of networks which generalize and those which do not remain unclear. Additionally, the tuning properties of single directions (defined as the activation of a single unit or some linear combination of units in response to some input) have been highlighted, but their importance has not been evaluated. Here, we connect these lines of inquiry to demonstrate that a network’s reliance on single directions is a good predictor of its generalization performance, across networks trained on datasets with different fractions of corrupted labels, across ensembles of networks trained on datasets with unmodified labels, across different hyperparameters, and over the course of training. While dropout only regularizes this quantity up to a point, batch normalization implicitly discourages single direction reliance, in part by decreasing the class selectivity of individual units. Finally, we find that class selectivity is a poor predictor of task importance, suggesting not only that networks which generalize well minimize their dependence on individual units by reducing their selectivity, but also that individually selective units may not be necessary for strong network performance.",
  "stance": -0.5
 },
 {
  "url": "https://openreview.net/forum?id=rJvJXZb0W",
  "title": "An efficient framework for learning sentence representations",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. Given a sentence and the context in which it appears, a classifier distinguishes context sentences from other contrastive sentences based on their vector representations. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform stateof-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=rkHVZWZAZ",
  "title": "The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "In this work, we present a new agent architecture, called Reactor, which combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN (Wang et al., 2017) and Categorical DQN (Bellemare et al., 2017), while giving better run-time performance than A3C (Mnih et al., 2016). Our first contribution is a new policy evaluation algorithm called Distributional Retrace, which brings multi-step off-policy updates to the distributional reinforcement learning setting. The same approach can be used to convert several classes of multi-step policy evaluation algorithms, designed for expected value evaluation, into distributional algorithms. Next, we introduce the β-leave-one-out policy gradient algorithm, which improves the trade-off between variance and bias by using action values as a baseline. Our final algorithmic contribution is a new prioritized replay algorithm for sequences, which exploits the temporal locality of neighboring observations for more efficient replay prioritization. Using the Atari 2600 benchmarks, we show that each of these innovations contribute to both sample efficiency and final agent performance. Finally, we demonstrate that Reactor reaches state-of-the-art performance after 200 million frames and less than a day of training.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=rkgOLb-0W",
  "title": "Neural Language Modeling by Jointly Learning Syntax and Lexicon",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "We propose a neural language model capable of unsupervised syntactic structure induction. The model leverages the structure information to form better semantic representations and better language modeling. Standard recurrent neural networks are limited by their structure and fail to efficiently use syntactic information. On the other hand, tree-structured recursive networks usually require additional structural supervision at the cost of human expert annotation. In this paper, We propose a novel neural language model, called the Parsing-Reading-Predict Networks (PRPN), that can simultaneously induce the syntactic structure from unannotated sentences and leverage the inferred structure to learn a better language model. In our model, the gradient can be directly back-propagated from the language model loss into the neural parsing network. Experiments show that the proposed model can discover the underlying syntactic structure and achieve state-of-the-art performance on word/character-level language model tasks.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=rkhlb8lCZ",
  "title": "Wavelet Pooling for Convolutional Neural Networks",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "Convolutional Neural Networks continuously advance the progress of 2D and 3D image and object classification. The steadfast usage of this algorithm requires constant evaluation and upgrading of foundational concepts to maintain progress. Network regularization techniques typically focus on convolutional layer operations, while leaving pooling layer operations without suitable options. We introduce Wavelet Pooling as another alternative to traditional neighborhood pooling. This method decomposes features into a second level decomposition, and discards the first-level subbands to reduce feature dimensions. This method addresses the overfitting problem encountered by max pooling, while reducing features in a more structurally compact manner than pooling via neighborhood regions. Experimental results on four benchmark classification datasets demonstrate our proposed method outperforms or performs comparatively with methods like max, mean, mixed, and stochastic pooling.",
  "stance": 0.7
 },
 {
  "url": "https://openreview.net/forum?id=ryQu7f-RZ",
  "title": "On the Convergence of Adam and Beyond",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSPROP, ADAM, ADADELTA, NADAM are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where ADAM does not converge to the optimal solution, and describe the precise problems with the previous analysis of ADAM algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with “long-term memory” of past gradients, and propose new variants of the ADAM algorithm which not only fix the convergence issues but often also lead to improved empirical performance.",
  "stance": 0.8
 },
 {
  "url": "https://openreview.net/forum?id=B1fpDsAqt7",
  "title": "Visual Reasoning by Progressive Module Networks",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "Humans learn to solve tasks of increasing complexity by building on top of previously acquired knowledge. Typically, there exists a natural progression in the tasks that we learn – most do not require completely independent solutions, but can be broken down into simpler subtasks. We propose to represent a solver for each task as a neural module that calls existing modules (solvers for simpler tasks) in a functional program-like manner. Lower modules are a black box to the calling module, and communicate only via a query and an output. Thus, a module for a new task learns to query existing modules and composes their outputs in order to produce its own output. Our model effectively combines previous skill-sets, does not suffer from forgetting, and is fully differentiable. We test our model in learning a set of visual reasoning tasks, and demonstrate improved performances in all tasks by learning progressively. By evaluating the reasoning process using human judges, we show that our model is more interpretable than an attention-based baseline.",
  "stance": 0.7
 },
 {
  "url": "https://openreview.net/forum?id=B1gabhRcYX",
  "title": "BA-Net: Dense Bundle Adjustment Networks",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "This paper introduces a network architecture to solve the structure-from-motion (SfM) problem via feature-metric bundle adjustment (BA), which explicitly enforces multi-view geometry constraints in the form of feature-metric error. The whole pipeline is differentiable, so that the network can learn suitable features that make the BA problem more tractable. Furthermore, this work introduces a novel depth parameterization to recover dense per-pixel depth. The network first generates several basis depth maps according to the input image, and optimizes the final depth as a linear combination of these basis depth maps via feature-metric BA. The basis depth maps generator is also learned via end-to-end training. The whole system nicely combines domain knowledge (i.e. hard-coded multi-view geometry constraints) and deep learning (i.e. feature learning and basis depth maps learning) to address the challenging dense SfM problem. Experiments on large scale real data prove the success of the proposed method.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=BkeStsCcKQ",
  "title": "Critical Learning Periods in Deep Networks",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "Similar to humans and animals, deep artificial neural networks exhibit critical periods during which a temporary stimulus deficit can impair the development of a skill. The extent of the impairment depends on the onset and length of the deficit window, as in animal models, and on the size of the neural network. Deficits that do not affect low-level statistics, such as vertical flipping of the images, have no lasting effect on performance and can be overcome with further training. To better understand this phenomenon, we use the Fisher Information of the weights to measure the effective connectivity between layers of a network during training. Counterintuitively, information rises rapidly in the early phases of training, and then decreases, preventing redistribution of information resources in a phenomenon we refer to as a loss of “Information Plasticity”. Our analysis suggests that the first few epochs are critical for the creation of strong connections that are optimal relative to the input data distribution. Once such strong connections are created, they do not appear to change during additional training. These findings suggest that the initial learning transient, under-scrutinized compared to asymptotic behavior, plays a key role in determining the outcome of the training process. Our findings, combined with recent theoretical results in the literature, also suggest that forgetting (decrease of information in the weights) is critical to achieving invariance and disentanglement in representation learning. Finally, critical periods are not restricted to biological systems, but can emerge naturally in learning systems, whether biological or artificial, due to fundamental constrains arising from learning dynamics and information processing.",
  "stance": 0.3
 },
 {
  "url": "https://openreview.net/forum?id=Bkl-43C9FQ",
  "title": "Spherical CNNs on Unstructured Grids",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "We present an efficient convolution kernel for Convolutional Neural Networks (CNNs) on unstructured grids using parameterized differential operators while focusing on spherical signals such as panorama images or planetary signals. To this end, we replace conventional convolution kernels with linear combinations of differential operators that are weighted by learnable parameters. Differential operators can be efficiently estimated on unstructured grids using one-ring neighbors, and learnable parameters can be optimized through standard back-propagation. As a result, we obtain extremely efficient neural networks that match or outperform state-of-the-art network architectures in terms of performance but with a significantly smaller number of network parameters. We evaluate our algorithm in an extensive series of experiments on a variety of computer vision and climate science tasks, including shape classification, climate pattern segmentation, and omnidirectional image semantic segmentation. Overall, we (1) present a novel CNN approach on unstructured grids using parameterized differential operators for spherical signals, and (2) show that our unique kernel parameterization allows our model to achieve the same or higher accuracy with significantly fewer network parameters.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=H1x-x309tm",
  "title": "On the Convergence of A Class of Adam-Type Algorithms for Non-Convex Optimization",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "This paper studies a class of adaptive gradient based momentum algorithms that update the search directions and learning rates simultaneously using past gradients. This class, which we refer to as the “Adam-type”, includes the popular algorithms such as Adam (Kingma & Ba, 2014) , AMSGrad (Reddi et al., 2018) , AdaGrad (Duchi et al., 2011). Despite their popularity in training deep neural networks (DNNs), the convergence of these algorithms for solving non-convex problems remains an open question. In this paper, we develop an analysis framework and a set of mild sufficient conditions that guarantee the convergence of the Adam-type methods, with a convergence rate of order O(log T/ √ T ) for non-convex stochastic optimization. Our convergence analysis applies to a new algorithm called AdaFom (AdaGrad with First Order Momentum). We show that the conditions are essential, by identifying concrete examples in which violating the conditions makes an algorithm diverge. Besides providing one of the first comprehensive analysis for Adam-type methods in the non-convex setting, our results can also help the practitioners to easily monitor the progress of algorithms and determine their convergence behavior.",
  "stance": 0.5
 },
 {
  "url": "https://openreview.net/forum?id=H1xwNhCcYm",
  "title": "Do Deep Generative Models Know What They Don't Know?",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data. A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong. Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel, out-of-distribution inputs. In this paper we challenge this assumption. We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former. Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN. To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood. We find such behavior persists even when we restrict the flows to constant-volume transformations. These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.",
  "stance": -0.8
 },
 {
  "url": "https://openreview.net/forum?id=Hk4fpoA5Km",
  "title": "Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "Algorithms for imitation learning based on adversarial optimization, such as generative adversarial imitation learning (GAIL) and adversarial inverse reinforcement learning (AIRL), can effectively mimic demonstrated behaviours by employing both reward and reinforcement learning (RL). However, applications of such algorithms are challenged by the inherent instability and poor sample efficiency of on-policy RL. In particular, the inadequate handling of absorbing states in canonical implementations of RL environments causes an implicit bias in reward functions used by these algorithms. While these biases might work well for some environments, they lead to sub-optimal behaviors in others. Moreover, despite the ability of these algorithms to learn from a few demonstrations, they require a prohibitively large number of the environment interactions for many real-world applications. To address these issues, we first propose to extend the environment MDP with absorbing states which leads to task-independent, and more importantly, unbiased rewards. Secondly, we introduce an off-policy learning algorithm, which we refer to as Discriminator-Actor-Critic. We demonstrate the effectiveness of proper handling of absorbing states, while empirically improving the sample efficiency by an average factor of 10. Our implementation is available online 1.",
  "stance": 0.5
 },
 {
  "url": "https://openreview.net/forum?id=HkezXnA9YX",
  "title": "Systematic Generalization: What Is Required and Can It Be Learned?",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "Numerous models for grounded language understanding have been recently proposed, including (i) generic models that can be easily adapted to any given task and (ii) intuitively appealing modular models that require background knowledge to be instantiated. We compare both types of models in how much they lend themselves to a particular form of systematic generalization. Using a synthetic VQA test, we evaluate which models are capable of reasoning about all possible object pairs after training on only a small subset of them. Our findings show that the generalization of modular models is much more systematic and that it is highly sensitive to the module layout, i.e. to how exactly the modules are connected. We furthermore investigate if modular models that generalize well could be made more end-to-end by learning their layout and parametrization. We find that endto-end methods from prior work often learn inappropriate layouts or parametrizations that do not facilitate systematic generalization. Our results suggest that, in addition to modularity, systematic generalization in language understanding may require explicit regularizers or priors.",
  "stance": -1.0
 },
 {
  "url": "https://openreview.net/forum?id=HkfPSh05K7",
  "title": "Multi-step Retriever-Reader Interaction for Scalable Open-domain Question Answering",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "This paper introduces a new framework for open-domain question answering in which the retriever and the reader iteratively interact with each other. The framework is agnostic to the architecture of the machine reading model, only requiring access to the token-level hidden representations of the reader. The retriever uses fast nearest neighbor search to scale to corpora containing millions of paragraphs. A gated recurrent unit updates the query at each step conditioned on the state of the reader and the reformulated query is used to re-rank the paragraphs by the retriever. We conduct analysis and show that iterative interaction helps in retrieving informative paragraphs from the corpus. Finally, we show that our multistep-reasoning framework brings consistent improvement when applied to two widely used reader architectures (DR.QA and BIDAF) on various large open-domain datasets — TRIVIAQA-unfiltered, QUASAR-T, SEARCHQA, and SQUAD-open1.",
  "stance": 0.7
 },
 {
  "url": "https://openreview.net/forum?id=HkgEQnRqYQ",
  "title": "RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links. The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations. In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition. Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space. In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model. Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=HkxKH2AcFm",
  "title": "Towards GAN Benchmarks Which Require Generalization",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "For many evaluation metrics commonly used as benchmarks for unconditional image generation, trivially memorizing the training set attains a better score than models which are considered state-of-the-art; we consider this problematic. We clarify a necessary condition for an evaluation metric not to behave this way: estimating the function must require a large sample from the model. In search of such a metric, we turn to neural network divergences (NNDs), which are defined in terms of a neural network trained to distinguish between distributions. The resulting benchmarks cannot be “won” by training set memorization, while still being perceptually correlated and computable only from samples. We survey past work on using NNDs for evaluation, implement an example black-box metric based on these ideas, and validate experimentally that it can measure a notion of generalization.",
  "stance": 0.4
 },
 {
  "url": "https://openreview.net/forum?id=HkxaFoC9KQ",
  "title": "Deep reinforcement learning with relational inductive biases",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "We introduce an approach for augmenting model-free deep reinforcement learning agents with a mechanism for relational reasoning over structured representations, which improves performance, learning efficiency, generalization, and interpretability. Our architecture encodes an image as a set of vectors, and applies an iterative message-passing procedure to discover and reason about relevant entities and relations in a scene. In six of seven StarCraft II Learning Environment mini-games, our agent achieved state-of-the-art performance, and surpassed human grandmasterlevel on four. In a novel navigation and planning task, our agent’s performance and learning efficiency far exceeded non-relational baselines, it was able to generalize to more complex scenes than it had experienced during training. Moreover, when we examined its learned internal representations, they reflected important structure about the problem and the agent’s intentions. The main contribution of this work is to introduce techniques for representing and reasoning about states in model-free deep reinforcement learning agents via relational inductive biases. Our experiments show this approach can offer advantages in efficiency, generalization, and interpretability, and can scale up to meet some of the most challenging test environments in modern artificial intelligence.",
  "stance": 0.7
 },
 {
  "url": "https://openreview.net/forum?id=HyEtjoCqFX",
  "title": "Soft Q-Learning with Mutual-Information Regularization",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "We propose a reinforcement learning (RL) algorithm that uses mutual-information regularization to optimize a prior action distribution for better performance and exploration. Entropy-based regularization has previously been shown to improve both exploration and robustness in challenging sequential decision-making tasks. It does so by encouraging policies to put probability mass on all actions. However, entropy regularization might be undesirable when actions have significantly different importance. In this paper, we propose a theoretically motivated framework that dynamically weights the importance of actions by using the mutualinformation. In particular, we express the RL problem as an inference problem where the prior probability distribution over actions is subject to optimization. We show that the prior optimization introduces a mutual-information regularizer in the RL objective. This regularizer encourages the policy to be close to a nonuniform distribution that assigns higher probability mass to more important actions. We empirically demonstrate that our method significantly improves over entropy regularization methods and unregularized methods.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=HyeGBj09Fm",
  "title": "Generating Liquid Simulations with Deformation-aware Neural Networks",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "We propose a novel approach for deformation-aware neural networks that learn the weighting and synthesis of dense volumetric deformation fields. Our method specifically targets the space-time representation of physical surfaces from liquid simulations. Liquids exhibit highly complex, non-linear behavior under changing simulation conditions such as different initial conditions. Our algorithm captures these complex phenomena in two stages: a first neural network computes a weighting function for a set of pre-computed deformations, while a second network directly generates a deformation field for refining the surface. Key for successful training runs in this setting is a suitable loss function that encodes the effect of the deformations, and a robust calculation of the corresponding gradients. To demonstrate the effectiveness of our approach, we showcase our method with several complex examples of flowing liquids with topology changes. Our representation makes it possible to rapidly generate the desired implicit surfaces. We have implemented a mobile application to demonstrate that real-time interactions with complex liquid effects are possible with our approach.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=Hyfn2jCcKm",
  "title": "Solving the Rubik's Cube with Approximate Policy Iteration",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "Recently, Approximate Policy Iteration (API) algorithms have achieved superhuman proficiency in two-player zero-sum games such as Go, Chess, and Shogi without human data. These API algorithms iterate between two policies: a slow policy (tree search), and a fast policy (a neural network). In these two-player games, a reward is always received at the end of the game. However, the Rubik’s Cube has only a single solved state, and episodes are not guaranteed to terminate. This poses a major problem for these API algorithms since they rely on the reward received at the end of the game. We introduce Autodidactic Iteration: an API algorithm that overcomes the problem of sparse rewards by training on a distribution of states that allows the reward to propagate from the goal state to states farther away. Autodidactic Iteration is able to learn how to solve the Rubik’s Cube without relying on human data. Our algorithm is able to solve 100% of randomly scrambled cubes while achieving a median solve length of 30 moves — less than or equal to solvers that employ human domain knowledge.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=Hygxb2CqKm",
  "title": "Stable Recurrent Models",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models.",
  "stance": 0.4
 },
 {
  "url": "https://openreview.net/forum?id=HylTBhA5tQ",
  "title": "The Limitations of Adversarial Training and the Blind-Spot Attack",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "The adversarial training procedure proposed by Madry et al. (2018) is one of the most effective methods to defend against adversarial examples in deep neural networks (DNNs). In our paper, we shed some lights on the practicality and the hardness of adversarial training by showing that the effectiveness (robustness on test set) of adversarial training has a strong correlation with the distance between a test point and the manifold of training data embedded by the network. Test examples that are relatively far away from this manifold are more likely to be vulnerable to adversarial attacks. Consequentially, an adversarial training based defense is susceptible to a new class of attacks, the “blind-spot attack”, where the input images reside in “blind-spots” (low density regions) of the empirical distribution of training data but is still on the ground-truth data manifold. For MNIST, we found that these blind-spots can be easily found by simply scaling and shifting image pixel values. Most importantly, for large datasets with high dimensional and complex data manifold (CIFAR, ImageNet, etc), the existence of blind-spots in adversarial training makes defending on any valid test examples difficult due to the curse of dimensionality and the scarcity of training data. Additionally, we find that blind-spots also exist on provable defenses including (Kolter & Wong, 2018) and (Sinha et al., 2018) because these trainable robustness certificates can only be practically optimized on a limited set of training data.",
  "stance": -0.5
 },
 {
  "url": "https://openreview.net/forum?id=S1gUsoR9YX",
  "title": "Multilingual Neural Machine Translation with Knowledge Distillation",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "Multilingual machine translation, which translates multiple languages with a single model, has attracted much attention due to its efficiency of offline training and online serving. However, traditional multilingual translation usually yields inferior accuracy compared with the counterpart using individual models for each language pair, due to language diversity and model capacity limitations. In this paper, we propose a distillation-based approach to boost the accuracy of multilingual machine translation. Specifically, individual models are first trained and regarded as teachers, and then the multilingual model is trained to fit the training data and match the outputs of individual models simultaneously through knowledge distillation. Experiments on IWSLT, WMT and Ted talk translation datasets demonstrate the effectiveness of our method. Particularly, we show that one model is enough to handle multiple languages (up to 44 languages in our experiment), with comparable or even better accuracy than individual models.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=S1x4ghC9tQ",
  "title": "Temporal Difference Variational Auto-Encoder",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "To act and plan in complex environments, we posit that agents should have a mental simulator of the world with three characteristics: (a) it should build an abstract state representing the condition of the world; (b) it should form a belief which represents uncertainty on the world; (c) it should go beyond simple step-by-step simulation, and exhibit temporal abstraction. Motivated by the absence of a model satisfying all these requirements, we propose TD-VAE, a generative sequence model that learns representations containing explicit beliefs about states several steps into the future, and that can be rolled out directly without single-step transitions. TD-VAE is trained on pairs of temporally separated time points, using an analogue of temporal difference learning used in reinforcement learning.",
  "stance": 0.3
 },
 {
  "url": "https://openreview.net/forum?id=S1xNEhR9KX",
  "title": "On the Sensitivity of Adversarial Robustness to Input Data Distributions",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "Neural networks are vulnerable to small adversarial perturbations. Existing literature largely focused on understanding and mitigating the vulnerability of learned models. In this paper, we demonstrate an intriguing phenomenon about the most popular robust training method in the literature, adversarial training: Adversarial robustness, unlike clean accuracy, is sensitive to the input data distribution. Even a semantics-preserving transformations on the input data distribution can cause a significantly different robustness for the adversarial trained model that is both trained and evaluated on the new distribution. Our discovery of such sensitivity on data distribution is based on a study which disentangles the behaviors of clean accuracy and robust accuracy of the Bayes classifier. Empirical investigations further confirm our finding. We construct semantically-identical variants for MNIST and CIFAR10 respectively, and show that standardly trained models achieve comparable clean accuracies on them, but adversarially trained models achieve significantly different robustness accuracies. This counter-intuitive phenomenon indicates that input data distribution alone can affect the adversarial robustness of trained neural networks, not necessarily the tasks themselves. Lastly, we discuss the practical implications on evaluating adversarial robustness, and make initial attempts to understand this complex phenomenon.",
  "stance": -0.5
 },
 {
  "url": "https://openreview.net/forum?id=S1xNb2A9YX",
  "title": "Minimal Images in Deep Neural Networks: Fragile Object Recognition in Natural Images",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "The human ability to recognize objects is impaired when the object is not shown in full. \"Minimal images\" are the smallest regions of an image that remain recognizable for humans. Ullman et al. (2016) show that a slight modification of the location and size of the visible region of the minimal image produces a sharp drop in human recognition accuracy. In this paper, we demonstrate that such drops in accuracy due to changes of the visible region are a common phenomenon between humans and existing state-of-the-art deep neural networks (DNNs), and are much more prominent in DNNs. We found many cases where DNNs classified one region correctly and the other incorrectly, though they only differed by one row or column of pixels, and were often bigger than the average human minimal image size. We show that this phenomenon is independent from previous works that have reported lack of invariance to minor modifications in object location in DNNs. Our results thus reveal a new failure mode of DNNs that also affects humans to a much lesser degree. They expose how fragile DNN recognition ability is for natural images even without adversarial patterns being introduced. Bringing the robustness of DNNs in natural images to the human level remains an open challenge for the community.",
  "stance": -0.8
 },
 {
  "url": "https://openreview.net/forum?id=SJggZnRcFQ",
  "title": "Learning Programmatically Structured Representations with Perceptor Gradients",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "We present the perceptor gradients algorithm – a novel approach to learning symbolic representations based on the idea of decomposing an agent’s policy into i) a perceptor network extracting symbols from raw observation data and ii) a task encoding program which maps the input symbols to output actions. We show that the proposed algorithm is able to learn representations that can be directly fed into a Linear-Quadratic Regulator (LQR) or a general purpose A* planner. Our experimental results confirm that the perceptor gradients algorithm is able to efficiently learn transferable symbolic representations as well as generate new observations according to a semantically meaningful specification.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=SkgEaj05t7",
  "title": "On the Relation Between the Sharpest Directions of DNN Loss and the SGD Step Length",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "Stochastic Gradient Descent (SGD) based training of neural networks with a large learning rate or a small batch-size typically ends in well-generalizing, flat regions of the weight space, as indicated by small eigenvalues of the Hessian of the training loss. However, the curvature along the SGD trajectory is poorly understood. An empirical investigation shows that initially SGD visits increasingly sharp regions, reaching a maximum sharpness determined by both the learning rate and the batch-size of SGD. When studying the SGD dynamics in relation to the sharpest directions in this initial phase, we find that the SGD step is large compared to the curvature and commonly fails to minimize the loss along the sharpest directions. Furthermore, using a reduced learning rate along these directions can improve training speed while leading to both sharper and better generalizing solutions compared to vanilla SGD. In summary, our analysis of the dynamics of SGD in the subspace of the sharpest directions shows that they influence the regions that SGD steers to (where larger learning rate or smaller batch size result in wider regions visited), the overall training speed, and the generalization ability of the final model.",
  "stance": 0.3
 },
 {
  "url": "https://openreview.net/forum?id=SyxAb30cY7",
  "title": "Robustness May Be at Odds with Accuracy",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "We show that there exists an inherent tension between the goal of adversarial robustness and that of standard generalization. Specifically, training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. We demonstrate that this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists even in a fairly simple and natural setting. These findings also corroborate a similar phenomenon observed in practice. Further, we argue that this phenomenon is a consequence of robust classifiers learning fundamentally different feature representations than standard classifiers. These differences, in particular, seem to result in unexpected benefits: the features learned by robust models tend to align better with salient data characteristics and human perception.",
  "stance": 0.0
 },
 {
  "url": "https://openreview.net/forum?id=r14EOsCqKX",
  "title": "A Closer Look at Deep Learning Heuristics: Learning rate restarts, Warmup and Distillation",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "The convergence rate and final performance of common deep learning models have significantly benefited from heuristics such as learning rate schedules, knowledge distillation, skip connections, and normalization layers. In the absence of theoretical underpinnings, controlled experiments aimed at explaining these strategies can aid our understanding of deep learning landscapes and the training dynamics. Existing approaches for empirical analysis rely on tools of linear interpolation and visualizations with dimensionality reduction, each with their limitations. Instead, we revisit such analysis of heuristics through the lens of recently proposed methods for loss surface and representation analysis, viz., mode connectivity and canonical correlation analysis (CCA), and hypothesize reasons for the success of the heuristics. In particular, we explore knowledge distillation and learning rate heuristics of (cosine) restarts and warmup using mode connectivity and CCA. Our empirical analysis suggests that: (a) the reasons often quoted for the success of cosine annealing are not evidenced in practice; (b) that the effect of learning rate warmup is to prevent the deeper layers from creating training instability; and (c) that the latent knowledge shared by the teacher is primarily disbursed to the deeper layers.",
  "stance": 0.4
 },
 {
  "url": "https://openreview.net/forum?id=r1efr3C9Ym",
  "title": "Interpolation-Prediction Networks for Irregularly Sampled Time Series",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "In this paper, we present a new deep learning architecture for addressing the problem of supervised learning with sparse and irregularly sampled multivariate time series. The architecture is based on the use of a semi-parametric interpolation network followed by the application of a prediction network. The interpolation network allows for information to be shared across multiple dimensions of a multivariate time series during the interpolation stage, while any standard deep learning model can be used for the prediction network. This work is motivated by the analysis of physiological time series data in electronic health records, which are sparse, irregularly sampled, and multivariate. We investigate the performance of this architecture on both classification and regression tasks, showing that our approach outperforms a range of baseline and recently proposed models.1",
  "stance": 0.8
 },
 {
  "url": "https://openreview.net/forum?id=r1lWUoA9FQ",
  "title": "Are adversarial examples inevitable?",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "A wide range of defenses have been proposed to harden neural networks against adversarial attacks. However, a pattern has emerged in which the majority of adversarial defenses are quickly broken by new attacks. Given the lack of success at generating robust defenses, we are led to ask a fundamental question: Are adversarial attacks inevitable? This paper analyzes adversarial examples from a theoretical perspective, and identifies fundamental bounds on the susceptibility of a classifier to adversarial attacks. We show that, for certain classes of problems, adversarial examples are inescapable. Using experiments, we explore the implications of theoretical guarantees for real-world problems and discuss how factors such as dimensionality and image complexity limit a classifier’s robustness against adversarial examples.",
  "stance": -1.0
 },
 {
  "url": "https://openreview.net/forum?id=r1lYRjC9F7",
  "title": "Enabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "Generating musical audio directly with neural networks is notoriously difficult because it requires coherently modeling structure at many different timescales. Fortunately, most music is also highly structured and can be represented as discrete note events played on musical instruments. Herein, we show that by using notes as an intermediate representation, we can train a suite of models capable of transcribing, composing, and synthesizing audio waveforms with coherent musical structure on timescales spanning six orders of magnitude (∼0.1 ms to ∼100 s), a process we call Wave2Midi2Wave. This large advance in the state of the art is enabled by our release of the new MAESTRO (MIDI and Audio Edited for Synchronous TRacks and Organization) dataset, composed of over 172 hours of virtuosic piano performances captured with fine alignment (≈3 ms) between note labels and audio waveforms. The networks and the dataset together present a promising approach toward creating new expressive and interpretable neural models of music.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=r1x4BnCqKX",
  "title": "A Generative Model For Electron Paths",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "Chemical reactions can be described as the stepwise redistribution of electrons in molecules. As such, reactions are often depicted using “arrow-pushing” diagrams which show this movement as a sequence of arrows. We propose an electron path prediction model (ELECTRO) to learn these sequences directly from raw reaction data. Instead of predicting product molecules directly from reactant molecules in one shot, learning a model of electron movement has the benefits of (a) being easy for chemists to interpret, (b) incorporating constraints of chemistry, such as balanced atom counts before and after the reaction, and (c) naturally encoding the sparsity of chemical reactions, which usually involve changes in only a small number of atoms in the reactants. We design a method to extract approximate reaction paths from any dataset of atom-mapped reaction SMILES strings. Our model achieves excellent performance on an important subset of the USPTO reaction dataset, comparing favorably to the strongest baselines. Furthermore, we show that our model recovers a basic knowledge of chemistry without being explicitly trained to do so.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=rJe10iC5K7",
  "title": "Unsupervised Discovery of Parts, Structure, and Dynamics",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "Humans easily recognize object parts and their hierarchical structure by watching how they move; they can then predict how each part moves in the future. In this paper, we propose a novel formulation that simultaneously learns a hierarchical, disentangled object representation and a dynamics model for object parts from unlabeled videos. Our Parts, Structure, and Dynamics (PSD) model learns to, first, recognize the object parts via a layered image representation; second, predict hierarchy via a structural descriptor that composes low-level concepts into a hierarchical structure; and third, model the system dynamics by predicting the future. Experiments on multiple real and synthetic datasets demonstrate that our PSD model works well on all three tasks: segmenting object parts, building their hierarchical structure, and capturing their motion distributions.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=rJfW5oA5KQ",
  "title": "Approximability of Discriminators Implies Diversity in GANs",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "While Generative Adversarial Networks (GANs) have empirically produced impressive results on learning complex real-world distributions, recent works have shown that they suffer from lack of diversity or mode collapse. The theoretical work of Arora et al. (2017a) suggests a dilemma about GANs’ statistical properties: powerful discriminators cause overfitting, whereas weak discriminators cannot detect mode collapse. By contrast, we show in this paper that GANs can in principle learn distributions in Wasserstein distance (or KL-divergence in many cases) with polynomial sample complexity, if the discriminator class has strong distinguishing power against the particular generator class (instead of against all possible generators). For various generator classes such as mixture of Gaussians, exponential families, and invertible and injective neural networks generators, we design corresponding discriminators (which are often neural nets of specific architectures) such that the Integral Probability Metric (IPM) induced by the discriminators can provably approximate the Wasserstein distance and/or KL-divergence. This implies that if the training is successful, then the learned distribution is close to the true distribution in Wasserstein distance or KL divergence, and thus cannot drop modes. Our preliminary experiments show that on synthetic datasets the test IPM is well correlated with KL divergence or the Wasserstein distance, indicating that the lack of diversity in GANs may be caused by the sub-optimality in optimization instead of statistical inefficiency.",
  "stance": 0.0
 },
 {
  "url": "https://openreview.net/forum?id=rJlnB3C5Ym",
  "title": "Rethinking the Value of Network Pruning",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "Network pruning is widely used for reducing the heavy inference cost of deep models in low-resource settings. A typical pruning algorithm is a three-stage pipeline, i.e., training (a large model), pruning and fine-tuning. During pruning, according to a certain criterion, redundant weights are pruned and important weights are kept to best preserve the accuracy. In this work, we make several surprising observations which contradict common beliefs. For all state-of-the-art structured pruning algorithms we examined, fine-tuning a pruned model only gives comparable or worse performance than training that model with randomly initialized weights. For pruning algorithms which assume a predefined target network architecture, one can get rid of the full pipeline and directly train the target network from scratch. Our observations are consistent for multiple network architectures, datasets, and tasks, which imply that: 1) training a large, over-parameterized model is often not necessary to obtain an efficient final model, 2) learned “important” weights of the large model are typically not useful for the small pruned model, 3) the pruned architecture itself, rather than a set of inherited “important” weights, is more crucial to the efficiency in the final model, which suggests that in some cases pruning can be useful as an architecture search paradigm. Our results suggest the need for more careful baseline evaluations in future research on structured pruning methods. We also compare with the “Lottery Ticket Hypothesis” (Frankle & Carbin, 2019), and find that with optimal learning rate, the “winning ticket” initialization as used in Frankle & Carbin (2019) does not bring improvement over random initialization.",
  "stance": -0.5
 },
 {
  "url": "https://openreview.net/forum?id=rkMW1hRqKX",
  "title": "Optimal Completion Distillation for Sequence Learning",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance. OCD is efficient, has no hyper-parameters of its own, and does not require pretraining or joint optimization with conditional log-likelihood. Given a partial sequence generated by the model, we first identify the set of optimal suffixes that minimize the total edit distance, using an efficient dynamic programming algorithm. Then, for each position of the generated sequence, we define a target distribution that puts an equal probability on the first token of each optimal suffix. OCD achieves the state-of-theart performance on end-to-end speech recognition, on both Wall Street Journal and Librispeech datasets, achieving 9.3% and 4.5% word error rates, respectively.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=rkxw-hAcFQ",
  "title": "Generating Multi-Agent Trajectories using Programmatic Weak Supervision",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "We study the problem of training sequential generative models for capturing coordinated multi-agent trajectory behavior, such as offensive basketball gameplay. When modeling such settings, it is often beneficial to design hierarchical models that can capture long-term coordination using intermediate variables. Furthermore, these intermediate variables should capture interesting high-level behavioral semantics in an interpretable and manipulatable way. We present a hierarchical framework that can effectively learn such sequential generative models. Our approach is inspired by recent work on leveraging programmatically produced weak labels, which we extend to the spatiotemporal regime. In addition to synthetic settings, we show how to instantiate our framework to effectively model complex interactions between basketball players and generate realistic multi-agent trajectories of basketball gameplay over long time periods. We validate our approach using both quantitative and qualitative evaluations, including a user study comparison conducted with professional sports analysts.1",
  "stance": 0.5
 },
 {
  "url": "https://openreview.net/forum?id=ryfMLoCqtQ",
  "title": "An analytic theory of generalization dynamics and transfer learning in deep linear networks",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "Much attention has been devoted recently to the generalization puzzle in deep learning: large, deep networks can generalize well, but existing theories bounding generalization error are exceedingly loose, and thus cannot explain this striking performance. Furthermore, a major hope is that knowledge may transfer across tasks, so that multi-task learning can improve generalization on individual tasks. However we lack analytic theories that can quantitatively predict how the degree of knowledge transfer depends on the relationship between the tasks. We develop an analytic theory of the nonlinear dynamics of generalization in deep linear networks, both within and across tasks. In particular, our theory provides analytic solutions to the training and testing error of deep networks as a function of training time, number of examples, network size and initialization, and the task structure and SNR. Our theory reveals that deep networks progressively learn the most important task structure first, so that generalization error at the early stopping time primarily depends on task structure and is independent of network size. This suggests any tight bound on generalization error must take into account task structure, and explains observations about real data being learned faster than random data. Intriguingly our theory also reveals the existence of a learning algorithm that proveably out-performs neural network training through gradient descent. Finally, for transfer learning, our theory reveals that knowledge transfer depends sensitively, but computably, on the SNRs and input feature alignments of pairs of tasks.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=B1elCp4KwH",
  "title": "Learning Hierarchical Discrete Linguistic Units from Visually-Grounded Speech",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "In this paper, we present a method for learning discrete linguistic units by incorporating vector quantization layers into neural models of visually grounded speech. We show that our method is capable of capturing both word-level and sub-word units, depending on how it is configured. What differentiates this paper from prior work on speech unit learning is the choice of training objective. Rather than using a reconstruction-based loss, we use a discriminative, multimodal grounding objective which forces the learned units to be useful for semantic image retrieval. We evaluate the sub-word units on the ZeroSpeech 2019 challenge, achieving a 27.3% reduction in ABX error rate over the top-performing submission, while keeping the bitrate approximately the same. We also present experiments demonstrating the noise robustness of these units. Finally, we show that a model with multiple quantizers can simultaneously learn phone-like detectors at a lower layer and word-like detectors at a higher layer. We show that these detectors are highly accurate, discovering 279 words with an F1 score of greater than 0.5.",
  "stance": 0.7
 },
 {
  "url": "https://openreview.net/forum?id=B1g8VkHFPH",
  "title": "Rethinking the Hyperparameters for Fine-tuning",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Fine-tuning from pre-trained ImageNet models has become the de-facto standard for various computer vision tasks. Current practices for fine-tuning typically involve selecting an ad-hoc choice of hyperparameters and keeping them fixed to values normally used for training from scratch. This paper re-examines several common practices of setting hyperparameters for fine-tuning. Our findings are based on extensive empirical evaluation for fine-tuning on various transfer learning benchmarks. (1) While prior works have thoroughly investigated learning rate and batch size, momentum for fine-tuning is a relatively unexplored parameter. We find that the value of momentum also affects fine-tuning performance and connect it with previous theoretical findings. (2) Optimal hyperparameters for fine-tuning, in particular, the effective learning rate, are not only dataset dependent but also sensitive to the similarity between the source domain and target domain. This is in contrast to hyperparameters for training from scratch. (3) Reference-based regularization that keeps models close to the initial model does not necessarily apply for “dissimilar” datasets. Our findings challenge common practices of finetuning and encourages deep learning practitioners to rethink the hyperparameters for fine-tuning.",
  "stance": -0.5
 },
 {
  "url": "https://openreview.net/forum?id=B1l2bp4YwS",
  "title": "What graph neural networks cannot learn: depth vs width",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "This paper studies the expressive power of graph neural networks falling within the message-passing framework (GNNmp). Two results are presented. First, GNNmp are shown to be Turing universal under sufficient conditions on their depth, width, node attributes, and layer expressiveness. Second, it is discovered that GNNmp can lose a significant portion of their power when their depth and width is restricted. The proposed impossibility statements stem from a new technique that enables the repurposing of seminal results from distributed computing and leads to lower bounds for an array of decision, optimization, and estimation problems involving graphs. Strikingly, several of these problems are deemed impossible unless the product of a GNNmp’s depth and width exceeds a polynomial of the graph size; this dependence remains significant even for tasks that appear simple or when considering approximation.",
  "stance": -0.7
 },
 {
  "url": "https://openreview.net/forum?id=B1lDoJSYDH",
  "title": "Lagrangian Fluid Simulation with Continuous Convolutions",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We present an approach to Lagrangian fluid simulation with a new type of convolutional network. Our networks process sets of moving particles, which describe fluids in space and time. Unlike previous approaches, we do not build an explicit graph structure to connect the particles but use spatial convolutions as the main differentiable operation that relates particles to their neighbors. To this end we present a simple, novel, and effective extension of N-D convolutions to the continuous domain. We show that our network architecture can simulate different materials, generalizes to arbitrary collision geometries, and can be used for inverse problems. In addition, we demonstrate that our continuous convolutions outperform prior formulations in terms of accuracy and speed.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=B1lJzyStvS",
  "title": "Self-Supervised Learning of Appliance Usage",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Learning home appliance usage is important for understanding people’s activities and optimizing energy consumption. The problem is modeled as an event detection task, where the objective is to learn when a user turns an appliance on, and which appliance it is (microwave, hair dryer, etc.). Ideally, we would like to solve the problem in an unsupervised way so that the method can be applied to new homes and new appliances without any labels. To this end, we introduce a new deep learning model that takes input from two home sensors: 1) a smart electricity meter that outputs the total energy consumed by the home as a function of time, and 2) a motion sensor that outputs the locations of the residents over time. The model learns the distribution of the residents’ locations conditioned on the home energy signal. We show that this cross-modal prediction task allows us to detect when a particular appliance is used, and the location of the appliance in the home, all in a self-supervised manner, without any labeled data.",
  "stance": 0.7
 },
 {
  "url": "https://openreview.net/forum?id=B1xMEerYvB",
  "title": "Smooth markets: A basic mechanism for organizing gradient-based learners",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "With the success of modern machine learning, it is becoming increasingly important to understand and control how learning algorithms interact. Unfortunately, negative results from game theory show there is little hope of understanding or controlling general n-player games. We therefore introduce smooth markets (SM-games), a class of n-player games with pairwise zero sum interactions. SM-games codify a common design pattern in machine learning that includes (some) GANs, adversarial training, and other recent algorithms. We show that SM-games are amenable to analysis and optimization using first-order methods. “I began to see legibility as a central problem in modern statecraft. The premodern state was, in many respects, partially blind [. . .] It lacked anything like a detailed ‘map’ of its terrain and its people. It lacked, for the most part, a measure, a metric that would allow it to ‘translate’ what it knew into a common standard necessary for a synoptic view. As a result, its interventions were often crude and self-defeating.” – from Seeing like a State by Scott (1999)",
  "stance": -0.8
 },
 {
  "url": "https://openreview.net/forum?id=BJe-91BtvH",
  "title": "Masked Based Unsupervised Content Transfer",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We consider the problem of translating, in an unsupervised manner, between two domains where one contains some additional information compared to the other. The proposed method disentangles the common and separate parts of these domains and, through the generation of a mask, focuses the attention of the underlying network to the desired augmentation alone, without wastefully reconstructing the entire target. This enables state-of-the-art quality and variety of content translation, as demonstrated through extensive quantitative and qualitative evaluation. Our method is also capable of adding the separate content of different guide images and domains as well as remove existing separate content. Furthermore, our method enables weakly-supervised semantic segmentation of the separate part of each domain, where only class labels are provided. Our code is available at https: //github.com/rmokady/mbu-content-tansfer.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=BJeKwTNFvB",
  "title": "Physics-as-Inverse-Graphics: Unsupervised Physical Parameter Estimation from Video",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We propose a model that is able to perform unsupervised physical parameter estimation of systems from video, where the differential equations governing the scene dynamics are known, but labeled states or objects are not available. Existing physical scene understanding methods require either object state supervision, or do not integrate with differentiable physics to learn interpretable system parameters and states. We address this problem through a physics-as-inverse-graphics approach that brings together vision-as-inverse-graphics and differentiable physics engines, enabling objects and explicit state and velocity representations to be discovered. This framework allows us to perform long term extrapolative video prediction, as well as vision-based model-predictive control. Our approach significantly outperforms related unsupervised methods in long-term future frame prediction of systems with interacting objects (such as ball-spring or 3-body gravitational systems), due to its ability to build dynamics into the model as an inductive bias. We further show the value of this tight vision-physics integration by demonstrating data-efficient learning of vision-actuated model-based control for a pendulum system. We also show that the controller’s interpretability provides unique capabilities in goal-driven control and physical reasoning for zero-data adaptation.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=BJg866NFvB",
  "title": "Estimating counterfactual treatment outcomes over time through adversarially balanced representations",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Identifying when to give treatments to patients and how to select among multiple treatments over time are important medical problems with a few existing solutions. In this paper, we introduce the Counterfactual Recurrent Network (CRN), a novel sequence-to-sequence model that leverages the increasingly available patient observational data to estimate treatment effects over time and answer such medical questions. To handle the bias from time-varying confounders, covariates affecting the treatment assignment policy in the observational data, CRN uses domain adversarial training to build balancing representations of the patient history. At each timestep, CRN constructs a treatment invariant representation which removes the association between patient history and treatment assignments and thus can be reliably used for making counterfactual predictions. On a simulated model of tumour growth, with varying degree of time-dependent confounding, we show how our model achieves lower error in estimating counterfactuals and in choosing the correct treatment and timing of treatment than current state-of-the-art methods.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=BJgZGeHFPH",
  "title": "Dynamics-Aware Embeddings",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "In this paper we consider self-supervised representation learning to improve sample efficiency in reinforcement learning (RL). We propose a forward prediction objective for simultaneously learning embeddings of states and action sequences. These embeddings capture the structure of the environment’s dynamics, enabling efficient policy learning. We demonstrate that our action embeddings alone improve the sample efficiency and peak performance of model-free RL on control from low-dimensional states. By combining state and action embeddings, we achieve efficient learning of high-quality policies on goal-conditioned continuous control from pixel observations in only 1-2 million environment steps.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=BJgqQ6NYvB",
  "title": "FasterSeg: Searching for Faster Real-time Semantic Segmentation",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We present FasterSeg, an automatically designed semantic segmentation network with not only state-of-the-art performance but also faster speed than current methods. Utilizing neural architecture search (NAS), FasterSeg is discovered from a novel and broader search space integrating multi-resolution branches, that has been recently found to be vital in manually designed segmentation models. To better calibrate the balance between the goals of high accuracy and low latency, we propose a decoupled and fine-grained latency regularization, that effectively overcomes our observed phenomenons that the searched networks are prone to “collapsing” to low-latency yet poor-accuracy models. Moreover, we seamlessly extend FasterSeg to a new collaborative search (co-searching) framework, simultaneously searching for a teacher and a student network in the same single run. The teacher-student distillation further boosts the student model’s accuracy. Experiments on popular segmentation benchmarks demonstrate the competency of FasterSeg. For example, FasterSeg can run over 30% faster than the closest manually designed competitor on Cityscapes, while maintaining comparable accuracy.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=BJlZ5ySKPH",
  "title": "U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based method which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new AdaLIN (Adaptive Layer-Instance Normalization) function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a fixed network architecture and hyper-parameters. Our code and datasets are available at https://github.com/taki0112/UGATIT or https://github.com/znxlwm/UGATITpytorch.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=Bke89JBtvB",
  "title": "Batch-shaping for learning conditional channel gated networks",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We present a method that trains large capacity neural networks with significantly improved accuracy and lower dynamic computational cost. We achieve this by gating the deep-learning architecture on a fine-grained-level. Individual convolutional maps are turned on/off conditionally on features in the network. To achieve this, we introduce a new residual block architecture that gates convolutional channels in a fine-grained manner. We also introduce a generally applicable tool batch-shaping that matches the marginal aggregate posteriors of features in a neural network to a pre-specified prior distribution. We use this novel technique to force gates to be more conditional on the data. We present results on CIFAR-10 and ImageNet datasets for image classification, and Cityscapes for semantic segmentation. Our results show that our method can slim down large architectures conditionally, such that the average computational cost on the data is on par with a smaller architecture, but with higher accuracy. In particular, on ImageNet, our ResNet50 and ResNet34 gated networks obtain 74.60% and 72.55% top-1 accuracy compared to the 69.76% accuracy of the baseline ResNet18 model, for similar complexity. We also show that the resulting networks automatically learn to use more features for difficult examples and fewer features for simple examples.",
  "stance": 0.8
 },
 {
  "url": "https://openreview.net/forum?id=BkgXT24tDS",
  "title": "Additive Powers-of-Two Quantization: An Efficient Non-uniform Discretization for Neural Networks",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We propose Additive Powers-of-Two (APoT) quantization, an efficient nonuniform quantization scheme for the bell-shaped and long-tailed distribution of weights and activations in neural networks. By constraining all quantization levels as the sum of Powers-of-Two terms, APoT quantization enjoys high computational efficiency and a good match with the distribution of weights. A simple reparameterization of the clipping function is applied to generate a better-defined gradient for learning the clipping threshold. Moreover, weight normalization is presented to refine the distribution of weights to make the training more stable and consistent. Experimental results show that our proposed method outperforms state-of-the-art methods, and is even competitive with the full-precision models, demonstrating the effectiveness of our proposed APoT quantization. For example, our 4-bit quantized ResNet-50 on ImageNet achieves 76.6% top-1 accuracy without bells and whistles; meanwhile, our model is capable to decrease 22% computational cost compared with the uniformly quantized counterpart. 1",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=BkxpMTEtPB",
  "title": "GLAD: Learning Sparse Graph Recovery",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Recovering sparse conditional independence graphs from data is a fundamental problem in machine learning with wide applications. A popular formulation of the problem is an `1 regularized maximum likelihood estimation. Many convex optimization algorithms have been designed to solve this formulation to recover the graph structure. Recently, there is a surge of interest to learn algorithms directly based on data, and in this case, learn to map empirical covariance to the sparse precision matrix. However, it is a challenging task in this case, since the symmetric positive definiteness (SPD) and sparsity of the matrix are not easy to enforce in learned algorithms, and a direct mapping from data to precision matrix may contain many parameters. We propose a deep learning architecture, GLAD, which uses an Alternating Minimization (AM) algorithm as our model inductive bias, and learns the model parameters via supervised learning. We show that GLAD learns a very compact and effective model for recovering sparse graphs from data.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=Byg1v1HKDB",
  "title": "Abductive Commonsense Reasoning",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Abductive reasoning is inference to the most plausible explanation. For example, if Jenny finds her house in a mess when she returns from work, and remembers that she left a window open, she can hypothesize that a thief broke into her house and caused the mess, as the most plausible explanation. While abduction has long been considered to be at the core of how people interpret and read between the lines in natural language (Hobbs et al., 1988), there has been relatively little research in support of abductive natural language inference and generation. We present the first study that investigates the viability of language-based abductive reasoning. We introduce a challenge dataset, ART, that consists of over 20k commonsense narrative contexts and 200k explanations. Based on this dataset, we conceptualize two new tasks – (i) Abductive NLI: a multiple-choice question answering task for choosing the more likely explanation, and (ii) Abductive NLG: a conditional generation task for explaining given observations in natural language. On Abductive NLI, the best model achieves 68.9% accuracy, well below human performance of 91.4%. On Abductive NLG, the current best language generators struggle even more, as they lack reasoning capabilities that are trivial for humans. Our analysis leads to new insights into the types of reasoning that deep pre-trained language models fail to perform—despite their strong performance on the related but more narrowly defined task of entailment NLI—pointing to interesting avenues for future research.",
  "stance": -1.0
 },
 {
  "url": "https://openreview.net/forum?id=Byl8hhNYPS",
  "title": "Neural Machine Translation with Universal Visual Representation",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Though visual information has been introduced for enhancing neural machine translation (NMT), its effectiveness strongly relies on the availability of large amounts of bilingual parallel sentence pairs with manual image annotations. In this paper, we present a universal visual representation learned over the monolingual corpora with image annotations, which overcomes the lack of largescale bilingual sentence-image pairs, thereby extending image applicability in NMT. In detail, a group of images with similar topics to the source sentence will be retrieved from a light topic-image lookup table learned over the existing sentence-image pairs, and then is encoded as image representations by a pretrained ResNet. An attention layer with a gated weighting is to fuse the visual information and text information as input to the decoder for predicting target translations. In particular, the proposed method enables the visual information to be integrated into large-scale text-only NMT in addition to the multimodal NMT. Experiments on four widely used translation datasets, including the WMT’16 English-to-Romanian, WMT’14 English-to-German, WMT’14 Englishto-French, and Multi30K, show that the proposed approach achieves significant improvements over strong baselines.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=ByxxgCEYDS",
  "title": "Inductive Matrix Completion Based on Graph Neural Networks",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We propose an inductive matrix completion model without using side information. By factorizing the (rating) matrix into the product of low-dimensional latent embeddings of rows (users) and columns (items), a majority of existing matrix completion methods are transductive, since the learned embeddings cannot generalize to unseen rows/columns or to new matrices. To make matrix completion inductive, most previous works use content (side information), such as user’s age or movie’s genre, to make predictions. However, high-quality content is not always available, and can be hard to extract. Under the extreme setting where not any side information is available other than the matrix to complete, can we still learn an inductive matrix completion model? In this paper, we propose an Inductive Graph-based Matrix Completion (IGMC) model to address this problem. IGMC trains a graph neural network (GNN) based purely on 1-hop subgraphs around (user, item) pairs generated from the rating matrix and maps these subgraphs to their corresponding ratings. It achieves highly competitive performance with state-of-the-art transductive baselines. In addition, IGMC is inductive – it can generalize to users/items unseen during the training (given that their interactions exist), and can even transfer to new tasks. Our transfer learning experiments show that a model trained out of the MovieLens dataset can be directly used to predict Douban movie ratings with surprisingly good performance. Our work demonstrates that: 1) it is possible to train inductive matrix completion models without using side information while achieving similar or better performances than state-of-the-art transductive methods; 2) local graph patterns around a (user, item) pair are effective predictors of the rating this user gives to the item; and 3) Long-range dependencies might not be necessary for modeling recommender systems.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=H1eCw3EKvH",
  "title": "On the Weaknesses of Reinforcement Learning for Neural Machine Translation",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Reinforcement learning (RL) is frequently used to increase performance in text generation tasks, including machine translation (MT), notably through the use of Minimum Risk Training (MRT) and Generative Adversarial Networks (GAN). However, little is known about what and how these methods learn in the context of MT. We prove that one of the most common RL methods for MT does not optimize the expected reward, as well as show that other methods take an infeasibly long time to converge. In fact, our results suggest that RL practices in MT are likely to improve performance only where the pre-trained parameters are already close to yielding the correct translation. Our findings further suggest that observed gains may be due to effects unrelated to the training signal, concretely, changes in the shape of the distribution curve.",
  "stance": -0.5
 },
 {
  "url": "https://openreview.net/forum?id=H1eqQeHFDS",
  "title": "AdvectiveNet: An Eulerian-Lagrangian Fluidic Reservoir for Point Cloud Processing",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "This paper presents a novel physics-inspired deep learning approach for point cloud processing motivated by the natural flow phenomena in fluid mechanics. Our learning architecture jointly defines data in an Eulerian world space, using a static background grid, and a Lagrangian material space, using moving particles. By introducing this Eulerian-Lagrangian representation, we are able to naturally evolve and accumulate particle features using flow velocities generated from a generalized, high-dimensional force field. We demonstrate the efficacy of this system by solving various point cloud classification and segmentation problems with state-of-the-art performance. The entire geometric reservoir and data flow mimics the pipeline of the classic PIC/FLIP scheme in modeling natural flow, bridging the disciplines of geometric machine learning and physical simulation.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=H1ezFREtwH",
  "title": "Composing Task-Agnostic Policies with Deep Reinforcement Learning",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "The composition of elementary behaviors to solve challenging transfer learning problems is one of the key elements in building intelligent machines. To date, there has been plenty of work on learning task-specific policies or skills but almost no focus on composing necessary, task-agnostic skills to find a solution to new problems. In this paper, we propose a novel deep reinforcement learning-based skill transfer and composition method that takes the agent’s primitive policies to solve unseen tasks. We evaluate our method in difficult cases where training policy through standard reinforcement learning (RL) or even hierarchical RL is either not feasible or exhibits high sample complexity. We show that our method not only transfers skills to new problem settings but also solves the challenging environments requiring both task planning and motion control with high data efficiency.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=H1gBsgBYwH",
  "title": "Generalization of Two-layer Neural Networks: An Asymptotic Viewpoint",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "This paper investigates the generalization properties of two-layer neural networks in high-dimensions, i.e. when the number of samples n, features d, and neurons h tend to infinity at the same rate. Specifically, we derive the exact population risk of the unregularized least squares regression problem with two-layer neural networks when either the first or the second layer is trained using a gradient flow under different initialization setups. When only the second layer coefficients are optimized, we recover the double descent phenomenon: a cusp in the population risk appears at h ≈ n and further overparameterization decreases the risk. In contrast, when the first layer weights are optimized, we highlight how different scales of initialization lead to different inductive bias, and show that the resulting risk is independent of overparameterization. Our theoretical and experimental results suggest that previously studied model setups that provably give rise to double descent might not translate to optimizing two-layer neural networks.",
  "stance": -0.5
 },
 {
  "url": "https://openreview.net/forum?id=H1gmHaEKwB",
  "title": "Data-Independent Neural Pruning via Coresets",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Previous work showed empirically that large neural networks can be significantly reduced in size while preserving their accuracy. Model compression became a central research topic, as it is crucial for deployment of neural networks on devices with limited computational and memory resources. The majority of the compression methods are based on heuristics and offer no worst-case guarantees on the trade-off between the compression rate and the approximation error for an arbitrarily new sample. We propose the first efficient, data-independent neural pruning algorithm with a provable trade-off between its compression rate and the approximation error for any future test sample. Our method is based on the coreset framework, which finds a small weighted subset of points that provably approximates the original inputs. Specifically, we approximate the output of a layer of neurons by a coreset of neurons in the previous layer and discard the rest. We apply this framework in a layer-by-layer fashion from the top to the bottom. Unlike previous works, our coreset is data independent, meaning that it provably guarantees the accuracy of the function for any input x ∈ R, including an adversarial one. We demonstrate the effectiveness of our method on popular network architectures. In particular, our coresets yield 90% compression of the LeNet-300-100 architecture on MNIST while improving classification accuracy.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=H1gzR2VKDH",
  "title": "Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Video prediction models combined with planning algorithms have shown promise in enabling robots to learn to perform many vision-based tasks through only selfsupervision, reaching novel goals in cluttered scenes with unseen objects. However, due to the compounding uncertainty in long horizon video prediction and poor scalability of sampling-based planning optimizers, one significant limitation of these approaches is the ability to plan over long horizons to reach distant goals. To that end, we propose a framework for subgoal generation and planning, hierarchical visual foresight (HVF), which generates subgoal images conditioned on a goal image, and uses them for planning. The subgoal images are directly optimized to decompose the task into easy to plan segments, and as a result, we observe that the method naturally identifies semantically meaningful states as subgoals. Across four simulated vision-based manipulation tasks, we find that our method achieves more than 20% absolute performance improvement over planning without subgoals and model-free RL approaches. Further, our experiments illustrate that our approach extends to real, cluttered visual scenes.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=HJgLZR4KvH",
  "title": "Dynamics-Aware Unsupervised Discovery of Skills",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Conventionally, model-based reinforcement learning (MBRL) aims to learn a global model for the dynamics of the environment. A good model can potentially enable planning algorithms to generate a large variety of behaviors and solve diverse tasks. However, learning an accurate model for complex dynamical systems is difficult, and even then, the model might not generalize well outside the distribution of states on which it was trained. In this work, we combine model-based learning with model-free learning of primitives that make modelbased planning easy. To that end, we aim to answer the question: how can we discover skills whose outcomes are easy to predict? We propose an unsupervised learning algorithm, Dynamics-Aware Discovery of Skills (DADS), which simultaneously discovers predictable behaviors and learns their dynamics. Our method can leverage continuous skill spaces, theoretically, allowing us to learn infinitely many behaviors even for high-dimensional state-spaces. We demonstrate that zero-shot planning in the learned latent space significantly outperforms standard MBRL and model-free goal-conditioned RL, can handle sparsereward tasks, and substantially improves over prior hierarchical RL methods for unsupervised skill discovery.",
  "stance": 0.8
 },
 {
  "url": "https://openreview.net/forum?id=HkgxW0EYDS",
  "title": "Scalable Model Compression by Entropy Penalized Reparameterization",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We describe a simple and general neural network weight compression approach, in which the network parameters (weights and biases) are represented in a “latent” space, amounting to a reparameterization. This space is equipped with a learned probability model, which is used to impose an entropy penalty on the parameter representation during training, and to compress the representation using a simple arithmetic coder after training. Classification accuracy and model compressibility is maximized jointly, with the bitrate–accuracy trade-off specified by a hyperparameter. We evaluate the method on the MNIST, CIFAR-10 and ImageNet classification benchmarks using six distinct model architectures. Our results show that state-of-the-art model compression can be achieved in a scalable and general way without requiring complex procedures such as multi-stage training.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=Hkl1iRNFwS",
  "title": "The Early Phase of Neural Network Training",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Recent studies have shown that many important aspects of neural network learning take place within the very earliest iterations or epochs of training. For example, sparse, trainable sub-networks emerge (Frankle et al., 2019), gradient descent moves into a small subspace (Gur-Ari et al., 2018), and the network undergoes a critical period (Achille et al., 2019). Here we examine the changes that deep neural networks undergo during this early phase of training. We perform extensive measurements of the network state during these early iterations of training and leverage the framework of Frankle et al. (2019) to quantitatively probe the weight distribution and its reliance on various aspects of the dataset. We find that, within this framework, deep networks are not robust to reinitializing with random weights while maintaining signs, and that weight distributions are highly non-independent even after only a few hundred iterations. Despite this behavior, pre-training with blurred inputs or an auxiliary self-supervised task can approximate the changes in supervised networks, suggesting that these changes are not inherently label-dependent, though labels significantly accelerate this process. Together, these results help to elucidate the network changes occurring during this pivotal initial period of learning.",
  "stance": -0.5
 },
 {
  "url": "https://openreview.net/forum?id=Hyg9anEFPS",
  "title": "Image-guided Neural Object Rendering",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We propose a learned image-guided rendering technique that combines the benefits of image-based rendering and GAN-based image synthesis. The goal of our method is to generate photo-realistic re-renderings of reconstructed objects for virtual and augmented reality applications (e.g., virtual showrooms, virtual tours & sightseeing, the digital inspection of historical artifacts). A core component of our work is the handling of view-dependent effects. Specifically, we directly train an object-specific deep neural network to synthesize the view-dependent appearance of an object. As input data we are using an RGB video of the object. This video is used to reconstruct a proxy geometry of the object via multi-view stereo. Based on this 3D proxy, the appearance of a captured view can be warped into a new target view as in classical image-based rendering. This warping assumes diffuse surfaces, in case of view-dependent effects, such as specular highlights, it leads to artifacts. To this end, we propose EffectsNet, a deep neural network that predicts view-dependent effects. Based on these estimations, we are able to convert observed images to diffuse images. These diffuse images can be projected into other views. In the target view, our pipeline reinserts the new view-dependent effects. To composite multiple reprojected images to a final output, we learn a composition network that outputs photo-realistic results. Using this image-guided approach, the network does not have to allocate capacity on “remembering” object appearance, instead it learns how to combine the appearance of captured images. We demonstrate the effectiveness of our approach both qualitatively and quantitatively on synthetic as well as on real data.",
  "stance": 0.7
 },
 {
  "url": "https://openreview.net/forum?id=HygrdpVKvr",
  "title": "NAS evaluation is frustratingly hard",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Neural Architecture Search (NAS) is an exciting new field which promises to be as much as a game-changer as Convolutional Neural Networks were in 2012. Despite many great works leading to substantial improvements on a variety of tasks, comparison between different methods is still very much an open issue. While most algorithms are tested on the same datasets, there is no shared experimental protocol followed by all. As such, and due to the under-use of ablation studies, there is a lack of clarity regarding why certain methods are more effective than others. Our first contribution is a benchmark of 8 NAS methods on 5 datasets. To overcome the hurdle of comparing methods with different search spaces, we propose using a method’s relative improvement over the randomly sampled average architecture, which effectively removes advantages arising from expertly engineered search spaces or training protocols. Surprisingly, we find that many NAS techniques struggle to significantly beat the average architecture baseline. We perform further experiments with the commonly used DARTS search space in order to understand the contribution of each component in the NAS pipeline. These experiments highlight that: (i) the use of tricks in the evaluation protocol has a predominant impact on the reported performance of architectures; (ii) the cell-based search space has a very narrow accuracy range, such that the seed has a considerable impact on architecture rankings; (iii) the hand-designed macrostructure (cells) is more important than the searched micro-structure (operations); and (iv) the depth-gap is a real phenomenon, evidenced by the change in rankings between 8 and 20 cell architectures. To conclude, we suggest best practices, that we hope will prove useful for the community and help mitigate current NAS pitfalls, e.g. difficulties in reproducibility and comparison of search methods. The code used is available at https://github.com/antoyang/NAS-Benchmark.",
  "stance": -0.8
 },
 {
  "url": "https://openreview.net/forum?id=HylsTT4FvB",
  "title": "On the \"steerability\" of generative adversarial networks",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "An open secret in contemporary machine learning is that many models work beautifully on standard benchmarks but fail to generalize outside the lab. This has been attributed to biased training data, which provide poor coverage over real world events. Generative models are no exception, but recent advances in generative adversarial networks (GANs) suggest otherwise – these models can now synthesize strikingly realistic and diverse images. Is generative modeling of photos a solved problem? We show that although current GANs can fit standard datasets very well, they still fall short of being comprehensive models of the visual manifold. In particular, we study their ability to fit simple transformations such as camera movements and color changes. We find that the models reflect the biases of the datasets on which they are trained (e.g., centered objects), but that they also exhibit some capacity for generalization: by “steering” in latent space, we can shift the distribution while still creating realistic images. We hypothesize that the degree of distributional shift is related to the breadth of the training data distribution. Thus, we conduct experiments to quantify the limits of GAN transformations and introduce techniques to mitigate the problem. Code is released on our project page: https://ali-design.github.io/gan_steerability/.",
  "stance": 0.1
 },
 {
  "url": "https://openreview.net/forum?id=HyxjNyrtPr",
  "title": "RGBD-GAN: Unsupervised 3D Representation Learning From Natural Image Datasets via RGBD Image Synthesis",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Understanding three-dimensional (3D) geometries from two-dimensional (2D) images without any labeled information is promising for understanding the real world without incurring annotation cost. We herein propose a novel generative model, RGBD-GAN, which achieves unsupervised 3D representation learning from 2D images. The proposed method enables camera parameter–conditional image generation and depth image generation without any 3D annotations, such as camera poses or depth. We use an explicit 3D consistency loss for two RGBD images generated from different camera parameters, in addition to the ordinal GAN objective. The loss is simple yet effective for any type of image generator such as DCGAN and StyleGAN to be conditioned on camera parameters. Through experiments, we demonstrated that the proposed method could learn 3D representations from 2D images with various generator architectures.",
  "stance": 0.9
 },
 {
  "url": "https://openreview.net/forum?id=HyxyIgHFvr",
  "title": "Truth or backpropaganda? An empirical investigation of deep learning theory",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike. In this work, we: (1) prove the widespread existence of suboptimal local minima in the loss landscape of neural networks, and we use our theory to find examples; (2) show that small-norm parameters are not optimal for generalization; (3) demonstrate that ResNets do not conform to wide-network theories, such as the neural tangent kernel, and that the interaction between skip connections and batch normalization plays a role; (4) find that rank does not correlate with generalization or robustness in a practical setting.",
  "stance": -0.5
 },
 {
  "url": "https://openreview.net/forum?id=S1xnXRVFwH",
  "title": "Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "The lottery ticket hypothesis proposes that over-parameterization of deep neural networks (DNNs) aids training by increasing the probability of a “lucky” sub-network initialization being present rather than by helping the optimization process (Frankle & Carbin, 2019). Intriguingly, this phenomenon suggests that initialization strategies for DNNs can be improved substantially, but the lottery ticket hypothesis has only previously been tested in the context of supervised learning for natural image tasks. Here, we evaluate whether “winning ticket” initializations exist in two different domains: natural language processing (NLP) and reinforcement learning (RL). For NLP, we examined both recurrent LSTM models and large-scale Transformer models (Vaswani et al., 2017). For RL, we analyzed a number of discrete-action space tasks, including both classic control and pixel control. Consistent with work in supervised image classification, we confirm that winning ticket initializations generally outperform parameter-matched random initializations, even at extreme pruning rates for both NLP and RL. Notably, we are able to find winning ticket initializations for Transformers which enable models one-third the size to achieve nearly equivalent performance. Together, these results suggest that the lottery ticket hypothesis is not restricted to supervised learning of natural images, but rather represents a broader phenomenon in DNNs.",
  "stance": 0.3
 },
 {
  "url": "https://openreview.net/forum?id=SJgIPJBFvH",
  "title": "Fantastic Generalization Measures and Where to Find Them",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Generalization of deep networks has lately been of great interest, resulting in a number of theoretically and empirically motivated complexity measures. However, most papers proposing such measures study only a small set of models, leaving open the question of whether the conclusion drawn from those experiments would remain valid in other settings. We present the first large scale study of generalization in deep networks. We investigate more then 40 complexity measures taken from both theoretical bounds and empirical studies. We train over 10,000 convolutional networks by systematically varying commonly used hyperparameters. Hoping to uncover potentially causal relationships between each measure and generalization, we analyze carefully controlled experiments and show surprising failures of some measures as well as promising measures for further research.",
  "stance": 0.0
 },
 {
  "url": "https://openreview.net/forum?id=SJxSOJStPr",
  "title": "A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Despite the growing interest in continual learning, most of its contemporary works have been studied in a rather restricted setting where tasks are clearly distinguishable, and task boundaries are known during training. However, if our goal is to develop an algorithm that learns as humans do, this setting is far from realistic, and it is essential to develop a methodology that works in a task-free manner. Meanwhile, among several branches of continual learning, expansion-based methods have the advantage of eliminating catastrophic forgetting by allocating new resources to learn new data. In this work, we propose an expansion-based approach for task-free continual learning. Our model, named Continual Neural Dirichlet Process Mixture (CN-DPM), consists of a set of neural network experts that are in charge of a subset of the data. CN-DPM expands the number of experts in a principled way under the Bayesian nonparametric framework. With extensive experiments, we show that our model successfully performs task-free continual learning for both discriminative and generative tasks such as image classification and image generation.",
  "stance": 0.9
 },
 {
  "url": "https://openreview.net/forum?id=SJxhNTNYwB",
  "title": "Black-Box Adversarial Attack with Transferable Model-based Embedding",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We present a new method for black-box adversarial attack. Unlike previous methods that combined transfer-based and scored-based methods by using the gradient or initialization of a surrogate white-box model, this new method tries to learn a low-dimensional embedding using a pretrained model, and then performs efficient search within the embedding space to attack an unknown target network. The method produces adversarial perturbations with high level semantic patterns that are easily transferable. We show that this approach can greatly improve the query efficiency of black-box adversarial attack across different target network architectures. We evaluate our approach on MNIST, ImageNet and Google Cloud Vision API, resulting in a significant reduction on the number of queries. We also attack adversarially defended networks on CIFAR10 and ImageNet, where our method not only reduces the number of queries, but also improves the attack success rate.",
  "stance": 0.7
 },
 {
  "url": "https://openreview.net/forum?id=SkgGCkrKvH",
  "title": "Decentralized Deep Learning with Arbitrary Communication Compression",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Decentralized training of deep learning models is a key element for enabling data privacy and on-device learning over networks, as well as for efficient scaling to large compute clusters. As current approaches are limited by network bandwidth, we propose the use of communication compression in the decentralized training context. We show that CHOCO-SGD achieves linear speedup in the number of workers for arbitrary high compression ratios on general non-convex functions, and non-IID training data. We demonstrate the practical performance of the algorithm in two key scenarios: the training of deep learning models (i) over decentralized user devices, connected by a peer-to-peer network and (ii) in a datacenter.",
  "stance": 0.5
 },
 {
  "url": "https://openreview.net/forum?id=Sklgs0NFvr",
  "title": "Learning The Difference That Makes A Difference With Counterfactually-Augmented Data",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Despite alarm over the reliance of machine learning systems on so-called spurious patterns, the term lacks coherent meaning in standard statistical frameworks. However, the language of causality offers clarity: spurious associations are due to confounding (e.g., a common cause), but not direct or indirect causal effects. In this paper, we focus on natural language processing, introducing methods and resources for training models less sensitive to spurious patterns. Given documents and their initial labels, we task humans with revising each document so that it (i) accords with a counterfactual target label; (ii) retains internal coherence; and (iii) avoids unnecessary changes. Interestingly, on sentiment analysis and natural language inference tasks, classifiers trained on original data fail on their counterfactually-revised counterparts and vice versa. Classifiers trained on combined datasets perform remarkably well, just shy of those specialized to either domain. While classifiers trained on either original or manipulated data alone are sensitive to spurious features (e.g., mentions of genre), models trained on the combined data are less sensitive to this signal. Both datasets are publicly available1.",
  "stance": 0.7
 },
 {
  "url": "https://openreview.net/forum?id=SylzhkBtDB",
  "title": "Understanding and Improving Information Transfer in Multi-Task Learning",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We investigate multi-task learning approaches that use a shared feature representation for all tasks. To better understand the transfer of task information, we study an architecture with a shared module for all tasks and a separate output module for each task. We study the theory of this setting on linear and ReLU-activated models. Our key observation is that whether or not tasks’ data are well-aligned can significantly affect the performance of multi-task learning. We show that misalignment between task data can cause negative transfer (or hurt performance) and provide sufficient conditions for positive transfer. Inspired by the theoretical insights, we show that aligning tasks’ embedding layers leads to performance gains for multi-task training and transfer learning on the GLUE benchmark and sentiment analysis tasks; for example, we obtain a 2.35% GLUE score average improvement on 5 GLUE tasks over BERTLARGE using our alignment method. We also design an SVD-based task reweighting scheme and show that it improves the robustness of multi-task training on a multi-label image dataset.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=r1etN1rtPB",
  "title": "Implementation Matters in Deep RL: A Case Study on PPO and TRPO",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We study the roots of algorithmic progress in deep policy gradient algorithms through a case study on two popular algorithms: Proximal Policy Optimization (PPO) and Trust Region Policy Optimization (TRPO). Specifically, we investigate the consequences of “code-level optimizations:” algorithm augmentations found only in implementations or described as auxiliary details to the core algorithm. Seemingly of secondary importance, such optimizations turn out to have a major impact on agent behavior. Our results show that they (a) are responsible for most of PPO’s gain in cumulative reward over TRPO, and (b) fundamentally change how RL methods function. These insights show the difficulty, and importance, of attributing performance gains in deep reinforcement learning.",
  "stance": -0.2
 },
 {
  "url": "https://openreview.net/forum?id=r1evOhEKvH",
  "title": "Graph inference learning for semi-supervised classification",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "In this work, we address semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem via advanced graph convolution in a conventionally supervised manner, but the performance could degrade significantly when labeled data is scarce. To this end, we propose a Graph Inference Learning (GIL) framework to boost the performance of semisupervised node classification by learning the inference of node labels on graph topology. To bridge the connection between two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths, and local topological structures together, which can make the inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted to testing nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed, and NELL) demonstrate the superiority of our proposed GIL when compared against state-of-the-art methods on the semi-supervised node classification task.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=r1genAVKPB",
  "title": "Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Modern deep learning methods provide effective means to learn good representations. However, is a good representation itself sufficient for sample efficient reinforcement learning? This question has largely been studied only with respect to (worst-case) approximation error, in the more classical approximate dynamic programming literature. With regards to the statistical viewpoint, this question is largely unexplored, and the extant body of literature mainly focuses on conditions which permit sample efficient reinforcement learning with little understanding of what are necessary conditions for efficient reinforcement learning. This work shows that, from the statistical viewpoint, the situation is far subtler than suggested by the more traditional approximation viewpoint, where the requirements on the representation that suffice for sample efficient RL are even more stringent. Our main results provide sharp thresholds for reinforcement learning methods, showing that there are hard limitations on what constitutes good function approximation (in terms of the dimensionality of the representation), where we focus on natural representational conditions relevant to value-based, model-based, and policy-based learning. These lower bounds highlight that having a good (valuebased, model-based, or policy-based) representation in and of itself is insufficient for efficient reinforcement learning, unless the quality of this approximation passes certain hard thresholds. Furthermore, our lower bounds also imply exponential separations on the sample complexity between 1) value-based learning with perfect representation and value-based learning with a good-but-not-perfect representation, 2) value-based learning and policy-based learning, 3) policy-based learning and supervised learning and 4) reinforcement learning and imitation learning.",
  "stance": -0.5
 },
 {
  "url": "https://openreview.net/forum?id=r1lF_CEYwS",
  "title": "On the Need for Topology-Aware Generative Models for Manifold-Based Defenses",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Machine-learning (ML) algorithms or models, especially deep neural networks (DNNs), have shown significant promise in several areas. However, researchers have recently demonstrated that ML algorithms, especially DNNs, are vulnerable to adversarial examples (slightly perturbed samples that cause misclassification). The existence of adversarial examples has hindered the deployment of ML algorithms in safety-critical sectors, such as security. Several defenses for adversarial examples exist in the literature. One of the important classes of defenses are manifoldbased defenses, where a sample is “pulled back” into the data manifold before classifying. These defenses rely on the assumption that data lie in a manifold of a lower dimension than the input space. These defenses use a generative model to approximate the input distribution. In this paper, we investigate the following question: do the generative models used in manifold-based defenses need to be topology-aware? We suggest the answer is yes, and we provide theoretical and empirical evidence to support our claim.",
  "stance": 0.0
 },
 {
  "url": "https://openreview.net/forum?id=r1lPleBFvH",
  "title": "Understanding the Limitations of Conditional Generative Models",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Class-conditional generative models hold promise to overcome the shortcomings of their discriminative counterparts. They are a natural choice to solve discriminative tasks in a robust manner as they jointly optimize for predictive performance and accurate modeling of the input distribution. In this work, we investigate robust classification with likelihood-based generative models from a theoretical and practical perspective to investigate if they can deliver on their promises. Our analysis focuses on a spectrum of robustness properties: (1) Detection of worst-case outliers in the form of adversarial examples; (2) Detection of average-case outliers in the form of ambiguous inputs and (3) Detection of incorrectly labeled in-distribution inputs. Our theoretical result reveals that it is impossible to guarantee detectability of adversarially-perturbed inputs even for near-optimal generative classifiers. Experimentally, we find that while we are able to train robust models for MNIST, robustness completely breaks down on CIFAR10. We relate this failure to various undesirable model properties that can be traced to the maximum likelihood training objective. Despite being a common choice in the literature, our results indicate that likelihood-based conditional generative models may are surprisingly ineffective for robust classification.",
  "stance": -0.5
 },
 {
  "url": "https://openreview.net/forum?id=rJe4_xSFDB",
  "title": "Lipschitz constant estimation of Neural Networks via sparse polynomial optimization",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We introduce LiPopt, a polynomial optimization framework for computing increasingly tighter upper bounds on the Lipschitz constant of neural networks. The underlying optimization problems boil down to either linear (LP) or semidefinite (SDP) programming. We show how to use the sparse connectivity of a network, to significantly reduce the complexity of computation. This is specially useful for convolutional as well as pruned neural networks. We conduct experiments on networks with random weights as well as networks trained on MNIST, showing that in the particular case of the `∞-Lipschitz constant, our approach yields superior estimates, compared to baselines available in the literature.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=rJeW1yHYwH",
  "title": "Inductive representation learning on temporal graphs",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Inductive representation learning on temporal graphs is an important step toward salable machine learning on real-world dynamic networks. The evolving nature of temporal dynamic graphs requires handling new nodes as well as capturing temporal patterns. The node embeddings, which are now functions of time, should represent both the static node features and the evolving topological structures. Moreover, node and topological features can be temporal as well, whose patterns the node embeddings should also capture. We propose the temporal graph attention (TGAT) layer to efficiently aggregate temporal-topological neighborhood features as well as to learn the time-feature interactions. For TGAT, we use the self-attention mechanism as building block and develop a novel functional time encoding technique based on the classical Bochner’s theorem from harmonic analysis. By stacking TGAT layers, the network recognizes the node embeddings as functions of time and is able to inductively infer embeddings for both new and observed nodes as the graph evolves. The proposed approach handles both node classification and link prediction task, and can be naturally extended to include the temporal edge features. We evaluate our method with transductive and inductive tasks under temporal settings with two benchmark and one industrial dataset. Our TGAT model compares favorably to state-of-the-art baselines as well as the previous temporal graph embedding approaches.",
  "stance": 0.8
 },
 {
  "url": "https://openreview.net/forum?id=rJeg7TEYwB",
  "title": "Pruned Graph Scattering Transforms",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Graph convolutional networks (GCNs) have achieved remarkable performance in a variety of network science learning tasks. However, theoretical analysis of such approaches is still at its infancy. Graph scattering transforms (GSTs) are non-trainable deep GCN models that are amenable to generalization and stability analyses. The present work addresses some limitations of GSTs by introducing a novel so-termed pruned (p)GST approach. The resultant pruning algorithm is guided by a graph-spectrum-inspired criterion, and retains informative scattering features on-the-fly while bypassing the exponential complexity associated with GSTs. It is further established that pGSTs are stable to perturbations of the input graph signals with bounded energy. Experiments showcase that i) pGST performs comparably to the baseline GST that uses all scattering features, while achieving significant computational savings; ii) pGST achieves comparable performance to state-of-the-art GCNs; and iii) Graph data from various domains lead to different scattering patterns, suggesting domain-adaptive pGST network architectures.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=rkgMkCEtPB",
  "title": "Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "An important research direction in machine learning has centered around developing meta-learning algorithms to tackle few-shot learning. An especially successful algorithm has been Model Agnostic Meta-Learning (MAML), a method that consists of two optimization loops, with the outer loop finding a meta-initialization, from which the inner loop can efficiently learn new tasks. Despite MAML’s popularity, a fundamental open question remains – is the effectiveness of MAML due to the meta-initialization being primed for rapid learning (large, efficient changes in the representations) or due to feature reuse, with the meta-initialization already containing high quality features? We investigate this question, via ablation studies and analysis of the latent representations, finding that feature reuse is the dominant factor. This leads to the ANIL (Almost No Inner Loop) algorithm, a simplification of MAML where we remove the inner loop for all but the (task-specific) head of the underlying neural network. ANIL matches MAML’s performance on benchmark few-shot image classification and RL and offers computational improvements over MAML. We further study the precise contributions of the head and body of the network, showing that performance on the test tasks is entirely determined by the quality of the learned features, and we can remove even the head of the network (the NIL algorithm). We conclude with a discussion of the rapid learning vs feature reuse question for meta-learning algorithms more broadly.",
  "stance": 0.0
 },
 {
  "url": "https://openreview.net/forum?id=rkgOlCVYvB",
  "title": "Pure and Spurious Critical Points: a Geometric Study of Linear Networks",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "The critical locus of the loss function of a neural network is determined by the geometry of the functional space and by the parameterization of this space by the network’s weights. We introduce a natural distinction between pure critical points, which only depend on the functional space, and spurious critical points, which arise from the parameterization. We apply this perspective to revisit and extend the literature on the loss function of linear neural networks. For this type of network, the functional space is either the set of all linear maps from input to output space, or a determinantal variety, i.e., a set of linear maps with bounded rank. We use geometric properties of determinantal varieties to derive new results on the landscape of linear networks with different loss functions and different parameterizations. Our analysis clearly illustrates that the absence of “bad” local minima in the loss landscape of linear networks is due to two distinct phenomena that apply in different settings: it is true for arbitrary smooth convex losses in the case of architectures that can express all linear maps (“filling architectures”) but it holds only for the quadratic loss when the functional space is a determinantal variety (“non-filling architectures”). Without any assumption on the architecture, smooth convex losses may lead to landscapes with many bad minima.",
  "stance": -1.0
 },
 {
  "url": "https://openreview.net/forum?id=rkgfdeBYvH",
  "title": "Effect of Activation Functions on the Training of Overparametrized Neural Nets",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "It is well-known that overparametrized neural networks trained using gradientbased methods quickly achieve small training error with appropriate hyperparameter settings. Recent papers have proved this statement theoretically for highly overparametrized networks under reasonable assumptions. These results either assume that the activation function is ReLU or they depend on the minimum eigenvalue of a certain Gram matrix. In the latter case, existing works only prove that this minimum eigenvalue is non-zero and do not provide quantitative bounds which require that this eigenvalue be large. Empirically, a number of alternative activation functions have been proposed which tend to perform better than ReLU at least in some settings but no clear understanding has emerged. This state of affairs underscores the importance of theoretically understanding the impact of activation functions on training. In the present paper, we provide theoretical results about the effect of activation function on the training of highly overparametrized 2-layer neural networks. A crucial property that governs the performance of an activation is whether or not it is smooth: • For non-smooth activations such as ReLU,SELU,ELU, which are not smooth because there is a point where either the first order or second order derivative is discontinuous, all eigenvalues of the associated Gram matrix are large under minimal assumptions on the data. • For smooth activations such as tanh, swish, polynomial, which have derivatives of all orders at all points, the situation is more complex: if the subspace spanned by the data has small dimension then the minimum eigenvalue of the Gram matrix can be small leading to slow training. But if the dimension is large and the data satisfies another mild condition, then the eigenvalues are large. If we allow deep networks, then the small data dimension is not a limitation provided that the depth is sufficient. We discuss a number of extensions and applications of these results.",
  "stance": 0.0
 },
 {
  "url": "https://openreview.net/forum?id=rkl3m1BFDB",
  "title": "Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Saliency maps are frequently used to support explanations of the behavior of deep reinforcement learning (RL) agents. However, a review of how saliency maps are used in practice indicates that the derived explanations are often unfalsifiable and can be highly subjective. We introduce an empirical approach grounded in counterfactual reasoning to test the hypotheses generated from saliency maps and assess the degree to which they correspond to the semantics of RL environments. We use Atari games, a common benchmark for deep RL, to evaluate three types of saliency maps. Our results show the extent to which existing claims about Atari games can be evaluated and suggest that saliency maps are best viewed as an exploratory tool rather than an explanatory tool.",
  "stance": 0.1
 },
 {
  "url": "https://openreview.net/forum?id=rkl8dlHYvB",
  "title": "Learning to Group: A Bottom-Up Framework for 3D Part Discovery in Unseen Categories",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We address the problem of discovering 3D parts for objects in unseen categories. Being able to learn the geometry prior of parts and transfer this prior to unseen categories pose fundamental challenges on data-driven shape segmentation approaches. Formulated as a contextual bandit problem, we propose a learningbased agglomerative clustering framework which learns a grouping policy to progressively group small part proposals into bigger ones in a bottom-up fashion. At the core of our approach is to restrict the local context for extracting part-level features, which encourages the generalizability to unseen categories. On the largescale fine-grained 3D part dataset, PartNet, we demonstrate that our method can transfer knowledge of parts learned from 3 training categories to 21 unseen testing categories without seeing any annotated samples. Quantitative comparisons against four shape segmentation baselines shows that our approach achieve the state-of-the-art performance.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=rklTmyBKPH",
  "title": "Fast Neural Network Adaptation via Parameter Remapping and Architecture Search",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Deep neural networks achieve remarkable performance in many computer vision tasks. Most state-of-the-art (SOTA) semantic segmentation and object detection approaches reuse neural network architectures designed for image classification as the backbone, commonly pre-trained on ImageNet. However, performance gains can be achieved by designing network architectures specifically for detection and segmentation, as shown by recent neural architecture search (NAS) research for detection and segmentation. One major challenge though, is that ImageNet pre-training of the search space representation (a.k.a. super network) or the searched networks incurs huge computational cost. In this paper, we propose a Fast Neural Network Adaptation (FNA) method, which can adapt both the architecture and parameters of a seed network (e.g. a high performing manually designed backbone) to become a network with different depth, width, or kernels via a Parameter Remapping technique, making it possible to utilize NAS for detection/segmentation tasks a lot more efficiently. In our experiments, we conduct FNA on MobileNetV2 to obtain new networks for both segmentation and detection that clearly out-perform existing networks designed both manually and by NAS. The total computation cost of FNA is significantly less than SOTA segmentation/detection NAS approaches: 1737× less than DPC, 6.8× less than Auto-DeepLab and 7.4× less than DetNAS. The code is available at https://github.com/JaminFong/FNA.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=rkxDoJBYPB",
  "title": "Reinforced Genetic Algorithm Learning for Optimizing Computation Graphs",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We present a deep reinforcement learning approach to minimizing the execution cost of neural network computation graphs in an optimizing compiler. Unlike earlier learning-based works that require training the optimizer on the same graph to be optimized, we propose a learning approach that trains an optimizer offline and then generalizes to previously unseen graphs without further training. This allows our approach to produce high-quality execution decisions on real-world TensorFlow graphs in seconds instead of hours. We consider two optimization tasks for computation graphs: minimizing running time and peak memory usage. In comparison to an extensive set of baselines, our approach achieves significant improvements over classical and other learning-based methods on these two tasks.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=ryeFY0EFwS",
  "title": "Coherent Gradients: An Approach to Understanding Generalization in Gradient Descent-based Optimization",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "An open question in the Deep Learning community is why neural networks trained with Gradient Descent generalize well on real datasets even though they are capable of fitting random data. We propose an approach to answering this question based on a hypothesis about the dynamics of gradient descent that we call Coherent Gradients: Gradients from similar examples are similar and so the overall gradient is stronger in certain directions where these reinforce each other. Thus changes to the network parameters during training are biased towards those that (locally) simultaneously benefit many examples when such similarity exists. We support this hypothesis with heuristic arguments and perturbative experiments and outline how this can explain several common empirical observations about Deep Learning. Furthermore, our analysis is not just descriptive, but prescriptive. It suggests a natural modification to gradient descent that can greatly reduce overfitting.",
  "stance": 0.6
 },
 {
  "url": "https://openreview.net/forum?id=rygGQyrFvH",
  "title": "The Curious Case of Neural Text Degeneration",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Despite considerable advances in neural language modeling, it remains an open question what the best decoding strategy is for text generation from a language model (e.g. to generate a story). The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, maximization-based decoding methods such as beam search lead to degeneration — output text that is bland, incoherent, or gets stuck in repetitive loops. To address this we propose Nucleus Sampling, a simple but effective method to draw considerably higher quality text out of neural language models than previous decoding strategies. Our approach avoids text degeneration by truncating the unreliable tail of the probability distribution, sampling from the dynamic nucleus of tokens containing the vast majority of the probability mass. To properly examine current maximization-based and stochastic decoding methods, we compare generations from each of these methods to the distribution of human text along several axes such as likelihood, diversity, and repetition. Our results show that (1) maximization is an inappropriate decoding objective for openended text generation, (2) the probability distributions of the best current language models have an unreliable tail which needs to be truncated during generation and (3) Nucleus Sampling is currently the best available decoding strategy for generating long-form text that is both high-quality — as measured by human evaluation — and as diverse as human-written text.",
  "stance": 0.9
 },
 {
  "url": "https://openreview.net/forum?id=ryxdEkHtPS",
  "title": "A Closer Look at Deep Policy Gradients",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We study how the behavior of deep policy gradient algorithms reflects the conceptual framework motivating their development. To this end, we propose a finegrained analysis of state-of-the-art methods based on key elements of this framework: gradient estimation, value prediction, and optimization landscapes. Our results show that the behavior of deep policy gradient algorithms often deviates from what their motivating framework would predict: the surrogate objective does not match the true reward landscape, learned value estimators fail to fit the true value function, and gradient estimates poorly correlate with the “true” gradient. The mismatch between predicted and empirical behavior we uncover highlights our poor understanding of current methods, and indicates the need to move beyond current benchmark-centric evaluation methods.",
  "stance": -1.0
 },
 {
  "url": "https://openreview.net/forum?id=-N7PBXqOUJZ",
  "title": "Lipschitz Recurrent Neural Networks",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Viewing recurrent neural networks (RNNs) as continuous-time dynamical systems, we propose a recurrent unit that describes the hidden state’s evolution with two parts: a well-understood linear component plus a Lipschitz nonlinearity. This particular functional form facilitates stability analysis of the long-term behavior of the recurrent unit using tools from nonlinear systems theory. In turn, this enables architectural design decisions before experimentation. Sufficient conditions for global stability of the recurrent unit are obtained, motivating a novel scheme for constructing hidden-to-hidden matrices. Our experiments demonstrate that the Lipschitz RNN can outperform existing recurrent units on a range of benchmark tasks, including computer vision, language modeling and speech prediction tasks. Finally, through Hessian-based analysis we demonstrate that our Lipschitz recurrent unit is more robust with respect to input and parameter perturbations as compared to other continuous-time RNNs.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=1rxHOBjeDUW",
  "title": "Drop-Bottleneck: Learning Discrete Compressed Representation for Noise-Robust Exploration",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "We propose a novel information bottleneck (IB) method named Drop-Bottleneck, which discretely drops features that are irrelevant to the target variable. DropBottleneck not only enjoys a simple and tractable compression objective but also additionally provides a deterministic compressed representation of the input variable, which is useful for inference tasks that require consistent representation. Moreover, it can jointly learn a feature extractor and select features considering each feature dimension’s relevance to the target task, which is unattainable by most neural network-based IB methods. We propose an exploration method based on Drop-Bottleneck for reinforcement learning tasks. In a multitude of noisy and reward sparse maze navigation tasks in VizDoom (Kempka et al., 2016) and DMLab (Beattie et al., 2016), our exploration method achieves state-of-the-art performance. As a new IB framework, we demonstrate that Drop-Bottleneck outperforms Variational Information Bottleneck (VIB) (Alemi et al., 2017) in multiple aspects including adversarial robustness and dimensionality reduction.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=30EvkP2aQLD",
  "title": "What are the Statistical Limits of Offline RL with Linear Function Approximation?",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Offline reinforcement learning seeks to utilize offline (observational) data to guide the learning of (causal) sequential decision making strategies. The hope is that offline reinforcement learning coupled with function approximation methods (to deal with the curse of dimensionality) can provide a means to help alleviate the excessive sample complexity burden in modern sequential decision making problems. However, the extent to which this broader approach can be effective is not well understood, where the literature largely consists of sufficient conditions. This work focuses on the basic question of what are necessary representational and distributional conditions that permit provable sample-efficient offline reinforcement learning. Perhaps surprisingly, our main result shows that even if: i) we have realizability in that the true value function of every policy is linear in a given set of features and 2) our off-policy data has good coverage over all features (under a strong spectral condition), any algorithm still (information-theoretically) requires a number of offline samples that is exponential in the problem horizon to nontrivially estimate the value of any given policy. Our results highlight that sampleefficient offline policy evaluation is not possible unless significantly stronger conditions hold; such conditions include either having low distribution shift (where the offline data distribution is close to the distribution of the policy to be evaluated) or significantly stronger representational conditions (beyond realizability).",
  "stance": -1.0
 },
 {
  "url": "https://openreview.net/forum?id=42kiJ7n_8xO",
  "title": "The geometry of integration in text classification RNNs",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Despite the widespread application of recurrent neural networks (RNNs), a unified understanding of how RNNs solve particular tasks remains elusive. In particular, it is unclear what dynamical patterns arise in trained RNNs, and how those patterns depend on the training dataset or task. This work addresses these questions in the context of text classification, building on earlier work studying the dynamics of binary sentiment-classification networks (Maheswaranathan et al., 2019). We study text-classification tasks beyond the binary case, exploring the dynamics of RNNs trained on both natural and synthetic datasets. These dynamics, which we find to be both interpretable and low-dimensional, share a common mechanism across architectures and datasets: specifically, these text-classification networks use low-dimensional attractor manifolds to accumulate evidence for each class as they process the text. The dimensionality and geometry of the attractor manifold are determined by the structure of the training dataset, with the dimensionality reflecting the number of scalar quantities the network remembers in order to classify. In categorical classification, for example, we show that this dimensionality is one less than the number of classes. Correlations in the dataset, such as those induced by ordering, can further reduce the dimensionality of the attractor manifold; we show how to predict this reduction using simple word-count statistics computed on the training dataset. To the degree that integration of evidence towards a decision is a common computational primitive, this work continues to lay the foundation for using dynamical systems techniques to study the inner workings of RNNs.",
  "stance": 0.0
 },
 {
  "url": "https://openreview.net/forum?id=71zCSP_HuBN",
  "title": "Individually Fair Rankings",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "We develop an algorithm to train individually fair learning-to-rank (LTR) models. The proposed approach ensures items from minority groups appear alongside similar items from majority groups. This notion of fair ranking is based on the definition of individual fairness from supervised learning and is more nuanced than prior fair LTR approaches that simply ensure the ranking model provides underrepresented items with a basic level of exposure. The crux of our method is an optimal transport-based regularizer that enforces individual fairness and an efficient algorithm for optimizing the regularizer. We show that our approach leads to certifiably individually fair LTR models and demonstrate the efficacy of our method on ranking tasks subject to demographic biases.",
  "stance": 0.8
 },
 {
  "url": "https://openreview.net/forum?id=8nl0k08uMi",
  "title": "Selectivity considered harmful: evaluating the causal impact of class selectivity in DNNs",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "The properties of individual neurons are often analyzed in order to understand the biological and artificial neural networks in which they’re embedded. Class selectivity—typically defined as how different a neuron’s responses are across different classes of stimuli or data samples—is commonly used for this purpose. However, it remains an open question whether it is necessary and/or sufficient for deep neural networks (DNNs) to learn class selectivity in individual units. We investigated the causal impact of class selectivity on network function by directly regularizing for or against class selectivity. Using this regularizer to reduce class selectivity across units in convolutional neural networks increased test accuracy by over 2% in ResNet18 and 1% in ResNet50 trained on Tiny ImageNet. For ResNet20 trained on CIFAR10 we could reduce class selectivity by a factor of 2.5 with no impact on test accuracy, and reduce it nearly to zero with only a small (∼2%) drop in test accuracy. In contrast, regularizing to increase class selectivity significantly decreased test accuracy across all models and datasets. These results indicate that class selectivity in individual units is neither sufficient nor strictly necessary, and can even impair DNN performance. They also encourage caution when focusing on the properties of single units as representative of the mechanisms by which DNNs function.",
  "stance": -1.0
 },
 {
  "url": "https://openreview.net/forum?id=9FWas6YbmB3",
  "title": "DrNAS: Dirichlet Neural Architecture Search",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "This paper proposes a novel differentiable architecture search method by formulating it into a distribution learning problem. We treat the continuously relaxed architecture mixing weight as random variables, modeled by Dirichlet distribution. With recently developed pathwise derivatives, the Dirichlet parameters can be easily optimized with gradient-based optimizer in an end-to-end manner. This formulation improves the generalization ability and induces stochasticity that naturally encourages exploration in the search space. Furthermore, to alleviate the large memory consumption of differentiable NAS, we propose a simple yet effective progressive learning scheme that enables searching directly on large-scale tasks, eliminating the gap between search and evaluation phases. Extensive experiments demonstrate the effectiveness of our method. Specifically, we obtain a test error of 2.46% for CIFAR-10, 23.7% for ImageNet under the mobile setting. On NASBench-201, we also achieve state-of-the-art results on all three datasets and provide insights for the effective design of neural architecture search algorithms.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=9SS69KwomAM",
  "title": "Solving Compositional Reinforcement Learning Problems via Task Reduction",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "We propose a novel learning paradigm, Self-Imitation via Reduction (SIR), for solving compositional reinforcement learning problems. SIR is based on two core ideas: task reduction and self-imitation. Task reduction tackles a hard-to-solve task by actively reducing it to an easier task whose solution is known by the RL agent. Once the original hard task is successfully solved by task reduction, the agent naturally obtains a self-generated solution trajectory to imitate. By continuously collecting and imitating such demonstrations, the agent is able to progressively expand the solved subspace in the entire task space. Experiment results show that SIR can significantly accelerate and improve learning on a variety of challenging sparse-reward continuous-control problems with compositional structures. Code and videos are available at https://sites.google.com/ view/sir-compositional.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=BbNIbVPJ-42",
  "title": "The Risks of Invariant Risk Minimization",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Invariant Causal Prediction (Peters et al., 2016) is a technique for out-of-distribution generalization which assumes that some aspects of the data distribution vary across the training set but that the underlying causal mechanisms remain constant. Recently, Arjovsky et al. (2019) proposed Invariant Risk Minimization (IRM), an objective based on this idea for learning deep, invariant features of data which are a complex function of latent variables; many alternatives have subsequently been suggested. However, formal guarantees for all of these works are severely lacking. In this paper, we present the first analysis of classification under the IRM objective—as well as these recently proposed alternatives—under a fairly natural and general model. In the linear case, we give simple conditions under which the optimal solution succeeds or, more often, fails to recover the optimal invariant predictor. We furthermore present the very first results in the non-linear regime: we demonstrate that IRM can fail catastrophically unless the test data are sufficiently similar to the training distribution—this is precisely the issue that it was intended to solve. Thus, in this setting we find that IRM and its alternatives fundamentally do not improve over standard Empirical Risk Minimization.",
  "stance": -1.0
 },
 {
  "url": "https://openreview.net/forum?id=Cb54AMqHQFP",
  "title": "Network Pruning That Matters: A Case Study on Retraining Variants",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Network pruning is an effective method to reduce the computational expense of over-parameterized neural networks for deployment on low-resource systems. Recent state-of-the-art techniques for retraining pruned networks such as weight rewinding and learning rate rewinding have been shown to outperform the traditional fine-tuning technique in recovering the lost accuracy (Renda et al., 2020), but so far it is unclear what accounts for such performance. In this work, we conduct extensive experiments to verify and analyze the uncanny effectiveness of learning rate rewinding. We find that the reason behind the success of learning rate rewinding is the usage of a large learning rate. Similar phenomenon can be observed in other learning rate schedules that involve large learning rates, e.g., the 1-cycle learning rate schedule (Smith & Topin, 2019). By leveraging the right learning rate schedule in retraining, we demonstrate a counter-intuitive phenomenon in that randomly pruned networks could even achieve better performance than methodically pruned networks (fine-tuned with the conventional approach). Our results emphasize the cruciality of the learning rate schedule in pruned network retraining – a detail often overlooked by practioners during the implementation of network pruning.",
  "stance": 0.5
 },
 {
  "url": "https://openreview.net/forum?id=Cz3dbFm5u-",
  "title": "SAFENet: A Secure, Accurate and Fast Neural Network Inference",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "The advances in neural networks have driven many companies to provide prediction services to users in a wide range of applications. However, current prediction systems raise privacy concerns regarding the user’s private data. A cryptographic neural network inference service is an efficient way to allow two parties to execute neural network inference without revealing either party’s data or model. Nevertheless, existing cryptographic neural network inference services suffer from enormous running latency; in particular, the latency of communication-expensive cryptographic activation function is 3 orders of magnitude higher than plaintextdomain activation function. And activations are the necessary components of the modern neural networks. Therefore, slow cryptographic activation has become the primary obstacle of efficient cryptographic inference. In this paper, we propose a new technique, called SAFENet, to enable a Secure, Accurate and Fast nEural Network inference service. To speedup secure inference and guarantee inference accuracy, SAFENet includes channel-wise activation approximation with multiple-degree options. This is implemented by keeping the most useful activation channels and replacing the remaining, less useful, channels with variousdegree polynomials. SAFENet also supports mixed-precision activation approximation by automatically assigning different replacement ratios to various layer; further increasing the approximation ratio and reducing inference latency. Our experimental results show SAFENet obtains the state-of-the-art inference latency and performance, reducing latency by 38% ∼ 61% or improving accuracy by 1.8% ∼ 4% over prior techniques on various encrypted datasets.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=EGdFhBzmAwB",
  "title": "Generalization bounds via distillation",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "This paper theoretically investigates the following empirical phenomenon: given a high-complexity network with poor generalization bounds, one can distill it into a network with nearly identical predictions but low complexity and vastly smaller generalization bounds. The main contribution is an analysis showing that the original network inherits this good generalization bound from its distillation, assuming the use of well-behaved data augmentation. This bound is presented both in an abstract and in a concrete form, the latter complemented by a reduction technique to handle modern computation graphs featuring convolutional layers, fully-connected layers, and skip connections, to name a few. To round out the story, a (looser) classical uniform convergence analysis of compression is also presented, as well as a variety of experiments on cifar10 and mnist demonstrating similar generalization performance between the original network and its distillation.",
  "stance": 0.0
 },
 {
  "url": "https://openreview.net/forum?id=EMHoBG0avc1",
  "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "We propose a simple and efficient multi-hop dense retrieval approach for answering complex open-domain questions, which achieves state-of-the-art performance on two multi-hop datasets, HotpotQA and multi-evidence FEVER. Contrary to previous work, our method does not require access to any corpus-specific information, such as inter-document hyperlinks or human-annotated entity markers, and can be applied to any unstructured text corpus. Our system also yields a much better efficiency-accuracy trade-off, matching the best published accuracy on HotpotQA while being 10 times faster at inference time.1",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=FOyuZ26emy",
  "title": "A Critique of Self-Expressive Deep Subspace Clustering",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Subspace clustering is an unsupervised clustering technique designed to cluster data that is supported on a union of linear subspaces, with each subspace defining a cluster with dimension lower than the ambient space. Many existing formulations for this problem are based on exploiting the self-expressive property of linear subspaces, where any point within a subspace can be represented as linear combination of other points within the subspace. To extend this approach to data supported on a union of non-linear manifolds, numerous studies have proposed learning an embedding of the original data using a neural network which is regularized by a self-expressive loss function on the data in the embedded space to encourage a union of linear subspaces prior on the data in the embedded space. Here we show that there are a number of potential flaws with this approach which have not been adequately addressed in prior work. In particular, we show the model formulation is often ill-posed in that it can lead to a degenerate embedding of the data, which need not correspond to a union of subspaces at all and is poorly suited for clustering. We validate our theoretical results experimentally and also repeat prior experiments reported in the literature, where we conclude that a significant portion of the previously claimed performance benefits can be attributed to an ad-hoc post processing step rather than the deep subspace clustering model.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=HHiiQKWsOcV",
  "title": "Explaining the Efficacy of Counterfactually Augmented Data",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "In attempts to produce machine learning models less reliant on spurious patterns in NLP datasets, researchers have recently proposed curating counterfactually augmented data (CAD) via a human-in-the-loop process in which given some documents and their (initial) labels, humans must revise the text to make a counterfactual label applicable. Importantly, edits that are not necessary to flip the applicable label are prohibited. Models trained on the augmented (original and revised) data appear, empirically, to rely less on semantically irrelevant words and to generalize better out of domain. While this work draws loosely on causal thinking, the underlying causal model (even at an abstract level) and the principles underlying the observed out-of-domain improvements remain unclear. In this paper, we introduce a toy analog based on linear Gaussian models, observing interesting relationships between causal models, measurement noise, out-of-domain generalization, and reliance on spurious signals. Our analysis provides some insights that help to explain the efficacy of CAD. Moreover, we develop the hypothesis that while adding noise to causal features should degrade both in-domain and out-ofdomain performance, adding noise to non-causal features should lead to relative improvements in out-of-domain performance. This idea inspires a speculative test for determining whether a feature attribution technique has identified the causal spans. If adding noise (e.g., by random word flips) to the highlighted spans degrades both in-domain and out-of-domain performance on a battery of challenge datasets, but adding noise to the complement gives improvements out-of-domain, this suggests we have identified causal spans. Thus, we present a large-scale empirical study comparing spans edited to create CAD to those selected by attention and saliency maps. Across numerous challenge domains and models, we find that the hypothesized phenomenon is pronounced for CAD.",
  "stance": 0.5
 },
 {
  "url": "https://openreview.net/forum?id=Ig-VyQc-MLK",
  "title": "Pruning Neural Networks at Initialization: Why Are We Missing the Mark?",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Recent work has explored the possibility of pruning neural networks at initialization. We assess proposals for doing so: SNIP (Lee et al., 2019), GraSP (Wang et al., 2020), SynFlow (Tanaka et al., 2020), and magnitude pruning. Although these methods surpass the trivial baseline of random pruning, we find that they remain below the accuracy of magnitude pruning after training. We show that, unlike magnitude pruning after training, randomly shuffling the weights these methods prune within each layer or sampling new initial values preserves or improves accuracy. As such, the per-weight pruning decisions made by these methods can be replaced by a per-layer choice of the fraction of weights to prune. This property suggests broader challenges with the underlying pruning heuristics, the desire to prune at initialization, or both.",
  "stance": 0.6
 },
 {
  "url": "https://openreview.net/forum?id=IrM64DGB21",
  "title": "On the role of planning in model-based deep reinforcement learning",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Model-based planning is often thought to be necessary for deep, careful reasoning and generalization in artificial agents. While recent successes of model-based reinforcement learning (MBRL) with deep function approximation have strengthened this hypothesis, the resulting diversity of model-based methods has also made it difficult to track which components drive success and why. In this paper, we seek to disentangle the contributions of recent methods by focusing on three questions: (1) How does planning benefit MBRL agents? (2) Within planning, what choices drive performance? (3) To what extent does planning improve generalization? To answer these questions, we study the performance of MuZero [58], a state-of-the-art MBRL algorithm with strong connections and overlapping components with many other MBRL algorithms. We perform a number of interventions and ablations of MuZero across a wide range of environments, including control tasks, Atari, and 9x9 Go. Our results suggest the following: (1) Planning is most useful in the learning process, both for policy updates and for providing a more useful data distribution. (2) Using shallow trees with simple Monte-Carlo rollouts is as performant as more complex methods, except in the most difficult reasoning tasks. (3) Planning alone is insufficient to drive strong generalization. These results indicate where and how to utilize planning in reinforcement learning settings, and highlight a number of open questions for future MBRL research.",
  "stance": 0.0
 },
 {
  "url": "https://openreview.net/forum?id=LGgdb4TS4Z",
  "title": "Topology-Aware Segmentation Using Discrete Morse Theory",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "In the segmentation of fine-scale structures from natural and biomedical images, per-pixel accuracy is not the only metric of concern. Topological correctness, such as vessel connectivity and membrane closure, is crucial for downstream analysis tasks. In this paper, we propose a new approach to train deep image segmentation networks for better topological accuracy. In particular, leveraging the power of discrete Morse theory (DMT), we identify global structures, including 1D skeletons and 2D patches, which are important for topological accuracy. Trained with a novel loss based on these global structures, the network performance is significantly improved especially near topologically challenging locations (such as weak spots of connections and membranes). On diverse datasets, our method achieves superior performance on both the DICE score and topological metrics.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=LVotkZmYyDi",
  "title": "Proximal Gradient Descent-Ascent: Variable Convergence under KŁ Geometry",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "The gradient descent-ascent (GDA) algorithm has been widely applied to solve minimax optimization problems. In order to achieve convergent policy parameters for minimax optimization, it is important that GDA generates convergent variable sequences rather than convergent sequences of function values or gradient norms. However, the variable convergence of GDA has been proved only under convexity geometries, and there lacks understanding for general nonconvex minimax optimization. This paper fills such a gap by studying the convergence of a more general proximal-GDA for regularized nonconvex-strongly-concave minimax optimization. Specifically, we show that proximal-GDA admits a novel Lyapunov function, which monotonically decreases in the minimax optimization process and drives the variable sequence to a critical point. By leveraging this Lyapunov function and the KŁ geometry that parameterizes the local geometries of general nonconvex functions, we formally establish the variable convergence of proximal-GDA to a critical point x∗, i.e., xt → x∗, yt → y∗(x∗). Furthermore, over the full spectrum of the KŁ-parameterized geometry, we show that proximal-GDA achieves different types of convergence rates ranging from sublinear convergence up to finite-step convergence, depending on the geometry associated with the KŁ parameter. This is the first theoretical result on the variable convergence for nonconvex minimax optimization.",
  "stance": 0.5
 },
 {
  "url": "https://openreview.net/forum?id=NQbnPjPYaG6",
  "title": "On the Impossibility of Global Convergence in Multi-Loss Optimization",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Under mild regularity conditions, gradient-based methods converge globally to a critical point in the single-loss setting. This is known to break down for vanilla gradient descent when moving to multi-loss optimization, but can we hope to build some algorithm with global guarantees? We negatively resolve this open problem by proving that desirable convergence properties cannot simultaneously hold for any algorithm. Our result has more to do with the existence of games with no satisfactory outcomes, than with algorithms per se. More explicitly we construct a two-player game with zero-sum interactions whose losses are both coercive and analytic, but whose only simultaneous critical point is a strict maximum. Any ‘reasonable’ algorithm, defined to avoid strict maxima, will therefore fail to converge. This is fundamentally different from single losses, where coercivity implies existence of a global minimum. Moreover, we prove that a wide range of existing gradient-based methods almost surely have bounded but non-convergent iterates in a constructed zero-sum game for suitably small learning rates. It nonetheless remains an open question whether such behavior can arise in high-dimensional games of interest to ML practitioners, such as GANs or multi-agent RL.",
  "stance": -1.0
 },
 {
  "url": "https://openreview.net/forum?id=Naqw7EHIfrv",
  "title": "Representation Learning for Sequence Data with Deep Autoencoding Predictive Components",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "We propose Deep Autoencoding Predictive Components (DAPC) – a selfsupervised representation learning method for sequence data, based on the intuition that useful representations of sequence data should exhibit a simple structure in the latent space. We encourage this latent structure by maximizing an estimate of predictive information of latent feature sequences, which is the mutual information between the past and future windows at each time step. In contrast to the mutual information lower bound commonly used by contrastive learning, the estimate of predictive information we adopt is exact under a Gaussian assumption. Additionally, it can be computed without negative sampling. To reduce the degeneracy of the latent space extracted by powerful encoders and keep useful information from the inputs, we regularize predictive information learning with a challenging masked reconstruction loss. We demonstrate that our method recovers the latent space of noisy dynamical systems, extracts predictive features for forecasting tasks, and improves automatic speech recognition when used to pretrain the encoder on large amounts of unlabeled data. 1",
  "stance": 0.8
 },
 {
  "url": "https://openreview.net/forum?id=O3Y56aqpChA",
  "title": "Self-training For Few-shot Transfer Across Extreme Task Differences",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Most few-shot learning techniques are pre-trained on a large, labeled “base dataset”. In problem domains where such large labeled datasets are not available for pre-training (e.g., X-ray, satellite images), one must resort to pre-training in a different “source” problem domain (e.g., ImageNet), which can be very different from the desired target task. Traditional few-shot and transfer learning techniques fail in the presence of such extreme differences between the source and target tasks. In this paper, we present a simple and effective solution to tackle this extreme domain gap: self-training a source domain representation on unlabeled data from the target domain. We show that this improves one-shot performance on the target domain by 2.9 points on average on the challenging BSCD-FSL benchmark consisting of datasets from multiple domains. Our code is available at https://github.com/cpphoo/STARTUP.",
  "stance": 0.9
 },
 {
  "url": "https://openreview.net/forum?id=OPyWRrcjVQw",
  "title": "Shapley explainability on the data manifold",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Explainability in AI is crucial for model development, compliance with regulation, and providing operational nuance to predictions. The Shapley framework for explainability attributes a model’s predictions to its input features in a mathematically principled and model-agnostic way. However, general implementations of Shapley explainability make an untenable assumption: that the model’s features are uncorrelated. In this work, we demonstrate unambiguous drawbacks of this assumption and develop two solutions to Shapley explainability that respect the data manifold. One solution, based on generative modelling, provides flexible access to data imputations; the other directly learns the Shapley value-function, providing performance and stability at the cost of flexibility. While “off-manifold” Shapley values can (i) give rise to incorrect explanations, (ii) hide implicit model dependence on sensitive attributes, and (iii) lead to unintelligible explanations in higher-dimensional data, on-manifold explainability overcomes these problems.",
  "stance": 0.5
 },
 {
  "url": "https://openreview.net/forum?id=PS3IMnScugk",
  "title": "Learning to Recombine and Resample Data For Compositional Generalization",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Flexible neural sequence models outperform grammarand automaton-based counterparts on a variety of tasks. However, neural models perform poorly in settings requiring compositional generalization beyond the training data—particularly to rare or unseen subsequences. Past work has found symbolic scaffolding (e.g. grammars or automata) essential in these settings. We describe R&R, a learned data augmentation scheme that enables a large category of compositional generalizations without appeal to latent symbolic structure. R&R has two components: recombination of original training examples via a prototype-based generative model and resampling of generated examples to encourage extrapolation. Training an ordinary neural sequence model on a dataset augmented with recombined and resampled examples significantly improves generalization in two language processing problems—instruction following (SCAN) and morphological analysis (SIGMORPHON 2018)—where R&R enables learning of new constructions and tenses from as few as eight initial examples.",
  "stance": 0.6
 },
 {
  "url": "https://openreview.net/forum?id=RSU17UoKfJF",
  "title": "R-GAP: Recursive Gradient Attack on Privacy",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Federated learning frameworks have been regarded as a promising approach to break the dilemma between demands on privacy and the promise of learning from large collections of distributed data. Many such frameworks only ask collaborators to share their local update of a common model, i.e. gradients, instead of exposing their raw data to other collaborators. However, recent optimization-based gradient attacks show that raw data can often be accurately recovered from gradients. It has been shown that minimizing the Euclidean distance between true gradients and those calculated from estimated data is often effective in fully recovering private data. However, there is a fundamental lack of theoretical understanding of how and when gradients can lead to unique recovery of original data. Our research fills this gap by providing a closed-form recursive procedure to recover data from gradients in deep neural networks. We name it Recursive Gradient Attack on Privacy (R-GAP). Experimental results demonstrate that R-GAP works as well as or even better than optimization-based approaches at a fraction of the computation under certain conditions. Additionally, we propose a Rank Analysis method, which can be used to estimate the risk of gradient attacks inherent in certain network architectures, regardless of whether an optimization-based or closed-form-recursive attack is used. Experimental results demonstrate the utility of the rank analysis towards improving the network’s security. Source code is available for download from https://github.com/JunyiZhu-AI/R-GAP.",
  "stance": 0.7
 },
 {
  "url": "https://openreview.net/forum?id=US-TP-xnXI",
  "title": "Structured Prediction as Translation between Augmented Natural Languages",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "We propose a new framework, Translation between Augmented Natural Languages (TANL), to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Instead of tackling the problem by training task-specific discriminative classifiers, we frame it as a translation task between augmented natural languages, from which the task-relevant information can be easily extracted. Our approach can match or outperform task-specific models on all tasks, and in particular, achieves new state-of-the-art results on joint entity and relation extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic role labeling (CoNLL-2005 and CoNLL2012). We accomplish this while using the same architecture and hyperparameters for all tasks and even when training a single model to solve all tasks at the same time (multi-task learning). Finally, we show that our framework can also significantly improve the performance in a low-resource regime, thanks to better use of label semantics.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=Xb8xvrtB8Ce",
  "title": "Bag of Tricks for Adversarial Training",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Adversarial training (AT) is one of the most effective strategies for promoting model robustness. However, recent benchmarks show that most of the proposed improvements on AT are less effective than simply early stopping the training procedure. This counter-intuitive fact motivates us to investigate the implementation details of tens of AT methods. Surprisingly, we find that the basic settings (e.g., weight decay, training schedule, etc.) used in these methods are highly inconsistent. In this work, we provide comprehensive evaluations on CIFAR-10, focusing on the effects of mostly overlooked training tricks and hyperparameters for adversarially trained models. Our empirical observations suggest that adversarial robustness is much more sensitive to some basic training settings than we thought. For example, a slightly different value of weight decay can reduce the model robust accuracy by more than 7%, which is probable to override the potential promotion induced by the proposed methods. We conclude a baseline training setting and re-implement previous defenses to achieve new state-of-the-art results1. These facts also appeal to more concerns on the overlooked confounders when benchmarking defenses.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=Y9McSeEaqUh",
  "title": "Predicting Classification Accuracy When Adding New Unobserved Classes",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Multiclass classifiers are often designed and evaluated only on a sample from the classes on which they will eventually be applied. Hence, their final accuracy remains unknown. In this work we study how a classifier’s performance over the initial class sample can be used to extrapolate its expected accuracy on a larger, unobserved set of classes. For this, we define a measure of separation between correct and incorrect classes that is independent of the number of classes: the reversed ROC (rROC), which is obtained by replacing the roles of classes and data-points in the common ROC. We show that the classification accuracy is a function of the rROC in multiclass classifiers, for which the learned representation of data from the initial class sample remains unchanged when new classes are added. Using these results we formulate a robust neural-network-based algorithm, CleaneX, which learns to estimate the accuracy of such classifiers on arbitrarily large sets of classes. Unlike previous methods, our method uses both the observed accuracies of the classifier and densities of classification scores, and therefore achieves remarkably better predictions than current state-of-the-art methods on both simulations and real datasets of object detection, face recognition, and brain decoding.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=YtMG5ex0ou",
  "title": "Tomographic Auto-Encoder: Unsupervised Bayesian Recovery of Corrupted Data",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "We propose a new probabilistic method for unsupervised recovery of corrupted data. Given a large ensemble of degraded samples, our method recovers accurate posteriors of clean values, allowing the exploration of the manifold of possible reconstructed data and hence characterising the underlying uncertainty. In this setting, direct application of classical variational methods often gives rise to collapsed densities that do not adequately explore the solution space. Instead, we derive our novel reduced entropy condition approximate inference method that results in rich posteriors. We test our model in a data recovery task under the common setting of missing values and noise, demonstrating superior performance to existing variational methods for imputation and de-noising with different real data sets. We further show higher classification accuracy after imputation, proving the advantage of propagating uncertainty to downstream tasks with our model.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=_WnwtieRHxM",
  "title": "Understanding the role of importance weighting for deep learning",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "The recent paper by Byrd & Lipton (2019), based on empirical observations, raises a major concern on the impact of importance weighting for the over-parameterized deep learning models. They observe that as long as the model can separate the training data, the impact of importance weighting diminishes as the training proceeds. Nevertheless, there lacks a rigorous characterization of this phenomenon. In this paper, we provide formal characterizations and theoretical justifications on the role of importance weighting with respect to the implicit bias of gradient descent and margin-based learning theory. We reveal both the optimization dynamics and generalization performance under deep learning models. Our work not only explains the various novel phenomenons observed for importance weighting in deep learning, but also extends to the studies where the weights are being optimized as part of the model, which applies to a number of topics under active research.",
  "stance": 0.5
 },
 {
  "url": "https://openreview.net/forum?id=_i3ASPp12WS",
  "title": "Online Adversarial Purification based on Self-supervised Learning",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Deep neural networks are known to be vulnerable to adversarial examples, where a perturbation in the input space leads to an amplified shift in the latent network representation. In this paper, we combine canonical supervised learning with selfsupervised representation learning, and present Self-supervised Online Adversarial Purification (SOAP), a novel defense strategy that uses a self-supervised loss to purify adversarial examples at test-time. Our approach leverages the labelindependent nature of self-supervised signals, and counters the adversarial perturbation with respect to the self-supervised tasks. SOAP yields competitive robust accuracy against state-of-the-art adversarial training and purification methods, with considerably less training complexity. In addition, our approach is robust even when adversaries are given knowledge of the purification defense strategy. To the best of our knowledge, our paper is the first that generalizes the idea of using self-supervised signals to perform online test-time purification.",
  "stance": 0.9
 },
 {
  "url": "https://openreview.net/forum?id=b7g3_ZMHnT0",
  "title": "Learning to Deceive Knowledge Graph Augmented Models via Targeted Perturbation",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Knowledge graphs (KGs) have helped neural models improve performance on various knowledge-intensive tasks, like question answering and item recommendation. By using attention over the KG, such KG-augmented models can also “explain” which KG information was most relevant for making a given prediction. In this paper, we question whether these models are really behaving as we expect. We show that, through a reinforcement learning policy (or even simple heuristics), one can produce deceptively perturbed KGs, which maintain the downstream performance of the original KG while significantly deviating from the original KG’s semantics and structure. Our findings raise doubts about KG-augmented models’ ability to reason about KG information and give sensible explanations.",
  "stance": -1.0
 },
 {
  "url": "https://openreview.net/forum?id=bM3L3I_853",
  "title": "AdaFuse: Adaptive Temporal Fusion Network for Efficient Action Recognition",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Temporal modelling is the key for efficient video action recognition. While understanding temporal information can improve recognition accuracy for dynamic actions, removing temporal redundancy and reusing past features can significantly save computation leading to efficient action recognition. In this paper, we introduce an adaptive temporal fusion network, called AdaFuse, that dynamically fuses channels from current and past feature maps for strong temporal modelling. Specifically, the necessary information from the historical convolution feature maps is fused with current pruned feature maps with the goal of improving both recognition accuracy and efficiency. In addition, we use a skipping operation to further reduce the computation cost of action recognition. Extensive experiments on Something V1&V2, Jester and Mini-Kinetics show that our approach can achieve about 40% computation savings with comparable accuracy to state-of-the-art methods. The project page can be found at https://mengyuest.github.io/AdaFuse/",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=c9-WeM-ceB",
  "title": "Saliency is a Possible Red Herring When Diagnosing Poor Generalization",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Poor generalization is one symptom of models that learn to predict target variables using spuriously-correlated image features present only in the training distribution instead of the true image features that denote a class. It is often thought that this can be diagnosed visually using attribution (aka saliency) maps. We study if this assumption is correct. In some prediction tasks, such as for medical images, one may have some images with masks drawn by a human expert, indicating a region of the image containing relevant information to make the prediction. We study multiple methods that take advantage of such auxiliary labels, by training networks to ignore distracting features which may be found outside of the region of interest. This mask information is only used during training and has an impact on generalization accuracy depending on the severity of the shift between the training and test distributions. Surprisingly, while these methods improve generalization performance in the presence of a covariate shift, there is no strong correspondence between the correction of attribution towards the features a human expert has labelled as important and generalization performance. These results suggest that the root cause of poor generalization may not always be spatially defined, and raise questions about the utility of masks as “attribution priors” as well as saliency maps for explainable predictions.",
  "stance": 0.0
 },
 {
  "url": "https://openreview.net/forum?id=g-wu9TMPODo",
  "title": "How Benign is Benign Overfitting ?",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "We investigate two causes for adversarial vulnerability in deep neural networks: bad data and (poorly) trained models. When trained with SGD, deep neural networks essentially achieve zero training error, even in the presence of label noise, while also exhibiting good generalization on natural test data, something referred to as benign overfitting (Bartlett et al., 2020; Chatterji & Long, 2020). However, these models are vulnerable to adversarial attacks. We identify label noise as one of the causes for adversarial vulnerability, and provide theoretical and empirical evidence in support of this. Surprisingly, we find several instances of label noise in datasets such as MNIST and CIFAR, and that robustly trained models incur training error on some of these, i.e. they don’t fit the noise. However, removing noisy labels alone does not suffice to achieve adversarial robustness. We conjecture that in part sub-optimal representation learning is also responsible for adversarial vulnerability. By means of simple theoretical setups, we show how the choice of representation can drastically affect adversarial robustness.",
  "stance": -0.5
 },
 {
  "url": "https://openreview.net/forum?id=gl3D-xY7wLq",
  "title": "Noise or Signal: The Role of Image Backgrounds in Object Recognition",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "We assess the tendency of state-of-the-art object recognition models to depend on signals from image backgrounds. We create a toolkit for disentangling foreground and background signal on ImageNet images, and find that (a) models can achieve non-trivial accuracy by relying on the background alone, (b) models often misclassify images even in the presence of correctly classified foregrounds—up to 88% of the time with adversarially chosen backgrounds, and (c) more accurate models tend to depend on backgrounds less. Our analysis of backgrounds brings us closer to understanding which correlations machine learning models use, and how they determine models’ out of distribution performance.",
  "stance": -1.0
 },
 {
  "url": "https://openreview.net/forum?id=hPWj1qduVw8",
  "title": "Learning Reasoning Paths over Semantic Graphs for Video-grounded Dialogues",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Compared to traditional visual question answering, video-grounded dialogues require additional reasoning over dialogue context to answer questions in a multiturn setting. Previous approaches to video-grounded dialogues mostly use dialogue context as a simple text input without modelling the inherent information flows at the turn level. In this paper, we propose a novel framework of Reasoning Paths in Dialogue Context (PDC). PDC model discovers information flows among dialogue turns through a semantic graph constructed based on lexical components in each question and answer. PDC model then learns to predict reasoning paths over this semantic graph. Our path prediction model predicts a path from the current turn through past dialogue turns that contain additional visual cues to answer the current question. Our reasoning model sequentially processes both visual and textual information through this reasoning path and the propagated features are used to generate the answer. Our experimental results demonstrate the effectiveness of our method and provide additional insights on how models use semantic dependencies in a dialogue context to retrieve visual cues.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=hsFN92eQEla",
  "title": "EVALUATION OF NEURAL ARCHITECTURES TRAINED WITH SQUARE LOSS VS CROSS-ENTROPY IN CLASSIFICATION TASKS",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Modern neural architectures for classification tasks are trained using the crossentropy loss, which is widely believed to be empirically superior to the square loss. In this work we provide evidence indicating that this belief may not be wellfounded. We explore several major neural architectures and a range of standard benchmark datasets for NLP, automatic speech recognition (ASR) and computer vision tasks to show that these architectures, with the same hyper-parameter settings as reported in the literature, perform comparably or better when trained with the square loss, even after equalizing computational resources. Indeed, we observe that the square loss produces better results in the dominant majority of NLP and ASR experiments. Cross-entropy appears to have a slight edge on computer vision tasks. We argue that there is little compelling empirical or theoretical evidence indicating a clear-cut advantage to the cross-entropy loss. Indeed, in our experiments, performance on nearly all non-vision tasks can be improved, sometimes significantly, by switching to the square loss. Furthermore, training with square loss appears to be less sensitive to the randomness in initialization. We posit that training using the square loss for classification needs to be a part of best practices of modern deep learning on equal footing with cross-entropy.",
  "stance": 0.5
 },
 {
  "url": "https://openreview.net/forum?id=hvdKKV2yt7T",
  "title": "Dataset Inference: Ownership Resolution in Machine Learning",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "With increasingly more data and computation involved in their training, machine learning models constitute valuable intellectual property. This has spurred interest in model stealing, which is made more practical by advances in learning with partial, little, or no supervision. Existing defenses focus on inserting unique watermarks in a model’s decision surface, but this is insufficient: the watermarks are not sampled from the training distribution and thus are not always preserved during model stealing. In this paper, we make the key observation that knowledge contained in the stolen model’s training set is what is common to all stolen copies. The adversary’s goal, irrespective of the attack employed, is always to extract this knowledge or its by-products. This gives the original model’s owner a strong advantage over the adversary: model owners have access to the original training data. We thus introduce dataset inference, the process of identifying whether a suspected model copy has private knowledge from the original model’s dataset, as a defense against model stealing. We develop an approach for dataset inference that combines statistical testing with the ability to estimate the distance of multiple data points to the decision boundary. Our experiments on CIFAR10, SVHN, CIFAR100 and ImageNet show that model owners can claim with confidence greater than 99% that their model (or dataset as a matter of fact) was stolen, despite only exposing 50 of the stolen model’s training points. Dataset inference defends against state-of-the-art attacks even when the adversary is adaptive. Unlike prior work, it does not require retraining or overfitting the defended model.1",
  "stance": 0.7
 },
 {
  "url": "https://openreview.net/forum?id=i80OPhOCVH2",
  "title": "On the Bottleneck of Graph Neural Networks and its Practical Implications",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Since the proposal of the graph neural network (GNN) by Gori et al. (2005) and Scarselli et al. (2008), one of the major problems in training GNNs was their struggle to propagate information between distant nodes in the graph. We propose a new explanation for this problem: GNNs are susceptible to a bottleneck when aggregating messages across a long path. This bottleneck causes the over-squashing of exponentially growing information into fixed-size vectors. As a result, GNNs fail to propagate messages originating from distant nodes and perform poorly when the prediction task depends on long-range interaction. In this paper, we highlight the inherent problem of over-squashing in GNNs: we demonstrate that the bottleneck hinders popular GNNs from fitting long-range signals in the training data; we further show that GNNs that absorb incoming edges equally, such as GCN and GIN, are more susceptible to over-squashing than GAT and GGNN; finally, we show that prior work, which extensively tuned GNN models of long-range problems, suffer from over-squashing, and that breaking the bottleneck improves their state-of-the-art results without any tuning or additional weights. Our code is available at https://github.com/tech-srl/bottleneck/ .",
  "stance": 0.8
 },
 {
  "url": "https://openreview.net/forum?id=iQQK02mxVIT",
  "title": "Why resampling outperforms reweighting for correcting sampling bias with stochastic gradients",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "A data set sampled from a certain population is biased if the subgroups of the population are sampled at proportions that are significantly different from their underlying proportions. Training machine learning models on biased data sets requires correction techniques to compensate for the bias. We consider two commonlyused techniques, resampling and reweighting, that rebalance the proportions of the subgroups to maintain the desired objective function. Though statistically equivalent, it has been observed that resampling outperforms reweighting when combined with stochastic gradient algorithms. By analyzing illustrative examples, we explain the reason behind this phenomenon using tools from dynamical stability and stochastic asymptotics. We also present experiments from regression, classification, and off-policy prediction to demonstrate that this is a general phenomenon. We argue that it is imperative to consider the objective function design and the optimization algorithm together while addressing the sampling bias.",
  "stance": 0.2
 },
 {
  "url": "https://openreview.net/forum?id=iWLByfvUhN",
  "title": "Decoupling Global and Local Representations via Invertible Generative Flows",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "In this work, we propose a new generative model that is capable of automatically decoupling global and local representations of images in an entirely unsupervised setting, by embedding a generative flow in the VAE framework to model the decoder. Specifically, the proposed model utilizes the variational auto-encoding framework to learn a (low-dimensional) vector of latent variables to capture the global information of an image, which is fed as a conditional input to a flow-based invertible decoder with architecture borrowed from style transfer literature. Experimental results on standard image benchmarks demonstrate the effectiveness of our model in terms of density estimation, image generation and unsupervised representation learning. Importantly, this work demonstrates that with only architectural inductive biases, a generative model with a likelihood-based objective is capable of learning decoupled representations, requiring no explicit supervision. The code for our model is available at https://github.com/XuezheMax/wolf.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=jh-rTtvkGeM",
  "title": "Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "We empirically demonstrate that full-batch gradient descent on neural network training objectives typically operates in a regime we call the Edge of Stability. In this regime, the maximum eigenvalue of the training loss Hessian hovers just above the value 2/(step size), and the training loss behaves non-monotonically over short timescales, yet consistently decreases over long timescales. Since this behavior is inconsistent with several widespread presumptions in the field of optimization, our findings raise questions as to whether these presumptions are relevant to neural network training. We hope that our findings will inspire future efforts aimed at rigorously understanding optimization at the Edge of Stability.",
  "stance": -0.5
 },
 {
  "url": "https://openreview.net/forum?id=kDnal_bbb-E",
  "title": "DialoGraph: Incorporating Interpretable Strategy-Graph Networks into Negotiation Dialogues",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "To successfully negotiate a deal, it is not enough to communicate fluently: pragmatic planning of persuasive negotiation strategies is essential. While modern dialogue agents excel at generating fluent sentences, they still lack pragmatic grounding and cannot reason strategically. We present DIALOGRAPH, a negotiation system that incorporates pragmatic strategies in a negotiation dialogue using graph neural networks. DIALOGRAPH explicitly incorporates dependencies between sequences of strategies to enable improved and interpretable prediction of next optimal strategies, given the dialogue context. Our graph-based method outperforms prior state-of-the-art negotiation models both in the accuracy of strategy/dialogue act prediction and in the quality of downstream dialogue response generation. We qualitatively show further benefits of learned strategy-graphs in providing explicit associations between effective negotiation strategies over the course of the dialogue, leading to interpretable and strategic dialogues.1",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=kyaIeYj4zZ",
  "title": "GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "We present GRAPPA, an effective pre-training approach for table semantic parsing that learns a compositional inductive bias in the joint representations of textual and tabular data. We construct synthetic question-SQL pairs over high-quality tables via a synchronous context-free grammar (SCFG). We pre-train GRAPPA on the synthetic data to inject important structural properties commonly found in table semantic parsing into the pre-training language model. To maintain the model’s ability to represent real-world data, we also include masked language modeling (MLM) on several existing table-and-language datasets to regularize our pre-training process. Our proposed pre-training strategy is much data-efficient. When incorporated with strong base semantic parsers, GRAPPA achieves new state-of-the-art results on four popular fully supervised and weakly supervised table semantic parsing tasks. The pre-trained embeddings can be downloaded at https://huggingface.co/Salesforce/grappa_large_jnt.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=l0V53bErniB",
  "title": "Combining Physics and Machine Learning for Network Flow Estimation",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "The flow estimation problem consists of predicting missing edge flows in a network (e.g., traffic, power, and water) based on partial observations. These missing flows depend both on the underlying physics (edge features and a flow conservation law) as well as the observed edge flows. This paper introduces an optimization framework for computing missing edge flows and solves the problem using bilevel optimization and deep learning. More specifically, we learn regularizers that depend on edge features (e.g., number of lanes in a road, resistance of a power line) using neural networks. Empirical results show that our method accurately predicts missing flows, outperforming the best baseline, and is able to capture relevant physical properties in traffic and power networks.",
  "stance": 0.8
 },
 {
  "url": "https://openreview.net/forum?id=lvRTC669EY_",
  "title": "Discovering Diverse Multi-Agent Strategic Behavior via Reward Randomization",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "We propose a simple, general and effective technique, Reward Randomization for discovering diverse strategic policies in complex multi-agent games. Combining reward randomization and policy gradient, we derive a new algorithm, RewardRandomized Policy Gradient (RPG). RPG is able to discover multiple distinctive human-interpretable strategies in challenging temporal trust dilemmas, including grid-world games and a real-world game Agar.io, where multiple equilibria exist but standard multi-agent policy gradient algorithms always converge to a fixed one with a sub-optimal payoff for every player even using state-of-the-art exploration techniques. Furthermore, with the set of diverse strategies from RPG, we can (1) achieve higher payoffs by fine-tuning the best policy from the set; and (2) obtain an adaptive agent by using this set of strategies as its training opponents. The source code and example videos can be found in our website: https://sites.google. com/view/staghuntrpg.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=m1CD7tPubNy",
  "title": "Mind the Pad -- CNNs Can Develop Blind Spots",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "We show how feature maps in convolutional networks are susceptible to spatial bias. Due to a combination of architectural choices, the activation at certain locations is systematically elevated or weakened. The major source of this bias is the padding mechanism. Depending on several aspects of convolution arithmetic, this mechanism can apply the padding unevenly, leading to asymmetries in the learned weights. We demonstrate how such bias can be detrimental to certain tasks such as small object detection: the activation is suppressed if the stimulus lies in the impacted area, leading to blind spots and misdetection. We propose solutions to mitigate spatial bias and demonstrate how they can improve model accuracy.",
  "stance": -0.1
 },
 {
  "url": "https://openreview.net/forum?id=qZzy5urZw9",
  "title": "Robust Overfitting may be mitigated by properly learned smoothening",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "A recent study (Rice et al., 2020) revealed overfitting to be a dominant phenomenon in adversarially robust training of deep networks, and that appropriate early-stopping of adversarial training (AT) could match the performance gains of most recent algorithmic improvements. This intriguing problem of robust overfitting motivates us to seek more remedies. As a pilot study, this paper investigates two empirical means to inject more learned smoothening during AT: one leveraging knowledge distillation and self-training to smooth the logits, the other performing stochastic weight averaging (Izmailov et al., 2018) to smooth the weights. Despite the embarrassing simplicity, the two approaches are surprisingly effective and hassle-free in mitigating robust overfitting. Experiments demonstrate that by plugging in them to AT, we can simultaneously boost the standard accuracy by 3.72% ∼ 6.68% and robust accuracy by 0.22% ∼ 2.03%, across multiple datasets (STL-10, SVHN, CIFAR-10, CIFAR-100, and Tiny ImageNet), perturbation types (`∞ and `2), and robustified methods (PGD, TRADES, and FSGM), establishing the new state-of-the-art bar in AT. We present systematic visualizations and analyses to dive into their possible working mechanisms. We also carefully exclude the possibility of gradient masking by evaluating our models’ robustness against transfer attacks. Codes are available at https: //github.com/VITA-Group/Alleviate-Robust-Overfitting.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=qbH974jKUVy",
  "title": "The role of Disentanglement in Generalisation",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Combinatorial generalisation — the ability to understand and produce novel combinations of familiar elements — is a core capacity of human intelligence that current AI systems struggle with. Recently, it has been suggested that learning disentangled representations may help address this problem. It is claimed that such representations should be able to capture the compositional structure of the world which can then be combined to support combinatorial generalisation. In this study, we systematically tested how the degree of disentanglement affects various forms of generalisation, including two forms of combinatorial generalisation that varied in difficulty. We trained three classes of variational autoencoders (VAEs) on two datasets on an unsupervised task by excluding combinations of generative factors during training. At test time we ask the models to reconstruct the missing combinations in order to measure generalisation performance. Irrespective of the degree of disentanglement, we found that the models supported only weak combinatorial generalisation. We obtained the same outcome when we directly input perfectly disentangled representations as the latents, and when we tested a model on a more complex task that explicitly required independent generative factors to be controlled. While learning disentangled representations does improve interpretability and sample efficiency in some downstream tasks, our results suggest that they are not sufficient for supporting more difficult forms of generalisation.",
  "stance": -0.5
 },
 {
  "url": "https://openreview.net/forum?id=rABUmU3ulQh",
  "title": "Learning to Generate 3D Shapes with Generative Cellular Automata",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "We present a probabilistic 3D generative model, named Generative Cellular Automata, which is able to produce diverse and high quality shapes. We formulate the shape generation process as sampling from the transition kernel of a Markov chain, where the sampling chain eventually evolves to the full shape of the learned distribution. The transition kernel employs the local update rules of cellular automata, effectively reducing the search space in a high-resolution 3D grid space by exploiting the connectivity and sparsity of 3D shapes. Our progressive generation only focuses on the sparse set of occupied voxels and their neighborhood, thus enabling the utilization of an expressive sparse convolutional network. We propose an effective training scheme to obtain the local homogeneous rule of generative cellular automata with sequences that are slightly different from the sampling chain but converge to the full shapes in the training data. Extensive experiments on probabilistic shape completion and shape generation demonstrate that our method achieves competitive performance against recent methods.",
  "stance": 0.9
 },
 {
  "url": "https://openreview.net/forum?id=tC6iW2UUbJf",
  "title": "What Makes Instance Discrimination Good for Transfer Learning?",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Contrastive visual pretraining based on the instance discrimination pretext task has made significant progress. Notably, recent work on unsupervised pretraining has shown to surpass the supervised counterpart for finetuning downstream applications such as object detection and segmentation. It comes as a surprise that image annotations would be better left unused for transfer learning. In this work, we investigate the following problems: What makes instance discrimination pretraining good for transfer learning? What knowledge is actually learned and transferred from these models? From this understanding of instance discrimination, how can we better exploit human annotation labels for pretraining? Our findings are threefold. First, what truly matters for the transfer is low-level and mid-level representations, not high-level representations. Second, the intra-category invariance enforced by the traditional supervised model weakens transferability by increasing task misalignment. Finally, supervised pretraining can be strengthened by following an exemplar-based approach without explicit constraints among the instances within the same category.",
  "stance": 0.0
 },
 {
  "url": "https://openreview.net/forum?id=uXl3bZLkr3c",
  "title": "Tent: Fully Test-Time Adaptation by Entropy Minimization",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "A model must adapt itself to generalize to new and different data during testing. In this setting of fully test-time adaptation the model has only the test data and its own parameters. We propose to adapt by test entropy minimization (tent1): we optimize the model for confidence as measured by the entropy of its predictions. Our method estimates normalization statistics and optimizes channel-wise affine transformations to update online on each batch. Tent reduces generalization error for image classification on corrupted ImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on ImageNet-C. Tent handles source-free domain adaptation on digit recognition from SVHN to MNIST/MNIST-M/USPS, on semantic segmentation from GTA to Cityscapes, and on the VisDA-C benchmark. These results are achieved in one epoch of test-time optimization without altering training.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=wb3wxCObbRT",
  "title": "Growing Efficient Deep Networks by Structured Continuous Sparsification",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "We develop an approach to growing deep network architectures over the course of training, driven by a principled combination of accuracy and sparsity objectives. Unlike existing pruning or architecture search techniques that operate on full-sized models or supernet architectures, our method can start from a small, simple seed architecture and dynamically grow and prune both layers and filters. By combining a continuous relaxation of discrete network structure optimization with a scheme for sampling sparse subnetworks, we produce compact, pruned networks, while also drastically reducing the computational expense of training. For example, we achieve 49.7% inference FLOPs and 47.4% training FLOPs savings compared to a baseline ResNet-50 on ImageNet, while maintaining 75.2% top-1 accuracy — all without any dedicated fine-tuning stage. Experiments across CIFAR, ImageNet, PASCAL VOC, and Penn Treebank, with convolutional networks for image classification and semantic segmentation, and recurrent networks for language modeling, demonstrate that we both train faster and produce more efficient networks than competing architecture pruning or search methods.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/Conferences/2010/papers/202.pdf",
  "title": "Exploiting Data-Independence for Fast Belief-Propagation.",
  "year": 2010,
  "venue": "ICML",
  "abstract": "Maximum a posteriori (MAP) inference in graphical models requires that we maximize the sum of two terms: a data-dependent term, encoding the conditional likelihood of a certain labeling given an observation, and a data-independent term, encoding some prior on labelings. Often, data-dependent factors contain fewer latent variables than dataindependent factors – for instance, many grid and tree-structured models contain only firstorder conditionals despite having pairwise priors. In this paper, we note that MAPinference in such models can be made substantially faster by appropriately preprocessing their data-independent terms. Our main result is to show that message-passing in any such pairwise model has an expected-case exponent of only 1.5 on the number of states per node, leading to significant improvements over existing quadratic-time solutions.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/Conferences/2010/papers/223.pdf",
  "title": "SVM Classifier Estimation from Group Probabilities.",
  "year": 2010,
  "venue": "ICML",
  "abstract": "A learning problem that has only recently gained attention in the machine learning community is that of learning a classifier from group probabilities. It is a learning task that lies somewhere between the well-known tasks of supervised and unsupervised learning, in the sense that for a set of observations we do not know the labels, but for some groups of observations, the frequency distribution of the label is known. This learning problem has important practical applications, for example in privacy-preserving data mining. This paper presents an approach to learn a classifier from group probabilities based on support vector regression and the idea of inverting a classifier calibration process. A detailed analysis will show that this new approach outperforms existing approaches.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/Conferences/2010/papers/227.pdf",
  "title": "Learning Sparse SVM for Feature Selection on Very High Dimensional Datasets.",
  "year": 2010,
  "venue": "ICML",
  "abstract": "A sparse representation of Support Vector Machines (SVMs) with respect to input features is desirable for many applications. In this paper, by introducing a 0-1 control variable to each input feature, l0-norm Sparse SVM (SSVM) is converted to a mixed integer programming (MIP) problem. Rather than directly solving this MIP, we propose an efficient cutting plane algorithm combining with multiple kernel learning to solve its convex relaxation. A global convergence proof for our method is also presented. Comprehensive experimental results on one synthetic and 10 real world datasets show that our proposed method can obtain better or competitive performance compared with existing SVM-based feature selection methods in term of sparsity and generalization performance. Moreover, our proposed method can effectively handle large-scale and extremely high dimensional problems.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/Conferences/2010/papers/233.pdf",
  "title": "Total Variation, Cheeger Cuts.",
  "year": 2010,
  "venue": "ICML",
  "abstract": "In this work, inspired by (Bühler & Hein, 2009), (Strang, 1983), and (Zhang et al., 2009), we give a continuous relaxation of the Cheeger cut problem on a weighted graph. We show that the relaxation is actually equivalent to the original problem. We then describe an algorithm for finding good cuts suggested by the similarities of the energy of the relaxed problem and various well studied energies in image processing. Finally we provide experimental validation of the proposed algorithm, demonstrating its efficiency in finding high quality cuts.",
  "stance": 0.9
 },
 {
  "url": "https://icml.cc/Conferences/2010/papers/352.pdf",
  "title": "Tree-Guided Group Lasso for Multi-Task Regression with Structured Sparsity.",
  "year": 2010,
  "venue": "ICML",
  "abstract": "We consider the problem of learning a sparse multi-task regression, where the structure in the outputs can be represented as a tree with leaf nodes as outputs and internal nodes as clusters of the outputs at multiple granularity. Our goal is to recover the common set of relevant inputs for each output cluster. Assuming that the tree structure is available as prior knowledge, we formulate this problem as a new multi-task regularized regression called tree-guided group lasso. Our structured regularization is based on a grouplasso penalty, where groups are defined with respect to the tree structure. We describe a systematic weighting scheme for the groups in the penalty such that each output variable is penalized in a balanced manner even if the groups overlap. We present an efficient optimization method that can handle a largescale problem. Using simulated and yeast datasets, we demonstrate that our method shows a superior performance in terms of both prediction errors and recovery of true sparsity patterns compared to other methods for multi-task learning.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/Conferences/2010/papers/505.pdf",
  "title": "Transfer Learning for Collective Link Prediction in Multiple Heterogenous Domains.",
  "year": 2010,
  "venue": "ICML",
  "abstract": "Link prediction is a key technique in many applications such as recommender systems, where potential links between users and items need to be predicted. A challenge in link prediction is the data sparsity problem. In this paper, we address this problem by jointly considering multiple heterogeneous link prediction tasks such as predicting links between users and different types of items including books, movies and songs, which we refer to as the collective link prediction (CLP) problem. We propose a nonparametric Bayesian framework for solving the CLP problem, which allows knowledge to be adaptively transferred across heterogeneous tasks while taking into account the similarities between tasks. We learn the inter-task similarity automatically. We also introduce link functions for different tasks to correct their biases and skewness of distributions in their link data. We conduct experiments on several real world datasets and demonstrate significant improvements over several existing state-of-the-art methods.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/Conferences/2010/papers/532.pdf",
  "title": "Learning the Linear Dynamical System with ASOS.",
  "year": 2010,
  "venue": "ICML",
  "abstract": "We develop a new algorithm, based on EM, for learning the Linear Dynamical System model. Called the method of Approximated Second-Order Statistics (ASOS) our approach achieves dramatically superior computational performance over standard EM through its use of approximations, which we justify with both intuitive explanations and rigorous convergence results. In particular, after an inexpensive precomputation phase, the iterations of ASOS can be performed in time independent of the length of the training dataset.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/Conferences/2010/papers/544.pdf",
  "title": "Active Learning for Networked Data.",
  "year": 2010,
  "venue": "ICML",
  "abstract": "We introduce a novel active learning algorithm for classification of network data. In this setting, training instances are connected by a set of links to form a network, the labels of linked nodes are correlated, and the goal is to exploit these dependencies and accurately label the nodes. This problem arises in many domains, including social and biological network analysis and document classification, and there has been much recent interest in methods that collectively classify the nodes in the network. While in many cases labeled examples are expensive, often network information is available. We show how an active learning algorithm can take advantage of network structure. Our algorithm effectively exploits the links between instances and the interaction between the local and collective aspects of a classifier to improve the accuracy of learning from fewer labeled examples. We experiment with two real-world benchmark collective classification domains, and show that we are able to achieve extremely accurate results even when only a small fraction of the data is labeled.",
  "stance": 0.7
 },
 {
  "url": "https://icml.cc/Conferences/2010/papers/556.pdf",
  "title": "A Fast Augmented Lagrangian Algorithm for Learning Low-Rank Matrices.",
  "year": 2010,
  "venue": "ICML",
  "abstract": "We propose a general and efficient algorithm for learning low-rank matrices. The proposed algorithm converges super-linearly and can keep the matrix to be learned in a compact factorized representation without the need of specifying the rank beforehand. Moreover, we show that the framework can be easily generalized to the problem of learning multiple matrices and general spectral regularization. Empirically we show that we can recover a 10,000×10,000 matrix from 1.2 million observations in about 5 minutes. Furthermore, we show that in a brain-computer interface problem, the proposed method can speed-up the optimization by two orders of magnitude against the conventional projected gradient method and produces more reliable solutions.",
  "stance": 0.9
 },
 {
  "url": "https://icml.cc/Conferences/2010/papers/587.pdf",
  "title": "Learning Efficiently with Approximate Inference via Dual Losses.",
  "year": 2010,
  "venue": "ICML",
  "abstract": "Many structured prediction tasks involve complex models where inference is computationally intractable, but where it can be well approximated using a linear programming relaxation. Previous approaches for learning for structured prediction (e.g., cuttingplane, subgradient methods, perceptron) repeatedly make predictions for some of the data points. These approaches are computationally demanding because each prediction involves solving a linear program to optimality. We present a scalable algorithm for learning for structured prediction. The main idea is to instead solve the dual of the structured prediction loss. We formulate the learning task as a convex minimization over both the weights and the dual variables corresponding to each data point. As a result, we can begin to optimize the weights even before completely solving any of the individual prediction problems. We show how the dual variables can be efficiently optimized using coordinate descent. Our algorithm is competitive with state-of-the-art methods such as stochastic subgradient and cutting-plane.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/Conferences/2010/papers/589.pdf",
  "title": "Bayes Optimal Multilabel Classification via Probabilistic Classifier Chains.",
  "year": 2010,
  "venue": "ICML",
  "abstract": "In the realm of multilabel classification (MLC), it has become an opinio communis that optimal predictive performance can only be achieved by learners that explicitly take label dependence into account. The goal of this paper is to elaborate on this postulate in a critical way. To this end, we formalize and analyze MLC within a probabilistic setting. Thus, it becomes possible to look at the problem from the point of view of risk minimization and Bayes optimal prediction. Moreover, inspired by our probabilistic setting, we propose a new method for MLC that generalizes and outperforms another approach, called classifier chains, that was recently introduced in the literature.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/Conferences/2010/papers/601.pdf",
  "title": "On the Interaction between Norm and Dimensionality: Multiple Regimes in Learning.",
  "year": 2010,
  "venue": "ICML",
  "abstract": "A learning problem might have several measures of complexity (e.g., norm and dimensionality) that affect the generalization error. What is the interaction between these complexities? Dimension-free learning theory bounds and parametric asymptotic analyses each provide a partial picture of the full learning curve. In this paper, we use high-dimensional asymptotics on two classical problems—mean estimation and linear regression—to explore the learning curve more completely. We show that these curves exhibit multiple regimes, where in each regime, the excess risk is controlled by a subset of the problem complexities.",
  "stance": 0.0
 },
 {
  "url": "https://icml.cc/Conferences/2010/papers/87.pdf",
  "title": "A Conditional Random Field for Multiple-Instance Learning.",
  "year": 2010,
  "venue": "ICML",
  "abstract": "We present MI-CRF, a conditional random field (CRF) model for multiple instance learning (MIL). MI-CRF models bags as nodes in a CRF with instances as their states. It combines discriminative unary instance classifiers and pairwise dissimilarity measures. We show that both forces improve the classification performance. Unlike other approaches, MI-CRF considers all bags jointly during training as well as during testing. This makes it possible to classify test bags in an imputation setup. The parameters of MI-CRF are learned using constraint generation. Furthermore, we show that MI-CRF can incorporate previous MIL algorithms to improve on their results. MICRF obtains competitive results on five standard MIL datasets.",
  "stance": 0.5
 },
 {
  "url": "https://icml.cc/Conferences/2010/papers/903.pdf",
  "title": "The Role of Machine Learning in Business Optimization.",
  "year": 2010,
  "venue": "ICML",
  "abstract": "In a trend that reflects the increasing demand for intelligent applications driven by business data, IBM today is building out a significant number of applications that leverage machine learning technologies to optimize business process decisions. This talk highlights this trend; and describes the many different ways in which leading edge machine learning concepts are being utilized in business applications developed by IBM for its internal use and for clients.",
  "stance": 0.0
 },
 {
  "url": "https://icml.cc/Conferences/2010/papers/906.pdf",
  "title": "FAB-MAP: Appearance-Based Place Recognition and Mapping using a Learned Visual Vocabulary Model.",
  "year": 2010,
  "venue": "ICML",
  "abstract": "We present an overview of FAB-MAP, an algorithm for place recognition and mapping developed for infrastructure-free mobile robot navigation in large environments. The system allows a robot to identify when it is revisiting a previously seen location, on the basis of imagery captured by the robot’s camera. We outline a complete probabilistic framework for the task, which is applicable even in visually repetitive environments where many locations may appear identical. Our work introduces a number of technical innovations notably we demonstrate that place recognition performance can be improved by learning an approximation to the joint distribution over visual elements. We also investigate several principled approaches to making the system robust in visually repetitive environments, and define an efficient bail-out strategy for multi-hypothesis testing to improve system speed. Our model has been shown to substantially outperform standard tf-idf ranking on our task of interest. We demonstrate the system performing reliable online appearance mapping and loop closure detection over a 1,000 km trajectory, with mean filter update times of 14ms.",
  "stance": 0.9
 },
 {
  "url": "https://icml.cc/2011/papers/108_icmlpaper.pdf",
  "title": "Dynamic Tree Block Coordinate Ascent.",
  "year": 2011,
  "venue": "ICML",
  "abstract": "This paper proposes a novel Linear Programming (LP) based algorithm, called Dynamic Tree-Block Coordinate Ascent (DTBCA), for performing maximum a posteriori (MAP) inference in probabilistic graphical models. Unlike traditional message passing algorithms, which operate uniformly on the whole factor graph, our method dynamically chooses regions of the factor graph on which to focus message-passing efforts. We propose two criteria for selecting regions, including an efficiently computable upperbound on the increase in the objective possible by passing messages in any particular region. This bound is derived from the theory of primal-dual methods from combinatorial optimization, and the forest that maximizes the bounds can be chosen efficiently using a maximum-spanning-tree-like algorithm. Experimental results show that our dynamic schedules significantly speed up state-of-theart LP-based message-passing algorithms on a wide variety of real-world problems.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2011/papers/125_icmlpaper.pdf",
  "title": "Parsing Natural Scenes and Natural Language with Recursive Neural Networks.",
  "year": 2011,
  "venue": "ICML",
  "abstract": "Recursive structure is commonly found in the inputs of different modalities such as natural scene images or natural language sentences. Discovering this recursive structure helps us to not only identify the units that an image or sentence contains but also how they interact to form a whole. We introduce a max-margin structure prediction architecture based on recursive neural networks that can successfully recover such structure both in complex scene images as well as sentences. The same algorithm can be used both to provide a competitive syntactic parser for natural language sentences from the Penn Treebank and to outperform alternative approaches for semantic scene segmentation, annotation and classification. For segmentation and annotation our algorithm obtains a new level of state-of-theart performance on the Stanford background dataset (78.1%). The features from the image parse tree outperform Gist descriptors for scene classification by 4%.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2011/papers/150_icmlpaper.pdf",
  "title": "An Augmented Lagrangian Approach to Constrained MAP Inference.",
  "year": 2011,
  "venue": "ICML",
  "abstract": "We propose a new algorithm for approximate MAP inference on factor graphs, by combining augmented Lagrangian optimization with the dual decomposition method. Each slave subproblem is given a quadratic penalty, which pushes toward faster consensus than in previous subgradient approaches. Our algorithm is provably convergent, parallelizable, and suitable for fine decompositions of the graph. We show how it can efficiently handle problems with (possibly global) structural constraints via simple sort operations. Experiments on synthetic and real-world data show that our approach compares favorably with the state-of-the-art.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2011/papers/210_icmlpaper.pdf",
  "title": "On optimization methods for deep learning.",
  "year": 2011,
  "venue": "ICML",
  "abstract": "The predominant methodology in training deep learning advocates the use of stochastic gradient descent methods (SGDs). Despite its ease of implementation, SGDs are difficult to tune and parallelize. These problems make it challenging to develop, debug and scale up deep learning algorithms with SGDs. In this paper, we show that more sophisticated off-the-shelf optimization methods such as Limited memory BFGS (L-BFGS) and Conjugate gradient (CG) with line search can significantly simplify and speed up the process of pretraining deep algorithms. In our experiments, the difference between LBFGS/CG and SGDs are more pronounced if we consider algorithmic extensions (e.g., sparsity regularization) and hardware extensions (e.g., GPUs or computer clusters). Our experiments with distributed optimization support the use of L-BFGS with locally connected networks and convolutional neural networks. Using L-BFGS, our convolutional network model achieves 0.69% on the standard MNIST dataset. This is a state-of-theart result on MNIST among algorithms that do not use distortions or pretraining.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2011/papers/216_icmlpaper.pdf",
  "title": "On the Necessity of Irrelevant Variables.",
  "year": 2011,
  "venue": "ICML",
  "abstract": "This work explores the effects of relevant and irrelevant boolean variables on the accuracy of classifiers. The analysis uses the assumption that the variables are conditionally independent given the class, and focuses on a natural family of learning algorithms for such sources when the relevant variables have a small advantage over random guessing. The main result is that algorithms relying predominately on irrelevant variables have error probabilities that quickly go to 0 in situations where algorithms that limit the use of irrelevant variables have errors bounded below by a positive constant. We also show that accurate learning is possible even when there are so few examples that one cannot determine with high confidence whether or not any individual variable is relevant.",
  "stance": -0.3
 },
 {
  "url": "https://icml.cc/2011/papers/323_icmlpaper.pdf",
  "title": "PILCO: A Model-Based and Data-Efficient Approach to Policy Search.",
  "year": 2011,
  "venue": "ICML",
  "abstract": "In this paper, we introduce pilco, a practical, data-efficient model-based policy search method. Pilco reduces model bias, one of the key problems of model-based reinforcement learning, in a principled way. By learning a probabilistic dynamics model and explicitly incorporating model uncertainty into long-term planning, pilco can cope with very little data and facilitates learning from scratch in only a few trials. Policy evaluation is performed in closed form using state-ofthe-art approximate inference. Furthermore, policy gradients are computed analytically for policy improvement. We report unprecedented learning efficiency on challenging and high-dimensional control tasks.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2011/papers/374_icmlpaper.pdf",
  "title": "Infinite SVM: a Dirichlet Process Mixture of Large-margin Kernel Machines.",
  "year": 2011,
  "venue": "ICML",
  "abstract": "We present Infinite SVM (iSVM), a Dirichlet process mixture of large-margin kernel machines for multi-way classification. An iSVM enjoys the advantages of both Bayesian nonparametrics in handling the unknown number of mixing components, and large-margin kernel machines in robustly capturing local nonlinearity of complex data. We develop an efficient variational learning algorithm for posterior inference of iSVM, and we demonstrate the advantages of iSVM over Dirichlet process mixture of generalized linear models and other benchmarks on both synthetic and real Flickr image classification datasets.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2011/papers/38_icmlpaper.pdf",
  "title": "A Graphbased Framework for Multi-Task Multi-View Learning.",
  "year": 2011,
  "venue": "ICML",
  "abstract": "Many real-world problems exhibit dualheterogeneity. A single learning task might have features in multiple views (i.e., feature heterogeneity); multiple learning tasks might be related with each other through one or more shared views (i.e., task heterogeneity). Existing multi-task learning or multi-view learning algorithms only capture one type of heterogeneity. In this paper, we introduce Multi-Task MultiView (MTV ) learning for such complicated learning problems with both feature heterogeneity and task heterogeneity. We propose a graph-based framework (GraM) to take full advantage of the dual-heterogeneous nature. Our framework has a natural connection to Reproducing Kernel Hilbert Space (RKHS). Furthermore, we propose an iterative algorithm (IteM) for GraM framework, and analyze its optimality, convergence and time complexity. Experimental results on various real data sets demonstrate its effectiveness.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2011/papers/419_icmlpaper.pdf",
  "title": "Clusterpath: an Algorithm for Clustering using Convex Fusion Penalties.",
  "year": 2011,
  "venue": "ICML",
  "abstract": "We present a new clustering algorithm by proposing a convex relaxation of hierarchical clustering, which results in a family of objective functions with a natural geometric interpretation. We give efficient algorithms for calculating the continuous regularization path of solutions, and discuss relative advantages of the parameters. Our method experimentally gives state-ofthe-art results similar to spectral clustering for non-convex clusters, and has the added benefit of learning a tree structure from the data.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2011/papers/491_icmlpaper.pdf",
  "title": "Large-Scale Learning of Embeddings with Reconstruction Sampling.",
  "year": 2011,
  "venue": "ICML",
  "abstract": "In this paper, we present a novel method to speed up the learning of embeddings for large-scale learning tasks involving very sparse data, as is typically the case for Natural Language Processing tasks. Our speed-up method has been developed in the context of Denoising Auto-encoders, which are trained in a purely unsupervised way to capture the input distribution, and learn embeddings for words and text similar to earlier neural language models. The main contribution is a new method to approximate reconstruction error by a sampling procedure. We show how this approximation can be made to obtain an unbiased estimator of the training criterion, and we show how it can be leveraged to make learning much more computationally efficient. We demonstrate the effectiveness of this method on the Amazon and RCV1 NLP datasets. Instead of reducing vocabulary size to make learning practical, our method allows us to train using very large vocabularies. In particular, reconstruction sampling requires 22x less training time on the full Amazon dataset.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2011/papers/546_icmlpaper.pdf",
  "title": "A Unified Probabilistic Model for Global and Local Unsupervised Feature Selection.",
  "year": 2011,
  "venue": "ICML",
  "abstract": "Existing algorithms for joint clustering and feature selection can be categorized as either global or local approaches. Global methods select a single cluster-independent subset of features, whereas local methods select cluster-specific subsets of features. In this paper, we present a unified probabilistic model that can perform both global and local feature selection for clustering. Our approach is based on a hierarchical beta-Bernoulli prior combined with a Dirichlet process mixture model. We obtain global or local feature selection by adjusting the variance of the beta prior. We provide a variational inference algorithm for our model. In addition to simultaneously learning the clusters and features, this Bayesian formulation allows us to learn both the number of clusters and the number of features to retain. Experiments on synthetic and real data show that our unified model can find global and local features and cluster data as well as competing methods of each type.",
  "stance": 0.5
 },
 {
  "url": "https://icml.cc/2011/papers/54_icmlpaper.pdf",
  "title": "Learning Output Kernels with Block Coordinate Descent.",
  "year": 2011,
  "venue": "ICML",
  "abstract": "We propose a method to learn simultaneously a vector-valued function and a kernel between its components. The obtained kernel can be used both to improve learning performance and to reveal structures in the output space which may be important in their own right. Our method is based on the solution of a suitable regularization problem over a reproducing kernel Hilbert space of vector-valued functions. Although the regularized risk functional is non-convex, we show that it is invex, implying that all local minimizers are global minimizers. We derive a block-wise coordinate descent method that efficiently exploits the structure of the objective functional. Then, we empirically demonstrate that the proposed method can improve classification accuracy. Finally, we provide a visual interpretation of the learned kernel matrix for some well known datasets.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2011/papers/9_icmlpaper.pdf",
  "title": "Efficient Sparse Modeling with Automatic Feature Grouping.",
  "year": 2011,
  "venue": "ICML",
  "abstract": "The grouping of features is highly beneficial in learning with high-dimensional data. It reduces the variance in the estimation and improves the stability of feature selection, leading to improved generalization. Moreover, it can also help in data understanding and interpretation. OSCAR is a recent sparse modeling tool that achieves this by using a `1-regularizer and a pairwise `∞-regularizer. However, its optimization is computationally expensive. In this paper, we propose an efficient solver based on the accelerated gradient methods. We show that its key projection step can be solved by a simple iterative group merging algorithm. It is highly efficient and reduces the empirical time complexity from O(d ∼ d) for the existing solvers to just O(d), where d is the number of features. Experimental results on toy and real-world data sets demonstrate that OSCAR is a competitive sparse modeling approach with the added ability of automatic feature grouping.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2012/papers/25.pdf",
  "title": "Quasi-Newton Methods: A New Direction.",
  "year": 2012,
  "venue": "ICML",
  "abstract": "Four decades after their invention, quasiNewton methods are still state of the art in unconstrained numerical optimization. Although not usually interpreted thus, these are learning algorithms that fit a local quadratic approximation to the objective function. We show that many, including the most popular, quasi-Newton methods can be interpreted as approximations of Bayesian linear regression under varying prior assumptions. This new notion elucidates some shortcomings of classical algorithms, and lights the way to a novel nonparametric quasi-Newton method, which is able to make more efficient use of available information at computational cost similar to its predecessors.",
  "stance": 0.7
 },
 {
  "url": "https://icml.cc/2012/papers/274.pdf",
  "title": "Modeling Latent Variable Uncertainty for Loss-based Learning.",
  "year": 2012,
  "venue": "ICML",
  "abstract": "We consider the problem of parameter estimation using weakly supervised datasets, where a training sample consists of the input and a partially specified annotation, which we refer to as the output. The missing information in the annotation is modeled using latent variables. Previous methods overburden a single distribution with two separate tasks: (i) modeling the uncertainty in the latent variables during training; and (ii) making accurate predictions for the output and the latent variables during testing. We propose a novel framework that separates the demands of the two tasks using two distributions: (i) a conditional distribution to model the uncertainty of the latent variables for a given input-output pair; and (ii) a delta distribution to predict the output and the latent variables for a given input. During learning, we encourage agreement between the two distributions by minimizing a loss-based dissimilarity coefficient. Our approach generalizes latent svm in two important ways: (i) it models the uncertainty over latent variables instead of relying on a pointwise estimate; and (ii) it allows the use of loss functions that depend on latent variables, which greatly increases its applicability. We demonstrate the efficacy of our approach on two challenging problems—object detection and action detection—using publicly available datasets.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2012/papers/349.pdf",
  "title": "Unachievable Region in Precision-Recall Space and Its Effect on Empirical Evaluation.",
  "year": 2012,
  "venue": "ICML",
  "abstract": "Precision-recall (PR) curves and the areas under them are widely used to summarize machine learning results, especially for data sets exhibiting class skew. They are often used analogously to ROC curves and the area under ROC curves. It is known that PR curves vary as class skew changes. What was not recognized before this paper is that there is a region of PR space that is completely unachievable, and the size of this region depends only on the skew. This paper precisely characterizes the size of that region and discusses its implications for empirical evaluation methodology in machine learning.",
  "stance": -0.3
 },
 {
  "url": "https://icml.cc/2012/papers/405.pdf",
  "title": "Evaluating Bayesian and L1 Approaches for Sparse Unsupervised Learning .",
  "year": 2012,
  "venue": "ICML",
  "abstract": "The use of L1 regularisation for sparse learning has generated immense research interest, with many successful applications in diverse areas such as signal acquisition, image coding, genomics and collaborative filtering. While existing work highlights the many advantages of L1 methods, in this paper we find that L1 regularisation often dramatically under-performs in terms of predictive performance when compared to other methods for inferring sparsity. We focus on unsupervised latent variable models, and develop L1 minimising factor models, Bayesian variants of “L1”, and Bayesian models with a stronger L0-like sparsity induced through spike-and-slab distributions. These spikeand-slab Bayesian factor models encourage sparsity while accounting for uncertainty in a principled manner, and avoid unnecessary shrinkage of non-zero values. We demonstrate on a number of data sets that in practice spike-and-slab Bayesian methods outperform L1 minimisation, even on a computational budget. We thus highlight the need to re-assess the wide use of L1 methods in sparsity-reliant applications, particularly when we care about generalising to previously unseen data, and provide an alternative that, over many varying conditions, provides improved generalisation performance. Appearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).",
  "stance": 0.9
 },
 {
  "url": "https://icml.cc/2012/papers/486.pdf",
  "title": "Copula Mixture Model for Dependency-seeking Clustering.",
  "year": 2012,
  "venue": "ICML",
  "abstract": "We introduce a copula mixture model to perform dependency-seeking clustering when cooccurring samples from different data sources are available. The model takes advantage of the great flexibility offered by the copulas framework to extend mixtures of Canonical Correlation Analysis to multivariate data with arbitrary continuous marginal densities. We formulate our model as a non-parametric Bayesian mixture, while providing efficient MCMC inference. Experiments on synthetic and real data demonstrate that the increased flexibility of the copula mixture significantly improves the clustering and the interpretability of the results.",
  "stance": 0.7
 },
 {
  "url": "https://icml.cc/2012/papers/501.pdf",
  "title": "Fast classification using sparse decision DAGs.",
  "year": 2012,
  "venue": "ICML",
  "abstract": "In this paper we propose an algorithm that builds sparse decision DAGs (directed acyclic graphs) from a list of base classifiers provided by an external learning method such as AdaBoost. The basic idea is to cast the DAG design task as a Markov decision process. Each instance can decide to use or to skip each base classifier, based on the current state of the classifier being built. The result is a sparse decision DAG where the base classifiers are selected in a data-dependent way. The method has a single hyperparameter with a clear semantics of controlling the accuracy/speed trade-off. The algorithm is competitive with state-of-the-art cascade detectors on three object-detection benchmarks, and it clearly outperforms them when there is a small number of base classifiers. Unlike cascades, it is also readily applicable for multi-class classification. Using the multi-class setup, we show on a benchmark Web page ranking data set that we can significantly improve the decision speed without harming the performance of the ranker.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2012/papers/507.pdf",
  "title": "Efficient Structured Prediction with Latent Variables for General Graphical Models.",
  "year": 2012,
  "venue": "ICML",
  "abstract": "In this paper we propose a unified framework for structured prediction with latent variables which includes hidden conditional random fields and latent structured support vector machines as special cases. We describe a local entropy approximation for this general formulation using duality, and derive an efficient message passing algorithm that is guaranteed to converge. We demonstrate its effectiveness in the tasks of image segmentation as well as 3D indoor scene understanding from single images, showing that our approach is superior to latent structured support vector machines and hidden conditional random fields.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2012/papers/625.pdf",
  "title": "On causal and anticausal learning.",
  "year": 2012,
  "venue": "ICML",
  "abstract": "We consider the problem of function estimation in the case where an underlying causal model can be inferred. This has implications for popular scenarios such as covariate shift, concept drift, transfer learning and semi-supervised learning. We argue that causal knowledge may facilitate some approaches for a given problem, and rule out others. In particular, we formulate a hypothesis for when semi-supervised learning can help, and corroborate it with empirical results.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2012/papers/665.pdf",
  "title": "Lognormal and Gamma Mixed Negative Binomial Regression.",
  "year": 2012,
  "venue": "ICML",
  "abstract": "In regression analysis of counts, a lack of simple and efficient algorithms for posterior computation has made Bayesian approaches appear unattractive and thus underdeveloped. We propose a lognormal and gamma mixed negative binomial (NB) regression model for counts, and present efficient closed-form Bayesian inference; unlike conventional Poisson models, the proposed approach has two free parameters to include two different kinds of random effects, and allows the incorporation of prior information, such as sparsity in the regression coefficients. By placing a gamma distribution prior on the NB dispersion parameter r, and connecting a lognormal distribution prior with the logit of the NB probability parameter p, efficient Gibbs sampling and variational Bayes inference are both developed. The closed-form updates are obtained by exploiting conditional conjugacy via both a compound Poisson representation and a Polya-Gamma distribution based data augmentation approach. The proposed Bayesian inference can be implemented routinely, while being easily generalizable to more complex settings involving multivariate dependence structures. The algorithms are illustrated using real examples.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2012/papers/690.pdf",
  "title": "Learning Task Grouping and Overlap in Multi-task Learning.",
  "year": 2012,
  "venue": "ICML",
  "abstract": "In the paradigm of multi-task learning, multiple related prediction tasks are learned jointly, sharing information across the tasks. We propose a framework for multi-task learning that enables one to selectively share the information across the tasks. We assume that each task parameter vector is a linear combination of a finite number of underlying basis tasks. The coefficients of the linear combination are sparse in nature and the overlap in the sparsity patterns of two tasks controls the amount of sharing across these. Our model is based on the assumption that task parameters within a group lie in a low dimensional subspace but allows the tasks in different groups to overlap with each other in one or more bases. Experimental results on four datasets show that our approach outperforms competing methods.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2012/papers/718.pdf",
  "title": "Large-Scale Feature Learning With Spike-and-Slab Sparse Coding.",
  "year": 2012,
  "venue": "ICML",
  "abstract": "We consider the problem of object recognition with a large number of classes. In order to overcome the low amount of labeled examples available in this setting, we introduce a new feature learning and extraction procedure based on a factor model we call spike-and-slab sparse coding (S3C). Prior work on S3C has not prioritized the ability to exploit parallel architectures and scale S3C to the enormous problem sizes needed for object recognition. We present a novel inference procedure for appropriate for use with GPUs which allows us to dramatically increase both the training set size and the amount of latent factors that S3C may be trained with. We demonstrate that this approach improves upon the supervised learning capabilities of both sparse coding and the spike-and-slab Restricted Boltzmann Machine (ssRBM) on the CIFAR-10 dataset. We use the CIFAR-100 dataset to demonstrate that our method scales to large numbers of classes better than previous methods. Finally, we use our method to win the NIPS 2011 Workshop on Challenges In Learning Hierarchical Models’ Transfer Learning Challenge.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2012/papers/726.pdf",
  "title": "Discovering Support and Affiliated Features from Very High Dimensions.",
  "year": 2012,
  "venue": "ICML",
  "abstract": "In this paper, a novel learning paradigm is presented to automatically identify groups of informative and correlated features from very high dimensions. Specifically, we explicitly incorporate correlation measures as constraints and then propose an efficient embedded feature selection method using recently developed cutting plane strategy. The benefits of the proposed algorithm are two-folds. First, it can identify the optimal discriminative and uncorrelated feature subset to the output labels, denoted here as Support Features, which brings about significant improvements in prediction performance over other state of the art feature selection methods considered in the paper. Second, during the learning process, the underlying group structures of correlated features associated with each support feature, denoted as Affiliated Features, can also be discovered without any additional cost. These affiliated features serve to improve the interpretations on the learning tasks. Extensive empirical studies on both synthetic and very high dimensional real-world datasets verify the validity and efficiency of the proposed method.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2012/papers/763.pdf",
  "title": "Using CCA to improve CCA: A new spectral method for estimating vector models of words.",
  "year": 2012,
  "venue": "ICML",
  "abstract": "Unlabeled data is often used to learn representations which can be used to supplement baseline features in a supervised learner. For example, for text applications where the words lie in a very high dimensional space (the size of the vocabulary), one can learn a low rank “dictionary” by an eigendecomposition of the word co-occurrence matrix (e.g. using PCA or CCA). In this paper, we present a new spectral method based on CCA to learn an eigenword dictionary. Our improved procedure computes two set of CCAs, the first one between the left and right contexts of the given word and the second one between the projections resulting from this CCA and the word itself. We prove theoretically that this two-step procedure has lower sample complexity than the simple single step procedure and also illustrate the empirical efficacy of our approach and the richness of representations learned by our Two Step CCA (TSCCA) procedure on the tasks of POS tagging and sentiment classification.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2012/papers/81.pdf",
  "title": "Bayesian Nonexhaustive Learning for Online Discovery and Modeling of Emerging Classes.",
  "year": 2012,
  "venue": "ICML",
  "abstract": "We present a framework for online inference in the presence of a nonexhaustively defined set of classes that incorporates supervised classification with class discovery and modeling. A Dirichlet process prior (DPP) model defined over class distributions ensures that both known and unknown class distributions originate according to a common base distribution. In an attempt to automatically discover potentially interesting class formations, the prior model is coupled with a suitably chosen data model, and sequential Monte Carlo sampling is used to perform online inference. Our research is driven by a biodetection application, where a new class of pathogen may suddenly appear, and the rapid increase in the number of samples originating from this class indicates the onset of an outbreak.",
  "stance": 0.3
 },
 {
  "url": "https://icml.cc/2012/papers/863.pdf",
  "title": "Predicting Consumer Behavior in Commerce Search.",
  "year": 2012,
  "venue": "ICML",
  "abstract": "Traditional approaches to ranking in web search follow the paradigm of rank-by-score: a learned function gives each query-URL combination an absolute score and URLs are ranked according to this score. This paradigm ensures that if the score of one URL is better than another then one will always be ranked higher than the other. Scoring contradicts prior work in behavioral economics that preference between items depends not only on the items but also on the presented alternatives. Thus, for the same query, preference between items A and B may depend on the presence or absence of item C. We propose a new model of ranking, the Random Shopper Model, that allows and explains such behavior. In this model, each feature is viewed as a Markov chain over the items to be ranked, and the goal is to find a weighting of the features that best reflects their importance. We show that our model can be learned under the empirical risk minimization framework, and give an efficient learning algorithm. Experiments on commerce search logs demonstrate that our algorithm outperforms scoring-based approaches including regression and listwise ranking.",
  "stance": 0.8
 },
 {
  "url": "https://icml.cc/2012/papers/94.pdf",
  "title": "Manifold Relevance Determination.",
  "year": 2012,
  "venue": "ICML",
  "abstract": "In this paper we present a fully Bayesian latent variable model which exploits conditional nonlinear (in)-dependence structures to learn an efficient latent representation. The latent space is factorized to represent shared and private information from multiple views of the data. In contrast to previous approaches, we introduce a relaxation to the discrete segmentation and allow for a “softly” shared latent space. Further, Bayesian techniques allow us to automatically estimate the dimensionality of the latent spaces. The model is capable of capturing structure underlying extremely high dimensional spaces. This is illustrated by modelling unprocessed images with tenths of thousands of pixels. This also allows us to directly generate novel images from the trained model by sampling from the discovered latent spaces. We also demonstrate the model by prediction of human pose in an ambiguous setting. Our Bayesian framework allows us to perform disambiguation in a principled manner by including latent space priors which incorporate the dynamic nature of the data.",
  "stance": 0.5
 },
 {
  "url": "https://proceedings.mlr.press/v28/balasubramanian13.pdf",
  "title": "Smooth Sparse Coding via Marginal Regression for Learning Sparse Representations",
  "year": 2013,
  "venue": "ICML",
  "abstract": "We propose and analyze a novel framework for learning sparse representations, based on two statistical techniques: kernel smoothing and marginal regression. The proposed approach provides a flexible framework for incorporating feature similarity or temporal information present in data sets, via nonparametric kernel smoothing. We provide generalization bounds for dictionary learning using smooth sparse coding and show how the sample complexity depends on the L1 norm of kernel function used. Furthermore, we propose using marginal regression for obtaining sparse codes, which significantly improves the speed and allows one to scale to large dictionary sizes easily. We demonstrate the advantages of the proposed approach, both in terms of accuracy and speed by extensive experimentation on several real data sets. In addition, we demonstrate how the proposed approach can be used for improving semisupervised sparse coding.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v28/bengio13.pdf",
  "title": "Better Mixing via Deep Representations",
  "year": 2013,
  "venue": "ICML",
  "abstract": "It has been hypothesized, and supported with experimental evidence, that deeper representations, when well trained, tend to do a better job at disentangling the underlying factors of variation. We study the following related conjecture: better representations, in the sense of better disentangling, can be exploited to produce Markov chains that mix faster between modes. Consequently, mixing between modes would be more efficient at higher levels of representation. To better understand this, we propose a secondary conjecture: the higher-level samples fill more uniformly the space they occupy and the highdensity manifolds tend to unfold when represented at higher levels. The paper discusses these hypotheses and tests them experimentally through visualization and measurements of mixing between modes and interpolating between samples.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v28/koppula13.pdf",
  "title": "Learning Spatio-Temporal Structure from RGB-D Videos for Human Activity Detection and Anticipation",
  "year": 2013,
  "venue": "ICML",
  "abstract": "We consider the problem of detecting past activities as well as anticipating which activity will happen in the future and how. We start by modeling the rich spatio-temporal relations between human poses and objects (called affordances) using a conditional random field (CRF). However, because of the ambiguity in the temporal segmentation of the sub-activities that constitute an activity, in the past as well as in the future, multiple graph structures are possible. In this paper, we reason about these alternate possibilities by reasoning over multiple possible graph structures. We obtain them by approximating the graph with only additive features, which lends to efficient dynamic programming. Starting with this proposal graph structure, we then design moves to obtain several other likely graph structures. We then show that our approach improves the state-of-the-art significantly for detecting past activities as well as for anticipating future activities, on a dataset of 120 activity videos collected from four subjects.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v28/nguyen13.pdf",
  "title": "Online Feature Selection for Model-based Reinforcement Learning",
  "year": 2013,
  "venue": "ICML",
  "abstract": "We propose a new framework for learning the world dynamics of feature-rich environments in model-based reinforcement learning. The main idea is formalized as a new, factored state-transition representation that supports efficient online-learning of the relevant features. We construct the transition models through predicting how the actions change the world. We introduce an online sparse coding learning technique for feature selection in high-dimensional spaces. We derive theoretical guarantees for our framework and empirically demonstrate its practicality in both simulated and real robotics domains.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v28/nguyen13a.pdf",
  "title": "Algorithms for Direct 0–1 Loss Optimization in Binary Classification",
  "year": 2013,
  "venue": "ICML",
  "abstract": "While convex losses for binary classification are attractive due to the existence of numerous (provably) efficient methods for finding their global optima, they are sensitive to outliers. On the other hand, while the nonconvex 0–1 loss is robust to outliers, it is NP-hard to optimize and thus rarely directly optimized in practice. In this paper, however, we do just that: we explore a variety of practical methods for direct (approximate) optimization of the 0–1 loss based on branch and bound search, combinatorial search, and coordinate descent on smooth, differentiable relaxations of 0–1 loss. Empirically, we compare our proposed algorithms to logistic regression, SVM, and the Bayes point machine showing that the proposed 0–1 loss optimization algorithms perform at least as well and offer a clear advantage in the presence of outliers. To this end, we believe this work reiterates the importance of 0–1 loss and its robustness properties while challenging the notion that it is difficult to directly optimize.",
  "stance": 0.5
 },
 {
  "url": "https://proceedings.mlr.press/v28/sutskever13.pdf",
  "title": "On the importance of initialization and momentum in deep learning",
  "year": 2013,
  "venue": "ICML",
  "abstract": "Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods su ce for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.",
  "stance": 0.3
 },
 {
  "url": "https://proceedings.mlr.press/v28/wang13b.pdf",
  "title": "Subproblem-Tree Calibration: A Unified Approach to Max-Product Message Passing",
  "year": 2013,
  "venue": "ICML",
  "abstract": "Max-product (max-sum) message passing algorithms are widely used for MAP inference in MRFs. It has many variants sharing a common flavor of passing “messages” over some graph-object. Recent advances revealed that its convergent versions (such as MPLP, MSD, TRW-S) can be viewed as performing block coordinate descent (BCD) in a dual objective. That is, each BCD step achieves dual-optimal w.r.t. a block of dual variables (messages), thereby decreases the dual objective monotonically. However, most existing algorithms are limited to updating blocks selected in rather restricted ways. In this paper, we show a “unified” message passing algorithm that: (a) subsumes MPLP, MSD, and TRW-S as special cases when applied to their respective choices of dual objective and blocks, and (b) is able to perform BCD under much more flexible choices of blocks (including very large blocks) as well as the dual objective itself (that arise from an arbitrary dual decomposition).",
  "stance": 0.4
 },
 {
  "url": "https://proceedings.mlr.press/v28/willemvandemeent13.pdf",
  "title": "Hierarchically-coupled hidden Markov models for learning kinetic rates from single-molecule data",
  "year": 2013,
  "venue": "ICML",
  "abstract": "We address the problem of analyzing sets of noisy time-varying signals that all report on the same process but confound straightforward analyses due to complex inter-signal heterogeneities and measurement artifacts. In particular we consider single-molecule experiments which indirectly measure the distinct steps in a biomolecular process via observations of noisy time-dependent signals such as a fluorescence intensity or bead position. Straightforward hidden Markov model (HMM) analyses attempt to characterize such processes in terms of a set of conformational states, the transitions that can occur between these states, and the associated rates at which those transitions occur; but require ad-hoc post-processing steps to combine multiple signals. Here we develop a hierarchically coupled HMM that allows experimentalists to deal with inter-signal variability in a principled and automatic way. Our approach is a generalized expectation maximization hyperparameter point estimation procedure with variational Bayes at the level of individual time series that learns an single interpretable representation of the overall data generating process.",
  "stance": 0.2
 },
 {
  "url": "https://proceedings.mlr.press/v28/zhai13.pdf",
  "title": "Online Latent Dirichlet Allocation with Infinite Vocabulary",
  "year": 2013,
  "venue": "ICML",
  "abstract": "Topic models based on latent Dirichlet allocation (LDA) assume a predefined vocabulary. This is reasonable in batch settings but not reasonable for streaming and online settings. To address this lacuna, we extend LDA by drawing topics from a Dirichlet process whose base distribution is a distribution over all strings rather than from a finite Dirichlet. We develop inference using online variational inference and—to only consider a finite number of words for each topic—propose heuristics to dynamically order, expand, and contract the set of words we consider in our vocabulary. We show our model can successfully incorporate new words and that it performs better than topic models with finite vocabularies in evaluations of topic quality and classification performance.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v32/eban14.pdf",
  "title": "Discrete Chebyshev Classifiers",
  "year": 2014,
  "venue": "ICML",
  "abstract": "In large scale learning problems it is often easy to collect simple statistics of the data, but hard or impractical to store all the original data. A key question in this setting is how to construct classifiers based on such partial information. One traditional approach to the problem has been to use maximum entropy arguments to induce a complete distribution on variables from statistics. However, this approach essentially makes conditional independence assumptions about the distribution, and furthermore does not optimize prediction loss. Here we present a framework for discriminative learning given a set of statistics. Specifically, we address the case where all variables are discrete and we have access to various marginals. Our approach minimizes the worst case hinge loss in this case, which upper bounds the generalization error. We show that for certain sets of statistics the problem is tractable, and in the general case can be approximated using MAP LP relaxations. Empirical results show that the method is competitive with other approaches that use the same input.",
  "stance": 0.9
 },
 {
  "url": "https://proceedings.mlr.press/v32/gal14.pdf",
  "title": "Pitfalls in the use of Parallel Inference for the Dirichlet Process",
  "year": 2014,
  "venue": "ICML",
  "abstract": "Recent work done by Lovell, Adams, and Mansingka (2012) and Williamson, Dubey, and Xing (2013) has suggested an alternative parametrisation for the Dirichlet process in order to derive non-approximate parallel MCMC inference for it – work which has been picked-up and implemented in several different fields. In this paper we show that the approach suggested is impractical due to an extremely unbalanced distribution of the data. We characterise the requirements of efficient parallel inference for the Dirichlet process and show that the proposed inference fails most of these requirements (while approximate approaches often satisfy most of them). We present both theoretical and experimental evidence, analysing the load balance for the inference and showing that it is independent of the size of the dataset and the number of nodes available in the parallel implementation. We end with suggestions of alternative paths of research for efficient non-approximate parallel inference for the Dirichlet process.",
  "stance": -1.0
 },
 {
  "url": "https://proceedings.mlr.press/v32/korattikara14.pdf",
  "title": "Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget",
  "year": 2014,
  "venue": "ICML",
  "abstract": "Can we make Bayesian posterior MCMC sampling more efficient when faced with very large datasets? We argue that computing the likelihood for N datapoints in the Metropolis-Hastings (MH) test to reach a single binary decision is computationally inefficient. We introduce an approximate MH rule based on a sequential hypothesis test that allows us to accept or reject samples with high confidence using only a fraction of the data required for the exact MH rule. While this method introduces an asymptotic bias, we show that this bias can be controlled and is more than offset by a decrease in variance due to our ability to draw more samples per unit of time.",
  "stance": 0.6
 },
 {
  "url": "https://proceedings.mlr.press/v32/nguyenb14.pdf",
  "title": "Bayesian Nonparametric Multilevel Clustering with Group-Level Contexts",
  "year": 2014,
  "venue": "ICML",
  "abstract": "We present a Bayesian nonparametric framework for multilevel clustering which utilizes grouplevel context information to simultaneously discover low-dimensional structures of the group contents and partitions groups into clusters. Using the Dirichlet process as the building block, our model constructs a product base-measure with a nested structure to accommodate content and context observations at multiple levels. The proposed model possesses properties that link the nested Dirichlet processes (nDP) and the Dirichlet process mixture models (DPM) in an interesting way: integrating out all contents results in the DPM over contexts, whereas integrating out group-specific contexts results in the nDP mixture over content variables. We provide a Polyaurn view of the model and an efficient collapsed Gibbs inference procedure. Extensive experiments on real-world datasets demonstrate the advantage of utilizing context information via our model in both text and image domains.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v32/scherrer14.pdf",
  "title": "Approximate Policy Iteration Schemes: A Comparison",
  "year": 2014,
  "venue": "ICML",
  "abstract": "We consider the infinite-horizon discounted optimal control problem formalized by Markov Decision Processes. We focus on several approximate variations of the Policy Iteration algorithm: Approximate Policy Iteration (API) (Bertsekas & Tsitsiklis, 1996), Conservative Policy Iteration (CPI) (Kakade & Langford, 2002), a natural adaptation of the Policy Search by Dynamic Programming algorithm (Bagnell et al., 2003) to the infinite-horizon case (PSDP∞), and the recently proposed Non-Stationary Policy Iteration (NSPI(m)) (Scherrer & Lesner, 2012). For all algorithms, we describe performance bounds with respect the per-iteration error , and make a comparison by paying a particular attention to the concentrability constants involved, the number of iterations and the memory required. Our analysis highlights the following points: 1) The performance guarantee of CPI can be arbitrarily better than that of API, but this comes at the cost of a relative—exponential in 1 —increase of the number of iterations. 2) PSDP∞ enjoys the best of both worlds: its performance guarantee is similar to that of CPI, but within a number of iterations similar to that of API. 3) Contrary to API that requires a constant memory, the memory needed by CPI and PSDP∞ is proportional to their number of iterations, which may be problematic when the discount factor γ is close to 1 or the approximation error is close to 0; we show that the NSPI(m) algorithm allows to make an overall trade-off between memory and performance. Simulations with these schemes confirm our analysis.",
  "stance": 0.0
 },
 {
  "url": "https://proceedings.mlr.press/v32/thomas14.pdf",
  "title": "Bias in Natural Actor-Critic Algorithms",
  "year": 2014,
  "venue": "ICML",
  "abstract": "We show that several popular discounted reward natural actor-critics, including the popular NACLSTD and eNAC algorithms, do not generate unbiased estimates of the natural policy gradient as claimed. We derive the first unbiased discounted reward natural actor-critics using batch and iterative approaches to gradient estimation. We argue that the bias makes the existing algorithms more appropriate for the average reward setting. We also show that, when Sarsa(λ) is guaranteed to converge to an optimal policy, the objective function used by natural actor-critics has only global optima, so policy gradient methods are guaranteed to converge to globally optimal policies as well.",
  "stance": 0.1
 },
 {
  "url": "https://proceedings.mlr.press/v32/zhong14.pdf",
  "title": "Fast Stochastic Alternating Direction Method of Multipliers",
  "year": 2014,
  "venue": "ICML",
  "abstract": "We propose a new stochastic alternating direction method of multipliers (ADMM) algorithm, which incrementally approximates the full gradient in the linearized ADMM formulation. Besides having a low per-iteration complexity as existing stochastic ADMM algorithms, it improves the convergence rate on convex problems fromO(1/ √ T ) toO(1/T ), where T is the number of iterations. This matches the convergence rate of the batch ADMM algorithm, but without the need to visit all the samples in each iteration. Experiments on the graph-guided fused lasso demonstrate that the new algorithm is significantly faster than state-of-the-art stochastic and batch ADMM algorithms.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v37/agarwal15.pdf",
  "title": "A Lower Bound for the Optimization of Finite Sums",
  "year": 2015,
  "venue": "ICML",
  "abstract": "This paper presents a lower bound for optimizing a finite sum of n functions, where each function is L-smooth and the sum is μ-strongly convex. We show that no algorithm can reach an error ε in minimizing all functions from this class in fewer than Ω(n+ √ n(κ−1) log(1/ε)) iterations, where κ = L/μ is a surrogate condition number. We then compare this lower bound to upper bounds for recently developed methods specializing to this setting. When the functions involved in this sum are not arbitrary, but based on i.i.d. random data, then we further contrast these complexity results with those for optimal first-order methods to directly optimize the sum. The conclusion we draw is that a lot of caution is necessary for an accurate comparison, and identify machine learning scenarios where the new methods help computationally.",
  "stance": 0.3
 },
 {
  "url": "https://proceedings.mlr.press/v37/avron15.pdf",
  "title": "Community Detection Using Time-Dependent Personalized PageRank",
  "year": 2015,
  "venue": "ICML",
  "abstract": "Local graph diffusions have proven to be valuable tools for solving various graph clustering problems. As such, there has been much interest recently in efficient local algorithms for computing them. We present an efficient local algorithm for approximating a graph diffusion that generalizes both the celebrated personalized PageRank and its recent competitor/companion the heat kernel. Our algorithm is based on writing the diffusion vector as the solution of an initial value problem, and then using a waveform relaxation approach to approximate the solution. Our experimental results suggest that it produces rankings that are distinct and competitive with the ones produced by high quality implementations of personalized PageRank and localized heat kernel, and that our algorithm is a useful addition to the toolset of localized graph diffusions.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v37/aybat15.pdf",
  "title": "An Asynchronous Distributed Proximal Gradient Method for Composite Convex Optimization",
  "year": 2015,
  "venue": "ICML",
  "abstract": "We propose a distributed first-order augmented Lagrangian (DFAL) algorithm to minimize the sum of composite convex functions, where each term in the sum is a private cost function belonging to a node, and only nodes connected by an edge can directly communicate with each other. This optimization model abstracts a number of applications in distributed sensing and machine learning. We show that any limit point of DFAL iterates is optimal; and for any ǫ > 0, an ǫ-optimal and ǫ-feasible solution can be computed within O(log(ǫ−1)) DFAL iterations, which require O( 1.5 max dmin ǫ−1) proximal gradient computations and communications per node in total, where ψmax denotes the largest eigenvalue of the graph Laplacian, and dmin is the minimum degree of the graph. We also propose an asynchronous version of DFAL by incorporating randomized block coordinate descent methods; and demonstrate the efficiency of DFAL on large scale sparse-group LASSO problems.",
  "stance": 0.9
 },
 {
  "url": "https://proceedings.mlr.press/v37/bai15.pdf",
  "title": "An Aligned Subtree Kernel for Weighted Graphs",
  "year": 2015,
  "venue": "ICML",
  "abstract": "In this paper, we develop a new entropic matching kernel for weighted graphs by aligning depthbased representations. We demonstrate that this kernel can be seen as an aligned subtree kernel that incorporates explicit subtree correspondences, and thus addresses the drawback of neglecting the relative locations between substructures that arises in the R-convolution kernels. Experiments on standard datasets demonstrate that our kernel can easily outperform state-of-the-art graph kernels in terms of classification accuracy.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v37/betancourt15.pdf",
  "title": "The Fundamental Incompatibility of Scalable Hamiltonian Monte Carlo and Naive Data Subsampling",
  "year": 2015,
  "venue": "ICML",
  "abstract": "Leveraging the coherent exploration of Hamiltonian flow, Hamiltonian Monte Carlo produces computationally efficient Monte Carlo estimators, even with respect to complex and highdimensional target distributions. When confronted with data-intensive applications, however, the algorithm may be too expensive to implement, leaving us to consider the utility of approximations such as data subsampling. In this paper I demonstrate how data subsampling fundamentally compromises the scalability of Hamiltonian Monte Carlo.",
  "stance": 0.1
 },
 {
  "url": "https://proceedings.mlr.press/v37/chenb15.pdf",
  "title": "Learning Deep Structured Models",
  "year": 2015,
  "venue": "ICML",
  "abstract": "Many problems in real-world applications involve predicting several random variables that are statistically related. Markov random fields (MRFs) are a great mathematical tool to encode such dependencies. The goal of this paper is to combine MRFs with deep learning to estimate complex representations while taking into account the dependencies between the output random variables. Towards this goal, we propose a training algorithm that is able to learn structured models jointly with deep features that form the MRF potentials. Our approach is efficient as it blends learning and inference and makes use of GPU acceleration. We demonstrate the effectiveness of our algorithm in the tasks of predicting words from noisy images, as well as tagging of Flickr photographs. We show that joint learning of the deep features and the MRF parameters results in significant performance gains.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v37/garbera15.pdf",
  "title": "Faster Rates for the Frank-Wolfe Method over Strongly-Convex Sets",
  "year": 2015,
  "venue": "ICML",
  "abstract": "The Frank-Wolfe method (a.k.a. conditional gradient algorithm) for smooth optimization has regained much interest in recent years in the context of large scale optimization and machine learning. A key advantage of the method is that it avoids projections the computational bottleneck in many applications replacing it by a linear optimization step. Despite this advantage, the known convergence rates of the FW method fall behind standard first order methods for most settings of interest. It is an active line of research to derive faster linear optimization-based algorithms for various settings of convex optimization.",
  "stance": -0.5
 },
 {
  "url": "https://proceedings.mlr.press/v37/gouws15.pdf",
  "title": "BilBOWA: Fast Bilingual Distributed Representations without Word Alignments",
  "year": 2015,
  "venue": "ICML",
  "abstract": "We introduce BilBOWA (Bilingual Bag-ofWords without Alignments), a simple and computationally-efficient model for learning bilingual distributed representations of words which can scale to large monolingual datasets and does not require word-aligned parallel training data. Instead it trains directly on monolingual data and extracts a bilingual signal from a smaller set of raw-text sentence-aligned data. This is achieved using a novel sampled bag-of-words cross-lingual objective, which is used to regularize two noise-contrastive language models for efficient cross-lingual feature learning. We show that bilingual embeddings learned using the proposed model outperform state-of-the-art methods on a cross-lingual document classification task as well as a lexical translation task on WMT11 data.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v37/kandemir15.pdf",
  "title": "Asymmetric Transfer Learning with Deep Gaussian Processes",
  "year": 2015,
  "venue": "ICML",
  "abstract": "We introduce a novel Gaussian process based Bayesian model for asymmetric transfer learning. We adopt a two-layer feed-forward deep Gaussian process as the task learner of source and target domains. The first layer projects the data onto a separate non-linear manifold for each task. We perform knowledge transfer by projecting the target data also onto the source domain and linearly combining its representations on the source and target domain manifolds. Our approach achieves the state-of-the-art in a benchmark real-world image categorization task, and improves on it in cross-tissue tumor detection from histopathology tissue slide images.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v37/lebret15.pdf",
  "title": "Phrase-based Image Captioning",
  "year": 2015,
  "venue": "ICML",
  "abstract": "Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely bilinear model that learns a metric between an image representation (generated from a previously trained Convolutional Neural Network) and phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on caption syntax statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results in two popular datasets for the task: Flickr30k and the recently proposed Microsoft COCO.",
  "stance": 0.7
 },
 {
  "url": "https://proceedings.mlr.press/v37/macdonald15.pdf",
  "title": "Controversy in mechanistic modelling with Gaussian processes",
  "year": 2015,
  "venue": "ICML",
  "abstract": "Parameter inference in mechanistic models based on non-affine differential equations is computationally onerous, and various faster alternatives based on gradient matching have been proposed. A particularly promising approach is based on nonparametric Bayesian modelling with Gaussian processes, which exploits the fact that a Gaussian process is closed under differentiation. However, two alternative paradigms have been proposed. The first paradigm, proposed at NIPS 2008 and AISTATS 2013, is based on a product of experts approach and a marginalization over the derivatives of the state variables. The second paradigm, proposed at ICML 2014, is based on a probabilistic generative model and a marginalization over the state variables. The claim has been made that this leads to better inference results. In the present article, we offer a new interpretation of the second paradigm, which highlights the underlying assumptions, approximations and limitations. In particular, we show that the second paradigm suffers from an intrinsic identifiability problem, which the first paradigm is not affected by.",
  "stance": -0.9
 },
 {
  "url": "https://proceedings.mlr.press/v37/qiu15.pdf",
  "title": "Robust Estimation of Transition Matrices in High Dimensional Heavy-tailed Vector Autoregressive Processes",
  "year": 2015,
  "venue": "ICML",
  "abstract": "Gaussian vector autoregressive (VAR) processes have been extensively studied in the literature. However, Gaussian assumptions are stringent for heavy-tailed time series that frequently arises in finance and economics. In this paper, we develop a unified framework for modeling and estimating heavy-tailed VAR processes. In particular, we generalize the Gaussian VAR model by an elliptical VAR model that naturally accommodates heavy-tailed time series. Under this model, we develop a quantile-based robust estimator for the transition matrix of the VAR process. We show that the proposed estimator achieves parametric rates of convergence in high dimensions. This is the first work in analyzing heavy-tailed high dimensional VAR processes. As an application of the proposed framework, we investigate Granger causality in the elliptical VAR process, and show that the robust transition matrix estimator induces sign-consistent estimators of Granger causality. The empirical performance of the proposed methodology is demonstrated by both synthetic and real data. We show that the proposed estimator is robust to heavy tails, and exhibit superior performance in stock price prediction. Proceedings of the 32 International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v37/sui15.pdf",
  "title": "Safe Exploration for Optimization with Gaussian Processes",
  "year": 2015,
  "venue": "ICML",
  "abstract": "We consider sequential decision problems under uncertainty, where we seek to optimize an unknown function from noisy samples. This requires balancing exploration (learning about the objective) and exploitation (localizing the maximum), a problem well-studied in the multiarmed bandit literature. In many applications, however, we require that the sampled function values exceed some prespecified “safety” threshold, a requirement that existing algorithms fail to meet. Examples include medical applications where patient comfort must be guaranteed, recommender systems aiming to avoid user dissatisfaction, and robotic control, where one seeks to avoid controls causing physical harm to the platform. We tackle this novel, yet rich, set of problems under the assumption that the unknown function satisfies regularity conditions expressed via a Gaussian process prior. We develop an efficient algorithm called SAFEOPT, and theoretically guarantee its convergence to a natural notion of optimum reachable under safety constraints. We evaluate SAFEOPT on synthetic data, as well as two real applications: movie recommendation, and therapeutic spinal cord stimulation. Proceedings of the 32 International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).",
  "stance": 0.5
 },
 {
  "url": "https://proceedings.mlr.press/v37/yogatama15.pdf",
  "title": "Learning Word Representations with Hierarchical Sparse Coding",
  "year": 2015,
  "venue": "ICML",
  "abstract": "We propose a new method for learning word representations using hierarchical regularization in sparse coding inspired by the linguistic study of word meanings. We show an efficient learning algorithm based on stochastic proximal methods that is significantly faster than previous approaches, making it possible to perform hierarchical sparse coding on a corpus of billions of word tokens. Experiments on various benchmark tasks—word similarity ranking, syntactic and semantic analogies, sentence completion, and sentiment analysis—demonstrate that the method outperforms or is competitive with state-of-the-art methods.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v37/zhoub15.pdf",
  "title": "\\ell_1,p-Norm Regularization: Error Bounds and Convergence Rate Analysis of First-Order Methods",
  "year": 2015,
  "venue": "ICML",
  "abstract": "In recent years, the `1,p-regularizer has been widely used to induce structured sparsity in the solutions to various optimization problems. Currently, such `1,p-regularized problems are typically solved by first-order methods. Motivated by the desire to analyze the convergence rates of these methods, we show that for a large class of `1,p-regularized problems, an error bound condition is satisfied when p ∈ [1, 2] or p = ∞ but fails to hold for any p ∈ (2,∞). Based on this result, we show that many first-order methods enjoy an asymptotic linear rate of convergence when applied to `1,p-regularized linear or logistic regression with p ∈ [1, 2] or p = ∞. By contrast, numerical experiments suggest that for the same class of problems with p ∈ (2,∞), the aforementioned methods may not converge linearly.",
  "stance": -0.3
 },
 {
  "url": "https://proceedings.mlr.press/v48/fernando16.pdf",
  "title": "Learning End-to-end Video Classification with Rank-Pooling",
  "year": 2016,
  "venue": "ICML",
  "abstract": "We introduce a new model for representation learning and classification of video sequences. Our model is based on a convolutional neural network coupled with a novel temporal pooling layer. The temporal pooling layer relies on an inner-optimization problem to efficiently encode temporal semantics over arbitrarily long video clips into a fixed-length vector representation. Importantly, the representation and classification parameters of our model can be estimated jointly in an end-to-end manner by formulating learning as a bilevel optimization problem. Furthermore, the model can make use of any existing convolutional neural network architecture (e.g., AlexNet or VGG) without modification or introduction of additional parameters. We demonstrate our approach on action and activity recognition tasks.",
  "stance": 0.6
 },
 {
  "url": "https://proceedings.mlr.press/v48/gea16.pdf",
  "title": "Rich Component Analysis",
  "year": 2016,
  "venue": "ICML",
  "abstract": "In many settings, we have multiple data sets (also called views) that capture different and overlapping aspects of the same phenomenon. We are often interested in finding patterns that are unique to one or to a subset of the views. For example, we might have one set of molecular observations and one set of physiological observations on the same group of individuals, and we want to quantify molecular patterns that are uncorrelated with physiology. Despite being a common problem, this is highly challenging when the correlations come from complex distributions. In this paper, we develop the general framework of Rich Component Analysis (RCA) to model settings where the observations from different views are driven by different sets of latent components, and each component can be a complex, high-dimensional distribution. We introduce algorithms based on cumulant extraction that provably learn each of the components without having to model the other components. We show how to integrate RCA with stochastic gradient descent into a meta-algorithm for learning general models, and demonstrate substantial improvement in accuracy on several synthetic and real datasets in both supervised and unsupervised tasks. Our method makes it possible to learn latent variable models when we don’t have samples from the true model but only samples after complex perturbations.",
  "stance": 0.8
 },
 {
  "url": "https://proceedings.mlr.press/v48/lic16.pdf",
  "title": "Fast k-Nearest Neighbour Search via Dynamic Continuous Indexing",
  "year": 2016,
  "venue": "ICML",
  "abstract": "Existing methods for retrieving k-nearest neighbours suffer from the curse of dimensionality. We argue this is caused in part by inherent deficiencies of space partitioning, which is the underlying strategy used by most existing methods. We devise a new strategy that avoids partitioning the vector space and present a novel randomized algorithm that runs in time linear in dimensionality of the space and sub-linear in the intrinsic dimensionality and the size of the dataset and takes space constant in dimensionality of the space and linear in the size of the dataset. The proposed algorithm allows fine-grained control over accuracy and speed on a per-query basis, automatically adapts to variations in data density, supports dynamic updates to the dataset and is easy-toimplement. We show appealing theoretical properties and demonstrate empirically that the proposed algorithm outperforms locality-sensitivity hashing (LSH) in terms of approximation quality, speed and space efficiency.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v48/papakonstantinou16.pdf",
  "title": "On the Power and Limits of Distance-Based Learning",
  "year": 2016,
  "venue": "ICML",
  "abstract": "We initiate the study of low-distortion finite metric embeddings in multi-class (and multi-label) classification where (i) both the space of input instances and the space of output classes have combinatorial metric structure, and (ii) the concepts we wish to learn are low-distortion embeddings. We develop new geometric techniques and prove strong learning lower bounds. These provable limits hold even when we allow learners and classifiers to get advice by one or more experts. Our study overwhelmingly indicates that post-geometry assumptions are necessary in multi-class classification, as in natural language processing (NLP). Technically, the mathematical tools we developed in this work could be of independent interest to NLP. To the best of our knowledge, this is the first work which formally studies classification problems in combinatorial spaces and where the concepts are low-distortion embeddings.",
  "stance": 0.6
 },
 {
  "url": "https://proceedings.mlr.press/v48/simsek16.pdf",
  "title": "Why Most Decisions Are Easy in Tetris—And Perhaps in Other Sequential Decision Problems, As Well",
  "year": 2016,
  "venue": "ICML",
  "abstract": "We examined the sequence of decision problems that are encountered in the game of Tetris and found that most of the problems are easy in the following sense: One can choose well among the available actions without knowing an evaluation function that scores well in the game. This is a consequence of three conditions that are prevalent in the game: simple dominance, cumulative dominance, and noncompensation. These conditions can be exploited to develop faster and more effective learning algorithms. In addition, they allow certain types of domain knowledge to be incorporated with ease into a learning algorithm. Among the sequential decision problems we encounter, it is unlikely that Tetris is unique or rare in having these properties.",
  "stance": 0.1
 },
 {
  "url": "https://proceedings.mlr.press/v48/ustinovskiy16.pdf",
  "title": "Meta–Gradient Boosted Decision Tree Model for Weight and Target Learning",
  "year": 2016,
  "venue": "ICML",
  "abstract": "Labeled training data is an essential part of any supervised machine learning framework. In practice, there is a trade-off between the quality of a label and its cost. In this paper, we consider a problem of learning to rank on a large-scale dataset with low-quality relevance labels aiming at maximizing the quality of a trained ranker on a small validation dataset with high-quality ground truth relevance labels. Motivated by the classical Gauss-Markov theorem for the linear regression problem, we formulate the problems of (1) reweighting training instances and (2) remapping learning targets. We propose meta– gradient decision tree learning framework for optimizing weight and target functions by applying gradient-based hyperparameter optimization. Experiments on a large-scale real-world dataset demonstrate that we can significantly improve state-of-the-art machine-learning algorithms by incorporating our framework.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v70/agarwal17a/agarwal17a.pdf",
  "title": "The Price of Differential Privacy for Online Learning",
  "year": 2017,
  "venue": "ICML",
  "abstract": "In the paradigm of online learning, a learning algorithm makes a sequence of predictions given the (possibly incomplete) knowledge of the correct answers for the past queries. In contrast to statistical learning, online learning algorithms typically offer distribution-free guarantees. Consequently, online learning algorithms are well suited to dynamic and adversarial environments, where real-time learning from changing data is essential making them ubiquitous in practical applications such as servicing search advertisements. In these settings often these algorithms interact with sensitive user data, making privacy a natural concern for these algorithms. A natural notion of privacy in such settings is differential privacy (Dwork et al., 2006) which ensures that the outputs of an algorithm are indistinguishable in the case when a user’s data is present as opposed to when it is absent in a dataset.",
  "stance": -0.2
 },
 {
  "url": "https://proceedings.mlr.press/v70/allen-zhu17a/allen-zhu17a.pdf",
  "title": "Natasha: Faster Non-Convex Stochastic Optimization via Strongly Non-Convex Parameter",
  "year": 2017,
  "venue": "ICML",
  "abstract": "Given a non-convex function f(x) that is an average of n smooth functions, we design stochastic first-order methods to find its approximate stationary points. The performance of our new methods depend on the smallest (negative) eigenvalue −σ of the Hessian. This parameter σ captures how strongly non-convex f(x) is, and is analogous to the strong convexity parameter for convex optimization. At least in theory, our methods outperform known results for a range of parameter σ, and can also be used to find approximate local minima. Our result implies an interesting dichotomy: there exists a threshold σ0 so that the (currently) fastest methods for σ > σ0 and for σ < σ0 have different behaviors: the former scales with n and the latter scales with n.",
  "stance": 0.8
 },
 {
  "url": "https://proceedings.mlr.press/v70/andreas17a/andreas17a.pdf",
  "title": "Modular Multitask Reinforcement Learning with Policy Sketches",
  "year": 2017,
  "venue": "ICML",
  "abstract": "We describe a framework for multitask deep reinforcement learning guided by policy sketches. Sketches annotate tasks with sequences of named subtasks, providing information about high-level structural relationships among tasks but not how to implement them—specifically not providing the detailed guidance used by much previous work on learning policy abstractions for RL (e.g. intermediate rewards, subtask completion signals, or intrinsic motivations). To learn from sketches, we present a model that associates every subtask with a modular subpolicy, and jointly maximizes reward over full task-specific policies by tying parameters across shared subpolicies. Optimization is accomplished via a decoupled actor–critic training objective that facilitates learning common behaviors from multiple dissimilar reward functions. We evaluate the effectiveness of our approach in three environments featuring both discrete and continuous control, and with sparse rewards that can be obtained only after completing a number of high-level subgoals. Experiments show that using our approach to learn policies guided by sketches gives better performance than existing techniques for learning task-specific or shared policies, while naturally inducing a library of interpretable primitive behaviors that can be recombined to rapidly adapt to new tasks.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v70/arik17a/arik17a.pdf",
  "title": "Deep Voice: Real-time Neural Text-to-Speech",
  "year": 2017,
  "venue": "ICML",
  "abstract": "We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-tophoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-tospeech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v70/arjevani17a/arjevani17a.pdf",
  "title": "Oracle Complexity of Second-Order Methods for Finite-Sum Problems",
  "year": 2017,
  "venue": "ICML",
  "abstract": "Finite-sum optimization problems are ubiquitous in machine learning, and are commonly solved using first-order methods which rely on gradient computations. Recently, there has been growing interest in second-order methods, which rely on both gradients and Hessians. In principle, second-order methods can require much fewer iterations than first-order methods, and hold the promise for more efficient algorithms. Although computing and manipulating Hessians is prohibitive for high-dimensional problems in general, the Hessians of individual functions in finite-sum problems can often be efficiently computed, e.g. because they possess a low-rank structure. Can second-order information indeed be used to solve such problems more efficiently? In this paper, we provide evidence that the answer – perhaps surprisingly – is negative, at least in terms of worst-case guarantees. We also discuss what additional assumptions and algorithmic approaches might potentially circumvent this negative result.",
  "stance": -0.2
 },
 {
  "url": "https://proceedings.mlr.press/v70/arpit17a/arpit17a.pdf",
  "title": "A Closer Look at Memorization in Deep Networks",
  "year": 2017,
  "venue": "ICML",
  "abstract": "We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization.",
  "stance": 0.1
 },
 {
  "url": "https://proceedings.mlr.press/v70/balog17a/balog17a.pdf",
  "title": "Lost Relatives of the Gumbel Trick",
  "year": 2017,
  "venue": "ICML",
  "abstract": "The Gumbel trick is a method to sample from a discrete probability distribution, or to estimate its normalizing partition function. The method relies on repeatedly applying a random perturbation to the distribution in a particular way, each time solving for the most likely configuration. We derive an entire family of related methods, of which the Gumbel trick is one member, and show that the new methods have superior properties in several settings with minimal additional computational cost. In particular, for the Gumbel trick to yield computational benefits for discrete graphical models, Gumbel perturbations on all configurations are typically replaced with socalled low-rank perturbations. We show how a subfamily of our new methods adapts to this setting, proving new upper and lower bounds on the log partition function and deriving a family of sequential samplers for the Gibbs distribution. Finally, we balance the discussion by showing how the simpler analytical form of the Gumbel trick enables additional theoretical results.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v70/bamler17a/bamler17a.pdf",
  "title": "Dynamic Word Embeddings",
  "year": 2017,
  "venue": "ICML",
  "abstract": "We present a probabilistic language model for time-stamped text data which tracks the semantic evolution of individual words over time. The model represents words and contexts by latent trajectories in an embedding space. At each moment in time, the embedding vectors are inferred from a probabilistic version of word2vec (Mikolov et al., 2013b). These embedding vectors are connected in time through a latent diffusion process. We describe two scalable variational inference algorithms—skipgram smoothing and skip-gram filtering—that allow us to train the model jointly over all times; thus learning on all data while simultaneously allowing word and context vectors to drift. Experimental results on three different corpora demonstrate that our dynamic model infers word embedding trajectories that are more interpretable and lead to higher predictive likelihoods than competing methods that are based on static models trained separately on time slices.",
  "stance": 0.9
 },
 {
  "url": "https://proceedings.mlr.press/v70/brown17a/brown17a.pdf",
  "title": "Reduced Space and Faster Convergence in Imperfect-Information Games via Pruning",
  "year": 2017,
  "venue": "ICML",
  "abstract": "Iterative algorithms such as Counterfactual Regret Minimization (CFR) are the most popular way to solve large zero-sum imperfect-information games. In this paper we introduce Best-Response Pruning (BRP), an improvement to iterative algorithms such as CFR that allows poorly-performing actions to be temporarily pruned. We prove that when using CFR in zero-sum games, adding BRP will asymptotically prune any action that is not part of a best response to some Nash equilibrium. This leads to provably faster convergence and lower space requirements. Experiments show that BRP results in a factor of 7 reduction in space, and the reduction factor increases with game size.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v70/czarnecki17a/czarnecki17a.pdf",
  "title": "Understanding Synthetic Gradients and Decoupled Neural Interfaces",
  "year": 2017,
  "venue": "ICML",
  "abstract": "When training neural networks, the use of Synthetic Gradients (SG) allows layers or modules to be trained without update locking – without waiting for a true error gradient to be backpropagated – resulting in Decoupled Neural Interfaces (DNIs). This unlocked ability of being able to update parts of a neural network asynchronously and with only local information was demonstrated to work empirically in Jaderberg et al. (2016). However, there has been very little demonstration of what changes DNIs and SGs impose from a functional, representational, and learning dynamics point of view. In this paper, we study DNIs through the use of synthetic gradients on feed-forward networks to better understand their behaviour and elucidate their effect on optimisation. We show that the incorporation of SGs does not affect the representational strength of the learning system for a neural network, and prove the convergence of the learning system for linear and deep linear models. On practical problems we investigate the mechanism by which synthetic gradient estimators approximate the true loss, and, surprisingly, how that leads to drastically different layer-wise representations. Finally, we also expose the relationship of using synthetic gradients to other error approximation techniques and find a unifying language for discussion and comparison.",
  "stance": 0.3
 },
 {
  "url": "https://proceedings.mlr.press/v70/dembczynski17a/dembczynski17a.pdf",
  "title": "Consistency Analysis for Binary Classification Revisited",
  "year": 2017,
  "venue": "ICML",
  "abstract": "Statistical learning theory is at an inflection point enabled by recent advances in understanding and optimizing a wide range of metrics. Of particular interest are non-decomposable metrics such as the F-measure and the Jaccard measure which cannot be represented as a simple average over examples. Non-decomposability is the primary source of difficulty in theoretical analysis, and interestingly has led to two distinct settings and notions of consistency. In this manuscript we analyze both settings, from statistical and algorithmic points of view, to explore the connections and to highlight differences between them for a wide range of metrics. The analysis complements previous results on this topic, clarifies common confusions around both settings, and provides guidance to the theory and practice of binary classification with complex metrics.",
  "stance": 0.0
 },
 {
  "url": "https://proceedings.mlr.press/v70/dinh17b/dinh17b.pdf",
  "title": "Sharp Minima Can Generalize For Deep Nets",
  "year": 2017,
  "venue": "ICML",
  "abstract": "Despite their overwhelming capacity to overfit, deep learning architectures tend to generalize relatively well to unseen data, allowing them to be deployed in practice. However, explaining why this is the case is still an open area of research. One standing hypothesis that is gaining popularity, e.g. Hochreiter & Schmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization. This paper argues that most notions of flatness are problematic for deep models and can not be directly applied to explain generalization. Specifically, when focusing on deep networks with rectifier units, we can exploit the particular geometry of parameter space induced by the inherent symmetries that these architectures exhibit to build equivalent models corresponding to arbitrarily sharper minima. Furthermore, if we allow to reparametrize a function, the geometry of its parameters can change drastically without affecting its generalization properties.",
  "stance": -0.7
 },
 {
  "url": "https://proceedings.mlr.press/v70/finn17a/finn17a.pdf",
  "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
  "year": 2017,
  "venue": "ICML",
  "abstract": "We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two fewshot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v70/grave17a/grave17a.pdf",
  "title": "Efficient softmax approximation for GPUs",
  "year": 2017,
  "venue": "ICML",
  "abstract": "We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computation time. Our approach further reduces the computational time by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax. The code of our method is available at https://github.com/ facebookresearch/adaptive-softmax.",
  "stance": 0.9
 },
 {
  "url": "https://proceedings.mlr.press/v70/graves17a/graves17a.pdf",
  "title": "Automated Curriculum Learning for Neural Networks",
  "year": 2017,
  "venue": "ICML",
  "abstract": "We introduce a method for automatically selecting the path, or syllabus, that a neural network follows through a curriculum so as to maximise learning efficiency. A measure of the amount that the network learns from each data sample is provided as a reward signal to a nonstationary multiarmed bandit algorithm, which then determines a stochastic syllabus. We consider a range of signals derived from two distinct indicators of learning progress: rate of increase in prediction accuracy, and rate of increase in network complexity. Experimental results for LSTM networks on three curricula demonstrate that our approach can significantly accelerate learning, in some cases halving the time required to attain a satisfactory performance level.",
  "stance": 0.9
 },
 {
  "url": "https://proceedings.mlr.press/v70/guo17a/guo17a.pdf",
  "title": "On Calibration of Modern Neural Networks",
  "year": 2017,
  "venue": "ICML",
  "abstract": "Confidence calibration – the problem of predicting probability estimates representative of the true correctness likelihood – is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-ofthe-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling – a singleparameter variant of Platt Scaling – is surprisingly effective at calibrating predictions.",
  "stance": 0.1
 },
 {
  "url": "https://proceedings.mlr.press/v70/ingraham17a/ingraham17a.pdf",
  "title": "Variational Inference for Sparse and Undirected Models",
  "year": 2017,
  "venue": "ICML",
  "abstract": "Undirected graphical models are applied in genomics, protein structure prediction, and neuroscience to identify sparse interactions that underlie discrete data. Although Bayesian methods for inference would be favorable in these contexts, they are rarely used because they require doubly intractable Monte Carlo sampling. Here, we develop a framework for scalable Bayesian inference of discrete undirected models based on two new methods. The first is Persistent VI, an algorithm for variational inference of discrete undirected models that avoids doubly intractable MCMC and approximations of the partition function. The second is Fadeout, a reparameterization approach for variational inference under sparsity-inducing priors that captures a posteriori correlations between parameters and hyperparameters with noncentered parameterizations. We find that, together, these methods for variational inference substantially improve learning of sparse undirected graphical models in simulated and real problems from physics and biology.",
  "stance": 0.0
 },
 {
  "url": "https://proceedings.mlr.press/v70/katt17a/katt17a.pdf",
  "title": "Learning in POMDPs with Monte Carlo Tree Search",
  "year": 2017,
  "venue": "ICML",
  "abstract": "The POMDP is a powerful framework for reasoning under outcome and information uncertainty, but constructing an accurate POMDP model is difficult. Bayes-Adaptive Partially Observable Markov Decision Processes (BA-POMDPs) extend POMDPs to allow the model to be learned during execution. BA-POMDPs are a Bayesian RL approach that, in principle, allows for an optimal trade-off between exploitation and exploration. Unfortunately, BA-POMDPs are currently impractical to solve for any non-trivial domain. In this paper, we extend the Monte-Carlo Tree Search method POMCP to BA-POMDPs and show that the resulting method, which we call BA-POMCP, is able to tackle problems that previous solution methods have been unable to solve. Additionally, we introduce several techniques that exploit the BA-POMDP structure to improve the efficiency of BA-POMCP along with proof of their convergence.",
  "stance": 0.9
 },
 {
  "url": "https://proceedings.mlr.press/v70/levy17a/levy17a.pdf",
  "title": "Learning to Align the Source Code to the Compiled Object Code",
  "year": 2017,
  "venue": "ICML",
  "abstract": "We propose a new neural network architecture and use it for the task of statement-by-statement alignment of source code and its compiled object code. Our architecture learns the alignment between the two sequences – one being the translation of the other – by mapping each statement to a context-dependent representation vector and aligning such vectors using a grid of the two sequence domains. Our experiments include short C functions, both artificial and human-written, and show that our neural network architecture is able to predict the alignment with high accuracy, outperforming known baselines. We also demonstrate that our model is general and can learn to solve graph problems such as the Traveling Salesman Problem.",
  "stance": 0.9
 },
 {
  "url": "https://proceedings.mlr.press/v70/maystre17b/maystre17b.pdf",
  "title": "ChoiceRank: Identifying Preferences from Node Traffic in Networks",
  "year": 2017,
  "venue": "ICML",
  "abstract": "Understanding how users navigate in a network is of high interest in many applications. We consider a setting where only aggregate node-level traffic is observed and tackle the task of learning edge transition probabilities. We cast it as a preference learning problem, and we study a model where choices follow Luce’s axiom. In this case, the O(n) marginal counts of node visits are a sufficient statistic for the O(n) transition probabilities. We show how to make the inference problem well-posed regardless of the network’s structure, and we present ChoiceRank, an iterative algorithm that scales to networks that contains billions of nodes and edges. We apply the model to two clickstream datasets and show that it successfully recovers the transition probabilities using only the network structure and marginal (nodelevel) traffic data. Finally, we also consider an application to mobility networks and apply the model to one year of rides on New York City’s bicycle-sharing system.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v70/mishra17a/mishra17a.pdf",
  "title": "Prediction and Control with Temporal Segment Models",
  "year": 2017,
  "venue": "ICML",
  "abstract": "We introduce a method for learning the dynamics of complex nonlinear systems based on deep generative models over temporal segments of states and actions. Unlike dynamics models that operate over individual discrete timesteps, we learn the distribution over future state trajectories conditioned on past state, past action, and planned future action trajectories, as well as a latent prior over action trajectories. Our approach is based on convolutional autoregressive models and variational autoencoders. It makes stable and accurate predictions over long horizons for complex, stochastic systems, effectively expressing uncertainty and modeling the effects of collisions, sensory noise, and action delays. The learned dynamics model and action prior can be used for end-to-end, fully differentiable trajectory optimization and model-based policy optimization, which we use to evaluate the performance and sample-efficiency of our method.",
  "stance": 0.8
 },
 {
  "url": "https://proceedings.mlr.press/v70/shalev-shwartz17a/shalev-shwartz17a.pdf",
  "title": "Failures of Gradient-Based Deep Learning",
  "year": 2017,
  "venue": "ICML",
  "abstract": "In recent years, Deep Learning has become the go-to solution for a broad range of applications, often outperforming state-of-the-art. However, it is important, for both theoreticians and practitioners, to gain a deeper understanding of the difficulties and limitations associated with common approaches and algorithms. We describe four types of simple problems, for which the gradientbased algorithms commonly used in deep learning either fail or suffer from significant difficulties. We illustrate the failures through practical experiments, and provide theoretical insights explaining their source, and how they might be remedied.",
  "stance": -1.0
 },
 {
  "url": "https://proceedings.mlr.press/v70/sun17c/sun17c.pdf",
  "title": "meProp: Sparsified Back Propagation for Accelerated Deep Learning with Reduced Overfitting",
  "year": 2017,
  "venue": "ICML",
  "abstract": "We propose a simple yet effective technique for neural network learning. The forward propagation is computed as usual. In back propagation, only a small subset of the full gradient is computed to update the model parameters. The gradient vectors are sparsified in such a way that only the top-k elements (in terms of magnitude) are kept. As a result, only k rows or columns (depending on the layout) of the weight matrix are modified, leading to a linear reduction (k divided by the vector dimension) in the computational cost. Surprisingly, experimental results demonstrate that we can update only 1–4% of the weights at each back propagation pass. This does not result in a larger number of training iterations. More interestingly, the accuracy of the resulting models is actually improved rather than degraded, and a detailed analysis is given.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v70/ye17b/ye17b.pdf",
  "title": "A Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization",
  "year": 2017,
  "venue": "ICML",
  "abstract": "Learning under a Wasserstein loss, a.k.a. Wasserstein loss minimization (WLM), is an emerging research topic for gaining insights from a large set of structured objects. Despite being conceptually simple, WLM problems are computationally challenging because they involve minimizing over functions of quantities (i.e. Wasserstein distances) that themselves require numerical algorithms to compute. In this paper, we introduce a stochastic approach based on simulated annealing for solving WLMs. Particularly, we have developed a Gibbs sampler to approximate effectively and efficiently the partial gradients of a sequence of Wasserstein losses. Our new approach has the advantages of numerical stability and readiness for warm starts. These characteristics are valuable for WLM problems that often require multiple levels of iterations in which the oracle for computing the value and gradient of a loss function is embedded. We applied the method to optimal transport with Coulomb cost and the Wasserstein non-negative matrix factorization problem, and made comparisons with the existing method of entropy regularization.",
  "stance": 0.4
 },
 {
  "url": "https://proceedings.mlr.press/v80/kamnitsas18a/kamnitsas18a.pdf",
  "title": "Semi-Supervised Learning via Compact Latent Space Clustering",
  "year": 2018,
  "venue": "ICML",
  "abstract": "We present a novel cost function for semisupervised learning of neural networks that encourages compact clustering of the latent space to facilitate separation. The key idea is to dynamically create a graph over embeddings of labeled and unlabeled samples of a training batch to capture underlying structure in feature space, and use label propagation to estimate its high and low density regions. We then devise a cost function based on Markov chains on the graph that regularizes the latent space to form a single compact cluster per class, while avoiding to disturb existing clusters during optimization. We evaluate our approach on three benchmarks and compare to state-of-the art with promising results. Our approach combines the benefits of graph-based regularization with efficient, inductive inference, does not require modifications to a network architecture, and can thus be easily applied to existing networks to enable an effective use of unlabeled data.",
  "stance": 0.8
 },
 {
  "url": "https://proceedings.mlr.press/v80/lake18a/lake18a.pdf",
  "title": "Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks",
  "year": 2018,
  "venue": "ICML",
  "abstract": "Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb “dax,” he or she can immediately understand the meaning of “dax twice” or “sing and dax.” In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can make successful zero-shot generalizations when the differences between training and test commands are small, so that they can apply “mix-and-match” strategies to solve the task. However, when generalization requires systematic compositional skills (as in the “dax” example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks’ notorious training data thirst.",
  "stance": -0.5
 },
 {
  "url": "https://proceedings.mlr.press/v80/li18d/li18d.pdf",
  "title": "On the Limitations of First-Order Approximation in GAN Dynamics",
  "year": 2018,
  "venue": "ICML",
  "abstract": "While Generative Adversarial Networks (GANs) have demonstrated promising performance on multiple vision tasks, their learning dynamics are not yet well understood, both in theory and in practice. To address this issue, we study GAN dynamics in a simple yet rich parametric model that exhibits several of the common problematic convergence behaviors such as vanishing gradients, mode collapse, and diverging or oscillatory behavior. In spite of the non-convex nature of our model, we are able to perform a rigorous theoretical analysis of its convergence behavior. Our analysis reveals an interesting dichotomy: a GAN with an optimal discriminator provably converges, while first order approximations of the discriminator steps lead to unstable GAN dynamics and mode collapse. Our result suggests that using first order discriminator steps (the de-facto standard in most existing GAN setups) might be one of the factors that makes GAN training challenging in practice.",
  "stance": 0.0
 },
 {
  "url": "https://proceedings.mlr.press/v80/liang18a/liang18a.pdf",
  "title": "Understanding the Loss Surface of Neural Networks for Binary Classification",
  "year": 2018,
  "venue": "ICML",
  "abstract": "It is widely conjectured that training algorithms for neural networks are successful because all local minima lead to similar performance; for example, see (LeCun et al., 2015; Choromanska et al., 2015; Dauphin et al., 2014). Performance is typically measured in terms of two metrics: training performance and generalization performance. Here we focus on the training performance of neural networks for binary classification, and provide conditions under which the training error is zero at all local minima of appropriately chosen surrogate loss functions. Our conditions are roughly in the following form: the neurons have to be increasing and strictly convex, the neural network should either be single-layered or is multi-layered with a shortcutlike connection, and the surrogate loss function should be a smooth version of hinge loss. We also provide counterexamples to show that, when these conditions are relaxed, the result may not hold.",
  "stance": -0.5
 },
 {
  "url": "https://proceedings.mlr.press/v80/liu18c/liu18c.pdf",
  "title": "Delayed Impact of Fair Machine Learning",
  "year": 2018,
  "venue": "ICML",
  "abstract": "Fairness in machine learning has predominantly been studied in static classification settings without concern for how decisions change the underlying population over time. Conventional wisdom suggests that fairness criteria promote the longterm well-being of those groups they aim to protect. We study how static fairness criteria interact with temporal indicators of well-being, such as long-term improvement, stagnation, and decline in a variable of interest. We demonstrate that even in a one-step feedback model, common fairness criteria in general do not promote improvement over time, and may in fact cause harm in cases where an unconstrained objective would not. We completely characterize the delayed impact of three standard criteria, contrasting the regimes in which these exhibit qualitatively different behavior. In addition, we find that a natural form of measurement error broadens the regime in which fairness criteria perform favorably. Our results highlight the importance of measurement and temporal modeling in the evaluation of fairness criteria, suggesting a range of new challenges and trade-offs.",
  "stance": 0.0
 },
 {
  "url": "https://proceedings.mlr.press/v80/loukas18a/loukas18a.pdf",
  "title": "Spectrally Approximating Large Graphs with Smaller Graphs",
  "year": 2018,
  "venue": "ICML",
  "abstract": "How does coarsening affect the spectrum of a general graph? We provide conditions such that the principal eigenvalues and eigenspaces of a coarsened and original graph Laplacian matrices are close. The achieved approximation is shown to depend on standard graph-theoretic properties, such as the degree and eigenvalue distributions, as well as on the ratio between the coarsened and actual graph sizes. Our results carry implications for learning methods that utilize coarsening. For the particular case of spectral clustering, they imply that coarse eigenvectors can be used to derive good quality assignments even without refinement—this phenomenon was previously observed, but lacked formal justification.",
  "stance": 0.3
 },
 {
  "url": "https://proceedings.mlr.press/v80/madras18a/madras18a.pdf",
  "title": "Learning Adversarially Fair and Transferable Representations",
  "year": 2018,
  "venue": "ICML",
  "abstract": "In this paper, we advocate for representation learning as the key to mitigating unfair prediction outcomes downstream. Motivated by a scenario where learned representations are used by third parties with unknown objectives, we propose and explore adversarial representation learning as a natural method of ensuring those parties act fairly. We connect group fairness (demographic parity, equalized odds, and equal opportunity) to different adversarial objectives. Through worst-case theoretical guarantees and experimental validation, we show that the choice of this objective is crucial to fair prediction. Furthermore, we present the first in-depth experimental demonstration of fair transfer learning and demonstrate empirically that our learned representations admit fair predictions on new tasks while maintaining utility, an essential goal of fair representation learning.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v80/nguyen18b/nguyen18b.pdf",
  "title": "Neural Networks Should Be Wide Enough to Learn Disconnected Decision Regions",
  "year": 2018,
  "venue": "ICML",
  "abstract": "In the recent literature the important role of depth in deep learning has been emphasized. In this paper we argue that sufficient width of a feedforward network is equally important by answering the simple question under which conditions the decision regions of a neural network are connected. It turns out that for a class of activation functions including leaky ReLU, neural networks having a pyramidal structure, that is no layer has more hidden units than the input dimension, produce necessarily connected decision regions. This implies that a sufficiently wide hidden layer is necessary to guarantee that the network can produce disconnected decision regions. We discuss the implications of this result for the construction of neural networks, in particular the relation to the problem of adversarial manipulation of classifiers.",
  "stance": 0.0
 },
 {
  "url": "https://proceedings.mlr.press/v80/reagan18a/reagan18a.pdf",
  "title": "Weightless: Lossy weight encoding for deep neural network compression",
  "year": 2018,
  "venue": "ICML",
  "abstract": "The large memory requirements of deep neural networks limit their deployment and adoption on many devices. Model compression methods effectively reduce the memory requirements of these models, usually through applying transformations such as weight pruning or quantization. In this paper, we present a novel scheme for lossy weight encoding co-designed with weight simplification techniques. The encoding is based on the Bloomier filter, a probabilistic data structure that can save space at the cost of introducing random errors. Leveraging the ability of neural networks to tolerate these imperfections and by re-training around the errors, the proposed technique, named Weightless, can compress weights by up to 496× without loss of model accuracy. This results in up to a 1.51× improvement over the state-of-the-art.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v97/allen19a/allen19a.pdf",
  "title": "Analogies Explained: Towards Understanding Word Embeddings",
  "year": 2019,
  "venue": "ICML",
  "abstract": "Word embeddings generated by neural network methods such as word2vec (W2V) are well known to exhibit seemingly linear behaviour, e.g. the embeddings of analogy “woman is to queen as man is to king” approximately describe a parallelogram. This property is particularly intriguing since the embeddings are not trained to achieve it. Several explanations have been proposed, but each introduces assumptions that do not hold in practice. We derive a probabilistically grounded definition of paraphrasing that we re-interpret as word transformation, a mathematical description of “wx is to wy”. From these concepts we prove existence of linear relationships between W2V-type embeddings that underlie the analogical phenomenon, identifying explicit error terms.",
  "stance": -0.5
 },
 {
  "url": "https://proceedings.mlr.press/v97/amin19a/amin19a.pdf",
  "title": "Bounding User Contributions: A Bias-Variance Trade-off in Differential Privacy",
  "year": 2019,
  "venue": "ICML",
  "abstract": "Differentially private learning algorithms protect individual participants in the training dataset by guaranteeing that their presence does not significantly change the resulting model. In order to make this promise, such algorithms need to know the maximum contribution that can be made by a single user: the more data an individual can contribute, the more noise will need to be added to protect them. While most existing analyses assume that the maximum contribution is known and fixed in advance—indeed, it is often assumed that each user contributes only a single example— we argue that in practice there is a meaningful choice to be made. On the one hand, if we allow users to contribute large amounts of data, we may end up adding excessive noise to protect a few outliers, even when the majority contribute only modestly. On the other hand, limiting users to small contributions keeps noise levels low at the cost of potentially discarding significant amounts of excess data, thus introducing bias. Here, we characterize this trade-off for an empirical risk minimization setting, showing that in general there is a “sweet spot” that depends on measurable properties of the dataset, but that there is also a concrete cost to privacy that cannot be avoided simply by collecting more data.",
  "stance": 0.0
 },
 {
  "url": "https://proceedings.mlr.press/v97/bouthillier19a/bouthillier19a.pdf",
  "title": "Unreproducible Research is Reproducible",
  "year": 2019,
  "venue": "ICML",
  "abstract": "The apparent contradiction in the title is a wordplay on the different meanings attributed to the word reproducible across different scientific fields. What we imply is that unreproducible findings can be built upon reproducible methods. Without denying the importance of facilitating the reproduction of methods, we deem important to reassert that reproduction of findings is a fundamental step of the scientific inquiry. We argue that the commendable quest towards easy deterministic reproducibility of methods and numerical results should not have us forget the even more important necessity of ensuring the reproducibility of empirical findings and conclusions by properly accounting for essential sources of variations. We provide experiments to exemplify the brittleness of current common practice in the evaluation of models in the field of deep learning, showing that even if the results could be reproduced, a slightly different experiment would not support the findings. We hope to help clarify the distinction between exploratory and empirical research in the field of deep learning and believe more energy should be devoted to proper empirical research in our community. This work is an attempt to promote the use of more rigorous and diversified methodologies. It is not an attempt to impose a new methodology and it is not a critique on the nature of exploratory research.",
  "stance": 0.0
 },
 {
  "url": "https://proceedings.mlr.press/v97/bubeck19a/bubeck19a.pdf",
  "title": "Adversarial examples from computational constraints",
  "year": 2019,
  "venue": "ICML",
  "abstract": "Why are classifiers in high dimension vulnerable to “adversarial” perturbations? We show that it is likely not due to information theoretic limitations, but rather it could be due to computational constraints. First we prove that, for a broad set of classification tasks, the mere existence of a robust classifier implies that it can be found by a possibly exponential-time algorithm with relatively few training examples. Then we give two particular classification tasks where learning a robust classifier is computationally intractable. More precisely we construct two binary classifications task in high dimensional space which are (i) information theoretically easy to learn robustly for large perturbations, (ii) efficiently learnable (nonrobustly) by a simple linear separator, (iii) yet are not efficiently robustly learnable, even for small perturbations. Specifically, for the first task hardness holds for any efficient algorithm in the statistical query (SQ) model, while for the second task we rule out any efficient algorithm under a cryptographic assumption. These examples give an exponential separation between classical learning and robust learning in the statistical query model or under a cryptographic assumption. It suggests that adversarial examples may be an unavoidable byproduct of computational limitations of learning algorithms.",
  "stance": -0.4
 },
 {
  "url": "https://proceedings.mlr.press/v97/byrd19a/byrd19a.pdf",
  "title": "What is the Effect of Importance Weighting in Deep Learning?",
  "year": 2019,
  "venue": "ICML",
  "abstract": "Importance-weighted risk minimization is a key ingredient in many machine learning algorithms for causal inference, domain adaptation, class imbalance, and off-policy reinforcement learning. While the effect of importance weighting is wellcharacterized for low-capacity misspecified models, little is known about how it impacts overparameterized, deep neural networks. Inspired by recent theoretical results showing that on (linearly) separable data, deep linear networks optimized by SGD learn weight-agnostic solutions, we ask, for realistic deep networks, for which many practical datasets are separable, what is the effect of importance weighting? We present the surprising finding that while importance weighting impacts deep nets early in training, so long as the nets are able to separate the training data, its effect diminishes over successive epochs. Moreover, while L2 regularization and batch normalization (but not dropout), restore some of the impact of importance weighting, they express the effect via (seemingly) the wrong abstraction: why should practitioners tweak the L2 regularization, and by how much, to produce the correct weighting effect? We experimentally confirm these findings across a range of architectures and datasets.",
  "stance": 0.0
 },
 {
  "url": "https://proceedings.mlr.press/v97/chai19a/chai19a.pdf",
  "title": "Automated Model Selection with Bayesian Quadrature",
  "year": 2019,
  "venue": "ICML",
  "abstract": "We present a novel technique for tailoring Bayesian quadrature (BQ) to model selection. The state-of-the-art for comparing the evidence of multiple models relies on Monte Carlo methods, which converge slowly and are unreliable for computationally expensive models. Although previous research has shown that BQ offers sample efficiency superior to Monte Carlo in computing the evidence of an individual model, applying BQ directly to model comparison may waste computation producing an overly-accurate estimate for the evidence of a clearly poor model. We propose an automated and efficient algorithm for computing the most-relevant quantity for model selection: the posterior model probability. Our technique maximizes the mutual information between this quantity and observations of the models’ likelihoods, yielding efficient sample acquisition across disparate model spaces when likelihood observations are limited. Our method produces moreaccurate posterior estimates using fewer likelihood evaluations than standard Bayesian quadrature and Monte Carlo estimators, as we demonstrate on synthetic and real-world examples.",
  "stance": 0.9
 },
 {
  "url": "https://proceedings.mlr.press/v97/chen19e/chen19e.pdf",
  "title": "Information-Theoretic Considerations in Batch Reinforcement Learning",
  "year": 2019,
  "venue": "ICML",
  "abstract": "Value-function approximation methods that operate in batch mode have foundational importance to reinforcement learning (RL). Finite sample guarantees for these methods often crucially rely on two types of assumptions: (1) mild distribution shift, and (2) representation conditions that are stronger than realizability. However, the necessity (“why do we need them?”) and the naturalness (“when do they hold?”) of such assumptions have largely eluded the literature. In this paper, we revisit these assumptions and provide theoretical results towards answering the above questions, and make steps towards a deeper understanding of value-function approximation.",
  "stance": 0.5
 },
 {
  "url": "https://proceedings.mlr.press/v97/cortes19b/cortes19b.pdf",
  "title": "Active Learning with Disagreement Graphs",
  "year": 2019,
  "venue": "ICML",
  "abstract": "We present two novel enhancements of an online importance-weighted active learning algorithm IWAL, using the properties of disagreements among hypotheses. The first enhancement, IWALD, prunes the hypothesis set with a more aggressive strategy based on the disagreement graph. We show that IWAL-D improves the generalization performance and the label complexity of the original IWAL, and quantify the improvement in terms of a disagreement graph coefficient. The second enhancement, IZOOM, further improves IWAL-D by adaptively zooming into the current version space and thus reducing the best-in-class error. We show that IZOOM admits favorable theoretical guarantees with the changing hypothesis set. We report experimental results on multiple datasets and demonstrate that the proposed algorithms achieve better test performances than IWAL given the same amount of labeling budget.",
  "stance": 0.8
 },
 {
  "url": "https://proceedings.mlr.press/v97/cranko19a/cranko19a.pdf",
  "title": "Monge blunts Bayes: Hardness Results for Adversarial Training",
  "year": 2019,
  "venue": "ICML",
  "abstract": "The last few years have seen a staggering number of empirical studies of the robustness of neural networks in a model of adversarial perturbations of their inputs. Most rely on an adversary which carries out local modifications within prescribed balls. None however has so far questioned the broader picture: how to frame a resource-bounded adversary so that it can be severely detrimental to learning, a non-trivial problem which entails at a minimum the choice of loss and classifiers. We suggest a formal answer for losses that satisfy the minimal statistical requirement of being proper. We pin down a simple sufficient property for any given class of adversaries to be detrimental to learning, involving a central measure of “harmfulness” which generalizes the well-known class of integral probability metrics. A key feature of our result is that it holds for all proper losses, and for a popular subset of these, the optimisation of this central measure appears to be independent of the loss. When classifiers are Lipschitz – a now popular approach in adversarial training –, this optimisation resorts to optimal transport to make a low-budget compression of class marginals. Toy experiments reveal a finding recently separately observed: training against a sufficiently budgeted adversary of this kind improves generalization.",
  "stance": 0.4
 },
 {
  "url": "https://proceedings.mlr.press/v97/crawford19a/crawford19a.pdf",
  "title": "Submodular Cost Submodular Cover with an Approximate Oracle",
  "year": 2019,
  "venue": "ICML",
  "abstract": "In this work, we study the Submodular Cost Submodular Cover problem, which is to minimize the submodular cost required to ensure that the submodular benefit function exceeds a given threshold. Existing approximation ratios for the greedy algorithm assume a value oracle to the benefit function. However, access to a value oracle is not a realistic assumption for many applications of this problem, where the benefit function is difficult to compute. We present two incomparable approximation ratios for this problem with an approximate value oracle and demonstrate that the ratios take on empirically relevant values through a case study with the Influence Threshold problem in online social networks.",
  "stance": 0.5
 },
 {
  "url": "https://proceedings.mlr.press/v97/dereli19a/dereli19a.pdf",
  "title": "A Multitask Multiple Kernel Learning Algorithm for Survival Analysis with Application to Cancer Biology",
  "year": 2019,
  "venue": "ICML",
  "abstract": "Predictive performance of machine learning algorithms on related problems can be improved using multitask learning approaches. Rather than performing survival analysis on each data set to predict survival times of cancer patients, we developed a novel multitask approach based on multiple kernel learning (MKL). Our multitask MKL algorithm both works on multiple cancer data sets and integrates cancer-related pathways/gene sets into survival analysis. We tested our algorithm, which is named as Path2MSurv, on the Cancer Genome Atlas data sets analyzing gene expression profiles of 7,655 patients from 20 cancer types together with cancer-specific pathway/gene set collections. Path2MSurv obtained better or comparable predictive performance when benchmarked against random survival forest, survival support vector machine, and single-task variant of our algorithm. Path2MSurv has the ability to identify key pathways/gene sets in predicting survival times of patients from different cancer types.",
  "stance": 0.9
 },
 {
  "url": "https://proceedings.mlr.press/v97/du19e/du19e.pdf",
  "title": "Task-Agnostic Dynamics Priors for Deep Reinforcement Learning",
  "year": 2019,
  "venue": "ICML",
  "abstract": "While model-based deep reinforcement learning (RL) holds great promise for sample efficiency and generalization, learning an accurate dynamics model is often challenging and requires substantial interaction with the environment. A wide variety of domains have dynamics that share common foundations like the laws of classical mechanics, which are rarely exploited by existing algorithms. In fact, humans continuously acquire and use such dynamics priors to easily adapt to operating in new environments. In this work, we propose an approach to learn task-agnostic dynamics priors from videos and incorporate them into an RL agent. Our method involves pre-training a frame predictor on task-agnostic physics videos to initialize dynamics models (and fine-tune them) for unseen target environments. Our frame prediction architecture, SpatialNet, is designed specifically to capture localized physical phenomena and interactions. Our approach allows for both faster policy learning and convergence to better policies, outperforming competitive approaches on several different environments. We also demonstrate that incorporating this prior allows for more effective transfer between environments.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v97/duncker19a/duncker19a.pdf",
  "title": "Learning interpretable continuous-time models of latent stochastic dynamical systems",
  "year": 2019,
  "venue": "ICML",
  "abstract": "We develop an approach to learn an interpretable semi-parametric model of a latent continuoustime stochastic dynamical system, assuming noisy high-dimensional outputs sampled at uneven times. The dynamics are described by a nonlinear stochastic differential equation (SDE) driven by a Wiener process, with a drift evolution function drawn from a Gaussian process (GP) conditioned on a set of learnt fixed points and corresponding local Jacobian matrices. This form yields a flexible nonparametric model of the dynamics, with a representation corresponding directly to the interpretable portraits routinely employed in the study of nonlinear dynamical systems. The learning algorithm combines inference of continuous latent paths underlying observed data with a sparse variational description of the dynamical process. We demonstrate our approach on simulated data from different nonlinear dynamical systems.",
  "stance": 0.4
 },
 {
  "url": "https://proceedings.mlr.press/v97/edwards19a/edwards19a.pdf",
  "title": "Imitating Latent Policies from Observation",
  "year": 2019,
  "venue": "ICML",
  "abstract": "In this paper, we describe a novel approach to imitation learning that infers latent policies directly from state observations. We introduce a method that characterizes the causal effects of latent actions on observations while simultaneously predicting their likelihood. We then outline an action alignment procedure that leverages a small amount of environment interactions to determine a mapping between the latent and real-world actions. We show that this corrected labeling can be used for imitating the observed behavior, even though no expert actions are given. We evaluate our approach within classic control environments and a platform game and demonstrate that it performs better than standard approaches. Code for this work is available at https://github. com/ashedwards/ILPO.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v97/engstrom19a/engstrom19a.pdf",
  "title": "Exploring the Landscape of Spatial Robustness",
  "year": 2019,
  "venue": "ICML",
  "abstract": "The study of adversarial robustness has so far largely focused on perturbations bound in `pnorms. However, state-of-the-art models turn out to be also vulnerable to other, more natural classes of perturbations such as translations and rotations. In this work, we thoroughly investigate the vulnerability of neural network–based classifiers to rotations and translations. While data augmentation offers relatively small robustness, we use ideas from robust optimization and testtime input aggregation to significantly improve robustness. Finally we find that, in contrast to the `p-norm case, first-order methods cannot reliably find worst-case perturbations. This highlights spatial robustness as a fundamentally different setting requiring additional study.",
  "stance": -0.8
 },
 {
  "url": "https://proceedings.mlr.press/v97/etmann19a/etmann19a.pdf",
  "title": "On the Connection Between Adversarial Robustness and Saliency Map Interpretability",
  "year": 2019,
  "venue": "ICML",
  "abstract": "Recent studies on the adversarial vulnerability of neural networks have shown that models trained to be more robust to adversarial attacks exhibit more interpretable saliency maps than their nonrobust counterparts. We aim to quantify this behavior by considering the alignment between input image and saliency map. We hypothesize that as the distance to the decision boundary grows, so does the alignment. This connection is strictly true in the case of linear models. We confirm these theoretical findings with experiments based on models trained with a local Lipschitz regularization and identify where the non-linear nature of neural networks weakens the relation.",
  "stance": 0.0
 },
 {
  "url": "https://proceedings.mlr.press/v97/gao19b/gao19b.pdf",
  "title": "Deep Generative Learning via Variational Gradient Flow",
  "year": 2019,
  "venue": "ICML",
  "abstract": "We propose a framework to learn deep generative models via Variational Gradient Flow (VGrow) on probability spaces. The evolving distribution that asymptotically converges to the target distribution is governed by a vector field, which is the negative gradient of the first variation of the f -divergence between them. We prove that the evolving distribution coincides with the pushforward distribution through the infinitesimal time composition of residual maps that are perturbations of the identity map along the vector field. The vector field depends on the density ratio of the pushforward distribution and the target distribution, which can be consistently learned from a binary classification problem. Connections of our proposed VGrow method with other popular methods, such as VAE, GAN and flow-based methods, have been established in this framework, gaining new insights of deep generative learning. We also evaluated several commonly used divergences, including Kullback-Leibler, Jensen-Shannon, Jeffreys divergences as well as our newly discovered “logD” divergence which serves as the objective function of the logD-trick GAN. Experimental results on benchmark datasets demonstrate that VGrow can generate high-fidelity images in a stable and efficient manner, achieving competitive performance with state-of-the-art GANs.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v97/ghorbani19a/ghorbani19a.pdf",
  "title": "An Instability in Variational Inference for Topic Models",
  "year": 2019,
  "venue": "ICML",
  "abstract": "Naive mean field variational methods are the stateof-the-art approach to inference in topic models. We show that these methods suffer from an instability that can produce misleading conclusions. Namely, for certain regimes of the model parameters, variational inference outputs a non-trivial decomposition into topics. However -–for the same parameter values-– the data contain no actual information about the true topic decomposition, and the output of the algorithm is uncorrelated with it. In particular, the estimated posterior mean is wrong, and estimated credible regions do not achieve the nominal coverage. We discuss how this instability is remedied by more accurate mean field approximations.",
  "stance": -1.0
 },
 {
  "url": "https://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf",
  "title": "Adversarial Examples Are a Natural Consequence of Test Error in Noise",
  "year": 2019,
  "venue": "ICML",
  "abstract": "Over the last few years, the phenomenon of adversarial examples — maliciously constructed inputs that fool trained machine learning models — has captured the attention of the research community, especially when the adversary is restricted to small modifications of a correctly handled input. Less surprisingly, image classifiers also lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this paper we provide both empirical and theoretical evidence that these are two manifestations of the same underlying phenomenon. We establish close connections between the adversarial robustness and corruption robustness research programs, with the strongest connection in the case of additive Gaussian noise. This suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. Based on our results we recommend that future adversarial defenses consider evaluating the robustness of their methods to distributional shift with benchmarks such as ImageNet-C.",
  "stance": -1.0
 },
 {
  "url": "https://proceedings.mlr.press/v97/greenfeld19a/greenfeld19a.pdf",
  "title": "Learning to Optimize Multigrid PDE Solvers",
  "year": 2019,
  "venue": "ICML",
  "abstract": "Constructing fast numerical solvers for partial differential equations (PDEs) is crucial for many scientific disciplines. A leading technique for solving large-scale PDEs is using multigrid methods. At the core of a multigrid solver is the prolongation matrix, which relates between different scales of the problem. This matrix is strongly problem-dependent, and its optimal construction is critical to the efficiency of the solver. In practice, however, devising multigrid algorithms for new problems often poses formidable challenges. In this paper we propose a framework for learning multigrid solvers. Our method learns a (single) mapping from a family of parameterized PDEs to prolongation operators. We train a neural network once for the entire class of PDEs, using an efficient and unsupervised loss function. Experiments on a broad class of 2D diffusion problems demonstrate improved convergence rates compared to the widely used Black-Box multigrid scheme, suggesting that our method successfully learned rules for constructing prolongation matrices.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v97/hanin19a/hanin19a.pdf",
  "title": "Complexity of Linear Regions in Deep Networks",
  "year": 2019,
  "venue": "ICML",
  "abstract": "It is well-known that the expressivity of a neural network depends on its architecture, with deeper networks expressing more complex functions. In the case of networks that compute piecewise linear functions, such as those with ReLU activation, the number of distinct linear regions is a natural measure of expressivity. It is possible to construct networks with merely a single region, or for which the number of linear regions grows exponentially with depth; it is not clear where within this range most networks fall in practice, either before or after training. In this paper, we provide a mathematical framework to count the number of linear regions of a piecewise linear network and measure the volume of the boundaries between these regions. In particular, we prove that for networks at initialization, the average number of regions along any one-dimensional subspace grows linearly in the total number of neurons, far below the exponential upper bound. We also find that the average distance to the nearest region boundary at initialization scales like the inverse of the number of neurons. Our theory suggests that, even after training, the number of linear regions is far below exponential, an intuition that matches our empirical observations. We conclude that the practical expressivity of neural networks is likely far below that of the theoretical maximum, and that this gap can be quantified.",
  "stance": 0.3
 },
 {
  "url": "https://proceedings.mlr.press/v97/heidari19a/heidari19a.pdf",
  "title": "On the Long-term Impact of Algorithmic Decision Policies: Effort Unfairness and Feature Segregation through Social Learning",
  "year": 2019,
  "venue": "ICML",
  "abstract": "Most existing notions of algorithmic fairness are one-shot: they ensure some form of allocative equality at the time of decision making, but do not account for the adverse impact of the algorithmic decisions today on the long-term welfare and prosperity of certain segments of the population. We take a broader perspective on algorithmic fairness. We propose an effort-based measure of fairness and present a data-driven framework for characterizing the long-term impact of algorithmic policies on reshaping the underlying population. Motivated by the psychological literature on social learning and the economic literature on equality of opportunity, we propose a micro-scale model of how individuals may respond to decision making algorithms. We employ existing measures of segregation from sociology and economics to quantify the resulting macroscale population-level change. Importantly, we observe that different models may shift the groupconditional distribution of qualifications in different directions. Our findings raise a number of important questions regarding the formalization of fairness for decision-making models.",
  "stance": 0.2
 },
 {
  "url": "https://proceedings.mlr.press/v97/hendrycks19a/hendrycks19a.pdf",
  "title": "Using Pre-Training Can Improve Model Robustness and Uncertainty",
  "year": 2019,
  "venue": "ICML",
  "abstract": "He et al. (2018) have called into question the utility of pre-training by showing that training from scratch can often yield similar performance to pre-training. We show that although pre-training may not improve performance on traditional classification metrics, it improves model robustness and uncertainty estimates. Through extensive experiments on label corruption, class imbalance, adversarial examples, out-of-distribution detection, and confidence calibration, we demonstrate large gains from pre-training and complementary effects with task-specific methods. We show approximately a 10% absolute improvement over the previous state-of-the-art in adversarial robustness. In some cases, using pre-training without task-specific methods also surpasses the stateof-the-art, highlighting the need for pre-training when evaluating future methods on robustness and uncertainty tasks.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v97/jaber19a/jaber19a.pdf",
  "title": "Causal Identification under Markov Equivalence: Completeness Results",
  "year": 2019,
  "venue": "ICML",
  "abstract": "Causal effect identification is the task of determining whether a causal distribution is computable from the combination of an observational distribution and substantive knowledge about the domain under investigation. One of the most studied versions of this problem assumes that knowledge is articulated in the form of a fully known causal diagram, which is arguably a strong assumption in many settings. In this paper, we relax this requirement and consider that the knowledge is articulated in the form of an equivalence class of causal diagrams, in particular, a partial ancestral graph (PAG). This is attractive because a PAG can be learned directly from data, and the scientist does not need to commit to a particular, unique diagram. There are different sufficient conditions for identification in PAGs, but none is complete. We derive a complete algorithm for identification given a PAG. This implies that whenever the causal effect is identifiable, the algorithm returns a valid identification expression; alternatively, it will throw a failure condition, which means that the effect is provably not identifiable. We further provide a graphical characterization of nonidentifiability of causal effects in PAGs.",
  "stance": 0.5
 },
 {
  "url": "https://proceedings.mlr.press/v97/jeong19d/jeong19d.pdf",
  "title": "Learning Discrete and Continuous Factors of Data via Alternating Disentanglement",
  "year": 2019,
  "venue": "ICML",
  "abstract": "We address the problem of unsupervised disentanglement of discrete and continuous explanatory factors of data. We first show a simple procedure for minimizing the total correlation of the continuous latent variables without having to use a discriminator network or perform importance sampling, via cascading the information flow in the β-vae framework. Furthermore, we propose a method which avoids offloading the entire burden of jointly modeling the continuous and discrete factors to the variational encoder by employing a separate discrete inference procedure. This leads to an interesting alternating minimization problem which switches between finding the most likely discrete configuration given the continuous factors and updating the variational encoder based on the computed discrete factors. Experiments show that the proposed method clearly disentangles discrete factors and significantly outperforms current disentanglement methods based on the disentanglement score and inference network classification score. The source code is available at https://github.com/snumllab/DisentanglementICML19.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v97/lawrence19a/lawrence19a.pdf",
  "title": "DP-GP-LVM: A Bayesian Non-Parametric Model for Learning Multivariate Dependency Structures",
  "year": 2019,
  "venue": "ICML",
  "abstract": "We present a non-parametric Bayesian latent variable model capable of learning dependency structures across dimensions in a multivariate setting. Our approach is based on flexible Gaussian process priors for the generative mappings and interchangeable Dirichlet process priors to learn the structure. The introduction of the Dirichlet process as a specific structural prior allows our model to circumvent issues associated with previous Gaussian process latent variable models. Inference is performed by deriving an efficient variational bound on the marginal log-likelihood of the model. We demonstrate the efficacy of our approach via analysis of discovered structure and superior quantitative performance on missing data imputation.",
  "stance": 0.8
 },
 {
  "url": "https://proceedings.mlr.press/v97/li19c/li19c.pdf",
  "title": "LGM-Net: Learning to Generate Matching Networks for Few-Shot Learning",
  "year": 2019,
  "venue": "ICML",
  "abstract": "In this work, we propose a novel meta-learning approach for few-shot classification, which learns transferable prior knowledge across tasks and directly produces network parameters for similar unseen tasks with training samples. Our approach, called LGM-Net, includes two key modules, namely, TargetNet and MetaNet. The TargetNet module is a neural network for solving a specific task and the MetaNet module aims at learning to generate functional weights for TargetNet by observing training samples. We also present an intertask normalization strategy for the training process to leverage common information shared across different tasks. The experimental results on Omniglot and miniImageNet datasets demonstrate that LGM-Net can effectively adapt to similar unseen tasks and achieve competitive performance, and the results on synthetic datasets show that transferable prior knowledge is learned by the MetaNet module via mapping training data to functional weights. LGM-Net enables fast learning and adaptation since no further tuning steps are required compared to other metalearning approaches.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v97/li19n/li19n.pdf",
  "title": "Alternating Minimizations Converge to Second-Order Optimal Solutions",
  "year": 2019,
  "venue": "ICML",
  "abstract": "This work studies the second-order convergence for both standard alternating minimization and proximal alternating minimization. We show that under mild assumptions on the (nonconvex) objective function, both algorithms avoid strict saddles almost surely from random initialization. Together with known first-order convergence results, this implies that both algorithms converge to a second-order stationary point. This solves an open problem for the second-order convergence of alternating minimization algorithms that have been widely used in practice to solve large-scale nonconvex problems due to their simple implementation, fast convergence, and superb empirical performance.",
  "stance": 0.5
 },
 {
  "url": "https://proceedings.mlr.press/v119/aitchison20a/aitchison20a.pdf",
  "title": "Why bigger is not always better: on finite and infinite neural networks",
  "year": 2020,
  "venue": "ICML",
  "abstract": "Recent work has argued that neural networks can be understood theoretically by taking the number of channels to infinity, at which point the outputs become Gaussian process (GP) distributed. However, we note that infinite Bayesian neural networks lack a key facet of the behaviour of real neural networks: the fixed kernel, determined only by network hyperparameters, implies that they cannot do any form of representation learning. The lack of representation or equivalently kernel learning leads to less flexibility and hence worse performance, giving a potential explanation for the inferior performance of infinite networks observed in the literature (e.g. Novak et al. 2019). We give analytic results characterising the prior over representations and representation learning in finite deep linear networks. We show empirically that the representations in SOTA architectures such as ResNets trained with SGD are much closer to those suggested by our deep linear results than by the corresponding infinite network. This motivates the introduction of a new class of network: infinite networks with bottlenecks, which inherit the theoretical tractability of infinite networks while at the same time allowing representation learning.",
  "stance": 0.1
 },
 {
  "url": "https://proceedings.mlr.press/v119/anokhin20a/anokhin20a.pdf",
  "title": "Low-loss connection of weight vectors: distribution-based approaches",
  "year": 2020,
  "venue": "ICML",
  "abstract": "Recent research shows that sublevel sets of the loss surfaces of overparameterized networks are connected, exactly or approximately. We describe and compare experimentally a panel of methods used to connect two low-loss points by a low-loss curve on this surface. Our methods vary in accuracy and complexity. Most of our methods are based on “macroscopic” distributional assumptions, and some are insensitive to the detailed properties of the points to be connected. Some methods require a prior training of a “global connection model” which can then be applied to any pair of points. The accuracy of the method generally correlates with its complexity and sensitivity to the endpoint detail.",
  "stance": 0.0
 },
 {
  "url": "https://proceedings.mlr.press/v119/assran20a/assran20a.pdf",
  "title": "On the Convergence of Nesterov’s Accelerated Gradient Method in Stochastic Settings",
  "year": 2020,
  "venue": "ICML",
  "abstract": "We study Nesterov’s accelerated gradient method with constant step-size and momentum parameters in the stochastic approximation setting (unbiased gradients with bounded variance) and the finite-sum setting (where randomness is due to sampling mini-batches). To build better insight into the behavior of Nesterov’s method in stochastic settings, we focus throughout on objectives that are smooth, strongly-convex, and twice continuously differentiable. In the stochastic approximation setting, Nesterov’s method converges to a neighborhood of the optimal point at the same accelerated rate as in the deterministic setting. Perhaps surprisingly, in the finite-sum setting, we prove that Nesterov’s method may diverge with the usual choice of step-size and momentum, unless additional conditions on the problem related to conditioning and data coherence are satisfied. Our results shed light as to why Nesterov’s method may fail to converge or achieve acceleration in the finite-sum setting.",
  "stance": -0.5
 },
 {
  "url": "https://proceedings.mlr.press/v119/ayoub20a/ayoub20a.pdf",
  "title": "Model-Based Reinforcement Learning with Value-Targeted Regression",
  "year": 2020,
  "venue": "ICML",
  "abstract": "This paper studies model-based reinforcement learning (RL) for regret minimization. We focus on finite-horizon episodic RL where the transition model P belongs to a known family of models P , a special case of which is when models in P take the form of linear mixtures: P✓ = P d i=1 ✓iPi. We propose a model based RL algorithm that is based on the optimism principle: In each episode, the set of models that are ‘consistent’ with the data collected is constructed. The criterion of consistency is based on the total squared error that the model incurs on the task of predicting state values as determined by the last value estimate along the transitions. The next value function is then chosen by solving the optimistic planning problem with the constructed set of models. We derive a bound on the regret, which, in the special case of linear mixtures, takes the form Õ(d p H3T ), where H , T and d are the horizon, the total number of steps and the dimension of ✓, respectively. In particular, this regret bound is independent of the total number of states or actions, and is close to a lower bound ⌦( p HdT ). For a general model family P , the regret bound is derived based on the Eluder dimension.",
  "stance": 0.0
 },
 {
  "url": "https://proceedings.mlr.press/v119/bengio20a/bengio20a.pdf",
  "title": "Interference and Generalization in Temporal Difference Learning",
  "year": 2020,
  "venue": "ICML",
  "abstract": "We study the link between generalization and interference in temporal-difference (TD) learning. Interference is defined as the inner product of two different gradients, representing their alignment; this quantity emerges as being of interest from a variety of observations about neural networks, parameter sharing and the dynamics of learning. We find that TD easily leads to low-interference, under-generalizing parameters, while the effect seems reversed in supervised learning. We hypothesize that the cause can be traced back to the interplay between the dynamics of interference and bootstrapping. This is supported empirically by several observations: the negative relationship between the generalization gap and interference in TD, the negative effect of bootstrapping on interference and the local coherence of targets, and the contrast between the propagation rate of information in TD(0) versus TD(λ) and regression tasks such as Monte-Carlo policy evaluation. We hope that these new findings can guide the future discovery of better bootstrapping methods.",
  "stance": 0.0
 },
 {
  "url": "https://proceedings.mlr.press/v119/blumenfeld20a/blumenfeld20a.pdf",
  "title": "Beyond Signal Propagation: Is Feature Diversity Necessary in Deep Neural Network Initialization?",
  "year": 2020,
  "venue": "ICML",
  "abstract": "Deep neural networks are typically initialized with random weights, with variances chosen to facilitate signal propagation and stable gradients. It is also believed that diversity of features is an important property of these initializations. We construct a deep convolutional network with identical features by initializing almost all the weights to 0. The architecture also enables perfect signal propagation and stable gradients, and achieves high accuracy on standard benchmarks. This indicates that random, diverse initializations are not necessary for training neural networks. An essential element in training this network is a mechanism of symmetry breaking; we study this phenomenon and find that standard GPU operations, which are non-deterministic, can serve as a sufficient source of symmetry breaking to enable training.",
  "stance": 0.9
 },
 {
  "url": "https://proceedings.mlr.press/v119/braverman20a/braverman20a.pdf",
  "title": "Calibration, Entropy Rates, and Memory in Language Models",
  "year": 2020,
  "venue": "ICML",
  "abstract": "Building accurate language models that capture meaningful long-term dependencies is a core challenge in natural language processing. Towards this end, we present a calibration-based approach to measure long-term discrepancies between a generative sequence model and the true distribution, and use these discrepancies to improve the model. Empirically, we show that stateof-the-art language models, including LSTMs and Transformers, are miscalibrated: the entropy rates of their generations drift dramatically upward over time. We then provide provable methods to mitigate this phenomenon. Furthermore, we show how this calibration-based approach can also be used to measure the amount of memory that language models use for prediction.",
  "stance": -0.5
 },
 {
  "url": "https://proceedings.mlr.press/v119/chatterjee20a/chatterjee20a.pdf",
  "title": "Circuit-Based Intrinsic Methods to Detect Overfitting",
  "year": 2020,
  "venue": "ICML",
  "abstract": "The focus of this paper is on intrinsic methods to detect overfitting. By intrinsic methods, we mean methods that rely only on the model and the training data, as opposed to traditional methods (we call them extrinsic methods) that rely on performance on a test set or on bounds from model complexity. We propose a family of intrinsic methods called Counterfactual Simulation (CFS) which analyze the flow of training examples through the model by identifying and perturbing rare patterns. By applying CFS to logic circuits we get a method that has no hyper-parameters and works uniformly across different types of models such as neural networks, random forests and lookup tables. Experimentally, CFS can separate models with different levels of overfit using only their logic circuit representations without any access to the high level structure. By comparing lookup tables, neural networks, and random forests using CFS, we get insight into why neural networks generalize. In particular, we find that stochastic gradient descent in neural nets does not lead to “brute force” memorization, but finds common patterns (whether we train with actual or randomized labels), and neural networks are not unlike forests in this regard. Finally, we identify a limitation with our proposal that makes it unsuitable in an adversarial setting, but points the way to future work on robust intrinsic methods.",
  "stance": -0.2
 },
 {
  "url": "https://proceedings.mlr.press/v119/chen20q/chen20q.pdf",
  "title": "More Data Can Expand The Generalization Gap Between Adversarially Robust and Standard Models",
  "year": 2020,
  "venue": "ICML",
  "abstract": "Despite remarkable success in practice, modern machine learning models have been found to be susceptible to adversarial attacks that make human-imperceptible perturbations to the data, but result in serious and potentially dangerous prediction errors. To address this issue, practitioners often use adversarial training to learn models that are robust against such attacks at the cost of higher generalization error on unperturbed test sets. The conventional wisdom is that more training data should shrink the gap between the generalization error of adversarially-trained models and standard models. However, we study the training of robust classifiers for both Gaussian and Bernoulli models under `∞ attacks, and we prove that more data may actually increase this gap. Furthermore, our theoretical results identify if and when additional data will finally begin to shrink the gap. Lastly, we experimentally demonstrate that our results also hold for linear regression models, which may indicate that this phenomenon occurs more broadly.",
  "stance": 0.0
 },
 {
  "url": "https://proceedings.mlr.press/v119/claici20a/claici20a.pdf",
  "title": "Model Fusion with Kullback-Leibler Divergence",
  "year": 2020,
  "venue": "ICML",
  "abstract": "We propose a method to fuse posterior distributions learned from heterogeneous datasets. Our algorithm relies on a mean field assumption for both the fused model and the individual dataset posteriors and proceeds using a simple assign-andaverage approach. The components of the dataset posteriors are assigned to the proposed global model components by solving a regularized variant of the assignment problem. The global components are then updated based on these assignments by their mean under a KL divergence. For exponential family variational distributions, our formulation leads to an efficient non-parametric algorithm for computing the fused model. Our algorithm is easy to describe and implement, efficient, and competitive with state-of-the-art on motion capture analysis, topic modeling, and federated learning of Bayesian neural networks.1",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v119/dai20c/dai20c.pdf",
  "title": "The Usual Suspects? Reassessing Blame for VAE Posterior Collapse",
  "year": 2020,
  "venue": "ICML",
  "abstract": "In narrow asymptotic settings Gaussian VAE models of continuous data have been shown to possess global optima aligned with ground-truth distributions. Even so, it is well known that poor solutions whereby the latent posterior collapses to an uninformative prior are sometimes obtained in practice. However, contrary to conventional wisdom that largely assigns blame for this phenomena on the undue influence of KL-divergence regularization, we will argue that posterior collapse is, at least in part, a direct consequence of bad local minima inherent to the loss surface of deep autoencoder networks. In particular, we prove that even small nonlinear perturbations of affine VAE decoder models can produce such minima, and in deeper models, analogous minima can force the VAE to behave like an aggressive truncation operator, provably discarding information along all latent dimensions in certain circumstances. Regardless, the underlying message here is not meant to undercut valuable existing explanations of posterior collapse, but rather, to refine the discussion and elucidate alternative risk factors that may have been previously underappreciated.",
  "stance": -0.7
 },
 {
  "url": "https://proceedings.mlr.press/v119/dong20e/dong20e.pdf",
  "title": "Collapsed Amortized Variational Inference for Switching Nonlinear Dynamical Systems",
  "year": 2020,
  "venue": "ICML",
  "abstract": "We propose an efficient inference method for switching nonlinear dynamical systems. The key idea is to learn an inference network which can be used as a proposal distribution for the continuous latent variables, while performing exact marginalization of the discrete latent variables. This allows us to use the reparameterization trick, and apply end-to-end training with stochastic gradient descent. We show that the proposed method can successfully segment time series data, including videos and 3D human pose, into meaningful “regimes” by using the piece-wise nonlinear dynamics.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v119/dupont20a/dupont20a.pdf",
  "title": "Equivariant Neural Rendering",
  "year": 2020,
  "venue": "ICML",
  "abstract": "We propose a framework for learning neural scene representations directly from images, without 3D supervision. Our key insight is that 3D structure can be imposed by ensuring that the learned representation transforms like a real 3D scene. Specifically, we introduce a loss which enforces equivariance of the scene representation with respect to 3D transformations. Our formulation allows us to infer and render scenes in real time while achieving comparable results to models requiring minutes for inference. In addition, we introduce two challenging new datasets for scene representation and neural rendering, including scenes with complex lighting and backgrounds. Through experiments, we show that our model achieves compelling results on these datasets as well as on standard ShapeNet benchmarks.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v119/dutta20a/dutta20a.pdf",
  "title": "Is There a Trade-Off Between Fairness and Accuracy? A Perspective Using Mismatched Hypothesis Testing",
  "year": 2020,
  "venue": "ICML",
  "abstract": "A trade-off between accuracy and fairness is almost taken as a given in the existing literature on fairness in machine learning. Yet, it is not preordained that accuracy should decrease with increased fairness. Novel to this work, we examine fair classification through the lens of mismatched hypothesis testing: trying to find a classifier that distinguishes between two ideal distributions when given two mismatched distributions that are biased. Using Chernoff information, a tool in information theory, we theoretically demonstrate that, contrary to popular belief, there always exist ideal distributions such that optimal fairness and accuracy (with respect to the ideal distributions) are achieved simultaneously: there is no trade-off. Moreover, the same classifier yields the lack of a trade-off with respect to ideal distributions while yielding a trade-off when accuracy is measured with respect to the given (possibly biased) dataset. To complement our main result, we formulate an optimization to find ideal distributions and derive fundamental limits to explain why a trade-off exists on the given biased dataset. We also derive conditions under which active data collection can alleviate the fairness-accuracy trade-off in the real world. Our results lead us to contend that it is problematic to measure accuracy with respect to data that reflects bias, and instead, we should be considering accuracy with respect to ideal, unbiased data.",
  "stance": 0.4
 },
 {
  "url": "https://proceedings.mlr.press/v119/eftekhari20a/eftekhari20a.pdf",
  "title": "Training Linear Neural Networks: Non-Local Convergence and Complexity Results",
  "year": 2020,
  "venue": "ICML",
  "abstract": "Linear networks provide valuable insights into the workings of neural networks in general. This paper identifies conditions under which the gradient flow provably trains a linear network, in spite of the non-strict saddle points present in the optimization landscape. This paper also provides the computational complexity of training linear networks with gradient flow. To achieve these results, this work develops a machinery to provably identify the stable set of gradient flow, which then enables us to improve over the state of the art in the literature of linear networks (Bah et al., 2019; Arora et al., 2018a). Crucially, our results appear to be the first to break away from the lazy training regime which has dominated the literature of neural networks. This work requires the network to have a layer with one neuron, which subsumes the networks with a scalar output, but extending the results of this theoretical work to all linear networks remains a challenging open problem.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v119/engstrom20a/engstrom20a.pdf",
  "title": "Identifying Statistical Bias in Dataset Replication",
  "year": 2020,
  "venue": "ICML",
  "abstract": "Dataset replication is a useful tool for assessing whether improvements in test accuracy on a specific benchmark correspond to improvements in models’ ability to generalize reliably. In this work, we present unintuitive yet significant ways in which standard approaches to dataset replication introduce statistical bias, skewing the resulting observations. We study ImageNet-v2, a replication of the ImageNet dataset on which models exhibit a significant (11-14%) drop in accuracy, even after controlling for selection frequency, a human-in-the-loop measure of data quality. We show that after remeasuring selection frequencies and correcting for statistical bias, only an estimated 3.6%±1.5% of the original 11.7%±1.0% accuracy drop remains unaccounted for. We conclude with concrete recommendations for recognizing and avoiding bias in dataset replication. Code for our study is publicly available.",
  "stance": 0.2
 },
 {
  "url": "https://proceedings.mlr.press/v119/fedus20a/fedus20a.pdf",
  "title": "Revisiting Fundamentals of Experience Replay",
  "year": 2020,
  "venue": "ICML",
  "abstract": "Experience replay is central to off-policy algorithms in deep reinforcement learning (RL), but there remain significant gaps in our understanding. We therefore present a systematic and extensive analysis of experience replay in Q-learning methods, focusing on two fundamental properties: the replay capacity and the ratio of learning updates to experience collected (replay ratio). Our additive and ablative studies upend conventional wisdom around experience replay — greater capacity is found to substantially increase the performance of certain algorithms, while leaving others unaffected. Counterintuitively we show that theoretically ungrounded, uncorrected n-step returns are uniquely beneficial while other techniques confer limited benefit for sifting through larger memory. Separately, by directly controlling the replay ratio we contextualize previous observations in the literature and empirically measure its importance across a variety of deep RL algorithms. Finally, we conclude by testing a set of hypotheses on the nature of these performance benefits.",
  "stance": 0.4
 },
 {
  "url": "https://proceedings.mlr.press/v119/feng20b/feng20b.pdf",
  "title": "Global Concavity and Optimization in a Class of Dynamic Discrete Choice Models",
  "year": 2020,
  "venue": "ICML",
  "abstract": "Discrete choice models with unobserved heterogeneity are commonly used Econometric models for dynamic Economic behavior which have been adopted in practice to predict behavior of individuals and firms from schooling and job choices to strategic decisions in market competition. These models feature optimizing agents who choose among a finite set of options in a sequence of periods and receive choice-specific payoffs that depend on both variables that are observed by the agent and recorded in the data and variables that are only observed by the agent but not recorded in the data. Existing work in Econometrics assumes that optimizing agents are fully rational and requires finding a functional fixed point to find the optimal policy. We show that in an important class of discrete choice models the value function is globally concave in the policy. That means that simple algorithms that do not require fixed point computation, such as the policy gradient algorithm, globally converge to the optimal policy. This finding can both be used to relax behavioral assumption regarding the optimizing agents and to facilitate Econometric analysis of dynamic behavior. In particular, we demonstrate significant computational advantages in using a simple implementation policy gradient algorithm over existing “nested fixed point” algorithms used in Econometrics.",
  "stance": 0.3
 },
 {
  "url": "https://proceedings.mlr.press/v119/ghari20a/ghari20a.pdf",
  "title": "Online Multi-Kernel Learning with Graph-Structured Feedback",
  "year": 2020,
  "venue": "ICML",
  "abstract": "is more powerful, as it learns the optimal kernel from a dicMulti-kernel learning (MKL) exhibits reliable performance in nonlinear function approximation tasks. Instead of using one kernel, it learns the optimal kernel from a pre-selected dictionary of kernels. The selection of the dictionary has crucial impact on both the performance and complexity of MKL. Specifcally, inclusion of a large number of irrelevant kernels may impair the accuracy, and increase the complexity of MKL algorithms. To enhance the accuracy, and alleviate the computational burden, the present paper develops a novel scheme which actively chooses relevant kernels. The proposed framework models the pruned kernel combination as feedback collected from a graph, that is refned ‘on the fy.’ Leveraging the random feature approximation, we propose an online scalable multi-kernel learning approach with graph feedback, and prove that the proposed algorithm enjoys sublinear regret. Numerical tests on real datasets demonstrate the effectiveness of the novel approach.",
  "stance": 0.7
 },
 {
  "url": "https://proceedings.mlr.press/v119/gottesman20a/gottesman20a.pdf",
  "title": "Interpretable Off-Policy Evaluation in Reinforcement Learning by Highlighting Influential Transitions",
  "year": 2020,
  "venue": "ICML",
  "abstract": "Off-policy evaluation in reinforcement learning offers the chance of using observational data to improve future outcomes in domains such as healthcare and education, but safe deployment in high stakes settings requires ways of assessing its validity. Traditional measures such as confidence intervals may be insufficient due to noise, limited data and confounding. In this paper we develop a method that could serve as a hybrid human-AI system, to enable human experts to analyze the validity of policy evaluation estimates. This is accomplished by highlighting observations in the data whose removal will have a large effect on the OPE estimate, and formulating a set of rules for choosing which ones to present to domain experts for validation. We develop methods to compute exactly the influence functions for fitted Q-evaluation with two different function classes: kernel-based and linear least squares, as well as importance sampling methods. Experiments on medical simulations and real-world intensive care unit data demonstrate that our method can be used to identify limitations in the evaluation process and make evaluation more robust.",
  "stance": 0.6
 },
 {
  "url": "https://proceedings.mlr.press/v119/goyal20a/goyal20a.pdf",
  "title": "PoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Elimination",
  "year": 2020,
  "venue": "ICML",
  "abstract": "We develop a novel method, called PoWER-BERT, for improving the inference time of the popular BERT model, while maintaining the accuracy. It works by: a) exploiting redundancy pertaining to word-vectors (intermediate transformer block outputs) and eliminating the redundant vectors. b) determining which wordvectors to eliminate by developing a strategy for measuring their significance, based on the selfattention mechanism. c) learning how many word-vectors to eliminate by augmenting the BERT model and the loss function. Experiments on the standard GLUE benchmark shows that PoWER-BERT achieves up to 4.5x reduction in inference time over BERT with < 1% loss in accuracy. We show that PoWER-BERT offers significantly better trade-off between accuracy and inference time compared to prior methods. We demonstrate that our method attains up to 6.8x reduction in inference time with < 1% loss in accuracy when applied over ALBERT, a highly compressed version of BERT. The code for PoWER-BERT is publicly available at https: //github.com/IBM/PoWER-BERT.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v119/goyal20b/goyal20b.pdf",
  "title": "PackIt: A Virtual Environment for Geometric Planning",
  "year": 2020,
  "venue": "ICML",
  "abstract": "The ability to jointly understand the geometry of objects and plan actions for manipulating them is crucial for intelligent agents. We refer to this ability as geometric planning. Recently, many interactive environments have been proposed to evaluate intelligent agents on various skills, however, none of them cater to the needs of geometric planning. We present PackIt, a virtual environment to evaluate and potentially learn the ability to do geometric planning, where an agent needs to take a sequence of actions to pack a set of objects into a box with limited space. We also construct a set of challenging packing tasks using an evolutionary algorithm. Further, we study various baselines for the task that include model-free learning-based and heuristic-based methods, as well as search-based optimization methods that assume access to the model of the environment.2",
  "stance": 0.6
 },
 {
  "url": "https://proceedings.mlr.press/v119/hacohen20a/hacohen20a.pdf",
  "title": "Let’s Agree to Agree: Neural Networks Share Classification Order on Real Datasets",
  "year": 2020,
  "venue": "ICML",
  "abstract": "We report a series of robust empirical observations, demonstrating that deep Neural Networks learn the examples in both the training and test sets in a similar order. This phenomenon is observed in all the commonly used benchmarks we evaluated, including many image classification benchmarks, and one text classification benchmark. While this phenomenon is strongest for models of the same architecture, it also crosses architectural boundaries – models of different architectures start by learning the same examples, after which the more powerful model may continue to learn additional examples. We further show that this pattern of results reflects the interplay between the way neural networks learn benchmark datasets. Specifically, when fixing the architecture, we describe synthetic datasets for which this pattern is no longer observed. When fixing the dataset, we show that other learning paradigms may learn the data in a different order. We hypothesize that our results reflect how neural networks discover structure in natural datasets.",
  "stance": 0.2
 },
 {
  "url": "https://proceedings.mlr.press/v119/hanzely20a/hanzely20a.pdf",
  "title": "Stochastic Subspace Cubic Newton Method",
  "year": 2020,
  "venue": "ICML",
  "abstract": "In this paper, we propose a new randomized second-order optimization algorithm—Stochastic Subspace Cubic Newton (SSCN)—for minimizing a high dimensional convex function f . Our method can be seen both as a stochastic extension of the cubically-regularized Newton method of Nesterov and Polyak (2006), and a second-order enhancement of stochastic subspace descent of Kozak et al. (2019). We prove that as we vary the minibatch size, the global convergence rate of SSCN interpolates between the rate of stochastic coordinate descent (CD) and the rate of cubic regularized Newton, thus giving new insights into the connection between first and second-order methods. Remarkably, the local convergence rate of SSCN matches the rate of stochastic subspace descent applied to the problem of minimizing the quadratic function 12 (x−x ∗)>∇2f(x∗)(x−x∗), where x∗ is the minimizer of f , and hence depends on the properties of f at the optimum only. Our numerical experiments show that SSCN outperforms non-accelerated first-order CD algorithms while being competitive to their accelerated variants.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v119/hasanzadeh20a/hasanzadeh20a.pdf",
  "title": "Bayesian Graph Neural Networks with Adaptive Connection Sampling",
  "year": 2020,
  "venue": "ICML",
  "abstract": "We propose a unified framework for adaptive connection sampling in graph neural networks (GNNs) that generalizes existing stochastic regularization methods for training GNNs. The proposed framework not only alleviates oversmoothing and over-fitting tendencies of deep GNNs, but also enables learning with uncertainty in graph analytic tasks with GNNs. Instead of using fixed sampling rates or hand-tuning them as model hyperparameters as in existing stochastic regularization methods, our adaptive connection sampling can be trained jointly with GNN model parameters in both global and local fashions. GNN training with adaptive connection sampling is shown to be mathematically equivalent to an efficient approximation of training Bayesian GNNs. Experimental results with ablation studies on benchmark datasets validate that adaptively learning the sampling rate given graph training data is the key to boosting the performance of GNNs in semi-supervised node classification, making them less prone to over-smoothing and over-fitting with more robust prediction.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/A00-1035",
  "title": "Spelling and Grammar Correction for Danish in SCARRIE",
  "year": 2000,
  "venue": "NAACL",
  "abstract": "This paper reports on work carried out to develop a spelling and grammar corrector for Danish, addressing in particular the issue of how a form of shallow parsing is combined with error detection and correction for the treatment of context-dependent spelling errors. The syntactic grammar for Danish used by the system has been developed with the aim of dealing with the most frequent error types found in a parallel corpus of unedited and proofread texts specifically collected by the project's end users. By focussing on certain grammatical constructions and certain error types, it has been possible to exploit the linguistic 'intelligence' provided by syntactic parsing and yet keep the system robust and efficient. The system described is thus superior to other existing spelling checkers for Danish in its ability to deal with contextdependent errors.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/A00-2002",
  "title": "The Automatic Translation of Discourse Structures",
  "year": 2000,
  "venue": "NAACL",
  "abstract": "We empirically show that there are significant differences between the discourse structure of Japanese texts and the discourse structure of their corresponding English translations. To improve translation quality, we propose a computational model for rewriting discourse structures. When we train our model on a parallel corpus of manually built Japanese and English discourse structure trees, we learn to rewrite Japanese trees as trees that are closer to the natural English rendering than the original ones.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/A00-2029",
  "title": "Predicting Automatic Speech Recognition Performance Using Prosodic Cues",
  "year": 2000,
  "venue": "NAACL",
  "abstract": "In spoken dialogue systems, it is important for a system to know how likely a speech recognition hypothesis is to be correct, so it can reprompt for fresh input, or, in cases where many errors have occurred, change its interaction strategy or switch the caller to a human attendant. We have discovered prosodic features which more accurately predict when a recognition hypothesis contains a word error than the acoustic confidence score thresholds traditionally used in automatic speech recognition. We present analytic results indicating that there are significant prosodic differences between correctly and incorrectly recognized turns in the T O O T train information corpus. We then present machine learning results showing how the use of prosodic features to automatically predict correct versus incorrectly recognized turns improves over the use of acoustic confidence scores alone. ",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/A00-2037",
  "title": "Acknowledgments in Human-Computer Interaction",
  "year": 2000,
  "venue": "NAACL",
  "abstract": "Acknowledgments are relatively rare in humancomputer interaction. Are people unwilling to use this human convention when talking to a machine, or is their scarcity due to the way that spoken-language interfaces are designed? We found that, given a simple spoken-language interface that provided opportunities for and responded to acknowle d g m e n t s , abou t h a l f of our sub j ec t s u s e d acknowledgments at least once and nearly 30% used them extensively during the interaction.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/N03-2020",
  "title": "A Robust Retrieval Engine for Proximal and Structural Search",
  "year": 2003,
  "venue": "NAACL",
  "abstract": "Katsuya Masuda† Takashi Ninomiya†‡ Yusuke Miyao† Tomoko Ohta†‡ Jun’ichi Tsujii†‡ † Department of Computer Science, Graduate School of Information Science and Technology, University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033, Japan ‡ CREST, JST (Japan Science and Technology Corporation) Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012, Japan {kmasuda,ninomi,yusuke,okap,tsujii}@is.s.u-tokyo.ac.jp",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/N06-1009",
  "title": "Role of Local Context in Automatic Deidentification of Ungrammatical, Fragmented Text",
  "year": 2006,
  "venue": "NAACL",
  "abstract": "Deidentification of clinical records is a crucial step before these records can be distributed to non-hospital researchers. Most approaches to deidentification rely heavily on dictionaries and heuristic rules; these approaches fail to remove most personal health information (PHI) that cannot be found in dictionaries. They also can fail to remove PHI that is ambiguous between PHI and non-PHI. Named entity recognition (NER) technologies can be used for deidentification. Some of these technologies exploit both local and global context of a word to identify its entity type. When documents are grammatically written, global context can improve NER. In this paper, we show that we can deidentify medical discharge summaries using support vector machines that rely on a statistical representation of local context. We compare our approach with three different systems. Comparison with a rulebased approach shows that a statistical representation of local context contributes more to deidentification than dictionaries and hand-tailored heuristics. Comparison with two well-known systems, SNoW and IdentiFinder, shows that when the language of documents is fragmented, local context contributes more to deidentification than global context.",
  "stance": 0.4
 },
 {
  "url": "https://aclanthology.org/N06-1050",
  "title": "Creating a Test Collection for Citation-based IR Experiments",
  "year": 2006,
  "venue": "NAACL",
  "abstract": "We present an approach to building a test collection of research papers. The approach is based on the Cranfield 2 tests but uses as its vehicle a current conference; research questions and relevance judgements of all cited papers are elicited from conference authors. The resultant test collection is different from TREC’s in that it comprises scientific articles rather than newspaper text and, thus, allows for IR experiments that include citation information. The test collection currently consists of 170 queries with relevance judgements; the document collection is the ACL Anthology. We describe properties of our queries and relevance judgements, and demonstrate the use of the test collection in an experimental setup. One potentially problematic property of our collection is that queries have a low number of relevant documents; we discuss ways of alleviating this.",
  "stance": 0.4
 },
 {
  "url": "https://aclanthology.org/N06-1055",
  "title": "Semantic role labeling of nominalized predicates in Chinese",
  "year": 2006,
  "venue": "NAACL",
  "abstract": "Recent work on semantic role labeling (SRL) has focused almost exclusively on the analysis of the predicate-argument structure of verbs, largely due to the lack of human-annotated resources for other types of predicates that can serve as training and test data for the semantic role labeling systems. However, it is wellknown that verbs are not the only type of predicates that can take arguments. Most notably, nouns that are nominalized forms of verbs and relational nouns generally are also considered to have their own predicate-argument structure. In this paper we report results of SRL experiments on nominalized predicates in Chinese, using a newly completed corpus, the Chinese Nombank. We also discuss the impact of using publicly available manually annotated verb data to improve the SRL accuracy of nouns, exploiting a widely-held assumption that verbs and their nominalizations share the same predicate-argument structure. Finally, we discuss the results of applying reranking techniques to improve SRL accuracy for nominalized predicates, which showed insignificant improvement.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/N06-1062",
  "title": "Unlimited vocabulary speech recognition for agglutinative languages",
  "year": 2006,
  "venue": "NAACL",
  "abstract": "It is practically impossible to build a word-based lexicon for speech recognition in agglutinative languages that would cover all the relevant words. The problem is that words are generally built by concatenating several prefixes and suffixes to the word roots. Together with compounding and inflections this leads to millions of different, but still frequent word forms. Due to inflections, ambiguity and other phenomena, it is also not trivial to automatically split the words into meaningful parts. Rule-based morphological analyzers can perform this splitting, but due to the handcrafted rules, they also suffer from an out-of-vocabulary problem. In this paper we apply a recently proposed fully automatic and rather language and vocabulary independent way to build subword lexica for three different agglutinative languages. We demonstrate the language portability as well by building a successful large vocabulary speech recognizer for each language and show superior recognition performance compared to the corresponding word-based reference systems.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/N06-2022",
  "title": "Automatic Recognition of Personality in Conversation",
  "year": 2006,
  "venue": "NAACL",
  "abstract": "The identification of personality by automatic analysis of conversation has many applications in natural language processing, from leader identification in meetings to partner matching on dating websites. We automatically train models of the main five personality dimensions, on a corpus of conversation extracts and personality ratings. Results show that the models perform better than the baseline, and their analysis confirms previous findings linking language and personality, while revealing many new linguistic and prosodic markers.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/N07-1001",
  "title": "Exploiting Acoustic and Syntactic Features for Prosody Labeling in a Maximum Entropy Framework",
  "year": 2007,
  "venue": "NAACL",
  "abstract": "In this paper we describe an automatic prosody labeling framework that exploits both language and speech information. We model the syntactic-prosodic information with a maximum entropy model that achieves an accuracy of 85.2% and 91.5% for pitch accent and boundary tone labeling on the Boston University Radio News corpus. We model the acousticprosodic stream with two different models, one a maximum entropy model and the other a traditional HMM. We finally couple the syntactic-prosodic and acousticprosodic components to achieve significantly improved pitch accent and boundary tone classification accuracies of 86.0% and 93.1% respectively. Similar experimental results are also reported on Boston Directions corpus.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N07-1030",
  "title": "Joint Determination of Anaphoricity and Coreference Resolution using Integer Programming",
  "year": 2007,
  "venue": "NAACL",
  "abstract": "Standard pairwise coreference resolution systems are subject to errors resulting from their performing anaphora identification as an implicit part of coreference resolution. In this paper, we propose an integer linear programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task, such that each local model informs the other for the final assignments. This joint ILP formulation provides f score improvements of 3.7-5.3% over a base coreference classifier on the ACE datasets.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N07-1046",
  "title": "A Log-Linear Block Transliteration Model based on Bi-Stream HMMs",
  "year": 2007,
  "venue": "NAACL",
  "abstract": "We propose a novel HMM-based framework to accurately transliterate unseen named entities. The framework leverages features in letteralignment and letter n-gram pairs learned from available bilingual dictionaries. Letter-classes, such as vowels/non-vowels, are integrated to further improve transliteration accuracy. The proposed transliteration system is applied to out-of-vocabulary named-entities in statistical machine translation (SMT), and a significant improvement over traditional transliteration approach is obtained. Furthermore, by incorporating an automatic spell-checker based on statistics collected from web search engines, transliteration accuracy is further improved. The proposed system is implemented within our SMT system and applied to a real translation scenario from Arabic to English.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N07-1048",
  "title": "Analysis of Morph-Based Speech Recognition and the Modeling of Out-of-Vocabulary Words Across Languages",
  "year": 2007,
  "venue": "NAACL",
  "abstract": "We analyze subword-based language models (LMs) in large-vocabulary continuous speech recognition across four “morphologically rich” languages: Finnish, Estonian, Turkish, and Egyptian Colloquial Arabic. By estimating n-gram LMs over sequences of morphs instead of words, better vocabulary coverage and reduced data sparsity is obtained. Standard word LMs suffer from high out-of-vocabulary (OOV) rates, whereas the morph LMs can recognize previously unseen word forms by concatenating morphs. We show that the morph LMs generally outperform the word LMs and that they perform fairly well on OOVs without compromising the accuracy obtained for in-vocabulary words.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/N07-1069",
  "title": "Can Semantic Roles Generalize Across Genres?",
  "year": 2007,
  "venue": "NAACL",
  "abstract": "PropBank has been widely used as training data for Semantic Role Labeling. However, because this training data is taken from the WSJ, the resulting machine learning models tend to overfit on idiosyncrasies of that text’s style, and do not port well to other genres. In addition, since PropBank was designed on a verb-by-verb basis, the argument labels Arg2 Arg5 get used for very diverse argument roles with inconsistent training instances. For example, the verb “make” uses Arg2 for the “Material” argument; but the verb “multiply” uses Arg2 for the “Extent” argument. As a result, it can be difficult for automatic classifiers to learn to distinguish arguments Arg2-Arg5. We have created a mapping between PropBank and VerbNet that provides a VerbNet thematic role label for each verb-specific PropBank label. Since VerbNet uses argument labels that are more consistent across verbs, we are able to demonstrate that these new labels are easier to learn.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/N07-2040",
  "title": "Are Some Speech Recognition Errors Easier to Detect than Others?",
  "year": 2007,
  "venue": "NAACL",
  "abstract": "This study investigates whether some speech recognition (SR) errors are easier to detect and what patterns can be identified from those errors. Specifically, SR errors were examined from both nonlinguistic and linguistic perspectives. The analyses of non-linguistic properties revealed that high error ratios and consecutive errors lowered the ease of error detection. The analyses of linguistic properties showed that ease of error detection was associated with changing parts-of-speech of reference words in SR errors. Additionally, syntactic relations themselves and the change of syntactic relations had impact on the ease of error detection.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/N07-2041",
  "title": "Simultaneous Identification of Biomedical Named-Entity and Functional Relation Using Statistical Parsing Techniques",
  "year": 2007,
  "venue": "NAACL",
  "abstract": "In this paper we propose a statistical parsing technique that simultaneously identifies biomedical named-entities (NEs) and extracts subcellular localization relations for bacterial proteins from the text in MEDLINE articles. We build a parser that derives both syntactic and domain-dependent semantic information and achieves an F-score of 48.4% for the relation extraction task. We then propose a semi-supervised approach that incorporates noisy automatically labeled data to improve the F-score of our parser to 83.2%. Our key contributions are: learning from noisy data, and building an annotated corpus that can benefit relation extraction research.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N07-2055",
  "title": "A Semi-Automatic Evaluation Scheme: Automated Nuggetization for Manual Annotation",
  "year": 2007,
  "venue": "NAACL",
  "abstract": "In this paper we describe automatic information nuggetization and its application to text comparison. More specifically, we take a close look at how machine-generated nuggets can be used to create evaluation material. A semiautomatic annotation scheme is designed to produce gold-standard data with exceptionally high inter-human agreement.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/N09-1004",
  "title": "A Fully Unsupervised Word Sense Disambiguation Method Using Dependency Knowledge",
  "year": 2009,
  "venue": "NAACL",
  "abstract": "Word sense disambiguation is the process of determining which sense of a word is used in a given context. Due to its importance in understanding semantics of natural languages, word sense disambiguation has been extensively studied in Computational Linguistics. However, existing methods either are brittle and narrowly focus on specific topics or words, or provide only mediocre performance in real-world settings. Broad coverage and disambiguation quality are critical for a word sense disambiguation system. In this paper we present a fully unsupervised word sense disambiguation method that requires only a dictionary and unannotated text as input. Such an automatic approach overcomes the problem of brittleness suffered in many existing methods and makes broad-coverage word sense disambiguation feasible in practice. We evaluated our approach using SemEval 2007 Task 7 (Coarse-grained English All-words Task), and our system significantly outperformed the best unsupervised system participating in SemEval 2007 and achieved the performance approaching top-performing supervised systems. Although our method was only tested with coarse-grained sense disambiguation, it can be directly applied to fine-grained sense disambiguation.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/N09-1017",
  "title": "The Role of Implicit Argumentation in Nominal SRL",
  "year": 2009,
  "venue": "NAACL",
  "abstract": "Nominals frequently surface without overtly expressed arguments. In order to measure the potential benefit of nominal SRL for downstream processes, such nominals must be accounted for. In this paper, we show that a state-of-the-art nominal SRL system with an overall argument F1 of 0.76 suffers a performance loss of more than 9% when nominals with implicit arguments are included in the evaluation. We then develop a system that takes implicit argumentation into account, improving overall performance by nearly 5%. Our results indicate that the degree of implicit argumentation varies widely across nominals, making automated detection of implicit argumentation an important step for nominal SRL.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/N09-1043",
  "title": "Assessing and Improving the Performance of Speech Recognition for Incremental Systems",
  "year": 2009,
  "venue": "NAACL",
  "abstract": "In incremental spoken dialogue systems, partial hypotheses about what was said are required even while the utterance is still ongoing. We define measures for evaluating the quality of incremental ASR components with respect to the relative correctness of the partial hypotheses compared to hypotheses that can optimize over the complete input, the timing of hypothesis formation relative to the portion of the input they are about, and hypothesis stability, defined as the number of times they are revised. We show that simple incremental post-processing can improve stability dramatically, at the cost of timeliness (from 90% of edits of hypotheses being spurious down to 10% at a lag of 320ms). The measures are not independent, and we show how system designers can find a desired operating point for their ASR. To our knowledge, we are the first to suggest and examine a variety of measures for assessing incremental ASR and improve performance on this basis.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N09-1055",
  "title": "An Iterative Reinforcement Approach for Fine-Grained Opinion Mining",
  "year": 2009,
  "venue": "NAACL",
  "abstract": "With the in-depth study of sentiment analysis research, finer-grained opinion mining, which aims to detect opinions on different review features as opposed to the whole review level, has been receiving more and more attention in the sentiment analysis research community recently. Most of existing approaches rely mainly on the template extraction to identify the explicit relatedness between product feature and opinion terms, which is insufficient to detect the implicit review features and mine the hidden sentiment association in reviews, which satisfies (1) the review features are not appear explicit in the review sentences; (2) it can be deduced by the opinion words in its context. From an information theoretic point of view, this paper proposed an iterative reinforcement framework based on the improved information bottleneck algorithm to address such problem. More specifically, the approach clusters product features and opinion words simultaneously and iteratively by fusing both their semantic information and co-occurrence information. The experimental results demonstrate that our approach outperforms the template extraction based approaches.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/N09-1074",
  "title": "Improved Syntactic Models for Parsing Speech with Repairs",
  "year": 2009,
  "venue": "NAACL",
  "abstract": "This paper introduces three new syntactic models for representing speech with repairs. These models are developed to test the intuition that the erroneous parts of speech repairs (reparanda) are not generated or recognized as such while occurring, but only after they have been corrected. Thus, they are designed to minimize the differences in grammar rule applications between fluent and disfluent speech containing similar structure. The three models considered in this paper are also designed to isolate the mechanism of impact, by systematically exploring different variables.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N09-2026",
  "title": "Multi-scale Personalization for Voice Search Applications",
  "year": 2009,
  "venue": "NAACL",
  "abstract": "Voice Search applications provide a very convenient and direct access to a broad variety of services and information. However, due to the vast amount of information available and the open nature of the spoken queries, these applications still suffer from recognition errors. This paper explores the utilization of personalization features for the post-processing of recognition results in the form of n-best lists. Personalization is carried out from three different angles: short-term, long-term and Web-based, and a large variety of features are proposed for use in a log-linear classification framework. Experimental results on data obtained from a commercially deployed Voice Search system show that the combination of the proposed features leads to a substantial sentence error rate reduction. In addition, it is shown that personalization features which are very different in nature can successfully complement each other.",
  "stance": 0.4
 },
 {
  "url": "https://aclanthology.org/N09-2042",
  "title": "Search Engine Adaptation by Feedback Control Adjustment for Time-sensitive Query",
  "year": 2009,
  "venue": "NAACL",
  "abstract": "We propose a new method to rank a special category of time-sensitive queries that are year qualified. The method adjusts the retrieval scores of a base ranking function according to time-stamps of web documents so that the freshest documents are ranked higher. Our method, which is based on feedback control theory, uses ranking errors to adjust the search engine behavior. For this purpose, we use a simple but effective method to extract year qualified queries by mining query logs and a time-stamp recognition method that considers titles and urls of web documents. Our method was tested on a commercial search engine. The experiments show that our approach can significantly improve relevance ranking for year qualified queries even if all the existing methods for comparison failed.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N09-2049",
  "title": "Analysing Recognition Errors in Unlimited-Vocabulary Speech Recognition",
  "year": 2009,
  "venue": "NAACL",
  "abstract": "We analyze the recognition errors made by a morph-based continuous speech recognition system, which practically allows an unlimited vocabulary. Examining the role of the acoustic and language models in erroneous regions shows how speaker adaptive training (SAT) and discriminative training with minimum phone frame error (MPFE) criterion decrease errors in different error classes. Analyzing the errors with respect to word frequencies and manually classified error types reveals the most potential areas for improving the system.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/N09-2050",
  "title": "The independence of dimensions in multidimensional dialogue act annotation",
  "year": 2009,
  "venue": "NAACL",
  "abstract": "This paper presents empirical evidence for the orthogonality of the DIT multidimensional dialogue act annotation scheme, showing that the ten dimensions of communication which underlie this scheme are addressed independently in natural dialogue.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N10-1003",
  "title": "Products of Random Latent Variable Grammars",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "We show that the automatically induced latent variable grammars of Petrov et al. (2006) vary widely in their underlying representations, depending on their EM initialization point. We use this to our advantage, combining multiple automatically learned grammars into an unweighted product model, which gives significantly improved performance over state-ofthe-art individual grammars. In our model, the probability of a constituent is estimated as a product of posteriors obtained from multiple grammars that differ only in the random seed used for initialization, without any learning or tuning of combination weights. Despite its simplicity, a product of eight automatically learned grammars improves parsing accuracy from 90.2% to 91.8% on English, and from 80.3% to 84.5% on German.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N10-1032",
  "title": "Testing a Grammar Customization System with Sahaptin",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "I briefly describe a system for automatically creating an implemented grammar of a natural language based on answers to a web-based questionnaire, then present a grammar of Sahaptin, a language of the Pacific Northwest with complex argument-marking and agreement patterns, that was developed to test the system. The development of this grammar has proved useful in three ways: (1) verifying the correct functioning of the grammar customization system, (2) motivating the addition of a new pattern of agreement to the system, and (3) making detailed predictions that uncovered gaps in the linguistic descriptions of Sahaptin.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N10-1034",
  "title": "Fast Query for Large Treebanks",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "A variety of query systems have been developed for interrogating parsed corpora, or treebanks. With the arrival of efficient, widecoverage parsers, it is feasible to create very large databases of trees. However, existing approaches that use in-memory search, or relational or XML database technologies, do not scale up. We describe a method for storage, indexing, and query of treebanks that uses an information retrieval engine. Several experiments with a large treebank demonstrate excellent scaling characteristics for a wide range of query types. This work facilitates the curation of much larger treebanks, and enables them to be used effectively in a variety of scientific and engineering tasks.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N10-1035",
  "title": "Efficient Parsing of Well-Nested Linear Context-Free Rewriting Systems",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "The use of well-nested linear context-free rewriting systems has been empirically motivated for modeling of the syntax of languages with discontinuous constituents or relatively free word order. We present a chart-based parsing algorithm that asymptotically improves the known running time upper bound for this class of rewriting systems. Our result is obtained through a linear space construction of a binary normal form for the grammar at hand.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N10-1037",
  "title": "Evaluation Metrics for the Lexical Substitution Task",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "We identify some problems of the evaluation metrics used for the English Lexical Substitution Task of SemEval-2007, and propose alternative metrics that avoid these problems, which we hope will better guide the future development of lexical substitution systems.",
  "stance": -0.4
 },
 {
  "url": "https://aclanthology.org/N10-1038",
  "title": "Movie Reviews and Revenues: An Experiment in Text Regression",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "We consider the problem of predicting a movie’s opening weekend revenue. Previous work on this problem has used metadata about a movie—e.g., its genre, MPAA rating, and cast—with very limited work making use of text about the movie. In this paper, we use the text of film critics’ reviews from several sources to predict opening weekend revenue. We describe a new dataset pairing movie reviews with metadata and revenue data, and show that review text can substitute for metadata, and even improve over it, for prediction.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/N10-1040",
  "title": "Improving Phrase-Based Translation with Prototypes of Short Phrases",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "We investigate methods of generating additional bilingual phrase pairs for a phrasebased decoder by translating short sequences of source text. Because our translation task is more constrained, we can use a model that employs more linguistically rich features than a traditional decoder. We have implemented an example of this approach. Experimental results suggest that the phrase pairs produced by our method are useful to the decoder, and lead to improved sentence translations.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N10-1045",
  "title": "Towards Cross-Lingual Textual Entailment",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "This paper investigates cross-lingual textual entailment as a semantic relation between two text portions in different languages, and proposes a prospective research direction. We argue that cross-lingual textual entailment (CLTE) can be a core technology for several cross-lingual NLP applications and tasks. Through preliminary experiments, we aim at proving the feasibility of the task, and providing a reliable baseline. We also introduce new applications for CLTE that will be explored in future work.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/N10-1052",
  "title": "Generalizing Hierarchical Phrase-based Translation using Rules with Adjacent Nonterminals",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "Hierarchical phrase-based translation (Hiero, (Chiang, 2005)) provides an attractive framework within which both shortand longdistance reorderings can be addressed consistently and ef ciently. However, Hiero is generally implemented with a constraint preventing the creation of rules with adjacent nonterminals, because such rules introduce computational and modeling challenges. We introduce methods to address these challenges, and demonstrate that rules with adjacent nonterminals can improve Hiero's generalization power and lead to signi cant performance gains in Chinese-English translation.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/N10-1053",
  "title": "The Effect of Ambiguity on the Automated Acquisition of WSD Examples",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "Several methods for automatically generating labeled examples that can be used as training data for WSD systems have been proposed, including a semisupervised approach based on relevance feedback (Stevenson et al., 2008a). This approach was shown to generate examples that improved the performance of a WSD system for a set of ambiguous terms from the biomedical domain. However, we find that this approach does not perform as well on other data sets. The levels of ambiguity in these data sets are analysed and we suggest this is the reason for this negative result.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/N10-1056",
  "title": "For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "We report on work in progress on extracting lexical simplifications (e.g., “collaborate” → “work together”), focusing on utilizing edit histories in Simple English Wikipedia for this task. We consider two main approaches: (1) deriving simplification probabilities via an edit model that accounts for a mixture of different operations, and (2) using metadata to focus on edits that are more likely to be simplification operations. We find our methods to outperform a reasonable baseline and yield many high-quality lexical simplifications not included in an independently-created manually prepared list.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N10-1060",
  "title": "\"cba to check the spelling\": Investigating Parser Performance on Discussion Forum Posts",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "We evaluate the Berkeley parser on text from an online discussion forum. We evaluate the parser output with and without gold tokens and spellings (using Sparseval and Parseval), and we compile a list of problematic phenomena for this domain. The Parseval f-score for a small development set is 77.56. This increases to 80.27 when we apply a set of simple transformations to the input sentences and to the Wall Street Journal (WSJ) training sections.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/N10-1072",
  "title": "Learning to Link Entities with Knowledge Base",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "This paper address the problem of entity linking. Specifically, given an entity mentioned in unstructured texts, the task is to link this entity with an entry stored in the existing knowledge base. This is an important task for information extraction. It can serve as a convenient gateway to encyclopedic information, and can greatly improve the web users’ experience. Previous learning based solutions mainly focus on classification framework. However, it’s more suitable to consider it as a ranking problem. In this paper, we propose a learning to rank algorithm for entity linking. It effectively utilizes the relationship information among the candidates when ranking. The experiment results on the TAC 20091 dataset demonstrate the effectiveness of our proposed framework. The proposed method achieves 18.5% improvement in terms of accuracy over the classification models for those entities which have corresponding entries in the Knowledge Base. The overall performance of the system is also better than that of the state-of-the-art methods.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N10-1075",
  "title": "Subword Variation in Text Message Classification",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "For millions of people in less resourced regions of the world, text messages (SMS) provide the only regular contact with their doctor. Classifying messages by medical labels supports rapid responses to emergencies, the early identification of epidemics and everyday administration, but challenges include textbrevity, rich morphology, phonological variation, and limited training data. We present a novel system that addresses these, working with a clinic in rural Malawi and texts in the Chichewa language. We show that modeling morphological and phonological variation leads to a substantial average gain of F=0.206 and an error reduction of up to 63.8% for specific labels, relative to a baseline system optimized over word-sequences. By comparison, there is no significant gain when applying the same system to the English translations of the same texts/labels, emphasizing the need for subword modeling in many languages. Language independent morphological models perform as accurately as language specific models, indicating a broad deployment potential.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N10-1080",
  "title": "The Best Lexical Metric for Phrase-Based Statistical MT System Optimization",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "Translation systems are generally trained to optimize BLEU, but many alternative metrics are available. We explore how optimizing toward various automatic evaluation metrics (BLEU, METEOR, NIST, TER) affects the resulting model. We train a state-of-the-art MT system using MERT on many parameterizations of each metric and evaluate the resulting models on the other metrics and also using human judges. In accordance with popular wisdom, we find that it’s important to train on the same metric used in testing. However, we also find that training to a newer metric is only useful to the extent that the MT model’s structure and features allow it to take advantage of the metric. Contrasting with TER’s good correlation with human judgments, we show that people tend to prefer BLEU and NIST trained models to those trained on edit distance based metrics like TER or WER. Human preferences for METEOR trained models varies depending on the source language. Since using BLEU or NIST produces models that are more robust to evaluation by other metrics and perform well in human judgments, we conclude they are still the best choice for training.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/N10-1089",
  "title": "Can Recognising Multiword Expressions Improve Shallow Parsing?",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "There is significant evidence in the literature that integrating knowledge about multiword expressions can improve shallow parsing accuracy. We present an experimental study to quantify this improvement, focusing on compound nominals, proper names and adjectivenoun constructions. The evaluation set of multiword expressions is derived from WordNet and the textual data are downloaded from the web. We use a classification method to aid human annotation of output parses. This method allows us to conduct experiments on a large dataset of unannotated data. Experiments show that knowledge about multiword expressions leads to an increase of between 7.5% and 9.5% in accuracy of shallow parsing in sentences containing these multiword expressions.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N10-1091",
  "title": "Ensemble Models for Dependency Parsing: Cheap and Good?",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "Previous work on dependency parsing used various kinds of combination models but a systematic analysis and comparison of these approaches is lacking. In this paper we implemented such a study for English dependency parsing and find several non-obvious facts: (a) the diversity of base parsers is more important than complex models for learning (e.g., stacking, supervised meta-classification), (b) approximate, linear-time re-parsing algorithms guarantee well-formed dependency trees without significant performance loss, and (c) the simplest scoring model for re-parsing (unweighted voting) performs essentially as well as other more complex models. This study proves that fast and accurate ensemble parsers can be built with minimal effort.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/N10-1098",
  "title": "Engaging learning groups using Social Interaction Strategies",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "Conversational Agents have been shown to be effective tutors in a wide range of educational domains. However, these agents are often ignored and abused in collaborative learning scenarios involving multiple students. In our work presented here, we design and evaluate interaction strategies motivated from prior research in small group communication. We will discuss how such strategies can be implemented in agents. As a first step towards evaluating agents that can interact socially, we report results showing that human tutors employing these strategies are able to cover more concepts with the students besides being rated as better integrated, likeable and friendlier.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/N10-1110",
  "title": "Investigations into the Crandem Approach to Word Recognition",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "We suggest improvements to a previously proposed framework for integrating Conditional Random Fields and Hidden Markov Models, dubbed a Crandem system (2009). The previous authors’ work suggested that local label posteriors derived from the CRF were too low-entropy for use in word-level automatic speech recognition. As an alternative to the log posterior representation used in their system, we explore frame-level representations derived from the CRF feature functions. We also describe a weight normalization transformation that leads to increased entropy of the CRF posteriors. We report significant gains over the previous Crandem system on the Wall Street Journal word recognition task.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/N12-1001",
  "title": "Multiple Narrative Disentanglement: Unraveling Infinite Jest",
  "year": 2012,
  "venue": "NAACL",
  "abstract": "Many works (of both fiction and non-fiction) span multiple, intersecting narratives, each of which constitutes a story in its own right. In this work I introduce the task of multiple narrative disentanglement (MND), in which the aim is to tease these narratives apart by assigning passages from a text to the sub-narratives to which they belong. The motivating example I use is David Foster Wallace’s fictional text Infinite Jest. I selected this book because it contains multiple, interweaving narratives within its sprawling 1,000-plus pages. I propose and evaluate a novel unsupervised approach to MND that is motivated by the theory of narratology. This method achieves strong empirical results, successfully disentangling the threads in Infinite Jest and significantly outperforming baseline strategies in doing so.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N12-1018",
  "title": "Apples to Oranges: Evaluating Image Annotations from Natural Language Processing Systems",
  "year": 2012,
  "venue": "NAACL",
  "abstract": "We examine evaluation methods for systems that automatically annotate images using cooccurring text. We compare previous datasets for this task using a series of baseline measures inspired by those used in information retrieval, computer vision, and extractive summarization. Some of our baselines match or exceed the best published scores for those datasets. These results illuminate incorrect assumptions and improper practices regarding preprocessing, evaluation metrics, and the collection of gold image annotations. We conclude with a list of recommended practices for future research combining language and vision processing techniques.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/N12-1022",
  "title": "Topical Segmentation: a Study of Human Performance and a New Measure of Quality.",
  "year": 2012,
  "venue": "NAACL",
  "abstract": "In a large-scale study of how people find topical shifts in written text, 27 annotators were asked to mark topically continuous segments in 20 chapters of a novel. We analyze the resulting corpus for inter-annotator agreement and examine disagreement patterns. The results suggest that, while the overall agreement is relatively low, the annotators show high agreement on a subset of topical breaks – places where most prominent topic shifts occur. We recommend taking into account the prominence of topical shifts when evaluating topical segmentation, effectively penalizing more severely the errors on more important breaks. We propose to account for this in a simple modification of the windowDiff metric. We discuss the experimental results of evaluating several topical segmenters with and without considering the importance of the individual breaks, and emphasize the more insightful nature of the latter analysis.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/N12-1026",
  "title": "Optimized Online Rank Learning for Machine Translation",
  "year": 2012,
  "venue": "NAACL",
  "abstract": "We present an online learning algorithm for statistical machine translation (SMT) based on stochastic gradient descent (SGD). Under the online setting of rank learning, a corpus-wise loss has to be approximated by a batch local loss when optimizing for evaluation measures that cannot be linearly decomposed into a sentence-wise loss, such as BLEU. We propose a variant of SGD with a larger batch size in which the parameter update in each iteration is further optimized by a passive-aggressive algorithm. Learning is efficiently parallelized and line search is performed in each round when merging parameters across parallel jobs. Experiments on the NIST Chinese-to-English Open MT task indicate significantly better translation results.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N12-1030",
  "title": "The Challenges of Parsing Chinese with Combinatory Categorial Grammar",
  "year": 2012,
  "venue": "NAACL",
  "abstract": "We apply Combinatory Categorial Grammar to wide-coverage parsing in Chinese with the new Chinese CCGbank, bringing a formalism capable of transparently recovering non-local dependencies to a language in which they are particularly frequent. We train two state-of-the-art English  parsers: the parser of Petrov and Klein (P&K), and the Clark and Curran (C&C) parser, uncovering a surprising performance gap between them not observed in English — 72.73 (P&K) and 67.09 (C&C) F -score on  6. We explore the challenges of Chinese  parsing through three novel ideas: developing corpus variants rather than treating the corpus as fixed; controlling noun/verb and other  ambiguities; and quantifying the impact of constructions like pro-drop.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N12-1043",
  "title": "A Comparative Investigation of Morphological Language Modeling for the Languages of the European Union",
  "year": 2012,
  "venue": "NAACL",
  "abstract": "We investigate a language model that combines morphological and shape features with a Kneser-Ney model and test it in a large crosslingual study of European languages. Even though the model is generic and we use the same architecture and features for all languages, the model achieves reductions in perplexity for all 21 languages represented in the Europarl corpus, ranging from 3% to 11%. We show that almost all of this perplexity reduction can be achieved by identifying suffixes by frequency.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N12-1062",
  "title": "Tuning as Linear Regression",
  "year": 2012,
  "venue": "NAACL",
  "abstract": "We propose a tuning method for statistical machine translation, based on the pairwise ranking approach. Hopkins and May (2011) presented a method that uses a binary classifier. In this work, we use linear regression and show that our approach is as effective as using a binary classifier and converges faster.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N12-1063",
  "title": "Ranking-based readability assessment for early primary children's literature",
  "year": 2012,
  "venue": "NAACL",
  "abstract": "Determining the reading level of children’s literature is an important task for providing educators and parents with an appropriate reading trajectory through a curriculum. Automating this process has been a challenge addressed before in the computational linguistics literature, with most studies attempting to predict the particular grade level of a text. However, guided reading levels developed by educators operate at a more fine-grained level, with multiple levels corresponding to each grade. We find that ranking performs much better than classification at the fine-grained leveling task, and that features derived from the visual layout of a book are just as predictive as standard text features of level; including both sets of features, we find that we can predict the reading level up to 83% of the time on a small corpus of children’s books.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N12-1064",
  "title": "How Text Segmentation Algorithms Gain from Topic Models",
  "year": 2012,
  "venue": "NAACL",
  "abstract": "This paper introduces a general method to incorporate the LDA Topic Model into text segmentation algorithms. We show that semantic information added by Topic Models significantly improves the performance of two wordbased algorithms, namely TextTiling and C99. Additionally, we introduce the new TopicTiling algorithm that is designed to take better advantage of topic information. We show consistent improvements over word-based methods and achieve state-of-the art performance on a standard dataset.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N12-1068",
  "title": "Are You Sure? Confidence in Prediction of Dependency Tree Edges",
  "year": 2012,
  "venue": "NAACL",
  "abstract": "We describe and evaluate several methods for estimating the confidence in the per-edge correctness of a predicted dependency parse. We show empirically that the confidence is associated with the probability that an edge is selected correctly and that it can be used to detect incorrect edges very efficiently. We evaluate our methods on parsing text in 14 languages.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N12-1079",
  "title": "Why Not Grab a Free Lunch? Mining Large Corpora for Parallel Sentences to Improve Translation Modeling",
  "year": 2012,
  "venue": "NAACL",
  "abstract": "It is well known that the output quality of statistical machine translation (SMT) systems increases with more training data. To obtain more parallel text for translation modeling, researchers have turned to the web to mine parallel sentences, but most previous approaches have avoided the difficult problem of pairwise similarity on cross-lingual documents and instead rely on heuristics. In contrast, we confront this challenge head on using the MapReduce framework. On a modest cluster, our scalable end-to-end processing pipeline was able to automatically gather 5.8m parallel sentence pairs from English and German Wikipedia. Augmenting existing bitext with these data yielded significant improvements over a state-of-the-art baseline (2.39 BLEU points in the best case).",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N13-1018",
  "title": "Towards Topic Labeling with Phrase Entailment and Aggregation",
  "year": 2013,
  "venue": "NAACL",
  "abstract": "We propose a novel framework for topic labeling that assigns the most representative phrases for a given set of sentences covering the same topic. We build an entailment graph over phrases that are extracted from the sentences, and use the entailment relations to identify and select the most relevant phrases. We then aggregate those selected phrases by means of phrase generalization and merging. We motivate our approach by applying over conversational data, and show that our framework improves performance significantly over baseline algorithms.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N13-1019",
  "title": "Topic Segmentation with a Structured Topic Model",
  "year": 2013,
  "venue": "NAACL",
  "abstract": "We present a new hierarchical Bayesian model for unsupervised topic segmentation. This new model integrates a point-wise boundary sampling algorithm used in Bayesian segmentation into a structured topic model that can capture a simple hierarchical topic structure latent in documents. We develop an MCMC inference algorithm to split/merge segment(s). Experimental results show that our model outperforms previous unsupervised segmentation methods using only lexical information on Choi’s datasets and two meeting transcripts and has performance comparable to those previous methods on two written datasets.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N13-1020",
  "title": "Text Alignment for Real-Time Crowd Captioning",
  "year": 2013,
  "venue": "NAACL",
  "abstract": "The primary way of providing real-time captioning for deaf and hard of hearing people is to employ expensive professional stenographers who can type as fast as natural speaking rates. Recent work has shown that a feasible alternative is to combine the partial captions of ordinary typists, each of whom types part of what they hear. In this paper, we describe an improved method for combining partial captions into a final output based on weighted A search and multiple sequence alignment (MSA). In contrast to prior work, our method allows the tradeoff between accuracy and speed to be tuned, and provides formal error bounds. Our method outperforms the current state-of-the-art on Word Error Rate (WER) (29.6%), BLEU Score (41.4%), and F-measure (36.9%). The end goal is for these captions to be used by people, and so we also compare how these metrics correlate with the judgments of 50 study participants, which may assist others looking to make further progress on this problem.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N13-1021",
  "title": "Discriminative Joint Modeling of Lexical Variation and Acoustic Confusion for Automated Narrative Retelling Assessment",
  "year": 2013,
  "venue": "NAACL",
  "abstract": "Automatically assessing the fidelity of a retelling to the original narrative – a task of growing clinical importance – is challenging, given extensive paraphrasing during retelling along with cascading automatic speech recognition (ASR) errors. We present a word tagging approach using conditional random fields (CRFs) that allows a diversity of features to be considered during inference, including some capturing acoustic confusions encoded in word confusion networks. We evaluate the approach under several scenarios, including both supervised and unsupervised training, the latter achieved by training on the output of a baseline automatic word-alignment model. We also adapt the ASR models to the domain, and evaluate the impact of error rate on performance. We find strong robustness to ASR errors, even using just the 1-best system output. A hybrid approach making use of both automatic alignment and CRFs trained tagging models achieves the best performance, yielding strong improvements over using either approach alone.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N13-1023",
  "title": "Segmentation Strategies for Streaming Speech Translation",
  "year": 2013,
  "venue": "NAACL",
  "abstract": "The study presented in this work is a first effort at real-time speech translation of TED talks, a compendium of public talks with different speakers addressing a variety of topics. We address the goal of achieving a system that balances translation accuracy and latency. In order to improve ASR performance for our diverse data set, adaptation techniques such as constrained model adaptation and vocal tract length normalization are found to be useful. In order to improve machine translation (MT) performance, techniques that could be employed in real-time such as monotonic and partial translation retention are found to be of use. We also experiment with inserting text segmenters of various types between ASR and MT in a series of real-time translation experiments. Among other results, our experiments demonstrate that a good segmentation is useful, and a novel conjunction-based segmentation strategy improves translation quality nearly as much as other strategies such as comma-based segmentation. It was also found to be important to synchronize various pipeline components in order to minimize latency.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N13-1032",
  "title": "Improving reordering performance using higher order and structural features",
  "year": 2013,
  "venue": "NAACL",
  "abstract": "Recent work has shown that word aligned data can be used to learn a model for reordering source sentences to match the target order. This model learns the cost of putting a word immediately before another word and finds the best reordering by solving an instance of the Traveling Salesman Problem (TSP). However, for efficiently solving the TSP, the model is restricted to pairwise features which examine only a pair of words and their neighborhood. In this work, we go beyond these pairwise features and learn a model to rerank the n-best reorderings produced by the TSP model using higher order and structural features which help in capturing longer range dependencies. In addition to using a more informative set of source side features, we also capture target side features indirectly by using the translation score assigned to a reordering. Our experiments, involving Urdu-English, show that the proposed approach outperforms a state-of-theart PBSMT system which uses the TSP model for reordering by 1.3 BLEU points, and a publicly available state-of-the-art MT system, Hiero, by 3 BLEU points.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N13-1039",
  "title": "Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters",
  "year": 2013,
  "venue": "NAACL",
  "abstract": "We consider the problem of part-of-speech tagging for informal, online conversational text. We systematically evaluate the use of large-scale unsupervised word clustering and new lexical features to improve tagging accuracy. With these features, our system achieves state-of-the-art tagging results on both Twitter and IRC POS tagging tasks; Twitter tagging is improved from 90% to 93% accuracy (more than 3% absolute). Qualitative analysis of these word clusters yields insights about NLP and linguistic phenomena in this genre. Additionally, we contribute the first POS annotation guidelines for such text and release a new dataset of English language tweets annotated using these guidelines. Tagging software, annotation guidelines, and large-scale word clusters are available at: http://www.ark.cs.cmu.edu/TweetNLP This paper describes release 0.3 of the “CMU Twitter Part-of-Speech Tagger” and annotated data.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N13-1053",
  "title": "Negative Deceptive Opinion Spam",
  "year": 2013,
  "venue": "NAACL",
  "abstract": "The rising influence of user-generated online reviews (Cone, 2011) has led to growing incentive for businesses to solicit and manufacture DECEPTIVE OPINION SPAM—fictitious reviews that have been deliberately written to sound authentic and deceive the reader. Recently, Ott et al. (2011) have introduced an opinion spam dataset containing gold standard deceptive positive hotel reviews. However, the complementary problem of negative deceptive opinion spam, intended to slander competitive offerings, remains largely unstudied. Following an approach similar to Ott et al. (2011), in this work we create and study the first dataset of deceptive opinion spam with negative sentiment reviews. Based on this dataset, we find that standard n-gram text categorization techniques can detect negative deceptive opinion spam with performance far surpassing that of human judges. Finally, in conjunction with the aforementioned positive review dataset, we consider the possible interactions between sentiment and deception, and present initial results that encourage further exploration of this relationship.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/N13-1054",
  "title": "Improving speech synthesis quality by reducing pitch peaks in the source recordings",
  "year": 2013,
  "venue": "NAACL",
  "abstract": "We present a method for improving the perceived naturalness of corpus-based speech synthesizers. It consists in removing pronounced pitch peaks in the original recordings, which typically lead to noticeable discontinuities in the synthesized speech. We perceptually evaluated this method using two concatenative and two HMM-based synthesis systems, and found that using it on the source recordings managed to improve the naturalness of the synthesizers and had no effect on their intelligibility.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N13-1055",
  "title": "Robust Systems for Preposition Error Correction Using Wikipedia Revisions",
  "year": 2013,
  "venue": "NAACL",
  "abstract": "We show that existing methods for training preposition error correction systems, whether using well-edited text or error-annotated corpora, do not generalize across very different test sets. We present a new, large errorannotated corpus and use it to train systems that generalize across three different test sets, each from a different domain and with different error characteristics. This new corpus is automatically extracted from Wikipedia revisions and contains over one million instances of preposition corrections.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/N13-1069",
  "title": "Systematic Comparison of Professional and Crowdsourced Reference Translations for Machine Translation",
  "year": 2013,
  "venue": "NAACL",
  "abstract": "We present a systematic study of the effect of crowdsourced translations on Machine Translation performance. We compare Machine Translation systems trained on the same data but with translations obtained using Amazon’s Mechanical Turk vs. professional translations, and show that the same performance is obtained from Mechanical Turk translations at 1/5th the cost. We also show that adding a Mechanical Turk reference translation of the development set improves parameter tuning and output evaluation.",
  "stance": 0.1
 },
 {
  "url": "https://aclanthology.org/N13-1080",
  "title": "What's in a Domain? Multi-Domain Learning for Multi-Attribute Data",
  "year": 2013,
  "venue": "NAACL",
  "abstract": "Multi-Domain learning assumes that a single metadata attribute is used in order to divide the data into so-called domains. However, real-world datasets often have multiple metadata attributes that can divide the data into domains. It is not always apparent which single attribute will lead to the best domains, and more than one attribute might impact classification. We propose extensions to two multi-domain learning techniques for our multi-attribute setting, enabling them to simultaneously learn from several metadata attributes. Experimentally, they outperform the multi-domain learning baseline, even when it selects the single “best” attribute.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N13-1122",
  "title": "To Link or Not to Link? A Study on End-to-End Tweet Entity Linking",
  "year": 2013,
  "venue": "NAACL",
  "abstract": "Information extraction from microblog posts is an important task, as today microblogs capture an unprecedented amount of information and provide a view into the pulse of the world. As the core component of information extraction, we consider the task of Twitter entity linking in this paper. In the current entity linking literature, mention detection and entity disambiguation are frequently cast as equally important but distinct problems. However, in our task, we find that mention detection is often the performance bottleneck. The reason is that messages on micro-blogs are short, noisy and informal texts with little context, and often contain phrases with ambiguous meanings. To rigorously address the Twitter entity linking problem, we propose a structural SVM algorithm for entity linking that jointly optimizes mention detection and entity disambiguation as a single end-to-end task. By combining structural learning and a variety of firstorder, second-order, and context-sensitive features, our system is able to outperform existing state-of-the art entity linking systems by 15% F1.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N15-1010",
  "title": "Not All Character N-grams Are Created Equal: A Study in Authorship Attribution",
  "year": 2015,
  "venue": "NAACL",
  "abstract": "Character n-grams have been identified as the most successful feature in both singledomain and cross-domain Authorship Attribution (AA), but the reasons for their discriminative value were not fully understood. We identify subgroups of character n-grams that correspond to linguistic aspects commonly claimed to be covered by these features: morphosyntax, thematic content and style. We evaluate the predictiveness of each of these groups in two AA settings: a single domain setting and a cross-domain setting where multiple topics are present. We demonstrate that character ngrams that capture information about affixes and punctuation account for almost all of the power of character n-grams as features. Our study contributes new insights into the use of n-grams for future AA work and other classification tasks.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N15-1015",
  "title": "What's Cookin'? Interpreting Cooking Videos using Text, Speech and Vision",
  "year": 2015,
  "venue": "NAACL",
  "abstract": "We present a novel method for aligning a sequence of instructions to a video of someone carrying out a task. In particular, we focus on the cooking domain, where the instructions correspond to the recipe. Our technique relies on an HMM to align the recipe steps to the (automatically generated) speech transcript. We then refine this alignment using a state-of-the-art visual food detector, based on a deep convolutional neural network. We show that our technique outperforms simpler techniques based on keyword spotting. It also enables interesting applications, such as automatically illustrating recipes with keyframes, and searching within a video for events of interest.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N15-1026",
  "title": "Personalized Page Rank for Named Entity Disambiguation",
  "year": 2015,
  "venue": "NAACL",
  "abstract": "The task of Named Entity Disambiguation is to map entity mentions in the document to their correct entries in some knowledge base. We present a novel graph-based disambiguation approach based on Personalized PageRank (PPR) that combines local and global evidence for disambiguation and effectively filters out noise introduced by incorrect candidates. Experiments show that our method outperforms state-of-the-art approaches by achieving 91.7% in microand 89.9% in macroaccuracy on a dataset of 27.8K named entity mentions.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N15-1027",
  "title": "When and why are log-linear models self-normalizing?",
  "year": 2015,
  "venue": "NAACL",
  "abstract": "Several techniques have recently been proposed for training “self-normalized” discriminative models. These attempt to find parameter settings for which unnormalized model scores approximate the true label probability. However, the theoretical properties of such techniques (and of self-normalization generally) have not been investigated. This paper examines the conditions under which we can expect self-normalization to work. We characterize a general class of distributions that admit self-normalization, and prove generalization bounds for procedures that minimize empirical normalizer variance. Motivated by these results, we describe a novel variant of an established procedure for training self-normalized models. The new procedure avoids computing normalizers for most training examples, and decreases training time by as much as factor of ten while preserving model quality.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N15-1029",
  "title": "Disfluency Detection with a Semi-Markov Model and Prosodic Features",
  "year": 2015,
  "venue": "NAACL",
  "abstract": "We present a discriminative model for detecting disfluencies in spoken language transcripts. Structurally, our model is a semiMarkov conditional random field with features targeting characteristics unique to speech repairs. This gives a significant performance improvement over standard chain-structured CRFs that have been employed in past work. We then incorporate prosodic features over silences and relative word duration into our semi-CRF model, resulting in further performance gains; moreover, these features are not easily replaced by discrete prosodic indicators such as ToBI breaks. Our final system, the semi-CRF with prosodic information, achieves an F-score of 85.4, which is 1.3 F1 better than the best prior reported F-score on this dataset.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N15-1034",
  "title": "Sign constraints on feature weights improve a joint model of word segmentation and phonology",
  "year": 2015,
  "venue": "NAACL",
  "abstract": "This paper describes a joint model of word segmentation and phonological alternations, which takes unsegmented utterances as input and infers word segmentations and underlying phonological representations. The model is a Maximum Entropy or log-linear model, which can express a probabilistic version of Optimality Theory (OT; Prince and Smolensky (2004)), a standard phonological framework. The features in our model are inspired by OT’s Markedness and Faithfulness constraints. Following the OT principle that such features indicate “violations”, we require their weights to be non-positive. We apply our model to a modified version of the Buckeye corpus (Pitt et al., 2007) in which the only phonological alternations are deletions of word-final /d/ and /t/ segments. The model sets a new state-ofthe-art for this corpus for word segmentation, identification of underlying forms, and identification of /d/ and /t/ deletions. We also show that the OT-inspired sign constraints on feature weights are crucial for accurate identification of deleted /d/s; without them our model posits approximately 10 times more deleted underlying /d/s than appear in the manually annotated data.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N15-1035",
  "title": "Semi-Supervised Word Sense Disambiguation Using Word Embeddings in General and Specific Domains",
  "year": 2015,
  "venue": "NAACL",
  "abstract": "One of the weaknesses of current supervised word sense disambiguation (WSD) systems is that they only treat a word as a discrete entity. However, a continuous-space representation of words (word embeddings) can provide valuable information and thus improve generalization accuracy. Since word embeddings are typically obtained from unlabeled data using unsupervised methods, this method can be seen as a semi-supervised word sense disambiguation approach. This paper investigates two ways of incorporating word embeddings in a word sense disambiguation setting and evaluates these two methods on some SensEval/SemEval lexical sample and all-words tasks and also a domain-specific lexical sample task. The obtained results show that such representations consistently improve the accuracy of the selected supervised WSD system. Moreover, our experiments on a domainspecific dataset show that our supervised baseline system beats the best knowledge-based systems by a large margin.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/N15-1054",
  "title": "Inferring Missing Entity Type Instances for Knowledge Base Completion: New Dataset and Methods",
  "year": 2015,
  "venue": "NAACL",
  "abstract": "Most of previous work in knowledge base (KB) completion has focused on the problem of relation extraction. In this work, we focus on the task of inferring missing entity type instances in a KB, a fundamental task for KB competition yet receives little attention. Due to the novelty of this task, we construct a large-scale dataset and design an automatic evaluation methodology. Our knowledge base completion method uses information within the existing KB and external information from Wikipedia. We show that individual methods trained with a global objective that considers unobserved cells from both the entity and the type side gives consistently higher quality predictions compared to baseline methods. We also perform manual evaluation on a small subset of the data to verify the effectiveness of our knowledge base completion methods and the correctness of our proposed automatic evaluation method.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/N15-1062",
  "title": "Constraint-Based Models of Lexical Borrowing",
  "year": 2015,
  "venue": "NAACL",
  "abstract": "Linguistic borrowing is the phenomenon of transferring linguistic constructions (lexical, phonological, morphological, and syntactic) from a “donor” language to a “recipient” language as a result of contacts between communities speaking different languages. Borrowed words are found in all languages, and—in contrast to cognate relationships—borrowing relationships may exist across unrelated languages (for example, about 40% of Swahili’s vocabulary is borrowed from Arabic). In this paper, we develop a model of morpho-phonological transformations across languages with features based on universal constraints from Optimality Theory (OT). Compared to several standard— but linguistically naïve—baselines, our OTinspired model obtains good performance with only a few dozen training examples, making this a cost-effective strategy for sharing lexical information across languages.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N15-1117",
  "title": "Removing the Training Wheels: A Coreference Dataset that Entertains Humans and Challenges Computers",
  "year": 2015,
  "venue": "NAACL",
  "abstract": "Coreference is a core nlp problem. However, newswire data, the primary source of existing coreference data, lack the richness necessary to truly solve coreference. We present a new domain with denser references—quiz bowl questions—that is challenging and enjoyable to humans, and we use the quiz bowl community to develop a new coreference dataset, together with an annotation framework that can tag any text data with coreferences and named entities. We also successfully integrate active learning into this annotation pipeline to collect documents maximally useful to coreference models. State-of-the-art coreference systems underperform a simple classifier on our new dataset, motivating non-newswire data for future coreference research.",
  "stance": -0.3
 },
 {
  "url": "https://aclanthology.org/N15-1124",
  "title": "Accurate Evaluation of Segment-level Machine Translation Metrics",
  "year": 2015,
  "venue": "NAACL",
  "abstract": "Evaluation of segment-level machine translation metrics is currently hampered by: (1) low inter-annotator agreement levels in human assessments; (2) lack of an effective mechanism for evaluation of translations of equal quality; and (3) lack of methods of significance testing improvements over a baseline. In this paper, we provide solutions to each of these challenges and outline a new human evaluation methodology aimed specifically at assessment of segment-level metrics. We replicate the human evaluation component of WMT-13 and reveal that the current state-of-the-art performance of segment-level metrics is better than previously believed. Three segment-level metrics — METEOR, NLEPOR and SENTBLEUMOSES — are found to correlate with human assessment at a level not significantly outperformed by any other metric in both the individual language pair assessment for Spanish-toEnglish and the aggregated set of 9 language pairs.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/N15-1126",
  "title": "Why Read if You Can Scan? Trigger Scoping Strategy for Biographical Fact Extraction",
  "year": 2015,
  "venue": "NAACL",
  "abstract": "The rapid growth of information sources brings a unique challenge to biographical information extraction: how to find specific facts without having to read all the words. An effective solution is to follow the human scanning strategy which keeps a specific keyword in mind and searches within a specific scope. In this paper, we mimic a scanning process to extract biographical facts. We use event and relation triggers as keywords, identify their scopes and apply type constraints to extract answers within the scope of a trigger. Experiments demonstrate that our approach outperforms state-of-the-art methods up to 26% absolute gain in F-score without using any syntactic analysis or external knowledge bases.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N15-1132",
  "title": "Unsupervised Most Frequent Sense Detection using Word Embeddings",
  "year": 2015,
  "venue": "NAACL",
  "abstract": "An acid test for any new Word Sense Disambiguation (WSD) algorithm is its performance against the Most Frequent Sense (MFS). The field of WSD has found the MFS baseline very hard to beat. Clearly, if WSD researchers had access to MFS values, their striving to better this heuristic will push the WSD frontier. However, getting MFS values requires sense annotated corpus in enormous amounts, which is out of bounds for most languages, even if their WordNets are available. In this paper, we propose an unsupervised method for MFS detection from the untagged corpora, which exploits word embeddings. We compare the word embedding of a word with all its sense embeddings and obtain the predominant sense with the highest similarity. We observe significant performance gain for Hindi WSD over the WordNet First Sense (WFS) baseline. As for English, the SemCor baseline is bettered for those words whose frequency is greater than 2. Our approach is language and domain independent.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/N15-1160",
  "title": "Large-Scale Native Language Identification with Cross-Corpus Evaluation",
  "year": 2015,
  "venue": "NAACL",
  "abstract": "We present a large-scale Native Language Identification (NLI) experiment on new data, with a focus on cross-corpus evaluation to identify corpusand genre-independent language transfer features. We test a new corpus and show it is comparable to other NLI corpora and suitable for this task. Cross-corpus evaluation on two large corpora achieves good accuracy and evidences the existence of reliable language transfer features, but lower performance also suggests that NLI models are not completely portable across corpora. Finally, we present a brief case study of features distinguishing Japanese learners’ English writing, demonstrating the presence of cross-corpus and cross-genre language transfer features that are highly applicable to SLA and ESL research.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/N15-1161",
  "title": "Unediting: Detecting Disfluencies Without Careful Transcripts",
  "year": 2015,
  "venue": "NAACL",
  "abstract": "Speech transcripts often only capture semantic content, omitting disfluencies that can be useful for analyzing social dynamics of a discussion. This work describes steps in building a model that can recover a large fraction of locations where disfluencies were present, by transforming carefully annotated text to match the standard transcription style, introducing a two-stage model for handling different types of disfluencies, and applying semi-supervised learning. Experiments show improvement in disfluency detection on Supreme Court oral arguments, nearly 23% improvement in F1.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N15-1179",
  "title": "Do We Really Need Lexical Information? Towards a Top-down Approach to Sentiment Analysis of Product Reviews",
  "year": 2015,
  "venue": "NAACL",
  "abstract": "Most of the current approaches to sentiment analysis of product reviews are dependent on lexical sentiment information and proceed in a bottom-up way, adding new layers of features to lexical data. In this paper, we maintain that a typical product review is not a bag of sentiments, but a narrative with an underlying structure and reoccurring patterns, which allows us to predict its sentiments knowing only its general polarity and discourse cues that occur in it. We hypothesize that knowing only the review’s score and its discourse patterns would allow us to accurately predict the sentiments of its individual sentences. The experiments we conducted prove this hypothesis and show a substantial improvement over the lexical baseline.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/N16-1005",
  "title": "Controlling Politeness in Neural Machine Translation via Side Constraints",
  "year": 2016,
  "venue": "NAACL",
  "abstract": "Many languages use honorifics to express politeness, social distance, or the relative social status between the speaker and their addressee(s). In machine translation from a language without honorifics such as English, it is difficult to predict the appropriate honorific, but users may want to control the level of politeness in the output. In this paper, we perform a pilot study to control honorifics in neural machine translation (NMT) via side constraints, focusing on English→German. We show that by marking up the (English) source side of the training data with a feature that encodes the use of honorifics on the (German) target side, we can control the honorifics produced at test time. Experiments show that the choice of honorifics has a big impact on translation quality as measured by BLEU, and oracle experiments show that substantial improvements are possible by constraining the translation to the desired level of politeness.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N16-1016",
  "title": "A Long Short-Term Memory Framework for Predicting Humor in Dialogues",
  "year": 2016,
  "venue": "NAACL",
  "abstract": "We propose a first-ever attempt to employ a Long Short-Term memory based framework to predict humor in dialogues. We analyze data from a popular TV-sitcom, whose canned laughters give an indication of when the audience would react. We model the setuppunchline relation of conversational humor with a Long Short-Term Memory, with utterance encodings obtained from a Convolutional Neural Network. Out neural network framework is able to improve the F-score of 8% over a Conditional Random Field baseline. We show how the LSTM effectively models the setup-punchline relation reducing the number of false positives and increasing the recall. We aim to employ our humor prediction model to build effective empathetic machine able to understand jokes.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N16-1018",
  "title": "Counter-fitting Word Vectors to Linguistic Constraints",
  "year": 2016,
  "venue": "NAACL",
  "abstract": "In this work, we present a novel counter-fitting method which injects antonymy and synonymy constraints into vector space representations in order to improve the vectors’ capability for judging semantic similarity. Applying this method to publicly available pre-trained word vectors leads to a new state of the art performance on the SimLex-999 dataset. We also show how the method can be used to tailor the word vector space for the downstream task of dialogue state tracking, resulting in robust improvements across different dialogue domains.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N16-1038",
  "title": "Questioning Arbitrariness in Language: a Data-Driven Study of Conventional Iconicity",
  "year": 2016,
  "venue": "NAACL",
  "abstract": "This paper presents a data-driven investigation of phonesthemes, phonetic units said to carry meaning associations, thus challenging the traditionally assumed arbitrariness of language. Phonesthemes have received a substantial amount of attention within the cognitive science literature on sound iconicity, but nevertheless remain a controversial and understudied phenomenon. Here we employ NLP techniques to address two main questions: How can the existence of phonesthemes be tested at a large scale with quantitative methods? And how can the meaning arguably carried by a phonestheme be induced automatically from word embeddings? We develop novel methods to make progress on these fronts and compare our results to previous work, obtaining substantial improvements.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N16-1086",
  "title": "What to talk about and how? Selective Generation using LSTMs with Coarse-to-Fine Alignment",
  "year": 2016,
  "venue": "NAACL",
  "abstract": "We propose an end-to-end, domainindependent neural encoder-aligner-decoder model for selective generation, i.e., the joint task of content selection and surface realization. Our model first encodes a full set of over-determined database event records via an LSTM-based recurrent neural network, then utilizes a novel coarse-to-fine aligner to identify the small subset of salient records to talk about, and finally employs a decoder to generate free-form descriptions of the aligned, selected records. Our model achieves the best selection and generation results reported to-date (with 59% relative improvement in generation) on the benchmark WEATHERGOV dataset, despite using no specialized features or linguistic resources. Using an improved k-nearest neighbor beam filter helps further. We also perform a series of ablations and visualizations to elucidate the contributions of our key model components. Lastly, we evaluate the generalizability of our model on the ROBOCUP dataset, and get results that are competitive with or better than the state-of-the-art, despite being severely data-starved.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N16-1098",
  "title": "A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories",
  "year": 2016,
  "venue": "NAACL",
  "abstract": "Representation and learning of commonsense knowledge is one of the foundational problems in the quest to enable deep language understanding. This issue is particularly challenging for understanding casual and correlational relationships between events. While this topic has received a lot of interest in the NLP community, research has been hindered by the lack of a proper evaluation framework. This paper attempts to address this problem with a new framework for evaluating story understanding and script learning: the ‘Story Cloze Test’. This test requires a system to choose the correct ending to a four-sentence story. We created a new corpus of 50k five-sentence commonsense stories, ROCStories, to enable this evaluation. This corpus is unique in two ways: (1) it captures a rich set of causal and temporal commonsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation. Experimental evaluation shows that a host of baselines and state-of-the-art models based on shallow language understanding struggle to achieve a high score on the Story Cloze Test. We discuss these implications for script and story learning, and offer suggestions for deeper language understanding.",
  "stance": -0.7
 },
 {
  "url": "https://aclanthology.org/N18-1007",
  "title": "Parsing Speech: a Neural Approach to Integrating Lexical and Acoustic-Prosodic Information",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "In conversational speech, the acoustic signal provides cues that help listeners disambiguate difficult parses. For automatically parsing spoken utterances, we introduce a model that integrates transcribed text and acoustic-prosodic features using a convolutional neural network over energy and pitch trajectories coupled with an attention-based recurrent neural network that accepts text and prosodic features. We find that different types of acoustic-prosodic features are individually helpful, and together give statistically significant improvements in parse and disfluency detection F1 scores over a strong text-only baseline. For this study with known sentence boundaries, error analyses show that the main benefit of acoustic-prosodic features is in sentences with disfluencies, attachment decisions are most improved, and transcription errors obscure gains from prosody.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N18-1008",
  "title": "Tied Multitask Learning for Neural Speech Translation",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "We explore multitask models for neural translation of speech, augmenting them in order to reflect two intuitive notions. First, we introduce a model where the second task decoder receives information from the decoder of the first task, since higher-level intermediate representations should provide useful information. Second, we apply regularization that encourages transitivity and invertibility. We show that the application of these notions on jointly trained models improves performance on the tasks of low-resource speech transcription and translation. It also leads to better performance when using attention information for word discovery over unsegmented input.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N18-1010",
  "title": "Attentive Interaction Model: Modeling Changes in View in Argumentation",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "We present a neural architecture for modeling argumentative dialogue that explicitly models the interplay between an Opinion Holder's (OH's) reasoning and a challenger's argument, with the goal of predicting if the argument successfully changes the OH's view. The model has two components: (1) vulnerable region detection, an attention model that identifies parts of the OH's reasoning that are amenable to change, and (2) interaction encoding, which identifies the relationship between the content of the OH's reasoning and that of the challenger's argument. Based on evaluation on discussions from the Change My View forum on Reddit, the two components work together to predict an OH's change in view, outperforming several baselines. A posthoc analysis suggests that sentences picked out by the attention model are addressed more frequently by successful arguments than by unsuccessful ones.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N18-1011",
  "title": "Automatic Focus Annotation: Bringing Formal Pragmatics Alive in Analyzing the Information Structure of Authentic Data",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "Analyzing language in context, both from a theoretical and from a computational perspective, is receiving increased interest. Complementing the research in linguistics on discourse and information structure, in computational linguistics identifying discourse concepts was also shown to improve the performance of certain applications, for example, Short Answer Assessment systems (Ziai and Meurers, 2014). Building on the research that established detailed annotation guidelines for manual annotation of information structural concepts for written (Dipper et al., 2007; Ziai and Meurers, 2014) and spoken language data (Calhoun et al., 2010), this paper presents the first approach automating the analysis of focus in authentic written data. Our classification approach combines a range of lexical, syntactic, and semantic features to achieve an accuracy of 78.1% for identifying focus.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N18-1023",
  "title": "Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "We present a reading comprehension challenge in which questions can only be answered by taking into account information from multiple sentences. We solicit and verify questions and answers for this challenge through a 4-step crowdsourcing experiment. Our challenge dataset contains 6,500+ questions for 1000+ paragraphs across 7 different domains (elementary school science, news, travel guides, fiction stories, etc) bringing in linguistic diversity to the texts and to the questions wordings. On a subset of our dataset, we found human solvers to achieve an F1-score of 88.1%. We analyze a range of baselines, including a recent state-of-art reading comprehension system, and demonstrate the difficulty of this challenge, despite a high human performance. The dataset is the first to study multi-sentence inference at scale, with an open-ended set of question types that requires reasoning skills.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N18-1025",
  "title": "QuickEdit: Editing Text & Translations by Crossing Words Out",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "We propose a framework for computer-assisted text editing. It applies to translation post-editing and to paraphrasing. Our proposal relies on very simple interactions: a human editor modifies a sentence by marking tokens they would like the system to change. Our model then generates a new sentence which reformulates the initial sentence by avoiding marked words. The approach builds upon neural sequence-to-sequence modeling and introduces a neural network which takes as input a sentence along with change markers. Our model is trained on translation bitext by simulating post-edits. We demonstrate the advantage of our approach for translation post-editing through simulated post-edits. We also evaluate our model for paraphrasing through a user study.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/N18-1086",
  "title": "Neural Syntactic Generative Models with Exact Marginalization",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "We present neural syntactic generative models with exact marginalization that support both dependency parsing and language modeling. Exact marginalization is made tractable through dynamic programming over shift-reduce parsing and minimal RNN-based feature sets. Our algorithms complement previous approaches by supporting batched training and enabling online computation of next word probabilities. For supervised dependency parsing, our model achieves a state-of-the-art result among generative approaches. We also report empirical results on unsupervised syntactic models and their role in language modeling. We find that our model formulation of latent dependencies with exact marginalization do not lead to better intrinsic language modeling performance than vanilla RNNs, and that parsing accuracy is not correlated with language modeling perplexity in stack-based models.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/N18-1175",
  "title": "The Argument Reasoning Comprehension Task: Identification and Reconstruction of Implicit Warrants",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "Reasoning is a crucial part of natural language argumentation. To comprehend an argument, one must analyze its warrant, which explains why its claim follows from its premises. As arguments are highly contextualized, warrants are usually presupposed and left implicit. Thus, the comprehension does not only require language understanding and logic skills, but also depends on common sense. In this paper we develop a methodology for reconstructing warrants systematically. We operationalize it in a scalable crowdsourcing process, resulting in a freely licensed dataset with warrants for 2k authentic arguments from news comments. On this basis, we present a new challenging task, the argument reasoning comprehension task. Given an argument with a claim and a premise, the goal is to choose the correct implicit warrant from two options. Both warrants are plausible and lexically close, but lead to contradicting claims. A solution to this task will define a substantial step towards automatic warrant reconstruction. However, experiments with several neural attention and language models reveal that current approaches do not suffice.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/N18-1195",
  "title": "Towards Understanding Text Factors in Oral Reading",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "Using a case study, we show that variation in oral reading rate across passages for professional narrators is consistent across readers and much of it can be explained using features of the texts being read. While text complexity is a poor predictor of the reading rate, a substantial share of variability can be explained by timing and story-based factors with performance reaching r=0.75 for unseen passages and narrator.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/N18-1200",
  "title": "Speaker Naming in Movies",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "We propose a new model for speaker naming in movies that leverages visual, textual, and acoustic modalities in an unified optimization framework. To evaluate the performance of our model, we introduce a new dataset consisting of six episodes of the Big Bang Theory TV show and eighteen full movies covering different genres. Our experiments show that our multimodal model significantly outperforms several competitive baselines on the average weighted F-score metric. To demonstrate the effectiveness of our framework, we design an end-to-end memory network model that leverages our speaker naming model and achieves state-of-the-art results on the subtitles task of the MovieQA 2017 Challenge.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N18-2005",
  "title": "Is Something Better than Nothing? Automatically Predicting Stance-based Arguments Using Deep Learning and Small Labelled Dataset",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "Online reviews have become a popular portal among customers making decisions about purchasing products. A number of corpora of reviews have been widely investigated in NLP in general, and, in particular, in argument mining. This is a subset of NLP that deals with extracting arguments and the relations among them from user-based content. A major problem faced by argument mining research is the lack of human-annotated data. In this paper, we investigate the use of weakly supervised and semi-supervised methods for automatically annotating data, and thus providing large annotated datasets. We do this by building on previous work that explores the classification of opinions present in reviews based whether the stance is expressed explicitly or implicitly. In the work described here, we automatically annotate stance as implicit or explicit and our results show that the datasets we generate, although noisy, can be used to learn better models for implicit/explicit opinion classification.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/N18-2015",
  "title": "A Simple and Effective Approach to the Story Cloze Test",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "In the Story Cloze Test, a system is presented with a 4-sentence prompt to a story, and must determine which one of two potential endings is the 'right' ending to the story. Previous work has shown that ignoring the training set and training a model on the validation set can achieve high accuracy on this task due to stylistic differences between the story endings in the training set and validation and test sets. Following this approach, we present a simpler fully-neural approach to the Story Cloze Test using skip-thought embeddings of the stories in a feed-forward network that achieves close to state-of-the-art performance on this task without any feature engineering. We also find that considering just the last sentence of the prompt instead of the whole prompt yields higher accuracy with our approach.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N18-2021",
  "title": "Training Structured Prediction Energy Networks with Indirect Supervision",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "This paper introduces rank-based training of structured prediction energy networks (SPENs). Our method samples from output structures using gradient descent and minimizes the ranking violation of the sampled structures with respect to a scalar scoring function defined with domain knowledge. We have successfully trained SPEN for citation field extraction without any labeled data instances, where the only source of supervision is a simple human-written scoring function. Such scoring functions are often easy to provide; the SPEN then furnishes an efficient structured prediction inference procedure.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N18-2022",
  "title": "Si O No, Que Penses? Catalonian Independence and Linguistic Identity on Social Media",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "Political identity is often manifested in language variation, but the relationship between the two is still relatively unexplored from a quantitative perspective. This study examines the use of Catalan, a language local to the semi-autonomous region of Catalonia in Spain, on Twitter in discourse related to the 2017 independence referendum. We corroborate prior findings that pro-independence tweets are more likely to include the local language than anti-independence tweets. We also find that Catalan is used more often in referendum-related discourse than in other contexts, contrary to prior findings on language variation. This suggests a strong role for the Catalan language in the expression of Catalonian political identity.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/N18-2041",
  "title": "Knowledge-Enriched Two-Layered Attention Network for Sentiment Analysis",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "We propose a novel two-layered attention network based on Bidirectional Long Short-Term Memory for sentiment analysis. The novel two-layered attention network takes advantage of the external knowledge bases to improve the sentiment prediction. It uses the Knowledge Graph Embedding generated using the WordNet. We build our model by combining the two-layered attention network with the supervised model based on Support Vector Regression using a Multilayer Perceptron network for sentiment analysis. We evaluate our model on the benchmark dataset of SemEval 2017 Task 5. Experimental results show that the proposed model surpasses the top system of SemEval 2017 Task 5. The model performs significantly better by improving the state-of-the-art system at SemEval 2017 Task 5 by 1.7 and 3.7 points for sub-tracks 1 and 2 respectively.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N18-2085",
  "title": "Are All Languages Equally Hard to Language-Model?",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "For general modeling methods applied to diverse languages, a natural question is: how well should we expect our models to work on languages with differing typological profiles? In this work, we develop an evaluation framework for fair cross-linguistic comparison of language models, using translated text so that all models are asked to predict approximately the same information. We then conduct a study on 21 languages, demonstrating that in some languages, the textual expression of the information is harder to predict with both n-gram and LSTM language models. We show complex inflectional morphology to be a cause of performance differences among languages.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N18-2113",
  "title": "Evaluating Historical Text Normalization Systems: How Well Do They Generalize?",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "We highlight several issues in the evaluation of historical text normalization systems that make it hard to tell how well these systems would actually work in practice-i.e., for new datasets or languages; in comparison to more naïve systems; or as a preprocessing step for downstream NLP tools. We illustrate these issues and exemplify our proposed evaluation practices by comparing two neural models against a naïve baseline system. We show that the neural models generalize well to unseen words in tests on five languages; nevertheless, they provide no clear benefit over the naïve baseline for downstream POS tagging of an English historical collection. We conclude that future work should include more rigorous evaluation, including both intrinsic and extrinsic measures where possible.",
  "stance": -0.7
 },
 {
  "url": "https://aclanthology.org/N18-2118",
  "title": "Slot-Gated Modeling for Joint Slot Filling and Intent Prediction",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "Attention-based recurrent neural network models for joint intent detection and slot filling have achieved the state-of-the-art performance, while they have independent attention weights. Considering that slot and intent have the strong relationship, this paper proposes a slot gate that focuses on learning the relationship between intent and slot attention vectors in order to obtain better semantic frame results by the global optimization. The experiments show that our proposed model significantly improves sentence-level semantic frame accuracy with 4.2% and 1.9% relative improvement compared to the attentional model on benchmark ATIS and Snips datasets respectively",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N18-2123",
  "title": "Visual Referring Expression Recognition: What Do Systems Actually Learn?",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "We present an empirical analysis of state-of-the-art systems for referring expression recognition - the task of identifying the object in an image referred to by a natural language expression - with the goal of gaining insight into how these systems reason about language and vision. Surprisingly, we find strong evidence that even sophisticated and linguistically-motivated models for this task may ignore linguistic structure, instead relying on shallow correlations introduced by unintended biases in the data selection and annotation process. For example, we show that a system trained and tested on the input image without the input referring expression can achieve a precision of 71.2% in top-2 predictions. Furthermore, a system that predicts only the object category given the input can achieve a precision of 84.2% in top-2 predictions. These surprisingly positive results for what should be deficient prediction scenarios suggest that careful analysis of what our models are learning - and further, how our data is constructed - is critical as we seek to make substantive progress on grounded language tasks.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/N19-1069",
  "title": "Adversarial Training for Satire Detection: Controlling for Confounding Variables",
  "year": 2019,
  "venue": "NAACL",
  "abstract": "The automatic detection of satire vs. regular news is relevant for downstream applications (for instance, knowledge base population) and to improve the understanding of linguistic characteristics of satire. Recent approaches build upon corpora which have been labeled automatically based on article sources. We hypothesize that this encourages the models to learn characteristics for different publication sources (e.g., \"The Onion\" vs. \"The Guardian\") rather than characteristics of satire, leading to poor generalization performance to unseen publication sources. We therefore propose a novel model for satire detection with an adversarial component to control for the confounding variable of publication source. On a large novel data set collected from German news (which we make available to the research community), we observe comparable satire classification performance and, as desired, a considerable drop in publication classification performance with adversarial training. Our analysis shows that the adversarial component is crucial for the model to learn to pay attention to linguistic properties of satire.",
  "stance": -0.3
 },
 {
  "url": "https://aclanthology.org/N19-1070",
  "title": "Keyphrase Generation: A Text Summarization Struggle",
  "year": 2019,
  "venue": "NAACL",
  "abstract": "Authors' keyphrases assigned to scientific articles are essential for recognizing content and topic aspects. Most of the proposed supervised and unsupervised methods for keyphrase generation are unable to produce terms that are valuable but do not appear in the text. In this paper, we explore the possibility of considering the keyphrase string as an abstractive summary of the title and the abstract. First, we collect, process and release a large dataset of scientific paper metadata that contains 2.2 million records. Then we experiment with popular text summarization neural architectures. Despite using advanced deep learning models, large quantities of data and many days of computation, our systematic evaluation on four test datasets reveals that the explored text summarization methods could not produce better keyphrases than the simpler unsupervised methods, or the existing supervised ones.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/N19-1132",
  "title": "Cross-Corpora Evaluation and Analysis of Grammatical Error Correction Models - Is Single-Corpus Evaluation Enough?",
  "year": 2019,
  "venue": "NAACL",
  "abstract": "This study explores the necessity of performing cross-corpora evaluation for grammatical error correction (GEC) models. GEC models have been previously evaluated based on a single commonly applied corpus: the CoNLL-2014 benchmark. However, the evaluation remains incomplete because the task difficulty varies depending on the test corpus and conditions such as the proficiency levels of the writers and essay topics. To overcome this limitation, we evaluate the performance of several GEC models, including NMT-based (LSTM, CNN, and transformer) and an SMT-based model, against various learner corpora (CoNLL-2013, CoNLL-2014, FCE, JFLEG, ICNALE, and KJ). Evaluation results reveal that the models' rankings considerably vary depending on the corpus, indicating that single-corpus evaluation is insufficient for GEC models.",
  "stance": -0.5
 },
 {
  "url": "https://aclanthology.org/N19-1143",
  "title": "Vector of Locally Aggregated Embeddings for Text Representation",
  "year": 2019,
  "venue": "NAACL",
  "abstract": "We present Vector of Locally Aggregated Embeddings (VLAE) for effective and, ultimately, lossless representation of textual content. Our model encodes each input text by effectively identifying and integrating the representations of its semantically-relevant parts. The proposed model generates high quality representation of textual content and improves the classification performance of current state-of-the-art deep averaging networks across several text classification tasks.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N19-1145",
  "title": "Biomedical Event Extraction based on Knowledge-driven Tree-LSTM",
  "year": 2019,
  "venue": "NAACL",
  "abstract": "Event extraction for the biomedical domain is more challenging than that in the general news domain since it requires broader acquisition of domain-specific knowledge and deeper understanding of complex contexts. To better encode contextual information and external background knowledge, we propose a novel knowledge base (KB)-driven tree-structured long short-term memory networks (Tree-LSTM) framework, incorporating two new types of features: (1) dependency structures to capture wide contexts; (2) entity properties (types and category descriptions) from external ontologies via entity linking. We evaluate our approach on the BioNLP shared task with Genia dataset and achieve a new state-of-the-art result. In addition, both quantitative and qualitative studies demonstrate the advancement of the Tree-LSTM and the external knowledge representation for biomedical event extraction.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N19-1165",
  "title": "Text Processing Like Humans Do: Visually Attacking and Shielding NLP Systems",
  "year": 2019,
  "venue": "NAACL",
  "abstract": "Visual modifications to text are often used to obfuscate offensive comments in social media (e.g., \"!d10t\") or as a writing style (\"1337\" in \"leet speak\"), among other scenarios. We consider this as a new type of adversarial attack in NLP, a setting to which humans are very robust, as our experiments with both simple and more difficult visual perturbations demonstrate. We investigate the impact of visual adversarial attacks on current NLP systems on character-, word-, and sentence-level tasks, showing that both neural and non-neural models are, in contrast to humans, extremely sensitive to such attacks, suffering performance decreases of up to 82%. We then explore three shielding methods-visual character embeddings, adversarial training, and rule-based recovery-which substantially improve the robustness of the models. However, the shielding methods still fall behind performances achieved in non-attack scenarios, which demonstrates the difficulty of dealing with visual attacks.",
  "stance": -0.3
 },
 {
  "url": "https://aclanthology.org/N19-1233",
  "title": "Evaluating Text GANs as Language Models",
  "year": 2019,
  "venue": "NAACL",
  "abstract": "Generative Adversarial Networks (GANs) are a promising approach for text generation that, unlike traditional language models (LM), does not suffer from the problem of \"exposure bias\". However, A major hurdle for understanding the potential of GANs for text generation is the lack of a clear evaluation metric. In this work, we propose to approximate the distribution of text generated by a GAN, which permits evaluating them with traditional probability-based LM metrics. We apply our approximation procedure on several GAN-based models and show that they currently perform substantially worse than state-of-the-art LMs. Our evaluation procedure promotes better understanding of the relation between GANs and LMs, and can accelerate progress in GAN-based text generation.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/N19-1237",
  "title": "Evaluating Rewards for Question Generation Models",
  "year": 2019,
  "venue": "NAACL",
  "abstract": "Recent approaches to question generation have used modifications to a Seq2Seq architecture inspired by advances in machine translation. Models are trained using teacher forcing to optimise only the one-step-ahead prediction. However, at test time, the model is asked to generate a whole sequence, causing errors to propagate through the generation process (exposure bias). A number of authors have suggested that optimising for rewards less tightly coupled to the training data might counter this mismatch. We therefore optimise directly for various objectives beyond simply replicating the ground truth questions, including a novel approach using an adversarial discriminator that seeks to generate questions that are indistinguishable from real examples. We confirm that training with policy gradient methods leads to increases in the metrics used as rewards. We perform a human evaluation, and show that although these metrics have previously been assumed to be good proxies for question quality, they are poorly aligned with human judgement and the model simply learns to exploit the weaknesses of the reward source.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/N19-1245",
  "title": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms",
  "year": 2019,
  "venue": "NAACL",
  "abstract": "We introduce a large-scale dataset of math word problems and an interpretable neural math problem solver by learning to map problems to their operation programs. Due to annotation challenges, current datasets in this domain have been either relatively small in scale or did not offer precise operational annotations over diverse problem types. We introduce a new representation language to model operation programs corresponding to each math problem that aim to improve both the performance and the interpretability of the learned models. Using this representation language, we significantly enhance the AQUA-RAT dataset with fully-specified operational programs. We additionally introduce a neural sequence-to-program model with automatic problem categorization. Our experiments show improvements over competitive baselines in our dataset as well as the AQUA-RAT dataset. The results are still lower than human performance indicating that the dataset poses new challenges for future research. Our dataset is available at https://math-qa.github.io/math-QA/",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/N19-1252",
  "title": "A Grounded Unsupervised Universal Part-of-Speech Tagger for Low-Resource Languages",
  "year": 2019,
  "venue": "NAACL",
  "abstract": "Unsupervised part of speech (POS) tagging is often framed as a clustering problem, but practical taggers need to ground their clusters as well. Grounding generally requires reference labeled data, a luxury a low-resource language might not have. In this work, we describe an approach for low-resource unsupervised POS tagging that yields fully grounded output and requires no labeled training data. We find the classic method of Brown et al. (1992) clusters well in our use case and employ a decipherment-based approach to grounding. This approach presumes a sequence of cluster IDs is a 'ciphertext' and seeks a POS tag-to-cluster ID mapping that will reveal the POS sequence. We show intrinsically that, despite the difficulty of the task, we obtain reasonable performance across a variety of languages. We also show extrinsically that incorporating our POS tagger into a name tagger leads to state-of-the-art tagging performance in Sinhalese and Kinyarwanda, two languages with nearly no labeled POS data available. We further demonstrate our tagger's utility by incorporating it into a true 'zero-resource' variant of the MALOPA (Ammar et al., 2016) dependency parser model that removes the current reliance on multilingual resources and gold POS tags for new languages. Experiments show that including our tagger makes up much of the accuracy lost when gold POS tags are unavailable.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N19-1298",
  "title": "A Richer-but-Smarter Shortest Dependency Path with Attentive Augmentation for Relation Extraction",
  "year": 2019,
  "venue": "NAACL",
  "abstract": "To extract the relationship between two entities in a sentence, two common approaches are (1) using their shortest dependency path (SDP) and (2) using an attention model to capture a context-based representation of the sentence. Each approach suffers from its own disadvantage of either missing or redundant information. In this work, we propose a novel model that combines the advantages of these two approaches. This is based on the basic information in the SDP enhanced with information selected by several attention mechanisms with kernel filters, namely RbSP (Richer-but-Smarter SDP). To exploit the representation behind the RbSP structure effectively, we develop a combined deep neural model with a LSTM network on word sequences and a CNN on RbSP. Experimental results on the SemEval-2010 dataset demonstrate improved performance over competitive baselines. The data and source code are available at https://github.com/catcd/RbSP.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/N19-1320",
  "title": "Reinforcement Learning Based Text Style Transfer without Parallel Training Corpus",
  "year": 2019,
  "venue": "NAACL",
  "abstract": "Text style transfer rephrases a text from a source style (e.g., informal) to a target style (e.g., formal) while keeping its original meaning. Despite the success existing works have achieved using a parallel corpus for the two styles, transferring text style has proven significantly more challenging when there is no parallel training corpus. In this paper, we address this challenge by using a reinforcement-learning-based generator-evaluator architecture. Our generator employs an attention-based encoder-decoder to transfer a sentence from the source style to the target style. Our evaluator is an adversarially trained style discriminator with semantic and syntactic constraints that score the generated sentence for style, meaning preservation, and fluency. Experimental results on two different style transfer tasks-sentiment transfer, and formality transfer-show that our model outperforms state-of-the-art approaches.Furthermore, we perform a manual evaluation that demonstrates the effectiveness of the proposed method using subjective metrics of generated text quality.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/N19-1378",
  "title": "What do Entity-Centric Models Learn? Insights from Entity Linking in Multi-Party Dialogue",
  "year": 2019,
  "venue": "NAACL",
  "abstract": "Humans use language to refer to entities in the external world. Motivated by this, in recent years several models that incorporate a bias towards learning entity representations have been proposed. Such entity-centric models have shown empirical success, but we still know little about why. In this paper we analyze the behavior of two recently proposed entity-centric models in a referential task, Entity Linking in Multi-party Dialogue (SemEval 2018 Task 4). We show that these models outperform the state of the art on this task, and that they do better on lower frequency entities than a counterpart model that is not entity-centric, with the same model size. We argue that making models entity-centric naturally fosters good architectural decisions. However, we also show that these models do not really build entity representations and that they make poor use of linguistic context. These negative results underscore the need for model analysis, to test whether the motivations for particular architectures are borne out in how models behave when deployed.",
  "stance": -0.5
 },
 {
  "url": "https://aclanthology.org/N19-1396",
  "title": "Understanding the Behaviour of Neural Abstractive Summarizers using Contrastive Examples",
  "year": 2019,
  "venue": "NAACL",
  "abstract": "Neural abstractive summarizers generate summary texts using a language model conditioned on the input source text, and have recently achieved high ROUGE scores on benchmark summarization datasets. We investigate how they achieve this performance with respect to human-written gold-standard abstracts, and whether the systems are able to understand deeper syntactic and semantic structures. We generate a set of contrastive summaries which are perturbed, deficient versions of human-written summaries, and test whether existing neural summarizers score them more highly than the human-written summaries. We analyze their performance on different datasets and find that these systems fail to understand the source text, in a majority of the cases.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/N19-1422",
  "title": "Probing the Need for Visual Context in Multimodal Machine Translation",
  "year": 2019,
  "venue": "NAACL",
  "abstract": "Current work on multimodal machine translation (MMT) has suggested that the visual modality is either unnecessary or only marginally beneficial. We posit that this is a consequence of the very simple, short and repetitive sentences used in the only available dataset for the task (Multi30K), rendering the source text sufficient as context. In the general case, however, we believe that it is possible to combine visual and textual information in order to ground translations. In this paper we probe the contribution of the visual modality to state-of-the-art MMT models by conducting a systematic analysis where we partially deprive the models from source-side textual context. Our results show that under limited textual context, models are capable of leveraging the visual input to generate better translations. This contradicts the current belief that MMT models disregard the visual modality because of either the quality of the image features or the way they are integrated into the model.",
  "stance": -1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1990/file/41ae36ecb9b3eee609d05b90c14222fb-Paper.pdf",
  "title": "A Short-Term Memory Architecture for the Learning of Morphophonemic Rules",
  "year": 1990,
  "venue": "NeurIPS",
  "abstract": "Despite its successes, Rumelhart and McClelland's (1986) well-known approach to the learning of morphophonemic rules suffers from two deficiencies: (1) It performs the artificial task of associating forms with forms rather than perception or production. (2) It is not constrained in ways that humans learners are. This paper describes a model which addresses both objections. Using a simple recurrent architecture which takes both forms and \"meanings\" as inputs, the model learns to generate verbs in one or another \"tense\", given arbitrary meanings, and to recognize the tenses of verbs. Furthermore, it fails to learn reversal processes unknown in human language.",
  "stance": -0.2
 },
 {
  "url": "https://papers.neurips.cc/paper/1990/file/5a4b25aaed25c2ee1b74de72dc03c14e-Paper.pdf",
  "title": "Designing Linear Threshold Based Neural Network Pattern Classifiers",
  "year": 1990,
  "venue": "NeurIPS",
  "abstract": "The three problems that concern us are identifying a natural domain of pattern classification applications of feedforward neural networks, selecting an appropriate feedforward network architecture, and assessing the tradeoff between network complexity, training set size, and statistical reliability as measured by the probability of incorrect classification. We close with some suggestions, for improving the bounds that come from VapnikChervonenkis theory, that can narrow, but not close, the chasm between theory and practice.",
  "stance": 0.2
 },
 {
  "url": "https://papers.neurips.cc/paper/1990/file/9de6d14fff9806d4bcd1ef555be766cd-Paper.pdf",
  "title": "Simple Spin Models for the Development of Ocular Dominance Columns and Iso-Orientation Patches",
  "year": 1990,
  "venue": "NeurIPS",
  "abstract": "Simple classical spin models well-known to physicists as the ANNNI and Heisenberg XY Models. in which long-range interactions occur in a pattern given by the Mexican Hat operator. can generate many of the structural properties characteristic of the ocular dominance columns and iso-orientation patches seen in cat and primate visual cortex.",
  "stance": 0.1
 },
 {
  "url": "https://papers.neurips.cc/paper/1990/file/c058f544c737782deacefa532d9add4c-Paper.pdf",
  "title": "Connectionist Approaches to the Use of Markov Models for Speech Recognition",
  "year": 1990,
  "venue": "NeurIPS",
  "abstract": "Previous work has shown the ability of Multilayer Perceptrons (MLPs) to estimate emission probabilities for Hidden Markov Models (HMMs). The advantages of a speech recognition system incorporating both MLPs and HMMs are the best discrimination and the ability to incorporate multiple sources of evidence (features, temporal context) without restrictive assumptions of distributions or statistical independence. This paper presents results on the speaker-dependent portion of DARPA's English language Resource Management database. Results support the previously reported utility of MLP probability estimation for continuous speech recognition. An additional approach we are pursuing is to use MLPs as nonlinear predictors for autoregressive HMMs. While this is shown to be more compatible with the HMM formalism, it still suffers from several limitations. This approach is generalized to take account of time correlation between successive observations, without any restrictive assumptions about the driving noise.",
  "stance": -0.2
 },
 {
  "url": "https://papers.neurips.cc/paper/1990/file/eed5af6add95a9a6f1252739b1ad8c24-Paper.pdf",
  "title": "The Devil and the Network: What Sparsity Implies to Robustness and Memory",
  "year": 1990,
  "venue": "NeurIPS",
  "abstract": "Robustness is a commonly bruited property of neural networks; in particular, a folk theorem in neural computation asserts that neural networks-in contexts with large interconnectivity-continue to function efficiently, albeit with some degradation, in the presence of component damage or loss. A second folk theorem in such contexts asserts that dense interconnectivity between neural elements is a sine qua non for the efficient usage of resources. These premises are formally examined in this communication in a setting that invokes the notion of the \"devil\" 1 in the network as an agent that produces sparsity by snipping connections.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1991/file/55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf",
  "title": "Fast Learning with Predictive Forward Models",
  "year": 1991,
  "venue": "NeurIPS",
  "abstract": "A method for transforming performance evaluation signals distal both in space and time into proximal signals usable by supervised learning algorithms, presented in [Jordan & Jacobs 90], is examined. A simple observation concerning differentiation through models trained with redundant inputs (as one of their networks is) explains a weakness in the original architecture and suggests a modification: an internal world model that encodes action-space exploration and, crucially, cancels input redundancy to the forward model is added. Learning time on an example task, cartpole balancing, is thereby reduced about 50 to 100 times.",
  "stance": 0.6
 },
 {
  "url": "https://papers.neurips.cc/paper/1991/file/8d34201a5b85900908db6cae92723617-Paper.pdf",
  "title": "Benchmarking Feed-Forward Neural Networks: Models and Measures",
  "year": 1991,
  "venue": "NeurIPS",
  "abstract": "Existing metrics for the learning performance of feed-forward neural networks do not provide a satisfactory basis for comparison because the choice of the training epoch limit can determine the results of the comparison. I propose new metrics which have the desirable property of being independent of the training epoch limit. The efficiency measures the yield of correct networks in proportion to the training effort expended. The optimal epoch limit provides the greatest efficiency. The learning performance is modelled statistically, and asymptotic performance is estimated. Implementation details may be found in (Harney, 1992).",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1992/file/48ab2f9b45957ab574cf005eb8a76760-Paper.pdf",
  "title": "Analogy-- Watershed or Waterloo? Structural alignment and the development of connectionist models of analogy",
  "year": 1992,
  "venue": "NeurIPS",
  "abstract": "Neural network models have been criticized for their inability to make use of compositional representations. In this paper, we describe a series of psychological phenomena that demonstrate the role of structured representations in cognition. These findings suggest that people compare relational representations via a process of structural alignment. This process will have to be captured by any model of cognition, symbolic or subsymbolic.",
  "stance": 0.3
 },
 {
  "url": "https://papers.neurips.cc/paper/1992/file/9cf81d8026a9018052c429cc4e56739b-Paper.pdf",
  "title": "Metamorphosis Networks: An Alternative to Constructive Models",
  "year": 1992,
  "venue": "NeurIPS",
  "abstract": "Given a set oft raining examples, determining the appropriate number of free parameters is a challenging problem. Constructive learning algorithms attempt to solve this problem automatically by adding hidden units, and therefore free parameters, during learning. We explore an alternative class of algorithms-called metamorphosis algorithms-in which the number of units is fixed, but the number of free parameters gradually increases during learning. The architecture we investigate is composed of RBF units on a lattice, which imposes flexible constraints on the parameters of the network. Virtues of this approach include variable subset selection, robust parameter selection, multiresolution processing, and interpolation of sparse training data.",
  "stance": 0.3
 },
 {
  "url": "https://papers.neurips.cc/paper/1992/file/a532400ed62e772b9dc0b86f46e583ff-Paper.pdf",
  "title": "Weight Space Probability Densities in Stochastic Learning: II. Transients and Basin Hopping Times",
  "year": 1992,
  "venue": "NeurIPS",
  "abstract": "In stochastic learning, weights are random variables whose time evolution is governed by a Markov process. At each time-step, n, the weights can be described by a probability density function P(w, n). We summarize the theory of the time evolution of P, and give graphical examples of the time evolution that contrast the behavior of stochastic learning with true gradient descent (batch learning). Finally, we use the formalism to obtain predictions of the time required for noise-induced hopping between basins of different optima. We compare the theoretical predictions with simulations of large ensembles of networks for simple problems in supervised and unsupervised learning.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1992/file/fae0b27c451c728867a567e8c1bb4e53-Paper.pdf",
  "title": "Transient Signal Detection with Neural Networks: The Search for the Desired Signal",
  "year": 1992,
  "venue": "NeurIPS",
  "abstract": "Matched filtering has been one of the most powerful techniques employed for transient detection. Here we will show that a dynamic neural network outperforms the conventional approach. When the artificial neural network (ANN) is trained with supervised learning schemes there is a need to supply the desired signal for all time, although we are only interested in detecting the transient. In this paper we also show the effects on the detection agreement of different strategies to construct the desired signal. The extension of the Bayes decision rule (011 desired signal), optimal in static classification, performs worse than desired signals constructed by random noise or prediction during the background.",
  "stance": -1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1993/file/1efa39bcaec6f3900149160693694536-Paper.pdf",
  "title": "How to Describe Neuronal Activity: Spikes, Rates, or Assemblies?",
  "year": 1993,
  "venue": "NeurIPS",
  "abstract": "What is the 'correct' theoretical description of neuronal activity? The analysis of the dynamics of a globally connected network of spiking neurons (the Spike Response Model) shows that a description by mean firing rates is possible only if active neurons fire incoherently. If firing occurs coherently or with spatio-temporal correlations, the spike structure of the neural code becomes relevant. Alternatively, neurons can be gathered into local or distributed ensembles or 'assemblies'. A description based on the mean ensemble activity is, in principle, possible but the interaction between different assemblies becomes highly nonlinear. A description with spikes should therefore be preferred.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1993/file/22ac3c5a5bf0b520d281c122d1490650-Paper.pdf",
  "title": "Emergence of Global Structure from Local Associations",
  "year": 1993,
  "venue": "NeurIPS",
  "abstract": "A variant of the encoder architecture, where units at the input and output layers represent nodes on a graph. is applied to the task of mapping locations to sets of neighboring locations. The degree to which the resuIting internal (i.e. hidden unit) representations reflect global properties of the environment depends upon several parameters of the learning procedure. Architectural bottlenecks. noise. and incremental learning of landmarks are shown to be important factors in maintaining topographic relationships at a global scale.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1993/file/22fb0cee7e1f3bde58293de743871417-Paper.pdf",
  "title": "Robot Learning: Exploration and Continuous Domains",
  "year": 1993,
  "venue": "NeurIPS",
  "abstract": "The goal of this workshop was to discuss two major issues: efficient exploration of a learner's state space, and learning in continuous domains. The common themes that emerged in presentations and in discussion were the importance of choosing one's domain assumptions carefully, mixing controllers/strategies, avoidance of catastrophic failure, new approaches with difficulties with reinforcement learning, and the importance of task transfer.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1993/file/d4c2e4a3297fe25a71d030b67eb83bfc-Paper.pdf",
  "title": "Bayesian Backpropagation Over I-O Functions Rather Than Weights",
  "year": 1993,
  "venue": "NeurIPS",
  "abstract": "The conventional Bayesian justification of backprop is that it finds the MAP weight vector. As this paper shows, to find the MAP i-o function instead one must add a correction tenn to backprop. That tenn biases one towards i-o functions with small description lengths, and in particular favors (some kinds of) feature-selection, pruning, and weight-sharing.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1994/file/1e48c4420b7073bc11916c6c1de226bb-Paper.pdf",
  "title": "Interference in Learning Internal Models of Inverse Dynamics in Humans",
  "year": 1994,
  "venue": "NeurIPS",
  "abstract": "Experiments were performed to reveal some of the computational properties of the human motor memory system. We show that as humans practice reaching movements while interacting with a novel mechanical environment, they learn an internal model of the inverse dynamics of that environment. Subjects show recall of this model at testing sessions 24 hours after the initial practice. The representation of the internal model in memory is such that there is interference when there is an attempt to learn a new inverse dynamics map immediately after an anticorrelated mapping was learned. We suggest that this interference is an indication that the same computational elements used to encode the first inverse dynamics map are being used to learn the second mapping. We predict that this leads to a forgetting of the initially learned skill.",
  "stance": -0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/1994/file/b56a18e0eacdf51aa2a5306b0f533204-Paper.pdf",
  "title": "A Study of Parallel Perturbative Gradient Descent",
  "year": 1994,
  "venue": "NeurIPS",
  "abstract": "We have continued our study of a parallel perturbative learning method [Alspector et al., 1993] and implications for its implementation in analog VLSI. Our new results indicate that, in most cases, a single parallel perturbation (per pattern presentation) of the function parameters (weights in a neural network) is theoretically the best course. This is not true, however, for certain problems and may not generally be true when faced with issues of implementation such as limited precision. In these cases, multiple parallel perturbations may be best as indicated in our previous results.",
  "stance": -1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1994/file/e6cb2a3c14431b55aa50c06529eaa21b-Paper.pdf",
  "title": "Hyperparameters Evidence and Generalisation for an Unrealisable Rule",
  "year": 1994,
  "venue": "NeurIPS",
  "abstract": "Using a statistical mechanical formalism we calculate the evidence, generalisation error and consistency measure for a linear perceptron trained and tested on a set of examples generated by a non linear teacher. The teacher is said to be unrealisable because the student can never model it without error. Our model allows us to interpolate between the known case of a linear teacher, and an unrealisable, nonlinear teacher. A comparison of the hyperparameters which maximise the evidence with those that optimise the performance measures reveals that, in the non-linear case, the evidence procedure is a misleading guide to optimising performance. Finally, we explore the extent to which the evidence procedure is unreliable and find that, despite being sub-optimal, in some circumstances it might be a useful method for fixing the hyperparameters.",
  "stance": 0.3
 },
 {
  "url": "https://papers.neurips.cc/paper/1996/file/459a4ddcb586f24efd9395aa7662bc7c-Paper.pdf",
  "title": "Why did TD-Gammon Work?",
  "year": 1996,
  "venue": "NeurIPS",
  "abstract": "Although TD-Gammon is one of the major successes in machine learning, it has not led to similar impressive breakthroughs in temporal difference learning for other applications or even other games. We were able to replicate some of the success of TD-Gammon, developing a competitive evaluation function on a 4000 parameter feed-forward neural network, without using back-propagation, reinforcement or temporal difference learning methods. Instead we apply simple hill-climbing in a relative fitness environment. These results and further analysis suggest that the surprising success of Tesauro's program had more to do with the co-evolutionary structure of the learning task and the dynamics of the backgammon game itself.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1996/file/6c14da109e294d1e8155be8aa4b1ce8e-Paper.pdf",
  "title": "Promoting Poor Features to Supervisors: Some Inputs Work Better as Outputs",
  "year": 1996,
  "venue": "NeurIPS",
  "abstract": "In supervised learning there is usually a clear distinction between inputs and outputs inputs are what you will measure, outputs are what you will predict from those measurements. This paper shows that the distinction between inputs and outputs is not this simple. Some features are more useful as extra outputs than as inputs. By using a feature as an output we get more than just the case values but can. learn a mapping from the other inputs to that feature. For many features this mapping may be more useful than the feature value itself. We present two regression problems and one classification problem where performance improves if features that could have been used as inputs are used as extra outputs instead. This result is surprising since a feature used as an output is not used during testing.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1996/file/dc87c13749315c7217cdc4ac692e704c-Paper.pdf",
  "title": "Limitations of Self-organizing Maps for Vector Quantization and Multidimensional Scaling",
  "year": 1996,
  "venue": "NeurIPS",
  "abstract": "The limitations of using self-organizing maps (SaM) for either clustering/vector quantization (VQ) or multidimensional scaling (MDS) are being discussed by reviewing recent empirical findings and the relevant theory. SaM 's remaining ability of doing both VQ and MDS at the same time is challenged by a new combined technique of online K-means clustering plus Sammon mapping of the cluster centroids. SaM are shown to perform significantly worse in terms of quantization error , in recovering the structure of the clusters and in preserving the topology in a comprehensive empirical study using a series of multivariate normal clustering problems.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1996/file/f47330643ae134ca204bf6b2481fec47-Paper.pdf",
  "title": "Balancing Between Bagging and Bumping",
  "year": 1996,
  "venue": "NeurIPS",
  "abstract": "We compare different methods to combine predictions from neural networks trained on different bootstrap samples of a regression problem. One of these methods, introduced in [6] and which we here call balancing, is based on the analysis of the ensemble generalization error into an ambiguity term and a term incorporating generalization performances of individual networks. We show how to estimate these individual errors from the residuals on validation patterns. Weighting factors for the different networks follow from a quadratic programming problem. On a real-world problem concerning the prediction of sales figures and on the well-known Boston housing data set, balancing clearly outperforms other recently proposed alternatives as bagging [1] and bumping [8].",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1998/file/efb76cff97aaf057654ef2f38cd77d73-Paper.pdf",
  "title": "Utilizing lime: Asynchronous Binding",
  "year": 1998,
  "venue": "NeurIPS",
  "abstract": "Historically, connectionist systems have not excelled at representing and manipulating complex structures. How can a system composed of simple neuron-like computing elements encode complex relations? Recently, researchers have begun to appreciate that representations can extend in both time and space. Many researchers have proposed that the synchronous firing of units can encode complex representations. I identify the limitations of this approach and present an asynchronous model of binding that effectively represents complex structures. The asynchronous model extends the synchronous approach. I argue that our cognitive architecture utilizes a similar mechanism.",
  "stance": 0.2
 },
 {
  "url": "https://papers.neurips.cc/paper/1999/file/86d7c8a08b4aaa1bc7c599473f5dddda-Paper.pdf",
  "title": "Rules and Similarity in Concept Learning",
  "year": 1999,
  "venue": "NeurIPS",
  "abstract": "This paper argues that two apparently distinct modes of generalizing concepts abstracting rules and computing similarity to exemplars should both be seen as special cases of a more general Bayesian learning framework. Bayes explains the specific workings of these two modes which rules are abstracted, how similarity is measured as well as why generalization should appear ruleor similarity-based in different situations. This analysis also suggests why the rules/similarity distinction, even if not computationally fundamental, may still be useful at the algorithmic level as part of a principled approximation to fully Bayesian learning.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1999/file/e9b73bccd1762555582b513ff9d02492-Paper.pdf",
  "title": "Acquisition in Autoshaping",
  "year": 1999,
  "venue": "NeurIPS",
  "abstract": "Quantitative data on the speed with which animals acquire behavioral responses during classical conditioning experiments should provide strong constraints on models of learning. However, most models have simply ignored these data; the few that have attempted to address them have failed by at least an order of magnitude. We discuss key data on the speed of acquisition, and show how to account for them using a statistically sound model of learning, in which differential reliabilities of stimuli playa crucial role.",
  "stance": 0.2
 },
 {
  "url": "https://papers.neurips.cc/paper/2000/file/04df4d434d481c5bb723be1b6df1ee65-Paper.pdf",
  "title": "Reinforcement Learning with Function Approximation Converges to a Region",
  "year": 2000,
  "venue": "NeurIPS",
  "abstract": "Many algorithms for approximate reinforcement learning are not known to converge. In fact, there are counterexamples showing that the adjustable weights in some algorithms may oscillate within a region rather than converging to a point. This paper shows that, for two popular algorithms, such oscillation is the worst that can happen: the weights cannot diverge, but instead must converge to a bounded region. The algorithms are SARSA(O) and V(O); the latter algorithm was used in the well-known TD-Gammon program.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2000/file/0950ca92a4dcf426067cfd2246bb5ff3-Paper.pdf",
  "title": "Occam's Razor",
  "year": 2000,
  "venue": "NeurIPS",
  "abstract": "The Bayesian paradigm apparently only sometimes gives rise to Occam's Razor; at other times very large models perform well. We give simple examples of both kinds of behaviour. The two views are reconciled when measuring complexity of functions, rather than of the machinery used to implement them. We analyze the complexity of functions for some linear in the parameter models that are equivalent to Gaussian Processes, and always find Occam's Razor at work.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2000/file/09b15d48a1514d8209b192a8b8f34e48-Paper.pdf",
  "title": "Exact Solutions to Time-Dependent MDPs",
  "year": 2000,
  "venue": "NeurIPS",
  "abstract": "We describe an extension of the Markov decision process model in which a continuous time dimension is included in the state space. This allows for the representation and exact solution of a wide range of problems in which transitions or rewards vary over time. We examine problems based on route planning with public transportation and telescope observation scheduling.",
  "stance": -0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2000/file/82cadb0649a3af4968404c9f6031b233-Paper.pdf",
  "title": "Stability and Noise in Biochemical Switches",
  "year": 2000,
  "venue": "NeurIPS",
  "abstract": "Many processes in biology, from the regulation of gene expression in bacteria to memory in the brain, involve switches constructed from networks of biochemical reactions. Crucial molecules are present in small numbers, raising questions about noise and stability. Analysis of noise in simple reaction schemes indicates that switches stable for years and switchable in milliseconds can be built from fewer than one hundred molecules. Prospects for direct tests of this prediction, as well as implications, are discussed.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2000/file/f0fcf351df4eb6786e9bb6fc4e2dee02-Paper.pdf",
  "title": "Whence Sparseness?",
  "year": 2000,
  "venue": "NeurIPS",
  "abstract": "It has been shown that the receptive fields of simple cells in VI can be explained by assuming optimal encoding, provided that an extra constraint of sparseness is added. This finding suggests that there is a reason, independent of optimal representation, for sparseness. However this work used an ad hoc model for the noise. Here I show that, if a biologically more plausible noise model, describing neurons as Poisson processes, is used sparseness does not have to be added as a constraint. Thus I conclude that sparseness is not a feature that evolution has striven for, but is simply the result of the evolutionary pressure towards an optimal representation.",
  "stance": -0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2001/file/2d579dc29360d8bbfbb4aa541de5afa9-Paper.pdf",
  "title": "Classifying Single Trial EEG: Towards Brain Computer Interfacing",
  "year": 2001,
  "venue": "NeurIPS",
  "abstract": "Driven by the progress in the field of single-trial analysis of EEG, there is a growing interest in brain computer interfaces (BCIs), i.e., systems that enable human subjects to control a computer only by means of their brain signals. In a pseudo-online simulation our BCI detects upcoming finger movements in a natural keyboard typing condition and predicts their laterality. This can be done on average 100–230ms before the respective key is actually pressed, i.e., long before the onset of EMG. Our approach is appealing for its short response time and high classification accuracy (>96%) in a binary decision where no human training is involved. We compare discriminative classifiers like Support Vector Machines (SVMs) and different variants of Fisher Discriminant that possess favorable regularization properties for dealing with high noise cases (inter-trial variablity).",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2001/file/7b7a53e239400a13bd6be6c91c4f6c4e-Paper.pdf",
  "title": "On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes",
  "year": 2001,
  "venue": "NeurIPS",
  "abstract": "We compare discriminative and generative learning as typified by logistic regression and naive Bayes. We show, contrary to a widelyheld belief that discriminative classifiers are almost always to be preferred, that there can often be two distinct regimes of performance as the training set size is increased, one in which each algorithm does better. This stems from the observationwhich is borne out in repeated experimentsthat while discriminative learning has lower asymptotic error, a generative classifier may also approach its (higher) asymptotic error much faster.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2001/file/a96d3afec184766bfeca7a9f989fc7e7-Paper.pdf",
  "title": "Associative memory in realistic neuronal networks",
  "year": 2001,
  "venue": "NeurIPS",
  "abstract": "Almost two decades ago, Hopfield [1] showed that networks of highly reduced model neurons can exhibit multiple attracting fixed points, thus providing a substrate for associative memory. It is still not clear, however, whether realistic neuronal networks can support multiple attractors. The main difficulty is that neuronal networks in vivo exhibit a stable background state at low firing rate, typically a few Hz. Embedding attractor is easy; doing so without destabilizing the background is not. Previous work [2 , 3] focused on the sparse coding limit, in which a vanishingly small number of neurons are involved in any memory. Here we investigate the case in which the number of neurons involved in a memory scales with the number of neurons in the network. In contrast to the sparse coding limit, we find that multiple attractors can co-exist robustly with a stable background state. Mean field theory is used to understand how the behavior of the network scales with its parameters, and simulations with analog neurons are presented.",
  "stance": 0.1
 },
 {
  "url": "https://papers.neurips.cc/paper/2001/file/d46e1fcf4c07ce4a69ee07e4134bcef1-Paper.pdf",
  "title": "Entropy and Inference, Revisited",
  "year": 2001,
  "venue": "NeurIPS",
  "abstract": "We study properties of popular near–uniform (Dirichlet) priors for learning undersampled probability distributions on discrete nonmetric spaces and show that they lead to disastrous results. However, an Occam–style phase space argument expands the priors into their infinite mixture and resolves most of the observed problems. This leads to a surprisingly good estimator of entropies of discrete distributions.",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2002/file/0233f3bb964cf325a30f8b1c2ed2da93-Paper.pdf",
  "title": "\"Name That Song!\" A Probabilistic Approach to Querying on Music and Text",
  "year": 2002,
  "venue": "NeurIPS",
  "abstract": "We present a novel, flexible statistical approach for modelling music and text jointly. The approach is based on multi-modal mixture models and maximum a posteriori estimation using EM. The learned models can be used to browse databases with documents containing music and text, to search for music using queries consisting of music and text (lyrics and other contextual information), to annotate text documents with music, and to automatically recommend or identify similar songs.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2002/file/04ad5632029cbfbed8e136e5f6f7ddfa-Paper.pdf",
  "title": "How the Poverty of the Stimulus Solves the Poverty of the Stimulus",
  "year": 2002,
  "venue": "NeurIPS",
  "abstract": "Language acquisition is a special kind of learning problem because the outcome of learning of one generation is the input for the next. That makes it possible for languages to adapt to the particularities of the learner. In this paper, I show that this type of language change has important consequences for models of the evolution and acquisition of syntax.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2002/file/43e4e6a6f341e00671e123714de019a8-Paper.pdf",
  "title": "An Impossibility Theorem for Clustering",
  "year": 2002,
  "venue": "NeurIPS",
  "abstract": "Although the study of clustering is centered around an intuitively compelling goal, it has been very difficult to develop a unified framework for reasoning about it at a technical level, and profoundly diverse approaches to clustering abound in the research community. Here we suggest a formal perspective on the difficulty in finding such a unification, in the form of an impossibility theorem: for a set of three simple properties, we show that there is no clustering function satisfying all three. Relaxations of these properties expose some of the interesting (and unavoidable) trade-offs at work in well-studied clustering techniques such as single-linkage, sum-of-pairs, k-means, and k-median.",
  "stance": -0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2002/file/635440afdfc39fe37995fed127d7df4f-Paper.pdf",
  "title": "Categorization Under Complexity: A Unified MDL Account of Human Learning of Regular and Irregular Categories",
  "year": 2002,
  "venue": "NeurIPS",
  "abstract": "We present an account of human concept learning-that is, learning of categories from examples-based on the principle of minimum description length (MDL). In support of this theory, we tested a wide range of two-dimensional concept types, including both regular (simple) and highly irregular (complex) structures, and found the MDL theory to give a good account of subjects' performance. This suggests that the intrinsic complexity of a concept (that is, its description -length) systematically influences its leamability. 1The Structure of Categories A number of different principles have been advanced to explain the manner in which humans learn to categorize objects. It has been variously suggested that the underlying principle might be the similarity structure of objects [1], the manipulability of decision bound~ aries [2], or Bayesian inference [3][4]. While many of these theories are mathematically well-grounded and have been successful in explaining a range of experimental findings, they have commonly only been tested on a narrow collection of concept types similar to the simple unimodal categories of Figure 1(a-e).",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2002/file/6e5025ccc7d638ae4e724da8938450a6-Paper.pdf",
  "title": "One-Class LP Classifiers for Dissimilarity Representations",
  "year": 2002,
  "venue": "NeurIPS",
  "abstract": "Problems in which abnormal or novel situations should be detected can be approached by describing the domain of the class of typical examples. These applications come from the areas of machine diagnostics, fault detection, illness identification or, in principle, refer to any problem where little knowledge is available outside the typical class. In this paper we explain why proximities are natural representations for domain descriptors and we propose a simple one-class classifier for dissimilarity representations. By the use of linear programming an efficient one-class description can be found, based on a small number of prototype objects. This classifier can be made (1) more robust by transforming the dissimilarities and (2) cheaper to compute by using a reduced representation set. Finally, a comparison to a comparable one-class classifier by Campbell and Bennett is given.",
  "stance": 0.7
 },
 {
  "url": "https://papers.neurips.cc/paper/2002/file/716e1b8c6cd17b771da77391355749f3-Paper.pdf",
  "title": "Modeling Midazolam's Effect on the Hippocampus and Recognition Memory",
  "year": 2002,
  "venue": "NeurIPS",
  "abstract": "The benz.odiaze:pine '~1idazolam causes dense,but temporary ~ anterograde amnesia, similar to that produced byhippocampal damage~Does the action of M'idazola:m on the hippocanlpus cause less storage, or less accurate storage, .of information in episodic. long-term menlory?\\rVe used a sinlple variant of theREJv1. JD.odel [18] to fit data collected. by IIirsbnla.n~Fisher, .IIenthorn,Arndt} and Passa.nnante [9] on the effects of Midazola.m, study time~ and normative \\vQrd... frequenc:y on both yes-no and remember-k.novv recognition m.emory. That a: simple strength. 'model fit well \\\\tas cont.rary to the expectations of 'flirshman et aLMore important,within the Bayesian based R.EM modeling frame\\vork, the data were consistentw'ith the view that Midazolam causes less accurate storage~ rather than less storage, of infornlation in episodic mcm.ory..",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2002/file/806fec5af7f5b48b8a31a003e171f3fb-Paper.pdf",
  "title": "Dynamical Constraints on Computing with Spike Timing in the Cortex",
  "year": 2002,
  "venue": "NeurIPS",
  "abstract": "If the cortex uses spike timing to compute, the timing of the spikes must be robust to perturbations. Based on a recent framework that provides a simple criterion to determine whether a spike sequence produced by a generic network is sensitive to initial conditions, and numerical simulations of a variety of network architectures, we argue within the limits set by our model of the neuron, that it is unlikely that precise sequences of spike timings are used for computation under conditions typically found in the cortex.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2002/file/8c5f6ecd29a0eb234459190ca51c16dd-Paper.pdf",
  "title": "A Minimal Intervention Principle for Coordinated Movement",
  "year": 2002,
  "venue": "NeurIPS",
  "abstract": "Behavioral goals are achieved reliably and repeatedly with movements rarely reproducible in their detail. Here we offer an explanation: we show that not only are variability and goal achievement compatible, but indeed that allowing variability in redundant dimensions is the optimal control strategy in the face of uncertainty. The optimal feedback control laws for typical motor tasks obey a “minimal intervention” principle: deviations from the average trajectory are only corrected when they interfere with the task goals. The resulting behavior exhibits task-constrained variability, as well as synergetic coupling among actuators—which is another unexplained empirical phenomenon.",
  "stance": -0.2
 },
 {
  "url": "https://papers.neurips.cc/paper/2002/file/ce6c92303f38d297e263c7180f03d402-Paper.pdf",
  "title": "Automatic Acquisition and Efficient Representation of Syntactic Structures",
  "year": 2002,
  "venue": "NeurIPS",
  "abstract": "The distributional principle according to which morphemes that occur in identical contexts belong, in some sense, to the same category [1] has been advanced as a means for extracting syntactic structures from corpus data. We extend this principle by applying it recursively, and by using mutual information for estimating category coherence. The resulting model learns, in an unsupervised fashion, highly structured, distributed representations of syntactic knowledge from corpora. It also exhibits promising behavior in tasks usually thought to require representations anchored in a grammar, such as systematicity.",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2002/file/d2a27e83d429f0dcae6b937cf440aeb1-Paper.pdf",
  "title": "Stable Fixed Points of Loopy Belief Propagation Are Local Minima of the Bethe Free Energy",
  "year": 2002,
  "venue": "NeurIPS",
  "abstract": "We extend recent work on the connection between loopy belief propagation and the Bethe free energy. Constrained minimization of the Bethe free energy can be turned into an unconstrained saddle-point problem. Both converging double-loop algorithms and standard loopy belief propagation can be interpreted as attempts to solve this saddle-point problem. Stability analysis then leads us to conclude that stable fixed points of loopy belief propagation must be (local) minima of the Bethe free energy. Perhaps surprisingly, the converse need not be the case: minima can be unstable fixed points. We illustrate this with an example and discuss implications.",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2002/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf",
  "title": "Replay, Repair and Consolidation",
  "year": 2002,
  "venue": "NeurIPS",
  "abstract": "A standard view of memory consolidation is that episodes are stored temporarily in the hippocampus, and are transferred to the neocortex through replay. Various recent experimental challenges to the idea of transfer, particularly for human memory, are forcing its re-evaluation. However, although there is independent neurophysiological evidence for replay, short of transfer, there are few theoretical ideas for what it might be doing. We suggest and demonstrate two important computational roles associated with neocortical indices.",
  "stance": 0.4
 },
 {
  "url": "https://papers.neurips.cc/paper/2002/file/e0f7a4d0ef9b84b83b693bbf3feb8e6e-Paper.pdf",
  "title": "Circuit Model of Short-Term Synaptic Dynamics",
  "year": 2002,
  "venue": "NeurIPS",
  "abstract": "We describe a model of short-term synaptic depression that is derived from a silicon circuit implementation. The dynamics of this circuit model are similar to the dynamics of some present theoretical models of shortterm depression except that the recovery dynamics of the variable describing the depression is nonlinear and it also depends on the presynaptic frequency. The equations describing the steady-state and transient responses of this synaptic model fit the experimental results obtained from a fabricated silicon network consisting of leaky integrate-and-fire neurons and different types of synapses. We also show experimental data demonstrating the possible computational roles of depression. One possible role of a depressing synapse is that the input can quickly bring the neuron up to threshold when the membrane potential is close to the resting potential.",
  "stance": 0.4
 },
 {
  "url": "https://papers.neurips.cc/paper/2003/file/063e26c670d07bb7c4d30e6fc69fe056-Paper.pdf",
  "title": "Information Dynamics and Emergent Computation in Recurrent Circuits of Spiking Neurons",
  "year": 2003,
  "venue": "NeurIPS",
  "abstract": "We employ an efficient method using Bayesian and linear classifiers for analyzing the dynamics of information in high-dimensional states of generic cortical microcircuit models. It is shown that such recurrent circuits of spiking neurons have an inherent capability to carry out rapid computations on complex spike patterns, merging information contained in the order of spike arrival with previously acquired context information.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2003/file/0bf727e907c5fc9d5356f11e4c45d613-Paper.pdf",
  "title": "A Model for Learning the Semantics of Pictures",
  "year": 2003,
  "venue": "NeurIPS",
  "abstract": "We propose an approach to learning the semantics of images which allows us to automatically annotate an image with keywords and to retrieve images based on text queries. We do this using a formalism that models the generation of annotated images. We assume that every image is divided into regions, each described by a continuous-valued feature vector. Given a training set of images with annotations, we compute a joint probabilistic model of image features and words which allow us to predict the probability of generating a word given the image regions. This may be used to automatically annotate and retrieve images given a word as a query. Experiments show that our model significantly outperforms the best of the previously reported results on the tasks of automatic image annotation and retrieval.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2003/file/1f34004ebcb05f9acda6016d5cc52d5e-Paper.pdf",
  "title": "Human and Ideal Observers for Detecting Image Curves",
  "year": 2003,
  "venue": "NeurIPS",
  "abstract": "This paper compares the ability of human observers to detect target image curves with that of an ideal observer. The target curves are sampled from a generative model which specifies (probabilistically) the geometry and local intensity properties of the curve. The ideal observer performs Bayesian inference on the generative model using MAP estimation. Varying the probability model for the curve geometry enables us investigate whether human performance is best for target curves that obey specific shape statistics, in particular those observed on natural shapes. Experiments are performed with data on both rectangular and hexagonal lattices. Our results show that human observers’ performance approaches that of the ideal observer and are, in general, closest to the ideal for conditions where the target curve tends to be straight or similar to natural statistics on curves. This suggests a bias of human observers towards straight curves and natural statistics.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2003/file/31c97cbb941d3e92d0e6f9925e9bc4d7-Paper.pdf",
  "title": "Sensory Modality Segregation",
  "year": 2003,
  "venue": "NeurIPS",
  "abstract": "Why are sensory modalities segregated the way they are? In this paper we show that sensory modalities are well designed for self-supervised cross-modal learning. Using the Minimizing-Disagreement algorithm on an unsupervised speech categorization task with visual (moving lips) and auditory (sound signal) inputs, we show that very informative auditory dimensions actually harm performance when moved to the visual side of the network. It is better to throw them away than to consider them part of the “visual input”. We explain this finding in terms of the statistical structure in sensory inputs.",
  "stance": 0.2
 },
 {
  "url": "https://papers.neurips.cc/paper/2003/file/8bdb5058376143fa358981954e7626b8-Paper.pdf",
  "title": "Ambiguous Model Learning Made Unambiguous with 1/f Priors",
  "year": 2003,
  "venue": "NeurIPS",
  "abstract": "What happens to the optimal interpretation of noisy data when there exists more than one equally plausible interpretation of the data? In a Bayesian model-learning framework the answer depends on the prior expectations of the dynamics of the model parameter that is to be inferred from the data. Local time constraints on the priors are insufficient to pick one interpretation over another. On the other hand, nonlocal time constraints, induced by a 1/f noise spectrum of the priors, is shown to permit learning of a specific model parameter even when there are infinitely many equally plausible interpretations of the data. This transition is inferred by a remarkable mapping of the model estimation problem to a dissipative physical system, allowing the use of powerful statistical mechanical methods to uncover the transition from indeterminate to determinate model learning.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2003/file/e82c4b19b8151ddc25d4d93baf7b908f-Paper.pdf",
  "title": "No Unbiased Estimator of the Variance of K-Fold Cross-Validation",
  "year": 2003,
  "venue": "NeurIPS",
  "abstract": "Most machine learning researchers perform quantitative experiments to estimate generalization error and compare algorithm performances. In order to draw statistically convincing conclusions, it is important to estimate the uncertainty of such estimates. This paper studies the estimation of uncertainty around the K-fold cross-validation estimator. The main theorem shows that there exists no universal unbiased estimator of the variance of K-fold cross-validation. An analysis based on the eigendecomposition of the covariance matrix of errors helps to better understand the nature of the problem and shows that naive estimators may grossly underestimate variance, as con£rmed by numerical experiments.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2003/file/f8b932c70d0b2e6bf071729a4fa68dfc-Paper.pdf",
  "title": "Learning Curves for Stochastic Gradient Descent in Linear Feedforward Networks",
  "year": 2003,
  "venue": "NeurIPS",
  "abstract": "Gradient-following learning methods can encounter problems of implementation in many applications, and stochastic variants are frequently used to overcome these difficulties. We derive quantitative learning curves for three online training methods used with a linear perceptron: direct gradient descent, node perturbation, and weight perturbation. The maximum learning rate for the stochastic methods scales inversely with the first power of the dimensionality of the noise injected into the system; with sufficiently small learning rate, all three methods give identical learning curves. These results suggest guidelines for when these stochastic methods will be limited in their utility, and considerations for architectures in which they will be effective.",
  "stance": 0.2
 },
 {
  "url": "https://papers.neurips.cc/paper/2004/file/1680e9fa7b4dd5d62ece800239bb53bd-Paper.pdf",
  "title": "Outlier Detection with One-class Kernel Fisher Discriminants",
  "year": 2004,
  "venue": "NeurIPS",
  "abstract": "The problem of detecting “atypical objects” or “outliers” is one of the classical topics in (robust) statistics. Recently, it has been proposed to address this problem by means of one-class SVM classifiers. The main conceptual shortcoming of most one-class approaches, however, is that in a strict sense they are unable to detect outliers, since the expected fraction of outliers has to be specified in advance. The method presented in this paper overcomes this problem by relating kernelized one-class classification to Gaussian density estimation in the induced feature space. Having established this relation, it is possible to identify “atypical objects” by quantifying their deviations from the Gaussian model. For RBF kernels it is shown that the Gaussian model is “rich enough” in the sense that it asymptotically provides an unbiased estimator for the true density. In order to overcome the inherent model selection problem, a cross-validated likelihood criterion for selecting all free model parameters is applied.",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2004/file/36e729ec173b94133d8fa552e4029f8b-Paper.pdf",
  "title": "Hierarchical Clustering of a Mixture Model",
  "year": 2004,
  "venue": "NeurIPS",
  "abstract": "In this paper we propose an efficient algorithm for reducing a large mixture of Gaussians into a smaller mixture while still preserving the component structure of the original model; this is achieved by clustering (grouping) the components. The method minimizes a new, easily computed distance measure between two Gaussian mixtures that can be motivated from a suitable stochastic model and the iterations of the algorithm use only the model parameters, avoiding the need for explicit resampling of datapoints. We demonstrate the method by performing hierarchical clustering of scenery images and handwritten digits.",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2004/file/921c2dc40d0b979c2910298d2f880152-Paper.pdf",
  "title": "An Auditory Paradigm for Brain-Computer Interfaces",
  "year": 2004,
  "venue": "NeurIPS",
  "abstract": "Motivated by the particular problems involved in communicating with “locked-in” paralysed patients, we aim to develop a braincomputer interface that uses auditory stimuli. We describe a paradigm that allows a user to make a binary decision by focusing attention on one of two concurrent auditory stimulus sequences. Using Support Vector Machine classification and Recursive Channel Elimination on the independent components of averaged eventrelated potentials, we show that an untrained user’s EEG data can be classified with an encouragingly high level of accuracy. This suggests that it is possible for users to modulate EEG signals in a single trial by the conscious direction of attention, well enough to be useful in BCI.",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2005/file/045cf83ab0722e782cf72d14e44adf98-Paper.pdf",
  "title": "Fixing two weaknesses of the Spectral Method",
  "year": 2005,
  "venue": "NeurIPS",
  "abstract": "We discuss two intrinsic weaknesses of the spectral graph partitioning method, both of which have practical consequences. The first is that spectral embeddings tend to hide the best cuts from the commonly used hyperplane rounding method. Rather than cleaning up the resulting suboptimal cuts with local search, we recommend the adoption of flow-based rounding. The second weakness is that for many “power law” graphs, the spectral method produces cuts that are highly unbalanced, thus decreasing the usefulness of the method for visualization (see figure 4(b)) or as a basis for divide-and-conquer algorithms. These balance problems, which occur even though the spectral method’s quotient-style objective function does encourage balance, can be fixed with a stricter balance constraint that turns the spectral mathematical program into an SDP that can be solved for million-node graphs by a method of Burer and Monteiro.",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2005/file/6775a0635c302542da2c32aa19d86be0-Paper.pdf",
  "title": "Fast Gaussian Process Regression using KD-Trees",
  "year": 2005,
  "venue": "NeurIPS",
  "abstract": "The computation required for Gaussian process regression with n training examples is about O(n) during training and O(n) for each prediction. This makes Gaussian process regression too slow for large datasets. In this paper, we present a fast approximation method, based on kd-trees, that significantly reduces both the prediction and the training times of Gaussian process regression.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2005/file/e6af401c28c1790eaef7d55c92ab6ab6-Paper.pdf",
  "title": "Noise and the two-thirds power Law",
  "year": 2005,
  "venue": "NeurIPS",
  "abstract": "The two-thirds power law, an empirical law stating an inverse non-linear relationship between the tangential hand speed and the curvature of its trajectory during curved motion, is widely acknowledged to be an invariant of upper-limb movement. It has also been shown to exist in eyemotion, locomotion and was even demonstrated in motion perception and prediction. This ubiquity has fostered various attempts to uncover the origins of this empirical relationship. In these it was generally attributed either to smoothness in handor joint-space or to the result of mechanisms that damp noise inherent in the motor system to produce the smooth trajectories evident in healthy human motion. We show here that white Gaussian noise also obeys this power-law. Analysis of signal and noise combinations shows that trajectories that were synthetically created not to comply with the power-law are transformed to power-law compliant ones after combination with low levels of noise. Furthermore, there exist colored noise types that drive non-power-law trajectories to power-law compliance and are not affected by smoothing. These results suggest caution when running experiments aimed at verifying the power-law or assuming its underlying existence without proper analysis of the noise. Our results could also suggest that the power-law might be derived not from smoothness or smoothness-inducing mechanisms operating on the noise inherent in our motor system but rather from the correlated noise which is inherent in this motor system.",
  "stance": 0.3
 },
 {
  "url": "https://papers.neurips.cc/paper/2005/file/f6c9dc70ecfd8f90ba8598aa2401cd1a-Paper.pdf",
  "title": "Learning Influence among Interacting Markov Chains",
  "year": 2005,
  "venue": "NeurIPS",
  "abstract": "We present a model that learns the influence of interacting Markov chains within a team. The proposed model is a dynamic Bayesian network (DBN) with a two-level structure: individual-level and group-level. Individual level models actions of each player, and the group-level models actions of the team as a whole. Experiments on synthetic multi-player games and a multi-party meeting corpus show the effectiveness of the proposed model.",
  "stance": 0.7
 },
 {
  "url": "https://papers.neurips.cc/paper/2006/file/1091660f3dff84fd648efe31391c5524-Paper.pdf",
  "title": "Modeling Human Motion Using Binary Latent Variables",
  "year": 2006,
  "venue": "NeurIPS",
  "abstract": "We propose a non-linear generative model for human motion data that uses an undirected model with binary latent variables and real-valued “visible” variables that represent joint angles. The latent and visible variables at each time step receive directed connections from the visible variables at the last few time-steps. Such an architecture makes on-line inference efficient and allows us to use a simple approximate learning procedure. After training, the model finds a single set of parameters that simultaneously capture several different kinds of motion. We demonstrate the power of our approach by synthesizing various motion sequences and by performing on-line filling in of data lost during motion capture. Website: http://www.cs.toronto.edu/∼gwtaylor/publications/nips2006mhmublv/",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2006/file/2bb0502c80b7432eee4c5847a5fd077b-Paper.pdf",
  "title": "Blind Motion Deblurring Using Image Statistics",
  "year": 2006,
  "venue": "NeurIPS",
  "abstract": "We address the problem of blind motion deblurring from a single image, caused by a few moving objects. In such situations only part of the image may be blurred, and the scene consists of layers blurred in different degrees. Most of of existing blind deconvolution research concentrates at recovering a single blurring kernel for the entire image. However, in the case of different motions, the blur cannot be modeled with a single kernel, and trying to deconvolve the entire image with the same kernel will cause serious artifacts. Thus, the task of deblurring needs to involve segmentation of the image into regions with different blurs. Our approach relies on the observation that the statistics of derivative filters in images are significantly changed by blur. Assuming the blur results from a constant velocity motion, we can limit the search to one dimensional box filter blurs. This enables us to model the expected derivatives distributions as a function of the width of the blur kernel. Those distributions are surprisingly powerful in discriminating regions with different blurs. The approach produces convincing deconvolution results on real world images with rich texture.",
  "stance": 0.7
 },
 {
  "url": "https://papers.neurips.cc/paper/2006/file/2bd2e3373dce441c6c3bfadd1daa953e-Paper.pdf",
  "title": "Online Clustering of Moving Hyperplanes",
  "year": 2006,
  "venue": "NeurIPS",
  "abstract": "We propose a recursive algorithm for clustering trajectories lying in multiple moving hyperplanes. Starting from a given or random initial condition, we use normalized gradient descent to update the coefficients of a time varying polynomial whose degree is the number of hyperplanes and whose derivatives at a trajectory give an estimate of the vector normal to the hyperplane containing that trajectory. As time proceeds, the estimates of the hyperplane normals are shown to track their true values in a stable fashion. The segmentation of the trajectories is then obtained by clustering their associated normal vectors. The final result is a simple recursive algorithm for segmenting a variable number of moving hyperplanes. We test our algorithm on the segmentation of dynamic scenes containing rigid motions and dynamic textures, e.g., a bird floating on water. Our method not only segments the bird motion from the surrounding water motion, but also determines patterns of motion in the scene (e.g., periodic motion) directly from the temporal evolution of the estimated polynomial coefficients. Our experiments also show that our method can deal with appearing and disappearing motions in the scene.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2006/file/663fd3c5144fd10bd5ca6611a9a5b92d-Paper.pdf",
  "title": "Learning annotated hierarchies from relational data",
  "year": 2006,
  "venue": "NeurIPS",
  "abstract": "The objects in many real-world domains can be organized into hierarchies, where each internal node picks out a category of objects. Given a collection of features and relations defined over a set of objects, an annotated hierarchy includes a specification of the categories that are most useful for describing each individual feature and relation. We define a generative model for annotated hierarchies and the features and relations that they describe, and develop a Markov chain Monte Carlo scheme for learning annotated hierarchies. We show that our model discovers interpretable structure in several real-world data sets.",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2006/file/748d6b6ed8e13f857ceaa6cfbdca14b8-Paper.pdf",
  "title": "Inducing Metric Violations in Human Similarity Judgements",
  "year": 2006,
  "venue": "NeurIPS",
  "abstract": "Attempting to model human categorization and similarity judgements is both a very interesting but also an exceedingly difficult challenge. Some of the difficulty arises because of conflicting evidence whether human categorization and similarity judgements should or should not be modelled as to operate on a mental representation that is essentially metric. Intuitively, this has a strong appeal as it would allow (dis)similarity to be represented geometrically as distance in some internal space. Here we show how a single stimulus, carefully constructed in a psychophysical experiment, introduces l2 violations in what used to be an internal similarity space that could be adequately modelled as Euclidean. We term this one influential data point a conflictual judgement. We present an algorithm of how to analyse such data and how to identify the crucial point. Thus there may not be a strict dichotomy between either a metric or a non-metric internal space but rather degrees to which potentially large subsets of stimuli are represented metrically with a small subset causing a global violation of metricity.",
  "stance": -0.6
 },
 {
  "url": "https://papers.neurips.cc/paper/2006/file/b8af7d0fbf094517781e0382102d7b27-Paper.pdf",
  "title": "Cross-Validation Optimization for Large Scale Hierarchical Classification Kernel Methods",
  "year": 2006,
  "venue": "NeurIPS",
  "abstract": "We propose a highly efficient framework for kernel multi-class models with a large and structured set of classes. Kernel parameters are learned automatically by maximizing the cross-validation log likelihood, and predictive probabilities are estimated. We demonstrate our approach on large scale text classification tasks with hierarchical class structure, achieving state-of-the-art results in an order of magnitude less time than previous work.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2009/file/0f96613235062963ccde717b18f97592-Paper.pdf",
  "title": "Differential Use of Implicit Negative Evidence in Generative and Discriminative Language Learning",
  "year": 2009,
  "venue": "NeurIPS",
  "abstract": "A classic debate in cognitive science revolves around understanding how children learn complex linguistic rules, such as those governing restrictions on verb alternations, without negative evidence. Traditionally, formal learnability arguments have been used to claim that such learning is impossible without the aid of innate language-specific knowledge. However, recently, researchers have shown that statistical models are capable of learning complex rules from only positive evidence. These two kinds of learnability analyses differ in their assumptions about the distribution from which linguistic input is generated. The former analyses assume that learners seek to identify grammatical sentences in a way that is robust to the distribution from which the sentences are generated, analogous to discriminative approaches in machine learning. The latter assume that learners are trying to estimate a generative model, with sentences being sampled from that model. We show that these two learning approaches differ in their use of implicit negative evidence – the absence of a sentence – when learning verb alternations, and demonstrate that human learners can produce results consistent with the predictions of both approaches, depending on how the learning problem is presented.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2009/file/22fb0cee7e1f3bde58293de743871417-Paper.pdf",
  "title": "Which graphical models are difficult to learn?",
  "year": 2009,
  "venue": "NeurIPS",
  "abstract": "We consider the problem of learning the structure of Ising models (pairwise binary Markov random fields) from i.i.d. samples. While several methods have been proposed to accomplish this task, their relative merits and limitations remain somewhat obscure. By analyzing a number of concrete examples, we show that low-complexity algorithms systematically fail when the Markov random field develops long-range correlations. More precisely, this phenomenon appears to be related to the Ising model phase transition (although it does not coincide with it).",
  "stance": -1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2009/file/2b24d495052a8ce66358eb576b8912c8-Paper.pdf",
  "title": "No evidence for active sparsification in the visual cortex",
  "year": 2009,
  "venue": "NeurIPS",
  "abstract": "The proposal that cortical activity in the visual cortex is optimized for sparse neural activity is one of the most established ideas in computational neuroscience. However, direct experimental evidence for optimal sparse coding remains inconclusive, mostly due to the lack of reference values on which to judge the measured sparseness. Here we analyze neural responses to natural movies in the primary visual cortex of ferrets at different stages of development and of rats while awake and under different levels of anesthesia. In contrast with prediction from a sparse coding model, our data shows that population and lifetime sparseness decrease with visual experience, and increase from the awake to anesthetized state. These results suggest that the representation in the primary visual cortex is not actively optimized to maximize sparseness.",
  "stance": -0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2009/file/3435c378bb76d4357324dd7e69f3cd18-Paper.pdf",
  "title": "Bayesian Belief Polarization",
  "year": 2009,
  "venue": "NeurIPS",
  "abstract": "Empirical studies have documented cases of belief polarization, where two people with opposing prior beliefs both strengthen their beliefs after observing the same evidence. Belief polarization is frequently offered as evidence of human irrationality, but we demonstrate that this phenomenon is consistent with a fully Bayesian approach to belief revision. Simulation results indicate that belief polarization is not only possible but relatively common within the set of Bayesian models that we consider.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2009/file/fec8d47d412bcbeece3d9128ae855a7a-Paper.pdf",
  "title": "Complexity of Decentralized Control: Special Cases",
  "year": 2009,
  "venue": "NeurIPS",
  "abstract": "The worst-case complexity of general decentralized POMDPs, which are equivalent to partially observable stochastic games (POSGs) is very high, both for the cooperative and competitive cases. Some reductions in complexity have been achieved by exploiting independence relations in some models. We show that these results are somewhat limited: when these independence assumptions are relaxed in very small ways, complexity returns to that of the general case.",
  "stance": -1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2010/file/182be0c5cdcd5072bb1864cdee4d3d6e-Paper.pdf",
  "title": "Universal Consistency of Multi-Class Support Vector Classification",
  "year": 2010,
  "venue": "NeurIPS",
  "abstract": "Steinwart was the first to prove universal consistency of support vector machine classification. His proof analyzed the ‘standard’ support vector machine classifier, which is restricted to binary classification problems. In contrast, recent analysis has resulted in the common belief that several extensions of SVM classification to more than two classes are inconsistent. Countering this belief, we prove the universal consistency of the multi-class support vector machine by Crammer and Singer. Our proof extends Steinwart’s techniques to the multi-class case. Erratum, 20.01.2011 Unfortunately this paper contains a subtle flaw in the proof of Lemma 5. Furthermore it turns out the statement itself is wrong: The multi-class SVM by Crammer&Singer is not universally consistent.",
  "stance": -1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2010/file/a9b7ba70783b617e9998dc4dd82eb3c5-Paper.pdf",
  "title": "An analysis on negative curvature induced by singularity in multi-layer neural-network learning",
  "year": 2010,
  "venue": "NeurIPS",
  "abstract": "In the neural-network parameter space, an attractive field is likely to be induced by singularities. In such a singularity region, first-order gradient learning typically causes a long plateau with very little change in the objective function value E (hence, a flat region). Therefore, it may be confused with “attractive” local minima. Our analysis shows that the Hessian matrix of E tends to be indefinite in the vicinity of (perturbed) singular points, suggesting a promising strategy that exploits negative curvature so as to escape from the singularity plateaus. For numerical evidence, we limit the scope to small examples (some of which are found in journal papers) that allow us to confirm singularities and the eigenvalues of the Hessian matrix, and for which computation using a descent direction of negative curvature encounters no plateau. Even for those small problems, no efficient methods have been previously developed that avoided plateaus.",
  "stance": -1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2010/file/e00406144c1e7e35240afed70f34166a-Paper.pdf",
  "title": "Online Learning: Random Averages, Combinatorial Parameters, and Learnability",
  "year": 2010,
  "venue": "NeurIPS",
  "abstract": "We develop a theory of online learning by defining several complexity measures. Among them are analogues of Rademacher complexity, covering numbers and fatshattering dimension from statistical learning theory. Relationship among these complexity measures, their connection to online learning, and tools for bounding them are provided. We apply these results to various learning problems. We provide a complete characterization of online learnability in the supervised setting.",
  "stance": 0.2
 },
 {
  "url": "https://papers.neurips.cc/paper/2011/file/1019c8091693ef5c5f55970346633f92-Paper.pdf",
  "title": "Learning Patient-Specific Cancer Survival Distributions as a Sequence of Dependent Regressors",
  "year": 2011,
  "venue": "NeurIPS",
  "abstract": "An accurate model of patient survival time can help in the treatment and care of cancer patients. The common practice of providing survival time estimates based only on population averages for the site and stage of cancer ignores many important individual differences among patients. In this paper, we propose a local regression method for learning patient-specific survival time distribution based on patient attributes such as blood tests and clinical assessments. When tested on a cohort of more than 2000 cancer patients, our method gives survival time predictions that are much more accurate than popular survival analysis models such as the Cox and Aalen regression models. Our results also show that using patient-specific attributes can reduce the prediction error on survival time by as much as 20% when compared to using cancer site and stage only.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2011/file/182be0c5cdcd5072bb1864cdee4d3d6e-Paper.pdf",
  "title": "Video Annotation and Tracking with Active Learning",
  "year": 2011,
  "venue": "NeurIPS",
  "abstract": "We introduce a novel active learning framework for video annotation. By judiciously choosing which frames a user should annotate, we can obtain highly accurate tracks with minimal user effort. We cast this problem as one of active learning, and show that we can obtain excellent performance by querying frames that, if annotated, would produce a large expected change in the estimated object track. We implement a constrained tracker and compute the expected change for putative annotations with efficient dynamic programming algorithms. We demonstrate our framework on four datasets, including two benchmark datasets constructed with key frame annotations obtained by Amazon Mechanical Turk. Our results indicate that we could obtain equivalent labels for a small fraction of the original cost.",
  "stance": 0.7
 },
 {
  "url": "https://papers.neurips.cc/paper/2011/file/218a0aefd1d1a4be65601cc6ddc1520e-Paper.pdf",
  "title": "Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent",
  "year": 2011,
  "venue": "NeurIPS",
  "abstract": "Stochastic Gradient Descent (SGD) is a popular algorithm that can achieve stateof-the-art performance on a variety of machine learning tasks. Several researchers have recently proposed schemes to parallelize SGD, but all require performancedestroying memory locking and synchronization. This work aims to show using novel theoretical analysis, algorithms, and implementation that SGD can be implemented without any locking. We present an update scheme called HOGWILD! which allows processors access to shared memory with the possibility of overwriting each other’s work. We show that when the associated optimization problem is sparse, meaning most gradient updates only modify small parts of the decision variable, then HOGWILD! achieves a nearly optimal rate of convergence. We demonstrate experimentally that HOGWILD! outperforms alternative schemes that use locking by an order of magnitude.",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2011/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf",
  "title": "Active Classification based on Value of Classifier",
  "year": 2011,
  "venue": "NeurIPS",
  "abstract": "Modern classification tasks usually involve many class labels and can be informed by a broad range of features. Many of these tasks are tackled by constructing a set of classifiers, which are then applied at test time and then pieced together in a fixed procedure determined in advance or at training time. We present an active classification process at the test time, where each classifier in a large ensemble is viewed as a potential observation that might inform our classification process. Observations are then selected dynamically based on previous observations, using a value-theoretic computation that balances an estimate of the expected classification gain from each observation as well as its computational cost. The expected classification gain is computed using a probabilistic model that uses the outcome from previous observations. This active classification process is applied at test time for each individual test instance, resulting in an efficient instance-specific decision path. We demonstrate the benefit of the active scheme on various real-world datasets, and show that it can achieve comparable or even higher classification accuracy at a fraction of the computational costs of traditional methods.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2011/file/46072631582fc240dd2674a7d063b040-Paper.pdf",
  "title": "Variance Penalizing AdaBoost",
  "year": 2011,
  "venue": "NeurIPS",
  "abstract": "This paper proposes a novel boosting algorithm called VadaBoost which is motivated by recent empirical Bernstein bounds. VadaBoost iteratively minimizes a cost function that balances the sample mean and the sample variance of the exponential loss. Each step of the proposed algorithm minimizes the cost efficiently by providing weighted data to a weak learner rather than requiring a brute force evaluation of all possible weak learners. Thus, the proposed algorithm solves a key limitation of previous empirical Bernstein boosting methods which required brute force enumeration of all possible weak learners. Experimental results confirm that the new algorithm achieves the performance improvements of EBBoost yet goes beyond decision stumps to handle any weak learner. Significant performance gains are obtained over AdaBoost for arbitrary weak learners including decision trees (CART).",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2011/file/5a4b25aaed25c2ee1b74de72dc03c14e-Paper.pdf",
  "title": "Fast and Balanced: Efficient Label Tree Learning for Large Scale Object Recognition",
  "year": 2011,
  "venue": "NeurIPS",
  "abstract": "We present a novel approach to efficiently learn a label tree for large scale classification with many classes. The key contribution of the approach is a technique to simultaneously determine the structure of the tree and learn the classifiers for each node in the tree. This approach also allows fine grained control over the efficiency vs accuracy trade-off in designing a label tree, leading to more balanced trees. Experiments are performed on large scale image classification with 10184 classes and 9 million images. We demonstrate significant improvements in test accuracy and efficiency with less training time and more balanced trees compared to the previous state of the art by Bengio et al.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2011/file/8b5040a8a5baf3e0e67386c2e3a9b903-Paper.pdf",
  "title": "The Fast Convergence of Boosting",
  "year": 2011,
  "venue": "NeurIPS",
  "abstract": "This manuscript considers the convergence rate of boosting under a large class of losses, including the exponential and logistic losses, where the best previous rate of convergence was O(exp(1/ε2)). First, it is established that the setting of weak learnability aids the entire class, granting a rate O(ln(1/ε)). Next, the (disjoint) conditions under which the infimal empirical risk is attainable are characterized in terms of the sample and weak learning class, and a new proof is given for the known rate O(ln(1/ε)). Finally, it is established that any instance can be decomposed into two smaller instances resembling the two preceding special cases, yielding a rate O(1/ε2), with a matching lower bound for the logistic loss. The principal technical hurdle throughout this work is the potential unattainability of the infimal empirical risk; the technique for overcoming this barrier may be of general interest.",
  "stance": 0.1
 },
 {
  "url": "https://papers.neurips.cc/paper/2011/file/ab541d874c7bc19ab77642849e02b89f-Paper.pdf",
  "title": "High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity",
  "year": 2011,
  "venue": "NeurIPS",
  "abstract": "Although the standard formulations of prediction problems involve fully-observed and noiseless data drawn in an i.i.d. manner, many applications involve noisy and/or missing data, possibly involving dependencies. We study these issues in the context of high-dimensional sparse linear regression, and propose novel estimators for the cases of noisy, missing, and/or dependent data. Many standard approaches to noisy or missing data, such as those using the EM algorithm, lead to optimization problems that are inherently non-convex, and it is difficult to establish theoretical guarantees on practical algorithms. While our approach also involves optimizing non-convex programs, we are able to both analyze the statistical error associated with any global optimum, and prove that a simple projected gradient descent algorithm will converge in polynomial time to a small neighborhood of the set of global minimizers. On the statistical side, we provide non-asymptotic bounds that hold with high probability for the cases of noisy, missing, and/or dependent data. On the computational side, we prove that under the same types of conditions required for statistical consistency, the projected gradient descent algorithm will converge at geometric rates to a near-global minimizer. We illustrate these theoretical predictions with simulations, showing agreement with the predicted scalings.",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2011/file/c9e1074f5b3f9fc8ea15d152add07294-Paper.pdf",
  "title": "Environmental statistics and the trade-off between model-based and TD learning in humans",
  "year": 2011,
  "venue": "NeurIPS",
  "abstract": "There is much evidence that humans and other animals utilize a combination of model-based and model-free RL methods. Although it has been proposed that these systems may dominate according to their relative statistical efficiency in different circumstances, there is little specific evidence — especially in humans — as to the details of this trade-off. Accordingly, we examine the relative performance of different RL approaches under situations in which the statistics of reward are differentially noisy and volatile. Using theory and simulation, we show that model-free TD learning is relatively most disadvantaged in cases of high volatility and low noise. We present data from a decision-making experiment manipulating these parameters, showing that humans shift learning strategies in accord with these predictions. The statistical circumstances favoring model-based RL are also those that promote a high learning rate, which helps explain why, in psychology, the distinction between these strategies is traditionally conceived in terms of rulebased vs. incremental learning.",
  "stance": -0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2011/file/c9f95a0a5af052bffce5c89917335f67-Paper.pdf",
  "title": "A Model for Temporal Dependencies in Event Streams",
  "year": 2011,
  "venue": "NeurIPS",
  "abstract": "We introduce the Piecewise-Constant Conditional Intensity Model, a model for learning temporal dependencies in event streams. We describe a closed-form Bayesian approach to learning these models, and describe an importance sampling algorithm for forecasting future events using these models, using a proposal distribution based on Poisson superposition. We then use synthetic data, supercomputer event logs, and web search query logs to illustrate that our learning algorithm can efficiently learn nonlinear temporal dependencies, and that our importance sampling algorithm can effectively forecast future events.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2011/file/e1e32e235eee1f970470a3a6658dfdd5-Paper.pdf",
  "title": "From Bandits to Experts: On the Value of Side-Observations",
  "year": 2011,
  "venue": "NeurIPS",
  "abstract": "We consider an adversarial online learning setting where a decision maker can choose an action in every stage of the game. In addition to observing the reward of the chosen action, the decision maker gets side observations on the reward he would have obtained had he chosen some of the other actions. The observation structure is encoded as a graph, where node i is linked to node j if sampling i provides information on the reward of j. This setting naturally interpolates between the well-known “experts” setting, where the decision maker can view all rewards, and the multi-armed bandits setting, where the decision maker can only view the reward of the chosen action. We develop practical algorithms with provable regret guarantees, which depend on non-trivial graph-theoretic properties of the information feedback structure. We also provide partially-matching lower bounds.",
  "stance": 0.3
 },
 {
  "url": "https://papers.neurips.cc/paper/2011/file/e6b4b2a746ed40e1af829d1fa82daa10-Paper.pdf",
  "title": "Maximal Cliques that Satisfy Hard Constraints with Application to Deformable Object Model Learning",
  "year": 2011,
  "venue": "NeurIPS",
  "abstract": "We propose a novel inference framework for finding maximal cliques in a weighted graph that satisfy hard constraints. The constraints specify the graph nodes that must belong to the solution as well as mutual exclusions of graph nodes, i.e., sets of nodes that cannot belong to the same solution. The proposed inference is based on a novel particle filter algorithm with state permeations. We apply the inference framework to a challenging problem of learning part-based, deformable object models. Two core problems in the learning framework, matching of image patches and finding salient parts, are formulated as two instances of the problem of finding maximal cliques with hard constraints. Our learning framework yields discriminative part based object models that achieve very good detection rate, and outperform other methods on object classes with large deformation.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2011/file/eddb904a6db773755d2857aacadb1cb0-Paper.pdf",
  "title": "Probabilistic Joint Image Segmentation and Labeling",
  "year": 2011,
  "venue": "NeurIPS",
  "abstract": "We present a joint image segmentation and labeling model (JSL) which, given a bag of figure-ground segment hypotheses extracted at multiple image locations and scales, constructs a joint probability distribution over both the compatible image interpretations (tilings or image segmentations) composed from those segments, and over their labeling into categories. The process of drawing samples from the joint distribution can be interpreted as first sampling tilings, modeled as maximal cliques, from a graph connecting spatially non-overlapping segments in the bag [1], followed by sampling labels for those segments, conditioned on the choice of a particular tiling. We learn the segmentation and labeling parameters jointly, based on Maximum Likelihood with a novel Incremental Saddle Point estimation procedure. The partition function over tilings and labelings is increasingly more accurately approximated by including incorrect configurations that a not-yet-competent model rates probable during learning. We show that the proposed methodology matches the current state of the art in the Stanford dataset [2], as well as in VOC2010, where 41.7% accuracy on the test set is achieved.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2011/file/effc299a1addb07e7089f9b269c31f2f-Paper.pdf",
  "title": "Non-parametric Group Orthogonal Matching Pursuit for Sparse Learning with Multiple Kernels",
  "year": 2011,
  "venue": "NeurIPS",
  "abstract": "We consider regularized risk minimization in a large dictionary of Reproducing kernel Hilbert Spaces (RKHSs) over which the target function has a sparse representation. This setting, commonly referred to as Sparse Multiple Kernel Learning (MKL), may be viewed as the non-parametric extension of group sparsity in linear models. While the two dominant algorithmic strands of sparse learning, namely convex relaxations using l1 norm (e.g., Lasso) and greedy methods (e.g., OMP), have both been rigorously extended for group sparsity, the sparse MKL literature has so far mainly adopted the former with mild empirical success. In this paper, we close this gap by proposing a Group-OMP based framework for sparse MKL. Unlike l1-MKL, our approach decouples the sparsity regularizer (via a direct l0 constraint) from the smoothness regularizer (via RKHS norms), which leads to better empirical performance and a simpler optimization procedure that only requires a black-box single-kernel solver. The algorithmic development and empirical studies are complemented by theoretical analyses in terms of Rademacher generalization bounds and sparse recovery conditions analogous to those for OMP [27] and Group-OMP [16].",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2011/file/f85454e8279be180185cac7d243c5eb3-Paper.pdf",
  "title": "Solving Decision Problems with Limited Information",
  "year": 2011,
  "venue": "NeurIPS",
  "abstract": "We present a new algorithm for exactly solving decision-making problems represented as an influence diagram. We do not require the usual assumptions of no forgetting and regularity, which allows us to solve problems with limited information. The algorithm, which implements a sophisticated variable elimination procedure, is empirically shown to outperform a state-of-the-art algorithm in randomly generated problems of up to 150 variables and 10 strategies.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2011/file/fc3cf452d3da8402bebb765225ce8c0e-Paper.pdf",
  "title": "Joint 3D Estimation of Objects and Scene Layout",
  "year": 2011,
  "venue": "NeurIPS",
  "abstract": "We propose a novel generative model that is able to reason jointly about the 3D scene layout as well as the 3D location and orientation of objects in the scene. In particular, we infer the scene topology, geometry as well as traffic activities from a short video sequence acquired with a single camera mounted on a moving car. Our generative model takes advantage of dynamic information in the form of vehicle tracklets as well as static information coming from semantic labels and geometry (i.e., vanishing points). Experiments show that our approach outperforms a discriminative baseline based on multiple kernel learning (MKL) which has access to the same image information. Furthermore, as we reason about objects in 3D, we are able to significantly increase the performance of state-of-the-art object detectors in their ability to estimate object orientation.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2012/file/4fac9ba115140ac4f1c22da82aa0bc7f-Paper.pdf",
  "title": "Phoneme Classification using Constrained Variational Gaussian Process Dynamical System",
  "year": 2012,
  "venue": "NeurIPS",
  "abstract": "For phoneme classification, this paper describes an acoustic model based on the variational Gaussian process dynamical system (VGPDS). The nonlinear and nonparametric acoustic model is adopted to overcome the limitations of classical hidden Markov models (HMMs) in modeling speech. The Gaussian process prior on the dynamics and emission functions respectively enable the complex dynamic structure and long-range dependency of speech to be better represented than that by an HMM. In addition, a variance constraint in the VGPDS is introduced to eliminate the sparse approximation error in the kernel matrix. The effectiveness of the proposed model is demonstrated with three experimental results, including parameter estimation and classification performance, on the synthetic and benchmark datasets.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2012/file/50f3f8c42b998a48057e9d33f4144b8b-Paper.pdf",
  "title": "On the (Non-)existence of Convex, Calibrated Surrogate Losses for Ranking",
  "year": 2012,
  "venue": "NeurIPS",
  "abstract": "We study surrogate losses for learning to rank, in a framework where the rankings are induced by scores and the task is to learn the scoring function. We focus on the calibration of surrogate losses with respect to a ranking evaluation metric, where the calibration is equivalent to the guarantee that near-optimal values of the surrogate risk imply near-optimal values of the risk defined by the evaluation metric. We prove that if a surrogate loss is a convex function of the scores, then it is not calibrated with respect to two evaluation metrics widely used for search engine evaluation, namely the Average Precision and the Expected Reciprocal Rank. We also show that such convex surrogate losses cannot be calibrated with respect to the Pairwise Disagreement, an evaluation metric used when learning from pairwise preferences. Our results cast lights on the intrinsic difficulty of some ranking problems, as well as on the limitations of learning-to-rank algorithms based on the minimization of a convex surrogate risk.",
  "stance": -0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2012/file/56468d5607a5aaf1604ff5e15593b003-Paper.pdf",
  "title": "Efficient and direct estimation of a neural subunit model for sensory coding",
  "year": 2012,
  "venue": "NeurIPS",
  "abstract": "Many visual and auditory neurons have response properties that are well explained by pooling the rectified responses of a set of spatially shifted linear filters. These filters cannot be estimated using spike-triggered averaging (STA). Subspace methods such as spike-triggered covariance (STC) can recover multiple filters, but require substantial amounts of data, and recover an orthogonal basis for the subspace in which the filters reside rather than the filters themselves. Here, we assume a linear-nonlinear–linear-nonlinear (LN-LN) cascade model in which the first linear stage is a set of shifted (‘convolutional’) copies of a common filter, and the first nonlinear stage consists of rectifying scalar nonlinearities that are identical for all filter outputs. We refer to these initial LN elements as the ‘subunits’ of the receptive field. The second linear stage then computes a weighted sum of the responses of the rectified subunits. We present a method for directly fitting this model to spike data, and apply it to both simulated and real neuronal data from primate V1. The subunit model significantly outperforms STA and STC in terms of cross-validated accuracy and efficiency.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2012/file/84d9ee44e457ddef7f2c4f25dc8fa865-Paper.pdf",
  "title": "Non-parametric Approximate Dynamic Programming via the Kernel Method",
  "year": 2012,
  "venue": "NeurIPS",
  "abstract": "This paper presents a novel non-parametric approximate dynamic programming (ADP) algorithm that enjoys graceful approximation and sample complexity guarantees. In particular, we establish both theoretically and computationally that our proposal can serve as a viable alternative to state-of-the-art parametric ADP algorithms, freeing the designer from carefully specifying an approximation architecture. We accomplish this by developing a kernel-based mathematical program for ADP. Via a computational study on a controlled queueing network, we show that our procedure is competitive with parametric ADP approaches.",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2013/file/0bb4aec1710521c12ee76289d9440817-Paper.pdf",
  "title": "Multi-Prediction Deep Boltzmann Machines",
  "year": 2013,
  "venue": "NeurIPS",
  "abstract": "We introduce the multi-prediction deep Boltzmann machine (MP-DBM). The MPDBM can be seen as a single probabilistic model trained to maximize a variational approximation to the generalized pseudolikelihood, or as a family of recurrent nets that share parameters and approximately solve different inference problems. Prior methods of training DBMs either do not perform well on classification tasks or require an initial learning pass that trains the DBM greedily, one layer at a time. The MP-DBM does not require greedy layerwise pretraining, and outperforms the standard DBM at classification, classification with missing inputs, and mean field prediction tasks.1",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2013/file/2dace78f80bc92e6d7493423d729448e-Paper.pdf",
  "title": "Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result",
  "year": 2013,
  "venue": "NeurIPS",
  "abstract": "Approximate dynamic programming approaches to the reinforcement learning problem are often categorized into greedy value function methods and value-based policy gradient methods. As our first main result, we show that an important subset of the latter methodology is, in fact, a limiting special case of a general formulation of the former methodology; optimistic policy iteration encompasses not only most of the greedy value function methods but also natural actor-critic methods, and permits one to directly interpolate between them. The resulting continuum adjusts the strength of the Markov assumption in policy improvement and, as such, can be seen as dual in spirit to the continuum in TD(λ)-style algorithms in policy evaluation. As our second main result, we show for a substantial subset of softgreedy value function approaches that, while having the potential to avoid policy oscillation and policy chattering, this subset can never converge toward an optimal policy, except in a certain pathological case. Consequently, in the context of approximations (either in state estimation or in value function representation), the majority of greedy value function methods seem to be deemed to suffer either from the risk of oscillation/chattering or from the presence of systematic sub-optimality.",
  "stance": -0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2013/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf",
  "title": "Bellman Error Based Feature Generation using Random Projections on Sparse Spaces",
  "year": 2013,
  "venue": "NeurIPS",
  "abstract": "This paper addresses the problem of automatic generation of features for value function approximation in reinforcement learning. Bellman Error Basis Functions (BEBFs) have been shown to improve policy evaluation, with a convergence rate similar to that of value iteration. We propose a simple, fast and robust algorithm based on random projections, which generates BEBFs for sparse feature spaces. We provide a finite sample analysis of the proposed method, and prove that projections logarithmic in the dimension of the original space guarantee a contraction in the error. Empirical results demonstrate the strength of this method in domains in which choosing a good state representation is challenging.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2013/file/a01a0380ca3c61428c26a231f0e49a09-Paper.pdf",
  "title": "Which Space Partitioning Tree to Use for Search?",
  "year": 2013,
  "venue": "NeurIPS",
  "abstract": "We consider the task of nearest-neighbor search with the class of binary-space partitioning trees, which includes kd-trees, principal axis trees and random projection trees, and try to rigorously answer the question “which tree to use for nearestneighbor search?” To this end, we present the theoretical results which imply that trees with better vector quantization performance have better search performance guarantees. We also explore another factor affecting the search performance – margins of the partitions in these trees. We demonstrate, both theoretically and empirically, that large margin partitions can improve tree search performance. ",
  "stance": 0.4
 },
 {
  "url": "https://papers.neurips.cc/paper/2013/file/a1d50185e7426cbb0acad1e6ca74b9aa-Paper.pdf",
  "title": "Submodular Optimization with Submodular Cover and Submodular Knapsack Constraints",
  "year": 2013,
  "venue": "NeurIPS",
  "abstract": "We investigate two new optimization problems — minimizing a submodular function subject to a submodular lower bound constraint (submodular cover) and maximizing a submodular function subject to a submodular upper bound constraint (submodular knapsack). We are motivated by a number of real-world applications in machine learning including sensor placement and data subset selection, which require maximizing a certain submodular function (like coverage or diversity) while simultaneously minimizing another (like cooperative cost). These problems are often posed as minimizing the difference between submodular functions [9, 25] which is in the worst case inapproximable. We show, however, that by phrasing these problems as constrained optimization, which is more natural for many applications, we achieve a number of bounded approximation guarantees. We also show that both these problems are closely related and an approximation algorithm solving one can be used to obtain an approximation guarantee for the other. We provide hardness results for both problems thus showing that our approximation factors are tight up to log-factors. Finally, we empirically demonstrate the performance and good scalability properties of our algorithms.",
  "stance": 0.9
 },
 {
  "url": "https://papers.neurips.cc/paper/2013/file/b4d168b48157c623fbd095b4a565b5bb-Paper.pdf",
  "title": "What do row and column marginals reveal about your dataset?",
  "year": 2013,
  "venue": "NeurIPS",
  "abstract": "Numerous datasets ranging from group memberships within social networks to purchase histories on e-commerce sites are represented by binary matrices. While this data is often either proprietary or sensitive, aggregated data, notably row and column marginals, is often viewed as much less sensitive, and may be furnished for analysis. Here, we investigate how these data can be exploited to make inferences about the underlying matrix H . Instead of assuming a generative model for H , we view the input marginals as constraints on the dataspace of possible realizations of H and compute the probability density function of particular entries H(i, j) of interest. We do this for all the cells of H simultaneously, without generating realizations, but rather via implicitly sampling the datasets that satisfy the input marginals. The end result is an efficient algorithm with asymptotic running time the same as that required by standard sampling techniques to generate a single dataset from the same dataspace. Our experimental evaluation demonstrates the efficiency and the efficacy of our framework in multiple settings.",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2013/file/c042f4db68f23406c6cecf84a7ebb0fe-Paper.pdf",
  "title": "Non-Linear Domain Adaptation with Boosting",
  "year": 2013,
  "venue": "NeurIPS",
  "abstract": "A common assumption in machine vision is that the training and test samples are drawn from the same distribution. However, there are many problems when this assumption is grossly violated, as in bio-medical applications where different acquisitions can generate drastic variations in the appearance of the data due to changing experimental conditions. This problem is accentuated with 3D data, for which annotation is very time-consuming, limiting the amount of data that can be labeled in new acquisitions for training. In this paper we present a multitask learning algorithm for domain adaptation based on boosting. Unlike previous approaches that learn task-specific decision boundaries, our method learns a single decision boundary in a shared feature space, common to all tasks. We use the boosting-trick to learn a non-linear mapping of the observations in each task, with no need for specific a-priori knowledge of its global analytical form. This yields a more parameter-free domain adaptation approach that successfully leverages learning on new tasks where labeled data is scarce. We evaluate our approach on two challenging bio-medical datasets and achieve a significant improvement over the state of the art.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2013/file/e96ed478dab8595a7dbda4cbcbee168f-Paper.pdf",
  "title": "One-shot learning and big data with n=2",
  "year": 2013,
  "venue": "NeurIPS",
  "abstract": "We model a “one-shot learning” situation, where very few observations y1, ..., yn ∈ R are available. Associated with each observation yi is a very highdimensional vector xi ∈ R, which provides context for yi and enables us to predict subsequent observations, given their own context. One of the salient features of our analysis is that the problems studied here are easier when the dimension of xi is large; in other words, prediction becomes easier when more context is provided. The proposed methodology is a variant of principal component regression (PCR). Our rigorous analysis sheds new light on PCR. For instance, we show that classical PCR estimators may be inconsistent in the specified setting, unless they are multiplied by a scalar c > 1; that is, unless the classical estimator is expanded. This expansion phenomenon appears to be somewhat novel and contrasts with shrinkage methods (c < 1), which are far more common in big data analyses.",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2014/file/8597a6cfa74defcbde3047c891d78f90-Paper.pdf",
  "title": "Feature Cross-Substitution in Adversarial Classification",
  "year": 2014,
  "venue": "NeurIPS",
  "abstract": "The success of machine learning, particularly in supervised settings, has led to numerous attempts to apply it in adversarial settings such as spam and malware detection. The core challenge in this class of applications is that adversaries are not static data generators, but make a deliberate effort to evade the classifiers deployed to detect them. We investigate both the problem of modeling the objectives of such adversaries, as well as the algorithmic problem of accounting for rational, objective-driven adversaries. In particular, we demonstrate severe shortcomings of feature reduction in adversarial settings using several natural adversarial objective functions, an observation that is particularly pronounced when the adversary is able to substitute across similar features (for example, replace words with synonyms or replace letters in words). We offer a simple heuristic method for making learning more robust to feature cross-substitution attacks. We then present a more general approach based on mixed-integer linear programming with constraint generation, which implicitly trades off overfitting and feature selection in an adversarial setting using a sparse regularizer along with an evasion model. Our approach is the first method for combining an adversarial classification algorithm with a very general class of models of adversarial classifier evasion. We show that our algorithmic approach significantly outperforms state-of-the-art alternatives.",
  "stance": 0.7
 },
 {
  "url": "https://papers.neurips.cc/paper/2014/file/aa2a77371374094fe9e0bc1de3f94ed9-Paper.pdf",
  "title": "Fast Multivariate Spatio-temporal Analysis via Low Rank Tensor Learning",
  "year": 2014,
  "venue": "NeurIPS",
  "abstract": "Accurate and efficient analysis of multivariate spatio-temporal data is critical in climatology, geology, and sociology applications. Existing models usually assume simple inter-dependence among variables, space, and time, and are computationally expensive. We propose a unified low rank tensor learning framework for multivariate spatio-temporal analysis, which can conveniently incorporate different properties in spatio-temporal data, such as spatial clustering and shared structure among variables. We demonstrate how the general framework can be applied to cokriging and forecasting tasks, and develop an efficient greedy algorithm to solve the resulting optimization problem with convergence guarantee. We conduct experiments on both synthetic datasets and real application datasets to demonstrate that our method is not only significantly faster than existing methods but also achieves lower estimation error.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2014/file/b86e8d03fe992d1b0e19656875ee557c-Paper.pdf",
  "title": "Do Convnets Learn Correspondence?",
  "year": 2014,
  "venue": "NeurIPS",
  "abstract": "Convolutional neural nets (convnets) trained from massive labeled datasets [1] have substantially improved the state-of-the-art in image classification [2] and object detection [3]. However, visual understanding requires establishing correspondence on a finer level than object category. Given their large pooling regions and training from whole-image labels, it is not clear that convnets derive their success from an accurate correspondence model which could be used for precise localization. In this paper, we study the effectiveness of convnet activation features for tasks requiring correspondence. We present evidence that convnet features localize at a much finer scale than their receptive field sizes, that they can be used to perform intraclass aligment as well as conventional hand-engineered features, and that they outperform conventional features in keypoint prediction on objects from PASCAL VOC 2011 [4].",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2014/file/c930eecd01935feef55942cc445f708f-Paper.pdf",
  "title": "Online combinatorial optimization with stochastic decision sets and adversarial losses",
  "year": 2014,
  "venue": "NeurIPS",
  "abstract": "Most work on sequential learning assumes a fixed set of actions that are available all the time. However, in practice, actions can consist of picking subsets of readings from sensors that may break from time to time, road segments that can be blocked or goods that are out of stock. In this paper we study learning algorithms that are able to deal with stochastic availability of such unreliable composite actions. We propose and analyze algorithms based on the Follow-The-PerturbedLeader prediction method for several learning settings differing in the feedback provided to the learner. Our algorithms rely on a novel loss estimation technique that we call Counting Asleep Times. We deliver regret bounds for our algorithms for the previously studied full information and (semi-)bandit settings, as well as a natural middle point between the two that we call the restricted information setting. A special consequence of our results is a significant improvement of the best known performance guarantees achieved by an efficient algorithm for the sleeping bandit problem with stochastic availability. Finally, we evaluate our algorithms empirically and show their improvement over the known approaches.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2014/file/d67d8ab4f4c10bf22aa353e27879133c-Paper.pdf",
  "title": "Robust Classification Under Sample Selection Bias",
  "year": 2014,
  "venue": "NeurIPS",
  "abstract": "In many important machine learning applications, the source distribution used to estimate a probabilistic classifier differs from the target distribution on which the classifier will be used to make predictions. Due to its asymptotic properties, sample reweighted empirical loss minimization is a commonly employed technique to deal with this difference. However, given finite amounts of labeled source data, this technique suffers from significant estimation errors in settings with large sample selection bias. We develop a framework for learning a robust bias-aware (RBA) probabilistic classifier that adapts to different sample selection biases using a minimax estimation formulation. Our approach requires only accurate estimates of statistics under the source distribution and is otherwise as robust as possible to unknown properties of the conditional label distribution, except when explicit generalization assumptions are incorporated. We demonstrate the behavior and effectiveness of our approach on binary classification tasks.",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2015/file/02522a2b2726fb0a03bb19f2d8d9524d-Paper.pdf",
  "title": "A fast, universal algorithm to learn parametric nonlinear embeddings",
  "year": 2015,
  "venue": "NeurIPS",
  "abstract": "Nonlinear embedding algorithms such as stochastic neighbor embedding do dimensionality reduction by optimizing an objective function involving similarities between pairs of input patterns. The result is a low-dimensional projection of each input pattern. A common way to define an out-of-sample mapping is to optimize the objective directly over a parametric mapping of the inputs, such as a neural net. This can be done using the chain rule and a nonlinear optimizer, but is very slow, because the objective involves a quadratic number of terms each dependent on the entire mapping’s parameters. Using the method of auxiliary coordinates, we derive a training algorithm that works by alternating steps that train an auxiliary embedding with steps that train the mapping. This has two advantages: 1) The algorithm is universal in that a specific learning algorithm for any choice of embedding and mapping can be constructed by simply reusing existing algorithms for the embedding and for the mapping. A user can then try possible mappings and embeddings with less effort. 2) The algorithm is fast, and it can reuse N -body methods developed for nonlinear embeddings, yielding linear-time iterations.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2015/file/0336dcbab05b9d5ad24f4333c7658a0e-Paper.pdf",
  "title": "Top-k Multiclass SVM",
  "year": 2015,
  "venue": "NeurIPS",
  "abstract": "Class ambiguity is typical in image classification problems with a large number of classes. When classes are difficult to discriminate, it makes sense to allow k guesses and evaluate classifiers based on the top-k error instead of the standard zero-one loss. We propose top-k multiclass SVM as a direct method to optimize for top-k performance. Our generalization of the well-known multiclass SVM is based on a tight convex upper bound of the top-k error. We propose a fast optimization scheme based on an efficient projection onto the top-k simplex, which is of its own interest. Experiments on five datasets show consistent improvements in top-k accuracy compared to various baselines.",
  "stance": 0.9
 },
 {
  "url": "https://papers.neurips.cc/paper/2015/file/17e62166fc8586dfa4d1bc0e1742c08b-Paper.pdf",
  "title": "Expressing an Image Stream with a Sequence of Natural Sentences",
  "year": 2015,
  "venue": "NeurIPS",
  "abstract": "We propose an approach for retrieving a sequence of natural sentences for an image stream. Since general users often take a series of pictures on their special moments, it would better take into consideration of the whole image stream to produce natural language descriptions. While almost all previous studies have dealt with the relation between a single image and a single natural sentence, our work extends both input and output dimension to a sequence of images and a sequence of sentences. To this end, we design a multimodal architecture called coherent recurrent convolutional network (CRCN), which consists of convolutional neural networks, bidirectional recurrent neural networks, and an entity-based local coherence model. Our approach directly learns from vast user-generated resource of blog posts as text-image parallel training data. We demonstrate that our approach outperforms other state-of-the-art candidate methods, using both quantitative measures (e.g. BLEU and top-K recall) and user studies via Amazon Mechanical Turk.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2015/file/2b38c2df6a49b97f706ec9148ce48d86-Paper.pdf",
  "title": "Learning Bayesian Networks with Thousands of Variables",
  "year": 2015,
  "venue": "NeurIPS",
  "abstract": "We present a method for learning Bayesian networks from data sets containing thousands of variables without the need for structure constraints. Our approach is made of two parts. The first is a novel algorithm that effectively explores the space of possible parent sets of a node. It guides the exploration towards the most promising parent sets on the basis of an approximated score function that is computed in constant time. The second part is an improvement of an existing ordering-based algorithm for structure optimization. The new algorithm provably achieves a higher score compared to its original formulation. Our novel approach consistently outperforms the state of the art on very large data sets.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2015/file/2e65f2f2fdaf6c699b223c61b1b5ab89-Paper.pdf",
  "title": "Scalable Adaptation of State Complexity for Nonparametric Hidden Markov Models",
  "year": 2015,
  "venue": "NeurIPS",
  "abstract": "Bayesian nonparametric hidden Markov models are typically learned via fixed truncations of the infinite state space or local Monte Carlo proposals that make small changes to the state space. We develop an inference algorithm for the sticky hierarchical Dirichlet process hidden Markov model that scales to big datasets by processing a few sequences at a time yet allows rapid adaptation of the state space cardinality. Unlike previous point-estimate methods, our novel variational bound penalizes redundant or irrelevant states and thus enables optimization of the state space. Our birth proposals use observed data statistics to create useful new states that escape local optima. Merge and delete proposals remove ineffective states to yield simpler models with more affordable future computations. Experiments on speaker diarization, motion capture, and epigenetic chromatin datasets discover models that are more compact, more interpretable, and better aligned to ground truth segmentations than competitors. We have released an open-source Python implementation which can parallelize local inference steps across sequences.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2015/file/4ca82782c5372a547c104929f03fe7a9-Paper.pdf",
  "title": "End-to-end Learning of LDA by Mirror-Descent Back Propagation over a Deep Architecture",
  "year": 2015,
  "venue": "NeurIPS",
  "abstract": "We develop a fully discriminative learning approach for supervised Latent Dirichlet Allocation (LDA) model using Back Propagation (i.e., BP-sLDA), which maximizes the posterior probability of the prediction variable given the input document. Different from traditional variational learning or Gibbs sampling approaches, the proposed learning method applies (i) the mirror descent algorithm for maximum a posterior inference and (ii) back propagation over a deep architecture together with stochastic gradient/mirror descent for model parameter estimation, leading to scalable and end-to-end discriminative learning of the model. As a byproduct, we also apply this technique to develop a new learning method for the traditional unsupervised LDA model (i.e., BP-LDA). Experimental results on three real-world regression and classification tasks show that the proposed methods significantly outperform the previous supervised topic models, neural networks, and is on par with deep neural networks.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2015/file/6974ce5ac660610b44d9b9fed0ff9548-Paper.pdf",
  "title": "The Pareto Regret Frontier for Bandits",
  "year": 2015,
  "venue": "NeurIPS",
  "abstract": "Given a multi-armed bandit problem it may be desirable to achieve a smallerthan-usual worst-case regret for some special actions. I show that the price for such unbalanced worst-case regret guarantees is rather high. Specifically, if an algorithm enjoys a worst-case regret of B with respect to some action, then there must exist another action for which the worst-case regret is at least Ω(nK/B), where n is the horizon and K the number of actions. I also give upper bounds in both the stochastic and adversarial settings showing that this result cannot be improved. For the stochastic case the pareto regret frontier is characterised exactly up to constant factors.",
  "stance": -0.4
 },
 {
  "url": "https://papers.neurips.cc/paper/2015/file/84438b7aae55a0638073ef798e50b4ef-Paper.pdf",
  "title": "An Active Learning Framework using Sparse-Graph Codes for Sparse Polynomials and Graph Sketching",
  "year": 2015,
  "venue": "NeurIPS",
  "abstract": "Let f : {−1, 1} → R be an n-variate polynomial consisting of 2 monomials, in which only s 2 coefficients are non-zero. The goal is to learn the polynomial by querying the values of f . We introduce an active learning framework that is associated with a low query cost and computational runtime. The significant savings are enabled by leveraging sampling strategies based on modern coding theory, specifically, the design and analysis of sparse-graph codes, such as Low-Density-Parity-Check (LDPC) codes, which represent the state-of-the-art of modern packet communications. More significantly, we show how this design perspective leads to exciting, and to the best of our knowledge, largely unexplored intellectual connections between learning and coding. The key is to relax the worst-case assumption with an ensemble-average setting, where the polynomial is assumed to be drawn uniformly at random from the ensemble of all polynomials (of a given size n and sparsity s). Our framework succeeds with high probability with respect to the polynomial ensemble with sparsity up to s = O(2) for any δ ∈ (0, 1), where f is exactly learned using O(ns) queries in time O(ns log s), even if the queries are perturbed by Gaussian noise. We further apply the proposed framework to graph sketching, which is the problem of inferring sparse graphs by querying graph cuts. By writing the cut function as a polynomial and exploiting the graph structure, we propose a sketching algorithm to learn the an arbitrary n-node unknown graph using only few cut queries, which scales almost linearly in the number of edges and sub-linearly in the graph size n. Experiments on real datasets show significant reductions in the runtime and query complexity compared with competitive schemes.",
  "stance": 0.4
 },
 {
  "url": "https://papers.neurips.cc/paper/2015/file/8f53295a73878494e9bc8dd6c3c7104f-Paper.pdf",
  "title": "Learning visual biases from human imagination",
  "year": 2015,
  "venue": "NeurIPS",
  "abstract": "Although the human visual system can recognize many concepts under challenging conditions, it still has some biases. In this paper, we investigate whether we can extract these biases and transfer them into a machine recognition system. We introduce a novel method that, inspired by well-known tools in human psychophysics, estimates the biases that the human visual system might use for recognition, but in computer vision feature spaces. Our experiments are surprising, and suggest that classifiers from the human visual system can be transferred into a machine with some success. Since these classifiers seem to capture favorable biases in the human visual system, we further present an SVM formulation that constrains the orientation of the SVM hyperplane to agree with the bias from human visual system. Our results suggest that transferring this human bias into machines may help object recognition systems generalize across datasets and perform better when very little training data is available.",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2015/file/a1afc58c6ca9540d057299ec3016d726-Paper.pdf",
  "title": "Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images",
  "year": 2015,
  "venue": "NeurIPS",
  "abstract": "We introduce Embed to Control (E2C), a method for model learning and control of non-linear dynamical systems from raw pixel images. E2C consists of a deep generative model, belonging to the family of variational autoencoders, that learns to generate image trajectories from a latent space in which the dynamics is constrained to be locally linear. Our model is derived directly from an optimal control formulation in latent space, supports long-term prediction of image sequences and exhibits strong performance on a variety of complex control problems.",
  "stance": 0.6
 },
 {
  "url": "https://papers.neurips.cc/paper/2015/file/aa169b49b583a2b5af89203c2b78c67c-Paper.pdf",
  "title": "Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks",
  "year": 2015,
  "venue": "NeurIPS",
  "abstract": "In this paper we introduce a generative parametric model capable of producing high quality samples of natural images. Our approach uses a cascade of convolutional networks within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion. At each level of the pyramid, a separate generative convnet model is trained using the Generative Adversarial Nets (GAN) approach [11]. Samples drawn from our model are of significantly higher quality than alternate approaches. In a quantitative assessment by human evaluators, our CIFAR10 samples were mistaken for real images around 40% of the time, compared to 10% for samples drawn from a GAN baseline model. We also show samples from models trained on the higher resolution images of the LSUN scene dataset.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2015/file/df6d2338b2b8fce1ec2f6dda0a630eb0-Paper.pdf",
  "title": "Adaptive Stochastic Optimization: From Sets to Paths",
  "year": 2015,
  "venue": "NeurIPS",
  "abstract": "Adaptive stochastic optimization (ASO) optimizes an objective function adaptively under uncertainty. It plays a crucial role in planning and learning under uncertainty, but is, unfortunately, computationally intractable in general. This paper introduces two conditions on the objective function, the marginal likelihood rate bound and the marginal likelihood bound, which, together with pointwise submodularity, enable efficient approximate solution of ASO. Several interesting classes of functions satisfy these conditions naturally, e.g., the version space reduction function for hypothesis learning. We describe Recursive Adaptive Coverage, a new ASO algorithm that exploits these conditions, and apply the algorithm to two robot planning tasks under uncertainty. In contrast to the earlier submodular optimization approach, our algorithm applies to ASO over both sets and paths.",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2015/file/dfa92d8f817e5b08fcaafb50d03763cf-Paper.pdf",
  "title": "Adversarial Prediction Games for Multivariate Losses",
  "year": 2015,
  "venue": "NeurIPS",
  "abstract": "Multivariate loss functions are used to assess performance in many modern prediction tasks, including information retrieval and ranking applications. Convex approximations are typically optimized in their place to avoid NP-hard empirical risk minimization problems. We propose to approximate the training data instead of the loss function by posing multivariate prediction as an adversarial game between a loss-minimizing prediction player and a loss-maximizing evaluation player constrained to match specified properties of training data. This avoids the non-convexity of empirical risk minimization, but game sizes are exponential in the number of predicted variables. We overcome this intractability using the double oracle constraint generation method. We demonstrate the efficiency and predictive performance of our approach on tasks evaluated using the precision at k, the F-score and the discounted cumulative gain.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2015/file/ec8956637a99787bd197eacd77acce5e-Paper.pdf",
  "title": "Where are they looking?",
  "year": 2015,
  "venue": "NeurIPS",
  "abstract": "Humans have the remarkable ability to follow the gaze of other people to identify what they are looking at. Following eye gaze, or gaze-following, is an important ability that allows us to understand what other people are thinking, the actions they are performing, and even predict what they might do next. Despite the importance of this topic, this problem has only been studied in limited scenarios within the computer vision community. In this paper, we propose a deep neural networkbased approach for gaze-following and a new benchmark dataset, GazeFollow, for thorough evaluation. Given an image and the location of a head, our approach follows the gaze of the person and identifies the object being looked at. Our deep network is able to discover how to extract head pose and gaze orientation, and to select objects in the scene that are in the predicted line of sight and likely to be looked at (such as televisions, balls and food). The quantitative evaluation shows that our approach produces reliable results, even when viewing only the back of the head. While our method outperforms several baseline approaches, we are still far from reaching human performance on this task. Overall, we believe that gazefollowing is a challenging and important problem that deserves more attention from the community.",
  "stance": 0.1
 },
 {
  "url": "https://papers.neurips.cc/paper/2015/file/f770b62bc8f42a0b66751fe636fc6eb0-Paper.pdf",
  "title": "Monotone k-Submodular Function Maximization with Size Constraints",
  "year": 2015,
  "venue": "NeurIPS",
  "abstract": "A k-submodular function is a generalization of a submodular function, where the input consists of k disjoint subsets, instead of a single subset, of the domain. Many machine learning problems, including influence maximization with k kinds of topics and sensor placement with k kinds of sensors, can be naturally modeled as the problem of maximizing monotone k-submodular functions. In this paper, we give constant-factor approximation algorithms for maximizing monotone ksubmodular functions subject to several size constraints. The running time of our algorithms are almost linear in the domain size. We experimentally demonstrate that our algorithms outperform baseline algorithms in terms of the solution quality.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/01eee509ee2f68dc6014898c309e86bf-Paper.pdf",
  "title": "Tagger: Deep Unsupervised Perceptual Grouping",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "We present a framework for efficient perceptual inference that explicitly reasons about the segmentation of its inputs and features. Rather than being trained for any specific segmentation, our framework learns the grouping process in an unsupervised manner or alongside any supervised task. We enable a neural network to group the representations of different objects in an iterative manner through a differentiable mechanism. We achieve very fast convergence by allowing the system to amortize the joint iterative inference of the groupings and their representations. In contrast to many other recently proposed methods for addressing multi-object scenes, our system does not assume the inputs to be images and can therefore directly handle other modalities. We evaluate our method on multi-digit classification of very cluttered images that require texture segmentation. Remarkably our method achieves improved classification performance over convolutional networks despite being fully connected, by making use of the grouping mechanism. Furthermore, we observe that our system greatly improves upon the semi-supervised result of a baseline Ladder network on our dataset. These results are evidence that grouping is a powerful tool that can help to improve sample efficiency.",
  "stance": 0.7
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/07cdfd23373b17c6b337251c22b7ea57-Paper.pdf",
  "title": "Budgeted stream-based active learning via adaptive submodular maximization",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "Active learning enables us to reduce the annotation cost by adaptively selecting unlabeled instances to be labeled. For pool-based active learning, several effective methods with theoretical guarantees have been developed through maximizing some utility function satisfying adaptive submodularity. In contrast, there have been few methods for stream-based active learning based on adaptive submodularity. In this paper, we propose a new class of utility functions, policy-adaptive submodular functions, which includes many existing adaptive submodular functions appearing in real world problems. We provide a general framework based on policy-adaptive submodularity that makes it possible to convert existing poolbased methods to stream-based methods and give theoretical guarantees on their performance. In addition we empirically demonstrate their effectiveness by comparing with existing heuristics on common benchmark datasets.",
  "stance": 0.7
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/0d73a25092e5c1c9769a9f3255caa65a-Paper.pdf",
  "title": "Maximal Sparsity with Deep Networks?",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "The iterations of many sparse estimation algorithms are comprised of a fixed linear filter cascaded with a thresholding nonlinearity, which collectively resemble a typical neural network layer. Consequently, a lengthy sequence of algorithm iterations can be viewed as a deep network with shared, hand-crafted layer weights. It is therefore quite natural to examine the degree to which a learned network model might act as a viable surrogate for traditional sparse estimation in domains where ample training data is available. While the possibility of a reduced computational budget is readily apparent when a ceiling is imposed on the number of layers, our work primarily focuses on estimation accuracy. In particular, it is well-known that when a signal dictionary has coherent columns, as quantified by a large RIP constant, then most tractable iterative algorithms are unable to find maximally sparse representations. In contrast, we demonstrate both theoretically and empirically the potential for a trained deep network to recover minimal `0-norm representations in regimes where existing methods fail. The resulting system, which can effectively learn novel iterative sparse estimation algorithms, is deployed on a practical photometric stereo estimation problem, where the goal is to remove sparse outliers that can disrupt the estimation of surface normals from a 3D scene.",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/0ed9422357395a0d4879191c66f4faa2-Paper.pdf",
  "title": "Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "In this paper, we propose a very deep fully convolutional encoding-decoding framework for image restoration such as denoising and super-resolution. The network is composed of multiple layers of convolution and deconvolution operators, learning end-to-end mappings from corrupted images to the original ones. The convolutional layers act as the feature extractor, which capture the abstraction of image contents while eliminating noises/corruptions. Deconvolutional layers are then used to recover the image details. We propose to symmetrically link convolutional and deconvolutional layers with skip-layer connections, with which the training converges much faster and attains a higher-quality local optimum. First, the skip connections allow the signal to be back-propagated to bottom layers directly, and thus tackles the problem of gradient vanishing, making training deep networks easier and achieving restoration performance gains consequently. Second, these skip connections pass image details from convolutional layers to deconvolutional layers, which is beneficial in recovering the original image. Significantly, with the large capacity, we can handle different levels of noises using a single model. Experimental results show that our network achieves better performance than recent state-of-the-art methods.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/140f6969d5213fd0ece03148e62e461e-Paper.pdf",
  "title": "Learning from Small Sample Sets by Combining Unsupervised Meta-Training with CNNs",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "This work explores CNNs for the recognition of novel categories from few examples. Inspired by the transferability properties of CNNs, we introduce an additional unsupervised meta-training stage that exposes multiple top layer units to a large amount of unlabeled real-world images. By encouraging these units to learn diverse sets of low-density separators across the unlabeled data, we capture a more generic, richer description of the visual world, which decouples these units from ties to a specific set of categories. We propose an unsupervised margin maximization that jointly estimates compact high-density regions and infers low-density separators. The low-density separator (LDS) modules can be plugged into any or all of the top layers of a standard CNN architecture. The resulting CNNs significantly improve the performance in scene classification, fine-grained recognition, and action recognition with small training samples.",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/14d9e8007c9b41f57891c48e07c23f57-Paper.pdf",
  "title": "Learning under uncertainty: a comparison between R-W and Bayesian approach",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "Accurately differentiating between what are truly unpredictably random and systematic changes that occur at random can have profound effect on affect and cognition. To examine the underlying computational principles that guide different learning behavior in an uncertain environment, we compared an R-W model and a Bayesian approach in a visual search task with different volatility levels. Both R-W model and the Bayesian approach reflected an individual’s estimation of the environmental volatility, and there is a strong correlation between the learning rate in R-W model and the belief of stationarity in the Bayesian approach in different volatility conditions. In a low volatility condition, R-W model indicates that learning rate positively correlates with lose-shift rate, but not choice optimality (inverted U shape). The Bayesian approach indicates that the belief of environmental stationarity positively correlates with choice optimality, but not lose-shift rate (inverted U shape). In addition, we showed that comparing to Expert learners, individuals with high lose-shift rate (sub-optimal learners) had significantly higher learning rate estimated from R-W model and lower belief of stationarity from the Bayesian model.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/1abb1e1ea5f481b589da52303b091cbb-Paper.pdf",
  "title": "A Probabilistic Model of Social Decision Making based on Reward Maximization",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "A fundamental problem in cognitive neuroscience is how humans make decisions, act, and behave in relation to other humans. Here we adopt the hypothesis that when we are in an interactive social setting, our brains perform Bayesian inference of the intentions and cooperativeness of others using probabilistic representations. We employ the framework of partially observable Markov decision processes (POMDPs) to model human decision making in a social context, focusing specifically on the volunteer’s dilemma in a version of the classic Public Goods Game. We show that the POMDP model explains both the behavior of subjects as well as neural activity recorded using fMRI during the game. The decisions of subjects can be modeled across all trials using two interpretable parameters. Furthermore, the expected reward predicted by the model for each subject was correlated with the activation of brain areas related to reward expectation in social interactions. Our results suggest a probabilistic basis for human social decision making within the framework of expected reward maximization.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/43351f7bf9a215be70c2c2caa7555002-Paper.pdf",
  "title": "Dual Space Gradient Descent for Online Learning",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "One crucial goal in kernel online learning is to bound the model size. Common approaches employ budget maintenance procedures to restrict the model sizes using removal, projection, or merging strategies. Although projection and merging, in the literature, are known to be the most effective strategies, they demand extensive computation whilst removal strategy fails to retain information of the removed vectors. An alternative way to address the model size problem is to apply random features to approximate the kernel function. This allows the model to be maintained directly in the random feature space, hence effectively resolve the curse of kernelization. However, this approach still suffers from a serious shortcoming as it needs to use a high dimensional random feature space to achieve a sufficiently accurate kernel approximation. Consequently, it leads to a significant increase in the computational cost. To address all of these aforementioned challenges, we present in this paper the Dual Space Gradient Descent (DualSGD), a novel framework that utilizes random features as an auxiliary space to maintain information from data points removed during budget maintenance. Consequently, our approach permits the budget to be maintained in a simple, direct and elegant way while simultaneously mitigating the impact of the dimensionality issue on learning performance. We further provide convergence analysis and extensively conduct experiments on five real-world datasets to demonstrate the predictive performance and scalability of our proposed method in comparison with the state-of-the-art baselines.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/44f683a84163b3523afe57c2e008bc8c-Paper.pdf",
  "title": "Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "We study the problem of 3D object generation. We propose a novel framework, namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets. The benefits of our model are three-fold: first, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3D objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of 3D objects, so that we can sample objects without a reference image or CAD models, and explore the 3D object manifold; third, the adversarial discriminator provides a powerful 3D shape descriptor which, learned without supervision, has wide applications in 3D object recognition. Experiments demonstrate that our method generates high-quality 3D objects, and our unsupervisedly learned features achieve impressive performance on 3D object recognition, comparable with those of supervised learning methods.",
  "stance": 0.6
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/49d4b2faeb4b7b9e745775793141e2b2-Paper.pdf",
  "title": "An Architecture for Deep, Hierarchical Generative Models",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "We present an architecture which lets us train deep, directed generative models with many layers of latent variables. We include deterministic paths between all latent variables and the generated output, and provide a richer set of connections between computations for inference and generation, which enables more effective communication of information throughout the model during training. To improve performance on natural images, we incorporate a lightweight autoregressive model in the reconstruction distribution. These techniques permit end-to-end training of models with 10+ layers of latent variables. Experiments show that our approach achieves state-of-the-art performance on standard image modelling benchmarks, can expose latent class structure in the absence of label information, and can provide convincing imputations of occluded regions in natural images.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/5d6646aad9bcc0be55b2c82f69750387-Paper.pdf",
  "title": "Stochastic Three-Composite Convex Minimization",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "We propose a stochastic optimization method for the minimization of the sum of three convex functions, one of which has Lipschitz continuous gradient as well as restricted strong convexity. Our approach is most suitable in the setting where it is computationally advantageous to process smooth term in the decomposition with its stochastic gradient estimate and the other two functions separately with their proximal operators, such as doubly regularized empirical risk minimization problems. We prove the convergence characterization of the proposed algorithm in expectation under the standard assumptions for the stochastic gradient estimate of the smooth term. Our method operates in the primal space and can be considered as a stochastic extension of the three-operator splitting method. Numerical evidence supports the effectiveness of our method in real-world problems.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/632cee946db83e7a52ce5e8d6f0fed35-Paper.pdf",
  "title": "Large Margin Discriminant Dimensionality Reduction in Prediction Space",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "In this paper we establish a duality between boosting and SVM, and use this to derive a novel discriminant dimensionality reduction algorithm. In particular, using the multiclass formulation of boosting and SVM we note that both use a combination of mapping and linear classification to maximize the multiclass margin. In SVM this is implemented using a pre-defined mapping (induced by the kernel) and optimizing the linear classifiers. In boosting the linear classifiers are pre-defined and the mapping (predictor) is learned through a combination of weak learners. We argue that the intermediate mapping, i.e. boosting predictor, is preserving the discriminant aspects of the data and that by controlling the dimension of this mapping it is possible to obtain discriminant low dimensional representations for the data. We use the aforementioned duality and propose a new method, Large Margin Discriminant Dimensionality Reduction (LADDER) that jointly learns the mapping and the linear classifiers in an efficient manner. This leads to a data-driven mapping which can embed data into any number of dimensions. Experimental results show that this embedding can significantly improve performance on tasks such as hashing and image/scene classification.",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/65ded5353c5ee48d0b7d48c591b8f430-Paper.pdf",
  "title": "SURGE: Surface Regularized Geometry Estimation from a Single Image",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "This paper introduces an approach to regularize 2.5D surface normal and depth predictions at each pixel given a single input image. The approach infers and reasons about the underlying 3D planar surfaces depicted in the image to snap predicted normals and depths to inferred planar surfaces, all while maintaining fine detail within objects. Our approach comprises two components: (i) a fourstream convolutional neural network (CNN) where depths, surface normals, and likelihoods of planar region and planar boundary are predicted at each pixel, followed by (ii) a dense conditional random field (DCRF) that integrates the four predictions such that the normals and depths are compatible with each other and regularized by the planar region and planar boundary information. The DCRF is formulated such that gradients can be passed to the surface normal and depth CNNs via backpropagation. In addition, we propose new planar-wise metrics to evaluate geometry consistency within planar surfaces, which are more tightly related to dependent 3D editing applications. We show that our regularization yields a 30% relative improvement in planar consistency on the NYU v2 dataset [24].",
  "stance": 0.7
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/6e7d2da6d3953058db75714ac400b584-Paper.pdf",
  "title": "Learning the Number of Neurons in Deep Networks",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "Nowadays, the number of layers and of neurons in each layer of a deep network are typically set manually. While very deep and wide networks have proven effective in general, they come at a high memory and computation cost, thus making them impractical for constrained platforms. These networks, however, are known to have many redundant parameters, and could thus, in principle, be replaced by more compact architectures. In this paper, we introduce an approach to automatically determining the number of neurons in each layer of a deep network during learning. To this end, we propose to make use of a group sparsity regularizer on the parameters of the network, where each group is defined to act on a single neuron. Starting from an overcomplete network, we show that our approach can reduce the number of parameters by up to 80% while retaining or even improving the network accuracy.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/bb7946e7d85c81a9e69fee1cea4a087c-Paper.pdf",
  "title": "Error Analysis of Generalized Nyström Kernel Regression",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "Nyström method has been successfully used to improve the computational efficiency of kernel ridge regression (KRR). Recently, theoretical analysis of Nyström KRR, including generalization bound and convergence rate, has been established based on reproducing kernel Hilbert space (RKHS) associated with the symmetric positive semi-definite kernel. However, in real world applications, RKHS is not always optimal and kernel function is not necessary to be symmetric or positive semi-definite. In this paper, we consider the generalized Nyström kernel regression (GNKR) with `2 coefficient regularization, where the kernel just requires the continuity and boundedness. Error analysis is provided to characterize its generalization performance and the column norm sampling strategy is introduced to construct the refined hypothesis space. In particular, the fast learning rate with polynomial decay is reached for the GNKR. Experimental analysis demonstrates the satisfactory performance of GNKR with the column norm sampling.",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/c4492cbe90fbdbf88a5aec486aa81ed5-Paper.pdf",
  "title": "Strategic Attentive Writer for Learning Macro-Actions",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "We present a novel deep recurrent neural network architecture that learns to build implicit plans in an end-to-end manner purely by interacting with an environment in reinforcement learning setting. The network builds an internal plan, which is continuously updated upon observation of the next input from the environment. It can also partition this internal representation into contiguous sub-sequences by learning for how long the plan can be committed to – i.e. followed without replaning. Combining these properties, the proposed model, dubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally abstracted macro-actions of varying lengths that are solely learnt from data without any prior information. These macro-actions enable both structured exploration and economic computation. We experimentally demonstrate that STRAW delivers strong improvements on several ATARI games by employing temporally extended planning strategies (e.g. Ms. Pacman and Frostbite). It is at the same time a general algorithm that can be applied on any sequence data. To that end, we also show that when trained on text prediction task, STRAW naturally predicts frequent n-grams (instead of macro-actions), demonstrating the generality of the approach.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/dc4c44f624d600aa568390f1f1104aa0-Paper.pdf",
  "title": "Satisfying Real-world Goals with Dataset Constraints",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "The goal of minimizing misclassification error on a training set is often just one of several real-world goals that might be defined on different datasets. For example, one may require a classifier to also make positive predictions at some specified rate for some subpopulation (fairness), or to achieve a specified empirical recall. Other real-world goals include reducing churn with respect to a previously deployed model, or stabilizing online training. In this paper we propose handling multiple goals on multiple datasets by training with dataset constraints, using the ramp penalty to accurately quantify costs, and present an efficient algorithm to approximately optimize the resulting non-convex constrained optimization problem. Experiments on both benchmark and real-world industry datasets demonstrate the effectiveness of our approach.",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/dc5c768b5dc76a084531934b34601977-Paper.pdf",
  "title": "Launch and Iterate: Reducing Prediction Churn",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "Practical applications of machine learning often involve successive training iterations with changes to features and training examples. Ideally, changes in the output of any new model should only be improvements (wins) over the previous iteration, but in practice the predictions may change neutrally for many examples, resulting in extra net-zero wins and losses, referred to as unnecessary churn. These changes in the predictions are problematic for usability for some applications, and make it harder and more expensive to measure if a change is statistically significant positive. In this paper, we formulate the problem and present a stabilization operator to regularize a classifier towards a previous classifier. We use a Markov chain Monte Carlo stabilization operator to produce a model with more consistent predictions without adversely affecting accuracy. We investigate the properties of the proposal with theoretical analysis. Experiments on benchmark datasets for different classification algorithms demonstrate the method and the resulting reduction in churn.",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/e2a2dcc36a08a345332c751b2f2e476c-Paper.pdf",
  "title": "Learning Treewidth-Bounded Bayesian Networks with Thousands of Variables",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "We present a method for learning treewidth-bounded Bayesian networks from data sets containing thousands of variables. Bounding the treewidth of a Bayesian network greatly reduces the complexity of inferences. Yet, being a global property of the graph, it considerably increases the difficulty of the learning process. Our novel algorithm accomplishes this task, scaling both to large domains and to large treewidths. Our novel approach consistently outperforms the state of the art on experiments with up to thousands of variables.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/f0adc8838f4bdedde4ec2cfad0515589-Paper.pdf",
  "title": "Adaptive Neural Compilation",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "This paper proposes an adaptive neural-compilation framework to address the problem of learning efficient programs. Traditional code optimisation strategies used in compilers are based on applying pre-specified set of transformations that make the code faster to execute without changing its semantics. In contrast, our work involves adapting programs to make them more efficient while considering correctness only on a target input distribution. Our approach is inspired by the recent works on differentiable representations of programs. We show that it is possible to compile programs written in a low-level language to a differentiable representation. We also show how programs in this representation can be optimised to make them efficient on a target input distribution. Experimental results demonstrate that our approach enables learning specifically-tuned algorithms for given data distributions with a high success rate.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/1680e9fa7b4dd5d62ece800239bb53bd-Paper.pdf",
  "title": "Efficient Modeling of Latent Information in Supervised Learning using Gaussian Processes",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "Often in machine learning, data are collected as a combination of multiple conditions, e.g., the voice recordings of multiple persons, each labeled with an ID. How could we build a model that captures the latent information related to these conditions and generalize to a new one with few data? We present a new model called Latent Variable Multiple Output Gaussian Processes (LVMOGP) that allows to jointly model multiple conditions for regression and generalize to a new condition with a few data points at test time. LVMOGP infers the posteriors of Gaussian processes together with a latent space representing the information about different conditions. We derive an efficient variational inference method for LVMOGP for which the computational complexity is as low as sparse Gaussian processes. We show that LVMOGP significantly outperforms related Gaussian process methods on various tasks with both synthetic and real data.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/253614bbac999b38b5b60cae531c4969-Paper.pdf",
  "title": "Collecting Telemetry Data Privately",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "The collection and analysis of telemetry data from user’s devices is routinely performed by many software companies. Telemetry collection leads to improved user experience but poses significant risks to users’ privacy. Locally differentially private (LDP) algorithms have recently emerged as the main tool that allows data collectors to estimate various population statistics, while preserving privacy. The guarantees provided by such algorithms are typically very strong for a single round of telemetry collection, but degrade rapidly when telemetry is collected regularly. In particular, existing LDP algorithms are not suitable for repeated collection of counter data such as daily app usage statistics. In this paper, we develop new LDP mechanisms geared towards repeated collection of counter data, with formal privacy guarantees even after being executed for an arbitrarily long period of time. For two basic analytical tasks, mean estimation and histogram estimation, our LDP mechanisms for repeated data collection provide estimates with comparable or even the same accuracy as existing single-round LDP collection mechanisms. We conduct empirical evaluation on real-world counter datasets to verify our theoretical results. Our mechanisms have been deployed by Microsoft to collect telemetry across millions of devices.",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/2650d6089a6d640c5e85b2b88265dc2b-Paper.pdf",
  "title": "What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model – uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/2cd4e8a2ce081c3d7c32c3cde4312ef7-Paper.pdf",
  "title": "InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "The goal of imitation learning is to mimic expert behavior without access to an explicit reward signal. Expert demonstrations provided by humans, however, often show significant variability due to latent factors that are typically not explicitly modeled. In this paper, we propose a new algorithm that can infer the latent structure of expert demonstrations in an unsupervised way. Our method, built on top of Generative Adversarial Imitation Learning, can not only imitate complex behaviors, but also learn interpretable and meaningful representations of complex behavioral data, including visual demonstrations. In the driving domain, we show that a model learned from human demonstrations is able to both accurately reproduce a variety of behaviors and accurately anticipate human actions using raw visual inputs. Compared with various baselines, our method can better capture the latent structure underlying expert demonstrations, often recovering semantically meaningful factors of variation in the data.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/3fb451ca2e89b3a13095b059d8705b15-Paper.pdf",
  "title": "ELF: An Extensive, Lightweight and Flexible Research Platform for Real-time Strategy Games",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "In this paper, we propose ELF, an Extensive, Lightweight and Flexible platform for fundamental reinforcement learning research. Using ELF, we implement a highly customizable real-time strategy (RTS) engine with three game environments (Mini-RTS, Capture the Flag and Tower Defense). Mini-RTS, as a miniature version of StarCraft, captures key game dynamics and runs at 40K frameper-second (FPS) per core on a laptop. When coupled with modern reinforcement learning methods, the system can train a full-game bot against built-in AIs endto-end in one day with 6 CPUs and 1 GPU. In addition, our platform is flexible in terms of environment-agent communication topologies, choices of RL methods, changes in game parameters, and can host existing C/C++-based game environments like ALE [4]. Using ELF, we thoroughly explore training parameters and show that a network with Leaky ReLU [17] and Batch Normalization [11] coupled with long-horizon training and progressive curriculum beats the rule-based built-in AI more than 70% of the time in the full game of Mini-RTS. Strong performance is also achieved on the other two games. In game replays, we show our agents learn interesting strategies. ELF, along with its RL platform, is open sourced at https://github.com/facebookresearch/ELF.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/643de7cf7ba769c7466ccbc4adfd7fac-Paper.pdf",
  "title": "Acceleration and Averaging in Stochastic Descent Dynamics",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "We formulate and study a general family of (continuous-time) stochastic dynamics for accelerated first-order minimization of smooth convex functions. Building on an averaging formulation of accelerated mirror descent, we propose a stochastic variant in which the gradient is contaminated by noise, and study the resulting stochastic differential equation. We prove a bound on the rate of change of an energy function associated with the problem, then use it to derive estimates of convergence rates of the function values (almost surely and in expectation), both for persistent and asymptotically vanishing noise. We discuss the interaction between the parameters of the dynamics (learning rate and averaging rates) and the covariation of the noise process. In particular, we show how the asymptotic rate of covariation affects the choice of parameters and, ultimately, the convergence rate.",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/7cc234202e98d2722580858573fd0817-Paper.pdf",
  "title": "Thy Friend is My Friend: Iterative Collaborative Filtering for Sparse Matrix Estimation",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "The sparse matrix estimation problem consists of estimating the distribution of an n× n matrix Y , from a sparsely observed single instance of this matrix where the entries of Y are independent random variables. This captures a wide array of problems; special instances include matrix completion in the context of recommendation systems, graphon estimation, and community detection in (mixed membership) stochastic block models. Inspired by classical collaborative filtering for recommendation systems, we propose a novel iterative, collaborative filteringstyle algorithm for matrix estimation in this generic setting. We show that the mean squared error (MSE) of our estimator converges to 0 at the rate of O(d2(pn)−2/5) as long as ω(dn) random entries from a total of n entries of Y are observed (uniformly sampled), E[Y ] has rank d, and the entries of Y have bounded support. The maximum squared error across all entries converges to 0 with high probability as long as we observe a little more, Ω(dn ln(n)) entries. Our results are the best known sample complexity results in this generality.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/81b3833e2504647f9d794f7d7b9bf341-Paper.pdf",
  "title": "The Marginal Value of Adaptive Gradient Methods in Machine Learning",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "Adaptive optimization methods, which perform local optimization with a metric constructed from the history of iterates, are becoming increasingly popular for training deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We show that for simple overparameterized problems, adaptive methods often find drastically different solutions than gradient descent (GD) or stochastic gradient descent (SGD). We construct an illustrative binary classification problem where the data is linearly separable, GD and SGD achieve zero test error, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to half. We additionally study the empirical generalization capability of adaptive methods on several stateof-the-art deep learning models. We observe that the solutions found by adaptive methods generalize worse (often significantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.",
  "stance": -1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/8597a6cfa74defcbde3047c891d78f90-Paper.pdf",
  "title": "Learned D-AMP: Principled Neural Network based Compressive Image Recovery",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "Compressive image recovery is a challenging problem that requires fast and accurate algorithms. Recently, neural networks have been applied to this problem with promising results. By exploiting massively parallel GPU processing architectures and oodles of training data, they can run orders of magnitude faster than existing techniques. However, these methods are largely unprincipled black boxes that are difficult to train and often-times specific to a single measurement matrix. It was recently demonstrated that iterative sparse-signal-recovery algorithms can be “unrolled” to form interpretable deep networks. Taking inspiration from this work, we develop a novel neural network architecture that mimics the behavior of the denoising-based approximate message passing (D-AMP) algorithm. We call this new network Learned D-AMP (LDAMP). The LDAMP network is easy to train, can be applied to a variety of different measurement matrices, and comes with a state-evolution heuristic that accurately predicts its performance. Most importantly, it outperforms the state-of-the-art BM3D-AMP and NLR-CS algorithms in terms of both accuracy and run time. At high resolutions, and when used with sensing matrices that have fast implementations, LDAMP runs over 50× faster than BM3D-AMP and hundreds of times faster than NLR-CS.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/8edd72158ccd2a879f79cb2538568fdc-Paper.pdf",
  "title": "Associative Embedding: End-to-End Learning for Joint Detection and Grouping",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "We introduce associative embedding, a novel method for supervising convolutional neural networks for the task of detection and grouping. A number of computer vision problems can be framed in this manner including multi-person pose estimation, instance segmentation, and multi-object tracking. Usually the grouping of detections is achieved with multi-stage pipelines, instead we propose an approach that teaches a network to simultaneously output detections and group assignments. This technique can be easily integrated into any state-of-the-art network architecture that produces pixel-wise predictions. We show how to apply this method to multi-person pose estimation and report state-of-the-art performance on the MPII and MS-COCO datasets.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/934815ad542a4a7c5e8a2dfa04fea9f5-Paper.pdf",
  "title": "Nearest-Neighbor Sample Compression: Efficiency, Consistency, Infinite Dimensions",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "We examine the Bayes-consistency of a recently proposed 1-nearest-neighbor-based multiclass learning algorithm. This algorithm is derived from sample compression bounds and enjoys the statistical advantages of tight, fully empirical generalization bounds, as well as the algorithmic advantages of a faster runtime and memory savings. We prove that this algorithm is strongly Bayes-consistent in metric spaces with finite doubling dimension — the first consistency result for an efficient nearest-neighbor sample compression scheme. Rather surprisingly, we discover that this algorithm continues to be Bayes-consistent even in a certain infinitedimensional setting, in which the basic measure-theoretic conditions on which classic consistency proofs hinge are violated. This is all the more surprising, since it is known that k-NN is not Bayes-consistent in this setting. We pose several challenging open problems for future research.",
  "stance": -0.3
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/a8baa56554f96369ab93e4f3bb068c22-Paper.pdf",
  "title": "Label Efficient Learning of Transferable Representations acrosss Domains and Tasks",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "We propose a framework that learns a representation transferable across different domains and tasks in a label efficient manner. Our approach battles domain shift with a domain adversarial loss, and generalizes the embedding to novel task using a metric learning-based approach. Our model is simultaneously optimized on labeled source data and unlabeled or sparsely labeled data in the target domain. Our method shows compelling results on novel classes within a new domain even when only a few labeled examples per class are available, outperforming the prevalent fine-tuning approach. In addition, we demonstrate the effectiveness of our framework on the transfer learning task from image object recognition to video action recognition.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/bdc4626aa1d1df8e14d80d345b2a442d-Paper.pdf",
  "title": "Conservative Contextual Linear Bandits",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "Safety is a desirable property that can immensely increase the applicability of learning algorithms in real-world decision-making problems. It is much easier for a company to deploy an algorithm that is safe, i.e., guaranteed to perform at least as well as a baseline. In this paper, we study the issue of safety in contextual linear bandits that have application in many different fields including personalized recommendation. We formulate a notion of safety for this class of algorithms. We develop a safe contextual linear bandit algorithm, called conservative linear UCB (CLUCB), that simultaneously minimizes its regret and satisfies the safety constraint, i.e., maintains its performance above a fixed percentage of the performance of a baseline strategy, uniformly over time. We prove an upper-bound on the regret of CLUCB and show that it can be decomposed into two terms: 1) an upper-bound for the regret of the standard linear UCB algorithm that grows with the time horizon and 2) a constant term that accounts for the loss of being conservative in order to satisfy the safety constraint. We empirically show that our algorithm is safe and validate our theoretical analysis.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/bf424cb7b0dea050a42b9739eb261a3a-Paper.pdf",
  "title": "Diving into the shallows: a computational perspective on large-scale shallow learning",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "Remarkable recent success of deep neural networks has not been easy to analyze theoretically. It has been particularly hard to disentangle relative significance of architecture and optimization in achieving accurate classification on large datasets. On the flip side, shallow methods (such as kernel methods) have encountered obstacles in scaling to large data, despite excellent performance on smaller datasets, and extensive theoretical analysis. Practical methods, such as variants of gradient descent used so successfully in deep learning, seem to perform below par when applied to kernel methods. This difficulty has sometimes been attributed to the limitations of shallow architecture. In this paper we identify a basic limitation in gradient descent-based optimization methods when used in conjunctions with smooth kernels. Our analysis demonstrates that only a vanishingly small fraction of the function space is reachable after a polynomial number of gradient descent iterations. That drastically limits the approximating power of gradient descent leading to over-regularization. The issue is purely algorithmic, persisting even in the limit of infinite data. To address this shortcoming in practice, we introduce EigenPro iteration, a simple and direct preconditioning scheme using a small number of approximately computed eigenvectors. It can also be viewed as learning a kernel optimized for gradient descent. Injecting this small, computationally inexpensive and SGD-compatible, amount of approximate second-order information leads to major improvements in convergence. For large data, this leads to a significant performance boost over the state-of-the-art kernel methods. In particular, we are able to match or improve the results reported in the literature at a small fraction of their computational budget. For complete version of this paper see https://arxiv.org/abs/1703.10622.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/c22abfa379f38b5b0411bc11fa9bf92f-Paper.pdf",
  "title": "Learning Affinity via Spatial Propagation Networks",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "In this paper, we propose spatial propagation networks for learning the affinity matrix for vision tasks. We show that by constructing a row/column linear propagation model, the spatially varying transformation matrix exactly constitutes an affinity matrix that models dense, global pairwise relationships of an image. Specifically, we develop a three-way connection for the linear propagation model, which (a) formulates a sparse transformation matrix, where all elements can be outputs from a deep CNN, but (b) results in a dense affinity matrix that effectively models any task-specific pairwise similarity matrix. Instead of designing the similarity kernels according to image features of two points, we can directly output all the similarities in a purely data-driven manner. The spatial propagation network is a generic framework that can be applied to many affinity-related tasks, such as image matting, segmentation and colorization, to name a few. Essentially, the model can learn semantically-aware affinity values for high-level vision tasks due to the powerful learning capability of deep CNNs. We validate the framework on the task of refinement of image segmentation boundaries. Experiments on the HELEN face parsing and PASCAL VOC-2012 semantic segmentation tasks show that the spatial propagation network provides a general, effective and efficient solution for generating high-quality segmentation results.",
  "stance": 0.9
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/c59b469d724f7919b7d35514184fdc0f-Paper.pdf",
  "title": "Deep Voice 2: Multi-Speaker Neural Text-to-Speech",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "We introduce a technique for augmenting neural text-to-speech (TTS) with lowdimensional trainable speaker embeddings to generate different voices from a single model. As a starting point, we show improvements over the two state-ofthe-art approaches for single-speaker neural TTS: Deep Voice 1 and Tacotron. We introduce Deep Voice 2, which is based on a similar pipeline with Deep Voice 1, but constructed with higher performance building blocks and demonstrates a significant audio quality improvement over Deep Voice 1. We improve Tacotron by introducing a post-processing neural vocoder, and demonstrate a significant audio quality improvement. We then demonstrate our technique for multi-speaker speech synthesis for both Deep Voice 2 and Tacotron on two multi-speaker TTS datasets. We show that a single neural TTS system can learn hundreds of unique voices from less than half an hour of data per speaker, while achieving high audio quality synthesis and preserving the speaker identities almost perfectly.",
  "stance": 0.9
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/c6036a69be21cb660499b75718a3ef24-Paper.pdf",
  "title": "Deliberation Networks: Sequence Generation Beyond One-Pass Decoding",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "The encoder-decoder framework has achieved promising progress for many sequence generation tasks, including machine translation, text summarization, dialog system, image captioning, etc. Such a framework adopts an one-pass forward process while decoding and generating a sequence, but lacks the deliberation process: A generated sequence is directly used as final output without further polishing. However, deliberation is a common behavior in human’s daily life like reading news and writing papers/articles/books. In this work, we introduce the deliberation process into the encoder-decoder framework and propose deliberation networks for sequence generation. A deliberation network has two levels of decoders, where the first-pass decoder generates a raw sequence and the second-pass decoder polishes and refines the raw sentence with deliberation. Since the second-pass deliberation decoder has global information about what the sequence to be generated might be, it has the potential to generate a better sequence by looking into future words in the raw sentence. Experiments on neural machine translation and text summarization demonstrate the effectiveness of the proposed deliberation networks. On the WMT 2014 English-to-French translation task, our model establishes a new state-of-the-art BLEU score of 41.5.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/c7558e9d1f956b016d1fdba7ea132378-Paper.pdf",
  "title": "Extracting low-dimensional dynamics from multiple large-scale neural population recordings by learning to predict correlations",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "A powerful approach for understanding neural population dynamics is to extract low-dimensional trajectories from population recordings using dimensionality reduction methods. Current approaches for dimensionality reduction on neural data are limited to single population recordings, and can not identify dynamics embedded across multiple measurements. We propose an approach for extracting low-dimensional dynamics from multiple, sequential recordings. Our algorithm scales to data comprising millions of observed dimensions, making it possible to access dynamics distributed across large populations or multiple brain areas. Building on subspace-identification approaches for dynamical systems, we perform parameter estimation by minimizing a moment-matching objective using a scalable stochastic gradient descent algorithm: The model is optimized to predict temporal covariations across neurons and across time. We show how this approach naturally handles missing data and multiple partial recordings, and can identify dynamics and predict correlations even in the presence of severe subsampling and small overlap between recordings. We demonstrate the effectiveness of the approach both on simulated data and a whole-brain larval zebrafish imaging dataset.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/dab49080d80c724aad5ebf158d63df41-Paper.pdf",
  "title": "Structured Bayesian Pruning via Log-Normal Multiplicative Noise",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "Dropout-based regularization methods can be regarded as injecting random noise with pre-defined magnitude to different parts of the neural network during training. It was recently shown that Bayesian dropout procedure not only improves generalization but also leads to extremely sparse neural architectures by automatically setting the individual noise magnitude per weight. However, this sparsity can hardly be used for acceleration since it is unstructured. In the paper, we propose a new Bayesian model that takes into account the computational structure of neural networks and provides structured sparsity, e.g. removes neurons and/or convolutional channels in CNNs. To do this we inject noise to the neurons outputs while keeping the weights unregularized. We establish the probabilistic model with a proper truncated log-uniform prior over the noise and truncated log-normal variational approximation that ensures that the KL-term in the evidence lower bound is computed in closed-form. The model leads to structured sparsity by removing elements with a low SNR from the computation graph and provides significant acceleration on a number of deep neural architectures. The model is easy to implement as it can be formulated as a separate dropout-like layer.",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/efdf562ce2fb0ad460fd8e9d33e57f57-Paper.pdf",
  "title": "Reconstructing perceived faces from brain activations with deep adversarial neural decoding",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "Here, we present a novel approach to solve the problem of reconstructing perceived stimuli from brain responses by combining probabilistic inference with deep learning. Our approach first inverts the linear transformation from latent features to brain responses with maximum a posteriori estimation and then inverts the nonlinear transformation from perceived stimuli to latent features with adversarial training of convolutional neural networks. We test our approach with a functional magnetic resonance imaging experiment and show that it can generate state-of-the-art reconstructions of perceived faces from brain activations. ConvNet (pretrained) + PCA ConvNet (adversarial training) la te nt fe at . prior (Gaussian) maximum a posteriori likelihood (Gaussian) posterior (Gaussian) pe rc ei ve d st im . br ai n re sp . *reconstruction *from brain resp. Figure 1: An illustration of our approach to solve the problem of reconstructing perceived stimuli from brain responses by combining probabilistic inference with deep learning.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/f22e4747da1aa27e363d86d40ff442fe-Paper.pdf",
  "title": "Deep Sets",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "We study the problem of designing models for machine learning tasks defined on sets. In contrast to traditional approach of operating on fixed dimensional vectors, we consider objective functions defined on sets that are invariant to permutations. Such problems are widespread, ranging from estimation of population statistics [1], to anomaly detection in piezometer data of embankment dams [2], to cosmology [3, 4]. Our main theorem characterizes the permutation invariant functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We also derive the necessary and sufficient conditions for permutation equivariance in deep models. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/f708f064faaf32a43e4d3c784e6af9ea-Paper.pdf",
  "title": "Overcoming Catastrophic Forgetting by Incremental Moment Matching",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "Catastrophic forgetting is a problem of neural networks that loses the information of the first task after training the second task. Here, we propose a method, i.e. incremental moment matching (IMM), to resolve this problem. IMM incrementally matches the moment of the posterior distribution of the neural network which is trained on the first and the second task, respectively. To make the search space of posterior parameter smooth, the IMM procedure is complemented by various transfer learning techniques including weight transfer, L2-norm of the old and the new parameter, and a variant of dropout with the old parameter. We analyze our approach on a variety of datasets including the MNIST, CIFAR-10, Caltech-UCSDBirds, and Lifelog datasets. The experimental results show that IMM achieves state-of-the-art performance by balancing the information between an old and a new network.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2018/file/01161aaa0b6d1345dd8fe4e481144d84-Paper.pdf",
  "title": "Self-Supervised Generation of Spatial Audio for 360° Video",
  "year": 2018,
  "venue": "NeurIPS",
  "abstract": "We introduce an approach to convert mono audio recorded by a 360◦ video camera into spatial audio, a representation of the distribution of sound over the full viewing sphere. Spatial audio is an important component of immersive 360◦ video viewing, but spatial audio microphones are still rare in current 360◦ video production. Our system consists of end-to-end trainable neural networks that separate individual sound sources and localize them on the viewing sphere, conditioned on multi-modal analysis of audio and 360◦ video frames. We introduce several datasets, including one filmed ourselves, and one collected in-the-wild from YouTube, consisting of 360◦ videos uploaded with spatial audio. During training, ground-truth spatial audio serves as self-supervision and a mixed down mono track forms the input to our network. Using our approach, we show that it is possible to infer the spatial location of sound sources based only on 360◦ video and a mono audio track.",
  "stance": 0.7
 },
 {
  "url": "https://papers.neurips.cc/paper/2018/file/09060616068d2b9544dc33f2fbe4ce2d-Paper.pdf",
  "title": "Unsupervised Learning of Artistic Styles with Archetypal Style Analysis",
  "year": 2018,
  "venue": "NeurIPS",
  "abstract": "In this paper, we introduce an unsupervised learning approach to automatically discover, summarize, and manipulate artistic styles from large collections of paintings. Our method is based on archetypal analysis, which is an unsupervised learning technique akin to sparse coding with a geometric interpretation. When applied to neural style representations from a collection of artworks, it learns a dictionary of archetypal styles, which can be easily visualized. After training the model, the style of a new image, which is characterized by local statistics of deep visual features, is approximated by a sparse convex combination of archetypes. This enables us to interpret which archetypal styles are present in the input image, and in which proportion. Finally, our approach allows us to manipulate the coefficients of the latent archetypal decomposition, and achieve various special effects such as style enhancement, transfer, and interpolation between multiple archetypes.",
  "stance": 0.6
 },
 {
  "url": "https://papers.neurips.cc/paper/2018/file/12b1e42dc0746f22cf361267de07073f-Paper.pdf",
  "title": "Semi-Supervised Learning with Declaratively Specified Entropy Constraints",
  "year": 2018,
  "venue": "NeurIPS",
  "abstract": "We propose a technique for declaratively specifying strategies for semi-supervised learning (SSL). SSL methods based on different assumptions perform differently on different tasks, which leads to difficulties applying them in practice. In this paper, we propose to use entropy to unify many types of constraints. Our method can be used to easily specify ensembles of semi-supervised learners, as well as agreement constraints and entropic regularization constraints between these learners, and can be used to model both well-known heuristics such as co-training, and novel domain-specific heuristics. Besides, our model is flexible as to the underlying learning mechanism. Compared to prior frameworks for specifying SSL techniques, our technique achieves consistent improvements on a suite of well-studied SSL benchmarks, and obtains a new state-of-the-art result on a difficult relation extraction task.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2018/file/1f36c15d6a3d18d52e8d493bc8187cb9-Paper.pdf",
  "title": "Unsupervised Learning of Object Landmarks through Conditional Image Generation",
  "year": 2018,
  "venue": "NeurIPS",
  "abstract": "We propose a method for learning landmark detectors for visual objects (such as the eyes and the nose in a face) without any manual supervision. We cast this as the problem of generating images that combine the appearance of the object as seen in a first example image with the geometry of the object as seen in a second example image, where the two examples differ by a viewpoint change and/or an object deformation. In order to factorize appearance and geometry, we introduce a tight bottleneck in the geometry-extraction process that selects and distils geometryrelated features. Compared to standard image generation problems, which often use generative adversarial networks, our generation task is conditioned on both appearance and geometry and thus is significantly less ambiguous, to the point that adopting a simple perceptual loss formulation is sufficient. We demonstrate that our approach can learn object landmarks from synthetic image deformations or videos, all without manual supervision, while outperforming state-of-the-art unsupervised landmark detectors. We further show that our method is applicable to a large variety of datasets — faces, people, 3D objects, and digits — without any modifications.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2018/file/201e5bacd665709851b77148e225b332-Paper.pdf",
  "title": "Simple, Distributed, and Accelerated Probabilistic Programming",
  "year": 2018,
  "venue": "NeurIPS",
  "abstract": "We describe a simple, low-level approach for embedding probabilistic programming in a deep learning ecosystem. In particular, we distill probabilistic programming down to a single abstraction—the random variable. Our lightweight implementation in TensorFlow enables numerous applications: a model-parallel variational auto-encoder (VAE) with 2nd-generation tensor processing units (TPUv2s); a data-parallel autoregressive model (Image Transformer) with TPUv2s; and multiGPU No-U-Turn Sampler (NUTS). For both a state-of-the-art VAE on 64x64 ImageNet and Image Transformer on 256x256 CelebA-HQ, our approach achieves an optimal linear speedup from 1 to 256 TPUv2 chips. With NUTS, we see a 100x speedup on GPUs over Stan and 37x over PyMC3.1",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2018/file/204da255aea2cd4a75ace6018fad6b4d-Paper.pdf",
  "title": "When do random forests fail?",
  "year": 2018,
  "venue": "NeurIPS",
  "abstract": "Random forests are learning algorithms that build large collections of random trees and make predictions by averaging the individual tree predictions. In this paper, we consider various tree constructions and examine how the choice of parameters affects the generalization error of the resulting random forests as the sample size goes to infinity. We show that subsampling of data points during the tree construction phase is important: Forests can become inconsistent with either no subsampling or too severe subsampling. As a consequence, even highly randomized trees can lead to inconsistent forests if no subsampling is used, which implies that some of the commonly used setups for random forests can be inconsistent. As a second consequence we can show that trees that have good performance in nearest-neighbor search can be a poor choice for random forests.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2018/file/22fb0cee7e1f3bde58293de743871417-Paper.pdf",
  "title": "Distilled Wasserstein Learning for Word Embedding and Topic Modeling",
  "year": 2018,
  "venue": "NeurIPS",
  "abstract": "We propose a novel Wasserstein method with a distillation mechanism, yielding joint learning of word embeddings and topics. The proposed method is based on the fact that the Euclidean distance between word embeddings may be employed as the underlying distance in the Wasserstein topic model. The word distributions of topics, their optimal transports to the word distributions of documents, and the embeddings of words are learned in a unified framework. When learning the topic model, we leverage a distilled underlying distance matrix to update the topic distributions and smoothly calculate the corresponding optimal transports. Such a strategy provides the updating of word embeddings with robust guidance, improving the algorithmic convergence. As an application, we focus on patient admission records, in which the proposed method embeds the codes of diseases and procedures and learns the topics of admissions, obtaining superior performance on clinically-meaningful disease network construction, mortality prediction as a function of admission codes, and procedure recommendation.",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2018/file/25db67c5657914454081c6a18e93d6dd-Paper.pdf",
  "title": "Importance Weighting and Variational Inference",
  "year": 2018,
  "venue": "NeurIPS",
  "abstract": "Recent work used importance sampling ideas for better variational bounds on likelihoods. We clarify the applicability of these ideas to pure probabilistic inference, by showing the resulting Importance Weighted Variational Inference (IWVI) technique is an instance of augmented variational inference, thus identifying the looseness in previous work. Experiments confirm IWVI’s practicality for probabilistic inference. As a second contribution, we investigate inference with elliptical distributions, which improves accuracy in low dimensions, and convergence in high dimensions.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2018/file/27e8e17134dd7083b050476733207ea1-Paper.pdf",
  "title": "GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration",
  "year": 2018,
  "venue": "NeurIPS",
  "abstract": "Despite advances in scalable models, the inference tools used for Gaussian processes (GPs) have yet to fully capitalize on developments in computing hardware. We present an efficient and general approach to GP inference based on Blackbox Matrix-Matrix multiplication (BBMM). BBMM inference uses a modified batched version of the conjugate gradients algorithm to derive all terms for training and inference in a single call. BBMM reduces the asymptotic complexity of exact GP inference from O(n3) to O(n2). Adapting this algorithm to scalable approximations and complex GP models simply requires a routine for efficient matrix-matrix multiplication with the kernel and its derivative. In addition, BBMM uses a specialized preconditioner to substantially speed up convergence. In experiments we show that BBMM effectively uses GPU hardware to dramatically accelerate both exact GP inference and scalable approximations. Additionally, we provide GPyTorch, a software platform for scalable GP inference via BBMM, built on PyTorch.",
  "stance": 0.6
 },
 {
  "url": "https://papers.neurips.cc/paper/2018/file/287e041302f34b11ddfb57afc8048cd8-Paper.pdf",
  "title": "Forward Modeling for Partial Observation Strategy Games - A StarCraft Defogger",
  "year": 2018,
  "venue": "NeurIPS",
  "abstract": "We formulate the problem of defogging as state estimation and future state prediction from previous, partial observations in the context of real-time strategy games. We propose to employ encoder-decoder neural networks for this task, and introduce proxy tasks and baselines for evaluation to assess their ability of capturing basic game rules and high-level dynamics. By combining convolutional neural networks and recurrent networks, we exploit spatial and sequential correlations and train well-performing models on a large dataset of human games of StarCraft R : Brood War R . Finally, we demonstrate the relevance of our models to downstream tasks by applying them for enemy unit prediction in a state-of-the-art, rule-based StarCraft bot. We observe improvements in win rates against several strong community bots.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2018/file/294a8ed24b1ad22ec2e7efea049b8737-Paper.pdf",
  "title": "Sanity Checks for Saliency Maps",
  "year": 2018,
  "venue": "NeurIPS",
  "abstract": "Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings2.",
  "stance": 0.3
 },
 {
  "url": "https://papers.neurips.cc/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf",
  "title": "Are Sixteen Heads Really Better than One?",
  "year": 2019,
  "venue": "NeurIPS",
  "abstract": "Attention is a powerful and ubiquitous mechanism for allowing neural models to focus on particular salient pieces of information by taking their weighted average when making predictions. In particular, multi-headed attention is a driving force behind many recent state-of-the-art natural language processing (NLP) models such as Transformer-based MT models and BERT. These models apply multiple attention mechanisms in parallel, with each attention “head” potentially focusing on different parts of the input, which makes it possible to express sophisticated functions beyond the simple weighted average. In this paper we make the surprising observation that even if models have been trained using multiple heads, in practice, a large percentage of attention heads can be removed at test time without significantly impacting performance. In fact, some layers can even be reduced to a single head. We further examine greedy algorithms for pruning down models, and the potential speed, memory efficiency, and accuracy improvements obtainable therefrom. Finally, we analyze the results with respect to which parts of the model are more reliant on having multiple heads, and provide precursory evidence that training dynamics play a role in the gains provided by multi-head attention.",
  "stance": -0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2019/file/3001ef257407d5a371a96dcd947c7d93-Paper.pdf",
  "title": "Generative Modeling by Estimating Gradients of the Data Distribution",
  "year": 2019,
  "venue": "NeurIPS",
  "abstract": "We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2019/file/3a24b25a7b092a252166a1641ae953e7-Paper.pdf",
  "title": "Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers",
  "year": 2019,
  "venue": "NeurIPS",
  "abstract": "Recent works have shown the effectiveness of randomized smoothing as a scalable technique for building neural network-based classifiers that are provably robust to `2-norm adversarial perturbations. In this paper, we employ adversarial training to improve the performance of randomized smoothing. We design an adapted attack for smoothed classifiers, and we show how this attack can be used in an adversarial training setting to boost the provable robustness of smoothed classifiers. We demonstrate through extensive experimentation that our method consistently outperforms all existing provably `2-robust classifiers by a significant margin on ImageNet and CIFAR-10, establishing the state-of-the-art for provable `2-defenses. Moreover, we find that pre-training and semi-supervised learning boost adversarially trained smoothed classifiers even further. Our code and trained models are available at http://github.com/Hadisalman/smoothing-adversarial2.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2019/file/44885837c518b06e3f98b41ab8cedc0f-Paper.pdf",
  "title": "Invariance and identifiability issues for word embeddings",
  "year": 2019,
  "venue": "NeurIPS",
  "abstract": "Word embeddings are commonly obtained as optimizers of a criterion function f of a text corpus, but assessed on word-task performance using a different evaluation function g of the test data. We contend that a possible source of disparity in performance on tasks is the incompatibility between classes of transformations that leave f and g invariant. In particular, word embeddings defined by f are not unique; they are defined only up to a class of transformations to which f is invariant, and this class is larger than the class to which g is invariant. One implication of this is that the apparent superiority of one word embedding over another, as measured by word task performance, may largely be a consequence of the arbitrary elements selected from the respective solution sets. We provide a formal treatment of the above identifiability issue, present some numerical examples, and discuss possible resolutions.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2019/file/46a558d97954d0692411c861cf78ef79-Paper.pdf",
  "title": "Limitations of the empirical Fisher approximation for natural gradient descent",
  "year": 2019,
  "venue": "NeurIPS",
  "abstract": "Natural gradient descent, which preconditions a gradient descent update with the Fisher information matrix of the underlying statistical model, is a way to capture partial second-order information. Several highly visible works have advocated an approximation known as the empirical Fisher, drawing connections between approximate second-order methods and heuristics like Adam. We dispute this argument by showing that the empirical Fisher—unlike the Fisher—does not generally capture second-order information. We further argue that the conditions under which the empirical Fisher approaches the Fisher (and the Hessian) are unlikely to be met in practice, and that, even on simple optimization problems, the pathologies of the empirical Fisher can have undesirable effects.",
  "stance": -1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2019/file/48aedb8880cab8c45637abc7493ecddd-Paper.pdf",
  "title": "vGraph: A Generative Model for Joint Community Detection and Node Representation Learning",
  "year": 2019,
  "venue": "NeurIPS",
  "abstract": "This paper focuses on two fundamental tasks of graph analysis: community detection and node representation learning, which capture the global and local structures of graphs, respectively. In the current literature, these two tasks are usually independently studied while they are actually highly correlated. We propose a probabilistic generative model called vGraph to learn community membership and node representation collaboratively. Specifically, we assume that each node can be represented as a mixture of communities, and each community is defined as a multinomial distribution over nodes. Both the mixing coefficients and the community distribution are parameterized by the low-dimensional representations of the nodes and communities. We designed an effective variational inference algorithm which regularizes the community membership of neighboring nodes to be similar in the latent space. Experimental results on multiple real-world graphs show that vGraph is very effective in both community detection and node representation learning, outperforming many competitive baselines in both tasks. We show that the framework of vGraph is quite flexible and can be easily extended to detect hierarchical communities.",
  "stance": 0.9
 },
 {
  "url": "https://papers.neurips.cc/paper/2019/file/48c8c3963853fff20bd9e8bee9bd4c07-Paper.pdf",
  "title": "Can Unconditional Language Models Recover Arbitrary Sentences?",
  "year": 2019,
  "venue": "NeurIPS",
  "abstract": "Neural network-based generative language models like ELMo and BERT can work effectively as general purpose sentence encoders in text classification without further fine-tuning. Is it possible to adapt them in a similar way for use as generalpurpose decoders? For this to be possible, it would need to be the case that for any target sentence of interest, there is some continuous representation that can be passed to the language model to cause it to reproduce that sentence. We set aside the difficult problem of designing an encoder that can produce such representations and, instead, ask directly whether such representations exist at all. To do this, we introduce a pair of effective, complementary methods for feeding representations into pretrained unconditional language models and a corresponding set of methods to map sentences into and out of this representation space, the reparametrized sentence space. We then investigate the conditions under which a language model can be made to generate a sentence through the identification of a point in such a space and find that it is possible to recover arbitrary sentences nearly perfectly with language models and representations of moderate size without modifying any model parameters.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2019/file/51c68dc084cb0b8467eafad1330bce66-Paper.pdf",
  "title": "Cold Case: The Lost MNIST Digits",
  "year": 2019,
  "venue": "NeurIPS",
  "abstract": "Although the popular MNIST dataset [LeCun et al., 1994] is derived from the NIST database [Grother and Hanaoka, 1995], the precise processing steps for this derivation have been lost to time. We propose a reconstruction that is accurate enough to serve as a replacement for the MNIST dataset, with insignificant changes in accuracy. We trace each MNIST digit to its NIST source and its rich metadata such as writer identifier, partition identifier, etc. We also reconstruct the complete MNIST test set with 60,000 samples instead of the usual 10,000. Since the balance 50,000 were never distributed, they can be used to investigate the impact of twenty-five years of MNIST experiments on the reported testing performances. Our limited results unambiguously confirm the trends observed by Recht et al. [2018, 2019]: although the misclassification rates are slightly off, classifier ordering and model selection remain broadly reliable. We attribute this phenomenon to the pairing benefits of comparing classifiers on the same digits.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2019/file/5481b2f34a74e427a2818014b8e103b0-Paper.pdf",
  "title": "On the Power and Limitations of Random Features for Understanding Neural Networks",
  "year": 2019,
  "venue": "NeurIPS",
  "abstract": "Recently, a spate of papers have provided positive theoretical results for training over-parameterized neural networks (where the network size is larger than what is needed to achieve low error). The key insight is that with sufficient overparameterization, gradient-based methods will implicitly leave some components of the network relatively unchanged, so the optimization dynamics will behave as if those components are essentially fixed at their initial random values. In fact, fixing these explicitly leads to the well-known approach of learning with random features (e.g. [27, 29]). In other words, these techniques imply that we can successfully learn with neural networks, whenever we can successfully learn with random features. In this paper, we formalize the link between existing results and random features, and argue that despite the impressive positive results, random feature approaches are also inherently limited in what they can explain. In particular, we prove that random features cannot be used to learn even a single ReLU neuron (over standard Gaussian inputs in R and poly(d) weights), unless the network size (or magnitude of its weights) is exponentially large in d. Since a single neuron is known to be learnable with gradient-based methods, we conclude that we are still far from a satisfying general explanation for the empirical success of neural networks. For completeness we also provide a simple self-contained proof, using a random features technique, that one-hidden-layer neural networks can learn low-degree polynomials.",
  "stance": -1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2019/file/56a3107cad6611c8337ee36d178ca129-Paper.pdf",
  "title": "SHE: A Fast and Accurate Deep Neural Network for Encrypted Data",
  "year": 2019,
  "venue": "NeurIPS",
  "abstract": "We present a novel nonnegative tensor decomposition method, called Legendre decomposition, which factorizes an input tensor into a multiplicative combination of parameters. Thanks to the well-developed theory of information geometry, the reconstructed tensor is unique and always minimizes the KL divergence from an input tensor. We empirically show that Legendre decomposition can more accurately reconstruct tensors than other nonnegative tensor decomposition methods.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/1fc30b9d4319760b04fab735fbfed9a9-Paper.pdf",
  "title": "Learning Dynamic Belief Graphs to Generalize on Text-Based Games",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "Playing text-based games requires skills in processing natural language and sequential decision making. Achieving human-level performance on text-based games remains an open challenge, and prior research has largely relied on hand-crafted structured representations and heuristics. In this work, we investigate how an agent can plan and generalize in text-based games using graph-structured representations learned end-to-end from raw text. We propose a novel graph-aided transformer agent (GATA) that infers and updates latent belief graphs during planning to enable effective action selection by capturing the underlying game dynamics. GATA is trained using a combination of reinforcement and self-supervised learning. Our work demonstrates that the learned graph-based representations help agents converge to better policies than their text-only counterparts and facilitate effective generalization across game configurations. Experiments on 500+ unique games from the TextWorld suite show that our best agent outperforms text-based baselines by an average of 24.2%.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/216f44e2d28d4e175a194492bde9148f-Paper.pdf",
  "title": "Reinforcement Learning for Control with Multiple Frequencies",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "Many real-world sequential decision problems involve multiple action variables whose control frequencies are different, such that actions take their effects at different periods. While these problems can be formulated with the notion of multiple action persistences in factored-action MDP (FA-MDP), it is non-trivial to solve them efficiently since an action-persistent policy constructed from a stationary policy can be arbitrarily suboptimal, rendering solution methods for the standard FA-MDPs hardly applicable. In this paper, we formalize the problem of multiple control frequencies in RL and provide its efficient solution method. Our proposed method, Action-Persistent Policy Iteration (AP-PI), provides a theoretical guarantee on the convergence to an optimal solution while incurring only a factor of |A| increase in time complexity during policy improvement step, compared to the standard policy iteration for FA-MDPs. Extending this result, we present ActionPersistent Actor-Critic (AP-AC), a scalable RL algorithm for high-dimensional control tasks. In the experiments, we demonstrate that AP-AC significantly outperforms the baselines on several continuous control tasks and a traffic control simulation, which highlights the effectiveness of our method that directly optimizes the periodic non-stationary policy for tasks with multiple control frequencies.",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/240ac9371ec2671ae99847c3ae2e6384-Paper.pdf",
  "title": "Restoring Negative Information in Few-Shot Object Detection",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "Few-shot learning has recently emerged as a new challenge in the deep learning field: unlike conventional methods that train the deep neural networks (DNNs) with a large number of labeled data, it asks for the generalization of DNNs on new classes with few annotated samples. Recent advances in few-shot learning mainly focus on image classification while in this paper we focus on object detection. The initial explorations in few-shot object detection tend to simulate a classification scenario by using the positive proposals in images with respect to certain object class while discarding the negative proposals of that class. Negatives, especially hard negatives, however, are essential to the embedding space learning in few-shot object detection. In this paper, we restore the negative information in few-shot object detection by introducing a new negativeand positive-representative based metric learning framework and a new inference scheme with negative and positive representatives. We build our work on a recent few-shot pipeline RepMet [1] with several new modules to encode negative information for both training and testing. Extensive experiments on ImageNet-LOC and PASCAL VOC show our method substantially improves the state-of-the-art few-shot object detection solutions. Our code is available at https://github.com/yang-yk/NP-RepMet.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/24bea84d52e6a1f8025e313c2ffff50a-Paper.pdf",
  "title": "Diverse Image Captioning with Context-Object Split Latent Spaces",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "Diverse image captioning models aim to learn one-to-many mappings that are innate to cross-domain datasets, such as of images and texts. Current methods for this task are based on generative latent variable models, e.g. VAEs with structured latent spaces. Yet, the amount of multimodality captured by prior work is limited to that of the paired training data – the true diversity of the underlying generative process is not fully captured. To address this limitation, we leverage the contextual descriptions in the dataset that explain similar contexts in different visual scenes. To this end, we introduce a novel factorization of the latent space, termed contextobject split, to model diversity in contextual descriptions across images and texts within the dataset. Our framework1 not only enables diverse captioning through context-based pseudo supervision, but extends this to images with novel objects and without paired captions in the training data. We evaluate our COS-CVAE approach on the standard COCO dataset and on the held-out COCO dataset consisting of images with novel objects, showing significant gains in accuracy and diversity.",
  "stance": 0.6
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/258be18e31c8188555c2ff05b4d542c3-Paper.pdf",
  "title": "Continual Learning with Node-Importance based Adaptive Group Sparse Regularization",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "We propose a novel regularization-based continual learning method, dubbed as Adaptive Group Sparsity based Continual Learning (AGS-CL), using two group sparsity-based penalties. Our method selectively employs the two penalties when learning each neural network node based on its the importance, which is adaptively updated after learning each task. By utilizing the proximal gradient descent method, the exact sparsity and freezing of the model is guaranteed during the learning process, and thus, the learner explicitly controls the model capacity. Furthermore, as a critical detail, we re-initialize the weights associated with unimportant nodes after learning each task in order to facilitate efficient learning and prevent the negative transfer. Throughout the extensive experimental results, we show that our AGS-CL uses orders of magnitude less memory space for storing the regularization parameters, and it significantly outperforms several state-of-the-art baselines on representative benchmarks for both supervised and reinforcement learning.",
  "stance": 0.9
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/26b58a41da329e0cbde0cbf956640a58-Paper.pdf",
  "title": "On ranking via sorting by estimated expected utility",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "Ranking tasks are defined through losses that measure trade-offs between different desiderata such as the relevance and the diversity of the items at the top of the list. This paper addresses the question of which of these tasks are asymptotically solved by sorting by decreasing order of expected utility, for some suitable notion of utility, or, equivalently, when is square loss regression consistent for ranking via score-andsort? We answer to this question by finding a characterization of ranking losses for which a suitable regression is consistent. This characterization has two strong corollaries. First, whenever there exists a consistent approach based on convex risk minimization, there also is a consistent approach based on regression. Second, when regression is not consistent, there are data distributions for which consistent surrogate approaches necessarily have non-trivial local minima, and for which optimal scoring function are necessarily discontinuous, even when the underlying data distribution is regular. In addition to providing a better understanding of surrogate approaches for ranking, these results illustrate the intrinsic difficulty of solving general ranking problems with the score-and-sort approach.",
  "stance": -0.3
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/293835c2cc75b585649498ee74b395f5-Paper.pdf",
  "title": "Dissecting Neural ODEs",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "Continuous deep learning architectures have recently re–emerged as Neural Ordinary Differential Equations (Neural ODEs). This infinite–depth approach theoretically bridges the gap between deep learning and dynamical systems, offering a novel perspective. However, deciphering the inner working of these models is still an open challenge, as most applications apply them as generic black–box modules. In this work we “open the box”, further developing the continuous–depth formulation with the aim of clarifying the influence of several design choices on the underlying dynamics.",
  "stance": 0.4
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/37bc5e7fb6931a50b3464ec66179085f-Paper.pdf",
  "title": "Learning Physical Constraints with Neural Projections",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "We propose a new family of neural networks to predict the behaviors of physical systems by learning their underpinning constraints. A neural projection operator lies at the heart of our approach, composed of a lightweight network with an embedded recursive architecture that interactively enforces learned underpinning constraints and predicts the various governed behaviors of different physical systems. Our neural projection operator is motivated by the position-based dynamics model that has been used widely in game and visual effects industries to unify the various fast physics simulators. Our method can automatically and effectively uncover a broad range of constraints from observation point data, such as length, angle, bending, collision, boundary effects, and their arbitrary combinations, without any connectivity priors. We provide a multi-group point representation in conjunction with a configurable network connection mechanism to incorporate prior inputs for processing complex physical systems. We demonstrated the efficacy of our approach by learning a set of challenging physical systems all in a unified and simple fashion including: rigid bodies with complex geometries, ropes with varying length and bending, articulated soft and rigid bodies, and multi-object collisions with complex boundaries.",
  "stance": 0.7
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/3f13cf4ddf6fc50c0d39a1d5aeb57dd8-Paper.pdf",
  "title": "Bayesian Bits: Unifying Quantization and Pruning",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "We introduce Bayesian Bits, a practical method for joint mixed precision quantization and pruning through gradient based optimization. Bayesian Bits employs a novel decomposition of the quantization operation, which sequentially considers doubling the bit width. At each new bit width, the residual error between the full precision value and the previously rounded value is quantized. We then decide whether or not to add this quantized residual error for a higher effective bit width and lower quantization noise. By starting with a power-of-two bit width, this decomposition will always produce hardware-friendly configurations, and through an additional 0-bit option, serves as a unified view of pruning and quantization. Bayesian Bits then introduces learnable stochastic gates, which collectively control the bit width of the given tensor. As a result, we can obtain low bit solutions by performing approximate inference over the gates, with prior distributions that encourage most of them to be switched off. We experimentally validate our proposed method on several benchmark datasets and show that we can learn pruned, mixed precision networks that provide a better trade-off between accuracy and efficiency than their static bit width equivalents.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/438124b4c06f3a5caffab2c07863b617-Paper.pdf",
  "title": "Meta-learning from Tasks with Heterogeneous Attribute Spaces",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "We propose a heterogeneous meta-learning method that trains a model on tasks with various attribute spaces, such that it can solve unseen tasks whose attribute spaces are different from the training tasks given a few labeled instances. Although many meta-learning methods have been proposed, they assume that all training and target tasks share the same attribute space, and they are inapplicable when attribute sizes are different across tasks. Our model infers latent representations of each attribute and each response from a few labeled instances using an inference network. Then, responses of unlabeled instances are predicted with the inferred representations using a prediction network. The attribute and response representations enable us to make predictions based on the task-specific properties of attributes and responses even when attribute and response sizes are different across tasks. In our experiments with synthetic datasets and 59 datasets in OpenML, we demonstrate that our proposed method can predict the responses given a few labeled instances in new tasks after being trained with tasks with heterogeneous attribute spaces.",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/47d40767c7e9df50249ebfd9c7cfff77-Paper.pdf",
  "title": "SMYRF - Efficient Attention using Asymmetric Clustering",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "We propose a novel type of balanced clustering algorithm to approximate attention. Attention complexity is reduced from O(N) to O(N logN), where N is the sequence length. Our algorithm, SMYRF, uses Locality Sensitive Hashing (LSH) in a novel way by defining new Asymmetric transformations and an adaptive scheme that produces balanced clusters. The biggest advantage of SMYRF is that it can be used as a drop-in replacement for dense attention layers without any retraining. On the contrary, prior fast attention methods impose constraints (e.g. queries and keys share the same vector representations) and require re-training from scratch. We apply our method to pre-trained state-of-the-art Natural Language Processing and Computer Vision models and we report significant memory and speed benefits. Notably, SMYRF-BERT outperforms (slightly) BERT on GLUE, while using 50% less memory. We also show that SMYRF can be used interchangeably with dense attention before and after training. Finally, we use SMYRF to train GANs with attention in high resolutions. Using a single TPU, we were able to scale attention to 128x128=16k and 256x256=65k tokens on BigGAN on CelebA-HQ.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/4b29fa4efe4fb7bc667c7b301b74d52d-Paper.pdf",
  "title": "BlockGAN: Learning 3D Object-aware Scene Representations from Unlabelled Images",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "We present BlockGAN, an image generative model that learns object-aware 3D scene representations directly from unlabelled 2D images. Current work on scene representation learning either ignores scene background or treats the whole scene as one object. Meanwhile, work that considers scene compositionality treats scene objects only as image patches or 2D layers with alpha maps. Inspired by the computer graphics pipeline, we design BlockGAN to learn to first generate 3D features of background and foreground objects, then combine them into 3D features for the whole scene, and finally render them into realistic images. This allows BlockGAN to reason over occlusion and interaction between objects’ appearance, such as shadow and lighting, and provides control over each object’s 3D pose and identity, while maintaining image realism. BlockGAN is trained end-to-end, using only unlabelled single images, without the need for 3D geometry, pose labels, object masks, or multiple views of the same scene. Our experiments show that using explicit 3D features to represent objects allows BlockGAN to learn disentangled representations both in terms of objects (foreground and background) and their properties (pose and identity). Our code is available at https://github.com/thunguyenphuoc/BlockGAN.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/518a38cc9a0173d0b2dc088166981cf8-Paper.pdf",
  "title": "Understanding the Role of Training Regimes in Continual Learning",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "Catastrophic forgetting affects the training of neural networks, limiting their ability to learn multiple tasks sequentially. From the perspective of the well established plasticity-stability dilemma, neural networks tend to be overly plastic, lacking the stability necessary to prevent the forgetting of previous knowledge, which means that as learning progresses, networks tend to forget previously seen tasks. This phenomenon coined in the continual learning literature, has attracted much attention lately, and several families of approaches have been proposed with different degrees of success. However, there has been limited prior work extensively analyzing the impact that different training regimes – learning rate, batch size, regularization method– can have on forgetting. In this work, we depart from the typical approach of altering the learning algorithm to improve stability. Instead, we hypothesize that the geometrical properties of the local minima found for each task play an important role in the overall degree of forgetting. In particular, we study the effect of dropout, learning rate decay, and batch size, on forming training regimes that widen the tasks’ local minima and consequently, on helping it not to forget catastrophically. Our study provides practical insights to improve stability via simple yet effective techniques that outperform alternative baselines.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/55d491cf951b1b920900684d71419282-Paper.pdf",
  "title": "Learning Certified Individually Fair Representations",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "Fair representation learning provides an effective way of enforcing fairness constraints without compromising utility for downstream users. A desirable family of such fairness constraints, each requiring similar treatment for similar individuals, is known as individual fairness. In this work, we introduce the first method that enables data consumers to obtain certificates of individual fairness for existing and new data points. The key idea is to map similar individuals to close latent representations and leverage this latent proximity to certify individual fairness. That is, our method enables the data producer to learn and certify a representation where for a data point all similar individuals are at `∞-distance at most , thus allowing data consumers to certify individual fairness by proving -robustness of their classifier. Our experimental evaluation on five real-world datasets and several fairness constraints demonstrates the expressivity and scalability of our approach.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/57cd30d9088b0185cf0ebca1a472ff1d-Paper.pdf",
  "title": "Can Implicit Bias Explain Generalization? Stochastic Convex Optimization as a Case Study",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "The notion of implicit bias, or implicit regularization, has been suggested as a means to explain the surprising generalization ability of modern-days overparameterized learning algorithms. This notion refers to the tendency of the optimization algorithm towards a certain structured solution that often generalizes well. Recently, several papers have studied implicit regularization and were able to identify this phenomenon in various scenarios. We revisit this paradigm in arguably the simplest non-trivial setup, and study the implicit bias of Stochastic Gradient Descent (SGD) in the context of Stochastic ConvexOptimization. As a first step, we provide a simple construction that rules out the existence of a distribution-independent implicit regularizer that governs the generalization ability of SGD. We then demonstrate a learning problem that rules out a very general class of distribution-dependent implicit regularizers from explaining generalization, which includes strongly convex regularizers as well as non-degenerate norm-based regularizations. Certain aspects of our constructions point out to significant difficulties in providing a comprehensive explanation of an algorithm’s generalization performance by solely arguing about its implicit regularization properties.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/588cb956d6bbe67078f29f8de420a13d-Paper.pdf",
  "title": "Critic Regularized Regression",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "Offline reinforcement learning (RL), also known as batch RL, offers the prospect of policy optimization from large pre-recorded datasets without online environment interaction. It addresses challenges with regard to the cost of data collection and safety, both of which are particularly pertinent to real-world applications of RL. Unfortunately, most off-policy algorithms perform poorly when learning from a fixed dataset. In this paper, we propose a novel offline RL algorithm to learn policies from data using a form of critic-regularized regression (CRR). We find that CRR performs surprisingly well and scales to tasks with high-dimensional state and action spaces – outperforming several state-of-the-art offline RL algorithms by a significant margin on a wide range of benchmark tasks.",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/5bf8aaef51c6e0d363cbe554acaf3f20-Paper.pdf",
  "title": "A Scalable Approach for Privacy-Preserving Collaborative Machine Learning",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "We consider a collaborative learning scenario in which multiple data-owners wish to jointly train a logistic regression model, while keeping their individual datasets private from the other parties. We propose COPML, a fully-decentralized training framework that achieves scalability and privacy-protection simultaneously. The key idea of COPML is to securely encode the individual datasets to distribute the computation load effectively across many parties and to perform the training computations as well as the model updates in a distributed manner on the securely encoded data. We provide the privacy analysis of COPML and prove its convergence. Furthermore, we experimentally demonstrate that COPML can achieve significant speedup in training over the benchmark protocols. Our protocol provides strong statistical privacy guarantees against colluding parties (adversaries) with unbounded computational power, while achieving up to 16× speedup in the training time against the benchmark protocols.",
  "stance": 0.4
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/5ef20b89bab8fed38253e98a12f26316-Paper.pdf",
  "title": "Greedy inference with structure-exploiting lazy maps",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "We propose a framework for solving high-dimensional Bayesian inference problems using structure-exploiting low-dimensional transport maps or flows. These maps are confined to a low-dimensional subspace (hence, lazy), and the subspace is identified by minimizing an upper bound on the Kullback–Leibler divergence (hence, structured). Our framework provides a principled way of identifying and exploiting low-dimensional structure in an inference problem. It focuses the expressiveness of a transport map along the directions of most significant discrepancy from the posterior, and can be used to build deep compositions of lazy maps, where low-dimensional projections of the parameters are iteratively transformed to match the posterior. We prove weak convergence of the generated sequence of distributions to the posterior, and we demonstrate the benefits of the framework on challenging inference problems in machine learning and differential equations, using inverse autoregressive flows and polynomial maps as examples of the underlying density estimators.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/618491e20a9b686b79e158c293ab4f91-Paper.pdf",
  "title": "Bad Global Minima Exist and SGD Can Reach Them",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "Several works have aimed to explain why overparameterized neural networks generalize well when trained by Stochastic Gradient Descent (SGD). The consensus explanation that has emerged credits the randomized nature of SGD for the bias of the training process towards low-complexity models and, thus, for implicit regularization. We take a careful look at this explanation in the context of image classification with common deep neural network architectures. We find that if we do not regularize explicitly, then SGD can be easily made to converge to poorlygeneralizing, high-complexity models: all it takes is to first train on a random labeling on the data, before switching to properly training with the correct labels. In contrast, we find that in the presence of explicit regularization, pretraining with random labels has no detrimental effect on SGD. We believe that our results give evidence that explicit regularization plays a far more important role in the success of overparameterized neural networks than what has been understood until now. Specifically, by penalizing complicated models independently of their fit to the data, regularization affects training dynamics also far away from optima, making simple models that fit the data well discoverable by local methods, such as SGD.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/6217b2f7e4634fa665d31d3b4df81b56-Paper.pdf",
  "title": "Spin-Weighted Spherical CNNs",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "Learning equivariant representations is a promising way to reduce sample and model complexity and improve the generalization performance of deep neural networks. The spherical CNNs are successful examples, producing SO(3)-equivariant representations of spherical inputs. There are two main types of spherical CNNs. The first type lifts the inputs to functions on the rotation group SO(3) and applies convolutions on the group, which are computationally expensive since SO(3) has one extra dimension. The second type applies convolutions directly on the sphere, which are limited to zonal (isotropic) filters, and thus have limited expressivity. In this paper, we present a new type of spherical CNN that allows anisotropic filters in an efficient way, without ever leaving the spherical domain. The key idea is to consider spin-weighted spherical functions, which were introduced in physics in the study of gravitational waves. These are complex-valued functions on the sphere whose phases change upon rotation. We define a convolution between spin-weighted functions and build a CNN based on it. The spin-weighted functions can also be interpreted as spherical vector fields, allowing applications to tasks where the inputs or outputs are vector fields. Experiments show that our method outperforms previous methods on tasks like classification of spherical images, classification of 3D shapes and semantic segmentation of spherical panoramas.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/690f44c8c2b7ded579d01abe8fdb6110-Paper.pdf",
  "title": "Geo-PIFu: Geometry and Pixel Aligned Implicit Functions for Single-view Human Reconstruction",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "We propose Geo-PIFu, a method to recover a 3D mesh from a monocular color image of a clothed person. Our method is based on a deep implicit function-based representation to learn latent voxel features using a structure-aware 3D U-Net, to constrain the model in two ways: first, to resolve feature ambiguities in query point encoding, second, to serve as a coarse human shape proxy to regularize the high-resolution mesh and encourage global shape regularity. We show that, by both encoding query points and constraining global shape using latent voxel features, the reconstruction we obtain for clothed human meshes exhibits less shape distortion and improved surface details compared to competing methods. We evaluate Geo-PIFu on a recent human mesh public dataset that is 10× larger than the private commercial dataset used in PIFu and previous derivative work. On average, we exceed the state of the art by 42.7% reduction in Chamfer and Point-to-Surface Distances, and 19.4% reduction in normal estimation errors.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S01-1004",
  "title": "English Lexical Sample Task Description",
  "year": 2001,
  "venue": "SemEval",
  "abstract": "The English lexical sample task (adjectives and nouns) for SENSEVAL 2 was set up according to the same principles as for SENSEVAL1, as reported in (Kilgarriff and Rosenzweig, 2000). (Adjectives and nouns only, because the data preparation for the verbs lexical sample was undertaken alongside that for the English all-words task, and is reported in Palmer et al (this volume). All discussion below up to the Results section covers only adjectives and nouns.)",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/S01-1007",
  "title": "The Italian Lexical Sample Task",
  "year": 2001,
  "venue": "SemEval",
  "abstract": "In this paper we give an overall description of the Italian lexical sample task for SENSEV AL-2, together with some general reflections about on the one hand the overall task of lexical-semantic annotation and on the other about the adequacy of existing lexical-semantic reference resources.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S01-1012",
  "title": "The SENSEVAL-2 Panel on Domains, Topics and Senses",
  "year": 2001,
  "venue": "SemEval",
  "abstract": "An important aspect of sense disambiguation is the wider semantic space (domain, topic) in which the ambiguous word occurs. This may be most clearly illustrated by some cross-lingual examples, as they would appear in (machine) translation. Consider for instance the English word housing. In a more general \"sense\", this translates in German into Wohnung. In an engineering setting however it translates into Gehiiuse. Also verbs may be translated differently (i.e. have a different sense) according to the semantic space in which they occur. For instance, English warming up translates into erhitzen in a more general sense, but into aufwiinnen in the sports domain. Because of the apparent relevance then of domains or topics on sense disambiguation, a panel was organized at SENSEV AL-2 to discuss some current and previous work in this area. The paper presents a more extended overview based on the relevant literature, besides giving a summary of the discussion that developed after the panel presentations.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/W04-0814",
  "title": "The University of Amsterdam at Senseval-3: Semantic roles and Logic forms",
  "year": 2004,
  "venue": "SemEval",
  "abstract": "We describe our participation in two of the tasks organized within Senseval-3: Automatic Labeling of Semantic Roles and Identification of Logic Forms in English.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/W04-0863",
  "title": "Joining forces to resolve lexical ambiguity: East meets West in Barcelona",
  "year": 2004,
  "venue": "SemEval",
  "abstract": "This paper describes the component models and combination model built as a joint effort between Swarthmore College, Hong Kong PolyU, and HKUST. Though other models described elsewhere contributed to the final combination model, this paper focuses solely on the joint contributions to the ”Swat-HK” effort.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S07-1011",
  "title": "SemEval-2007 Task 12: Turkish Lexical Sample Task",
  "year": 2007,
  "venue": "SemEval",
  "abstract": "This paper presents the task definition, resources, and the single participant system for Task 12: Turkish Lexical Sample Task (TLST), which was organized in the SemEval-2007 evaluation exercise. The methodology followed for developing the specific linguistic resources necessary for the task has been described in this context. A language-specific feature set was defined for Turkish. TLST consists of three pieces of data: The dictionary, the training data, and the evaluation data. Finally, a single system that utilizes a simple statistical method was submitted for the task and evaluated.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/S07-1033",
  "title": "GYDER: Maxent Metonymy Resolution",
  "year": 2007,
  "venue": "SemEval",
  "abstract": "Though the GYDER system has achieved the highest accuracy scores for the metonymy resolution shared task at SemEval-2007 in all six subtasks, we don’t consider the results (72.80% accuracy for org, 84.36% for loc) particularly impressive, and argue that metonymy resolution needs more features.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/S07-1055",
  "title": "OE: WSD Using Optimal Ensembling (OE) Method",
  "year": 2007,
  "venue": "SemEval",
  "abstract": "Optimal ensembling (OE) is a word sense disambiguation (WSD) method using word-specific training factors (average positive vs negative training per sense, posex and negex) to predict best system (classifier algorithm / applicable feature set) for given target word. Our official entry (OE1) in Senseval-4 Task 17 (coarse-grained English lexical sample task) contained many design flaws and thus failed to show the whole potential of the method, finishing -4.9% behind top system (+0.5 gain over best base system). A fixed system (OE2) finished only -3.4% behind (+2.0% net gain). All our systems were 'closed', i.e. used the official training data only (average 56 training examples per each sense). We also show that the official evaluation measure tends to favor systems that do well with high-trained words.",
  "stance": -0.4
 },
 {
  "url": "https://aclanthology.org/S07-1087",
  "title": "UMND2 : SenseClusters Applied to the Sense Induction Task of Senseval-4",
  "year": 2007,
  "venue": "SemEval",
  "abstract": "SenseClusters is a freely–available open– source system that served as the University of Minnesota, Duluth entry in the SENSEVAL-4 sense induction task. For this task SenseClusters was configured to construct representations of the instances to be clustered using the centroid of word cooccurrence vectors that replace the words in an instance. These instances are then clustered using k–means where the number of clusters is discovered automatically using the Adapted Gap Statistic. In these experiments SenseClusters did not use any information outside of the raw untagged text that was to be clustered, and no tuning of the system was performed using external corpora.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/S07-1100",
  "title": "USYD: WSD and Lexical Substitution using the Web1T corpus",
  "year": 2007,
  "venue": "SemEval",
  "abstract": "This paper describes the University of Sydney’s WSD and Lexical Substitution systems for SemEval-2007. These systems are principally based on evaluating the substitutability of potential synonyms in the context of the target word. Substitutability is measured using Pointwise Mutual Information as obtained from the Web1T corpus. The WSD systems are supervised, while the Lexical Substitution system is unsupervised. The lexical sample sub-task also used syntactic category information given from a CCG-based parse to assist in verb disambiguation, while both WSD tasks also make use of more traditional features. These related systems participated in the Coarse-Grained English All-Words WSD task (task 7), the Lexical Substitution Task (task 10) and the English Lexical Sample WSD sub-task (task 17).",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/S10-1005",
  "title": "SemEval-2010 Task 7: Argument Selection and Coercion",
  "year": 2010,
  "venue": "SemEval",
  "abstract": "We describe the Argument Selection and Coercion task for the SemEval-2010 evaluation exercise. This task involves characterizing the type of compositional operation that exists between a predicate and the arguments it selects. Specifically, the goal is to identify whether the type that a verb selects is satisfied directly by the argument, or whether the argument must change type to satisfy the verb typing. We discuss the problem in detail, describe the data preparation for the task, and analyze the results of the submissions.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/S10-1013",
  "title": "SemEval-2010 Task 17: All-Words Word Sense Disambiguation on a Specific Domain",
  "year": 2010,
  "venue": "SemEval",
  "abstract": "Domain portability and adaptation of NLP components and Word Sense Disambiguation systems present new challenges. The difficulties found by supervised systems to adapt might change the way we assess the strengths and weaknesses of supervised and knowledge-based WSD systems. Unfortunately, all existing evaluation datasets for specific domains are lexical-sample corpora. This task presented all-words datasets on the environment domain for WSD in four languages (Chinese, Dutch, English, Italian). 11 teams participated, with supervised and knowledge-based systems, mainly in the English dataset. The results show that in all languages the participants where able to beat the most frequent sense heuristic as estimated from general corpora. The most successful approaches used some sort of supervision in the form of hand-tagged examples from the domain.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/S10-1018",
  "title": "SUCRE: A Modular System for Coreference Resolution",
  "year": 2010,
  "venue": "SemEval",
  "abstract": "This paper presents SUCRE, a new software tool for coreference resolution and its feature engineering. It is able to separately do noun, pronoun and full coreference resolution. SUCRE introduces a new approach to the feature engineering of coreference resolution based on a relational database model and a regular feature definition language. SUCRE successfully participated in SemEval-2010 Task 1 on Coreference Resolution in Multiple Languages (Recasens et al., 2010) for gold and regular closed annotation tracks of six languages. It obtained the best results in several categories, including the regular closed annotation tracks of English and German.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S10-1044",
  "title": "FBK_NK: A WordNet-Based System for Multi-Way Classification of Semantic Relations",
  "year": 2010,
  "venue": "SemEval",
  "abstract": "We describe a WordNet-based system for the extraction of semantic relations between pairs of nominals appearing in English texts. The system adopts a lightweight approach, based on training a Bayesian Network classifier using large sets of binary features. Our features consider: i) the context surrounding the annotated nominals, and ii) different types of knowledge extracted from WordNet, including direct and explicit relations between the annotated nominals, and more general and implicit evidence (e.g. semantic boundary collocations). The system achieved a Macro-averaged F1 of 68.02% on the “Multi-Way Classification of Semantic Relations Between Pairs of Nominals” task (Task #8) at SemEval-2010.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S10-1067",
  "title": "PKU_HIT: An Event Detection System Based on Instances Expansion and Rich Syntactic Features",
  "year": 2010,
  "venue": "SemEval",
  "abstract": "This paper describes the PKU_HIT system on event detection in the SemEval-2010 Task. We construct three modules for the three sub-tasks of this evaluation. For target verb WSD, we build a Naïve Bayesian classifier which uses additional training instances expanded from an untagged Chinese corpus automatically. For sentence SRL and event detection, we use a feature-based machine learning method which makes combined use of both constituent-based and dependencybased features. Experimental results show that the Macro Accuracy of the WSD module reaches 83.81% and F-Score of the SRL module is 55.71%.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S12-1076",
  "title": "ETS: Discriminative Edit Models for Paraphrase Scoring",
  "year": 2012,
  "venue": "SemEval",
  "abstract": "Many problems in natural language processing can be viewed as variations of the task of measuring the semantic textual similarity between short texts. However, many systems that address these tasks focus on a single task and may or may not generalize well. In this work, we extend an existing machine translation metric, TERp (Snover et al., 2009a), by adding support for more detailed feature types and by implementing a discriminative learning algorithm. These additions facilitate applications of our system, called PERP, to similarity tasks other than machine translation evaluation, such as paraphrase recognition. In the SemEval 2012 Semantic Textual Similarity task, PERP performed competitively, particularly at the two surprise subtasks revealed shortly before the submission deadline.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/S13-1037",
  "title": "More Words and Bigger Pictures",
  "year": 2013,
  "venue": "SemEval",
  "abstract": "Object recognition is a little like translation: a picture (text in a source language) goes in, and a description (text in a target language) comes out. I will use this analogy, which has proven fertile, to describe recent progress in object recognition. We have very good methods to spot some objects in images, but extending these methods to produce descriptions of images remains very difficult. The description might come in the form of a set of words, indicating objects, and boxes or regions spanned by the object. This representation is difficult to work with, because some objects seem to be much more important than others, and because objects interact. An alternative is a sentence or a paragraph describing the picture, and recent work indicates how one might generate rich structures like this. Furthermore, recent work suggests that it is easier and more effective to generate descriptions of images in terms of chunks of meaning (”person on a horse”) rather than just objects (”person”; ”horse”). Finally, if the picture contains objects that are unfamiliar, then we need to generate useful descriptions that will make it possible to interact with them, even though we don’t know what they are. ",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/S13-2018",
  "title": "IIRG: A Naive Approach to Evaluating Phrasal Semantics",
  "year": 2013,
  "venue": "SemEval",
  "abstract": "This paper describes the IIRG 1 system entered in SemEval-2013, the 7th International Workshop on Semantic Evaluation. We participated in Task 5 Evaluating Phrasal Semantics. We have adopted a token-based approach to solve this task using 1) Naı̈ve Bayes methods and 2) Word Overlap methods, both of which rely on the extraction of syntactic features. We found that the word overlap method significantly out-performs the Naı̈ve Bayes methods, achieving our highest overall score with an accuracy of approximately 78%.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S13-2108",
  "title": "UTurku: Drug Named Entity Recognition and Drug-Drug Interaction Extraction Using SVM Classification and Domain Knowledge",
  "year": 2013,
  "venue": "SemEval",
  "abstract": "The DDIExtraction 2013 task in the SemEval conference concerns the detection of drug names and statements of drug-drug interactions (DDI) from text. Extraction of DDIs is important for providing up-to-date knowledge on adverse interactions between coadministered drugs. We apply the machine learning based Turku Event Extraction System to both tasks. We evaluate three feature sets, syntactic features derived from deep parsing, enhanced optionally with features derived from DrugBank or from both DrugBank and MetaMap. TEES achieves F-scores of 60% for the drug name recognition task and 59% for the DDI extraction task.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/S14-1001",
  "title": "More or less supervised supersense tagging of Twitter",
  "year": 2014,
  "venue": "SemEval",
  "abstract": "We present two Twitter datasets annotated with coarse-grained word senses (supersenses), as well as a series of experiments with three learning scenarios for supersense tagging: weakly supervised learning, as well as unsupervised and supervised domain adaptation. We show that (a) off-the-shelf tools perform poorly on Twitter, (b) models augmented with embeddings learned from Twitter data perform much better, and (c) errors can be reduced using type-constrained inference with distant supervision from WordNet.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S15-2153",
  "title": "SemEval 2015 Task 18: Broad-Coverage Semantic Dependency Parsing",
  "year": 2015,
  "venue": "SemEval",
  "abstract": "Task 18 at SemEval 2015 defines BroadCoverage Semantic Dependency Parsing (SDP) as the problem of recovering sentence-internal predicate–argument relationships for all content words, i.e. the semantic structure constituting the relational core of sentence meaning. In this task description, we position the problem in comparison to other language analysis sub-tasks, introduce and compare the semantic dependency target representations used, and summarize the task setup, participating systems, and main results.",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/S15-2162",
  "title": "Lisbon: Evaluating TurboSemanticParser on Multiple Languages and Out-of-Domain Data",
  "year": 2015,
  "venue": "SemEval",
  "abstract": "As part of the SemEval-2015 shared task on Broad-Coverage Semantic Dependency Parsing, we evaluate the performace of our last year’s system (TurboSemanticParser) on multiple languages and out-of-domain data. Our system is characterized by a feature-rich linear model, that includes scores for first and second-order dependencies (arcs, siblings, grandparents and co-parents). For decoding this second-order model, we solve a linear relaxation of that problem using alternating directions dual decomposition (AD). The experiments have shown that, even though the parser’s performance in Chinese and Czech attains around 80% (not too far from English performance), domain shift is a serious issue, suggesting domain adaptation as an interesting avenue for future research.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S16-1005",
  "title": "CUFE at SemEval-2016 Task 4: A Gated Recurrent Model for Sentiment Classification",
  "year": 2016,
  "venue": "SemEval",
  "abstract": "In this paper we describe a deep learning system that has been built for SemEval 2016 Task4 (Subtask A and B). In this work we trained a Gated Recurrent Unit (GRU) neural network model on top of two sets of word embeddings: (a) general word embeddings generated from unsupervised neural language model; and (b) task specific word embeddings generated from supervised neural language model that was trained to classify tweets into positive and negative categories. We also added a method for analyzing and splitting multi-words hashtags and appending them to the tweet body before feeding it to our model. Our models achieved 0.58 F1-measure for Subtask A (ranked 12/34) and 0.679 Recall for Subtask B (ranked 12/19).",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S16-1141",
  "title": "WHUNlp at SemEval-2016 Task DiMSUM: A Pilot Study in Detecting Minimal Semantic Units and their Meanings using Supervised Models",
  "year": 2016,
  "venue": "SemEval",
  "abstract": "This paper describes our approach towards the SemEval-2016 Task 10: Detecting Minimal Semantic Units and their Meanings (DiMSUM). We consider that the two problems are similar to multiword expression detection and supersense tagging, respectively. The former problem is formalized as a sequence labeling problem solved by first-order CRFs, and the latter one is formalized as a classification problem solved by Maximum Entropy Algorithm. To carry out our pilot study quickly, we extract some simple features such as words or part-of-speech tags from the training set, and avoid using external resources such as WordNet or Brown clusters which are allowed in the supervised closed condition. Experimental results show that much further work on feature engineering and model optimization needs to be explored.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/S16-2011",
  "title": "When Hyperparameters Help: Beneficial Parameter Combinations in Distributional Semantic Models",
  "year": 2016,
  "venue": "SemEval",
  "abstract": "Distributional semantic models can predict many linguistic phenomena, including word similarity, lexical ambiguity, and semantic priming, or even to pass TOEFL synonymy and analogy tests (Landauer and Dumais, 1997; Griffiths et al., 2007; Turney and Pantel, 2010). But what does it take to create a competitive distributional model? Levy et al. (2015) argue that the key to success lies in hyperparameter tuning rather than in the model’s architecture. More hyperparameters trivially lead to potential performance gains, but what do they actually do to improve the models? Are individual hyperparameters’ contributions independent of each other? Or are only specific parameter combinations beneficial? To answer these questions, we perform a quantitative and qualitative evaluation of major hyperparameters as identified in previous research.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/S17-1030",
  "title": "Ways of Asking and Replying in Duplicate Question Detection",
  "year": 2017,
  "venue": "SemEval",
  "abstract": "This paper presents the results of systematic experimentation on the impact in duplicate question detection of different types of questions across both a number of established approaches and a novel, superior one used to address this language processing task. This study permits to gain a novel insight on the different levels of robustness of the diverse detection methods with respect to different conditions of their application, including the ones that approximate real usage scenarios.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/S17-2048",
  "title": "FA3L at SemEval-2017 Task 3: A ThRee Embeddings Recurrent Neural Network for Question Answering",
  "year": 2017,
  "venue": "SemEval",
  "abstract": "In this paper we present ThReeNN, a model for Community Question Answering, Task 3, of SemEval-2017. The proposed model exploits both syntactic and semantic information to build a single and meaningful embedding space. Using a dependency parser in combination with word embeddings, the model creates sequences of inputs for a Recurrent Neural Network, which are then used for the ranking purposes of the Task. The score obtained on the official test data shows promising results.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S17-2054",
  "title": "SwissAlps at SemEval-2017 Task 3: Attention-based Convolutional Neural Network for Community Question Answering",
  "year": 2017,
  "venue": "SemEval",
  "abstract": "In this paper we propose a system for reranking answers for a given question. Our method builds on a siamese CNN architecture which is extended by two attention mechanisms. The approach was evaluated on the datasets of the SemEval-2017 competition for Community Question Answering (cQA), where it achieved 7th place obtaining a MAP score of 86:24 points on the Question-Comment Similarity subtask.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S17-2065",
  "title": "DataStories at SemEval-2017 Task 6: Siamese LSTM with Attention for Humorous Text Comparison",
  "year": 2017,
  "venue": "SemEval",
  "abstract": "In this paper we present a deep-learning system that competed at SemEval-2017 Task 6 ''#HashtagWars: Learning a Sense of Humor\". We participated in Subtask A, in which the goal was, given two Twitter messages, to identify which one is funnier. We propose a Siamese architecture with bidirectional Long Short-Term Memory (LSTM) networks, augmented with an attention mechanism. Our system works on the token-level, leveraging word embeddings trained on a big collection of unlabeled Twitter messages. We ranked 2nd in 7 teams. A post-completion improvement of our model, achieves state-of-the-art results on #HashtagWars dataset.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S17-2092",
  "title": "SemEval-2017 Task 11: End-User Development using Natural Language",
  "year": 2017,
  "venue": "SemEval",
  "abstract": "This task proposes a challenge to support the interaction between users and applications, micro-services and software APIs using natural language. The task aims for supporting the evaluation and evolution of the discussions surrounding the natural language processing approaches within the context of end-user natural language programming, under scenarios of high semantic heterogeneity/gap.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S17-2119",
  "title": "YNUDLG at SemEval-2017 Task 4: A GRU-SVM Model for Sentiment Classification and Quantification in Twitter",
  "year": 2017,
  "venue": "SemEval",
  "abstract": "Sentiment analysis is one of the central issues in Natural Language Processing and has become more and more important in many fields. Typical sentiment analysis classifies the sentiment of sentences into several discrete classes (e.g.,positive or negative). In this paper we describe our deep learning system(combining GRU and SVM) to solve both two-, three- and five-tweet polarity classifications. We first trained a gated recurrent neural network using pre-trained word embeddings, then we extracted features from GRU layer and input these features into support vector machine to fulfill both the classification and quantification subtasks. The proposed approach achieved 37th, 19th, and 14rd places in subtasks A, B and C, respectively.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S17-2134",
  "title": "YNU-HPCC at SemEval 2017 Task 4: Using A Multi-Channel CNN-LSTM Model for Sentiment Classification",
  "year": 2017,
  "venue": "SemEval",
  "abstract": "In this paper, we propose a multi-channel convolutional neural network-long short-term memory (CNN-LSTM) model that consists of two parts: multi-channel CNN and LSTM to analyze the sentiments of short English messages from Twitter. Un-like a conventional CNN, the proposed model applies a multi-channel strategy that uses several filters of different length to extract active local n-gram features in different scales. This information is then sequentially composed using LSTM. By combining both CNN and LSTM, we can consider both local information within tweets and long-distance dependency across tweets in the classification process. Officially released results show that our system outperforms the baseline algo-rithm.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S18-1019",
  "title": "AttnConvnet at SemEval-2018 Task 1: Attention-based Convolutional Neural Networks for Multi-label Emotion Classification",
  "year": 2018,
  "venue": "SemEval",
  "abstract": "In this paper, we propose an attention-based classifier that predicts multiple emotions of a given sentence. Our model imitates human's two-step procedure of sentence understanding and it can effectively represent and classify sentences. With emoji-to-meaning preprocessing and extra lexicon utilization, we further improve the model performance. We train and evaluate our model with data provided by SemEval-2018 task 1-5, each sentence of which has several labels among 11 given emotions. Our model achieves 5th/1st rank in English/Spanish respectively.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S18-1045",
  "title": "DeepMiner at SemEval-2018 Task 1: Emotion Intensity Recognition Using Deep Representation Learning",
  "year": 2018,
  "venue": "SemEval",
  "abstract": "In this paper, we propose a regression system to infer the emotion intensity of a tweet. We develop a multi-aspect feature learning mechanism to capture the most discriminative semantic features of a tweet as well as the emotion information conveyed by each word in it. We combine six types of feature groups: (1) a tweet representation learned by an LSTM deep neural network on the training data, (2) a tweet representation learned by an LSTM network on a large corpus of tweets that contain emotion words (a distant supervision corpus), (3) word embeddings trained on the distant supervision corpus and averaged over all words in a tweet, (4) word and character n-grams, (5) features derived from various sentiment and emotion lexicons, and (6) other hand-crafted features. As part of the word embedding training, we also learn the distributed representations of multi-word expressions (MWEs) and negated forms of words. An SVR regressor is then trained over the full set of features. We evaluate the effectiveness of our ensemble feature sets on the SemEval-2018 Task 1 datasets and achieve a Pearson correlation of 72% on the task of tweet emotion intensity prediction.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S18-1075",
  "title": "TAJJEB at SemEval-2018 Task 2: Traditional Approaches Just Do the Job with Emoji Prediction",
  "year": 2018,
  "venue": "SemEval",
  "abstract": "Emojis are widely used on social media andunderstanding their meaning is important forboth practical purposes (e.g. opinion mining,sentiment detection) and theoretical purposes(e.g. how different L1 speakers use them, dothey have some syntax?); this paper presents aset of experiments that aim to predict a singleemoji from a tweet. We built different mod-els and we found that the test results are verydifferent from the validation results.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S18-1128",
  "title": "SIRIUS-LTG-UiO at SemEval-2018 Task 7: Convolutional Neural Networks with Shortest Dependency Paths for Semantic Relation Extraction and Classification in Scientific Papers",
  "year": 2018,
  "venue": "SemEval",
  "abstract": "This article presents the SIRIUS-LTG-UiO system for the SemEval 2018 Task 7 on Semantic Relation Extraction and Classification in Scientific Papers. First we extract the shortest dependency path (sdp) between two entities, then we introduce a convolutional neural network (CNN) which takes the shortest dependency path embeddings as input and performs relation classification with differing objectives for each subtask of the shared task. This approach achieved overall F1 scores of 76.7 and 83.2 for relation classification on clean and noisy data, respectively. Furthermore, for combined relation extraction and classification on clean data, it obtained F1 scores of 37.4 and 33.6 for each phase. Our system ranks 3rd in all three sub-tasks of the shared task.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S18-1139",
  "title": "Talla at SemEval-2018 Task 7: Hybrid Loss Optimization for Relation Classification using Convolutional Neural Networks",
  "year": 2018,
  "venue": "SemEval",
  "abstract": "This paper describes our approach to SemEval-2018 Task 7 - given an entity-tagged text from the ACL Anthology corpus, identify and classify pairs of entities that have one of six possible semantic relationships. Our model consists of a convolutional neural network leveraging pre-trained word embeddings, unlabeled ACL-abstracts, and multiple window sizes to automatically learn useful features from entity-tagged sentences. We also experiment with a hybrid loss function, a combination of cross-entropy loss and ranking loss, to boost the separation in classification scores. Lastly, we include WordNet-based features to further improve the performance of our model. Our best model achieves an F1(macro) score of 74.2 and 84.8 on subtasks 1.1 and 1.2, respectively.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S18-1145",
  "title": "Digital Operatives at SemEval-2018 Task 8: Using dependency features for malware NLP",
  "year": 2018,
  "venue": "SemEval",
  "abstract": "The four sub-tasks of SecureNLP build towards a capability for quickly highlighting critical information from malware reports, such as the specific actions taken by a malware sample. Digital Operatives (DO) submitted to sub-tasks 1 and 2, using standard text analysis technology (text classification for sub-task 1, and a CRF for sub-task 2). Performance is broadly competitive with other submitted systems on sub-task 1 and weak on sub-task 2. The annotation guidelines for the intermediate sub-tasks create a linkage to the final task, which is both an annotation challenge and a potentially useful feature of the task. The methods that DO chose do not attempt to make use of this linkage, which may be a missed opportunity. This motivates a post-hoc error analysis. It appears that the annotation task is very hard, and that in some cases both deep conceptual knowledge and substantial surrounding context are needed in order to correctly classify sentences.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/S18-2010",
  "title": "The Limitations of Cross-language Word Embeddings Evaluation",
  "year": 2018,
  "venue": "SemEval",
  "abstract": "The aim of this work is to explore the possible limitations of existing methods of cross-language word embeddings evaluation, addressing the lack of correlation between intrinsic and extrinsic cross-language evaluation methods. To prove this hypothesis, we construct English-Russian datasets for extrinsic and intrinsic evaluation tasks and compare performances of 5 different cross-language models on them. The results say that the scores even on different intrinsic benchmarks do not correlate to each other. We can conclude that the use of human references as ground truth for cross-language word embeddings is not proper unless one does not understand how do native speakers process semantics in their cognition.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/S18-2021",
  "title": "Deep Affix Features Improve Neural Named Entity Recognizers",
  "year": 2018,
  "venue": "SemEval",
  "abstract": "We propose a practical model for named entity recognition (NER) that combines word and character-level information with a specific learned representation of the prefixes and suffixes of the word. We apply this approach to multilingual and multi-domain NER and show that it achieves state of the art results on the CoNLL 2002 Spanish and Dutch and CoNLL 2003 German NER datasets, consistently achieving 1.5-2.3 percent over the state of the art without relying on any dictionary features. Additionally, we show improvement on SemEval 2013 task 9.1 DrugNER, achieving state of the art results on the MedLine dataset and the second best results overall (-1.3% from state of the art). We also establish a new benchmark on the I2B2 2010 Clinical NER dataset with 84.70 F-score.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S19-1023",
  "title": "Word Embeddings (Also) Encode Human Personality Stereotypes",
  "year": 2019,
  "venue": "SemEval",
  "abstract": "Word representations trained on text reproduce human implicit bias related to gender, race and age. Methods have been developed to remove such bias. Here, we present results that show that human stereotypes exist even for much more nuanced judgments such as personality, for a variety of person identities beyond the typically legally protected attributes and that these are similarly captured in word representations. Specifically, we collected human judgments about a person's Big Five personality traits formed solely from information about the occupation, nationality or a common noun description of a hypothetical person. Analysis of the data reveals a large number of statistically significant stereotypes in people. We then demonstrate the bias captured in lexical representations is statistically significantly correlated with the documented human bias. Our results, showing bias for a large set of person descriptors for such nuanced traits put in doubt the feasibility of broadly and fairly applying debiasing methods and call for the development of new methods for auditing language technology systems and resources.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/S19-1028",
  "title": "On Adversarial Removal of Hypothesis-only Bias in Natural Language Inference",
  "year": 2019,
  "venue": "SemEval",
  "abstract": "Popular Natural Language Inference (NLI) datasets have been shown to be tainted by hypothesis-only biases. Adversarial learning may help models ignore sensitive biases and spurious correlations in data. We evaluate whether adversarial learning can be used in NLI to encourage models to learn representations free of hypothesis-only biases. Our analyses indicate that the representations learned via adversarial learning may be less biased, with only small drops in NLI accuracy.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/S19-2026",
  "title": "CoAStaL at SemEval-2019 Task 3: Affect Classification in Dialogue using Attentive BiLSTMs",
  "year": 2019,
  "venue": "SemEval",
  "abstract": "This work describes the system presented by the CoAStaL Natural Language Processing group at University of Copenhagen. The main system we present uses the same attention mechanism presented in (Yang et al., 2016). Our overall model architecture is also inspired by their hierarchical classification model and adapted to deal with classification in dialogue by encoding information at the turn level. We use different encodings for each turn to create a more expressive representation of dialogue context which is then fed into our classifier.We also define a custom preprocessing step in order to deal with language commonly used in interactions across many social media outlets. Our proposed system achieves a micro F1 score of 0.7340 on the test set and shows significant gains in performance compared to a system using dialogue level encoding.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/S19-2034",
  "title": "EmoSense at SemEval-2019 Task 3: Bidirectional LSTM Network for Contextual Emotion Detection in Textual Conversations",
  "year": 2019,
  "venue": "SemEval",
  "abstract": "In this paper, we describe a deep-learning system for emotion detection in textual conversations that participated in SemEval-2019 Task 3 \"EmoContext\". We designed a specific architecture of bidirectional LSTM which allows not only to learn semantic and sentiment feature representation, but also to capture user-specific conversation features. To fine-tune word embeddings using distant supervision we additionally collected a significant amount of emotional texts. The system achieved 72.59% micro-average F1 score for emotion classes on the test dataset, thereby significantly outperforming the officially-released baseline. Word embeddings and the source code were released for the research community.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S19-2035",
  "title": "EPITA-ADAPT at SemEval-2019 Task 3: Detecting emotions in textual conversations using deep learning models combination",
  "year": 2019,
  "venue": "SemEval",
  "abstract": "Messaging platforms like WhatsApp, Facebook Messenger and Twitter have gained recently much popularity owing to their ability in connecting users in real-time. The content of these textual messages can be a useful resource for text mining to discover and unhide various aspects, including emotions. In this paper we present our submission for SemEval 2019 task 'EmoContext'. The task consists of classifying a given textual dialogue into one of four emotion classes: Angry, Happy, Sad and Others. Our proposed system is based on the combination of different deep neural networks techniques. In particular, we use Recurrent Neural Networks (LSTM, B-LSTM, GRU, B-GRU), Convolutional Neural Network (CNN) and Transfer Learning (TL) methodes. Our final system, achieves an F1 score of 74.51% on the subtask evaluation dataset.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S19-2058",
  "title": "TDBot at SemEval-2019 Task 3: Context Aware Emotion Detection Using A Conditioned Classification Approach",
  "year": 2019,
  "venue": "SemEval",
  "abstract": "With the system description it is shown how to use the context information while detecting the emotion in a dialogue. Some guidelines about how to handle emojis was also laid out. While developing this system I realized the importance of pre-processing in conversational text data, or in general NLP related tasks; it can not be over emphasized.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/S19-2127",
  "title": "nlpUP at SemEval-2019 Task 6: A Deep Neural Language Model for Offensive Language Detection",
  "year": 2019,
  "venue": "SemEval",
  "abstract": "This paper presents our submission for the SemEval shared task 6, sub-task A on the identification of offensive language. Our proposed model, C-BiGRU, combines a Convolutional Neural Network (CNN) with a bidirectional Recurrent Neural Network (RNN). We utilize word2vec to capture the semantic similarities between words. This composition allows us to extract long term dependencies in tweets and distinguish between offensive and non-offensive tweets. In addition, we evaluate our approach on a different dataset and show that our model is capable of detecting online aggressiveness in both English and German tweets. Our model achieved a macro F1-score of 79.40% on the SemEval dataset.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S19-2155",
  "title": "SemEval-2019 Task 12: Toponym Resolution in Scientific Papers",
  "year": 2019,
  "venue": "SemEval",
  "abstract": "We present the SemEval-2019 Task 12 which focuses on toponym resolution in scientific articles. Given an article from PubMed, the task consists of detecting mentions of names of places, or toponyms, and mapping the mentions to their corresponding entries in GeoNames.org, a database of geospatial locations. We proposed three subtasks. In Subtask 1, we asked participants to detect all toponyms in an article. In Subtask 2, given toponym mentions as input, we asked participants to disambiguate them by linking them to entries in GeoNames. In Subtask 3, we asked participants to perform both the detection and the disambiguation steps for all toponyms. A total of 29 teams registered, and 8 teams submitted a system run. We summarize the corpus and the tools created for the challenge. They are freely available at https://competitions.codalab.org/competitions/19948. We also analyze the methods, the results and the errors made by the competing systems with a focus on toponym disambiguation.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/S19-2216",
  "title": "OleNet at SemEval-2019 Task 9: BERT based Multi-Perspective Models for Suggestion Mining",
  "year": 2019,
  "venue": "SemEval",
  "abstract": "This paper describes our system partici- pated in Task 9 of SemEval-2019: the task is focused on suggestion mining and it aims to classify given sentences into sug- gestion and non-suggestion classes in do- main specific and cross domain training setting respectively. We propose a multi- perspective architecture for learning rep- resentations by using different classical models including Convolutional Neural Networks (CNN), Gated Recurrent Units (GRU), Feed Forward Attention (FFA), etc. To leverage the semantics distributed in large amount of unsupervised data, we also have adopted the pre-trained Bidi- rectional Encoder Representations from Transformers (BERT) model as an en- coder to produce sentence and word rep- resentations. The proposed architecture is applied for both sub-tasks, and achieved f1-score of 0.7812 for subtask A, and 0.8579 for subtask B. We won the first and second place for the two tasks respec- tively in the final competition.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S19-2223",
  "title": "YNU_DYX at SemEval-2019 Task 9: A Stacked BiLSTM for Suggestion Mining Classification",
  "year": 2019,
  "venue": "SemEval",
  "abstract": "In this paper we describe a deep-learning system that competed as SemEval 2019 Task 9-SubTask A: Suggestion Mining from Online Reviews and Forums. We use Word2Vec to learn the distributed representations from sentences. This system is composed of a Stacked Bidirectional Long-Short Memory Network (SBiLSTM) for enriching word representations before and after the sequence relationship with context. We perform an ensemble to improve the effectiveness of our model. Our official submission results achieve an F1-score 0.5659.",
  "stance": 1.0
 }
]