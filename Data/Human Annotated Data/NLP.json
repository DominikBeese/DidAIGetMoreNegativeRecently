[
 {
  "url": "https://aclanthology.org/P00-1012",
  "title": "The Order of Prenominal Adjectives in Natural Language Generation",
  "year": 2000,
  "venue": "ACL",
  "abstract": "The order of prenominal adjectival modifiers in English is governed by complex and difficult to describe constraints which straddle the boundary between competence and performance. This paper describes and compares a number of statistical and machine learning techniques for ordering sequences of adjectives in the context of a natural language generation system.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P00-1017",
  "title": "Using Existing Systems to Supplement Small Amounts of Annotated Grammatical Relations Training Data",
  "year": 2000,
  "venue": "ACL",
  "abstract": "Grammatical relationships (GRs) form an important level of natural language processing, but di erent sets of GRs are useful for di erent purposes. Therefore, one may often only have time to obtain a small training corpus with the desired GR annotations. To boost the performance from using such a small training corpus on a transformation rule learner, we use existing systems that nd related types of annotations.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P00-1052",
  "title": "The Role of Centering Theory's Rough-Shift in the Teaching and Evaluation of Writing Skills",
  "year": 2000,
  "venue": "ACL",
  "abstract": "Existing software systems for automated essay scoring can provide NLP researchers with opportunities to test certain theoretical hypotheses, including some derived from Centering Theory. In this study we employ ETS's e-rater essay scoring system to examine whether local discourse coherence, as de ned by a measure of Rough-Shift transitions, might be a signi cant contributor to the evaluation of essays. Our positive results indicate that Rough-Shifts do indeed capture a source of incoherence, one that has not been closely examined in the Centering literature. These results not only justify Rough-Shifts as a valid transition type, but they also support the original formulation of Centering as a measure of discourse continuity even in pronominal-free text.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P00-1077",
  "title": "Panel: Computational Linguistics in India: An Overview",
  "year": 2000,
  "venue": "ACL",
  "abstract": "In the anusaaraka systems, the load between the human reader and the machine is divided as follows: language-based analysis of the text is carried out by the machine, and knowledge-based analysis or interpretation is left to the reader. The machine uses a dictionary and grammar rules, to produce the output. Most importantly, it does not use world knowledge to interpret (or disambiguate), as it is an error prone task and involves guessing or inferring based on knowledge other than the text. Anusaaraka aims for perfect \"information preservation\". We relax the requirement that the output be grammatical. In fact, anusaaraka output follows the grammar of the source language (where the grammar rules differ, and cannot be applied with 100 percent confidence). This requires that the reader undergo a short training to read and understand the output.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P03-2014",
  "title": "ACL-03 Interactive Posters and Demonstrations",
  "year": 2003,
  "venue": "ACL",
  "abstract": "On behalf of the program committee for ACL-2003 Interactive Posters/Demonstrations Sessions, it is my great pleasure to present you with this proceedings. Though ACL occasionally have had demonstration sessions, this will be the first attempt for ACL to have this kind of sessions, which aim to provide researchers or developers of natural language technologies with a generous environment for presentation and discussion of their works. Each paper receives an opportuity to give a five-minute oral preview presentatin and a poster or poster with demo presentation for two and half hours. To this attempt, there were initially 62 submissions, one paper was withdrawn during the review process, and we could accept only 28 papers. One paper was withdrawn after the selection and we finally have 27 papers, which you find in this volume.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P05-1027",
  "title": "Question Answering as Question-Biased Term Extraction: A New Approach toward Multilingual QA",
  "year": 2005,
  "venue": "ACL",
  "abstract": "This paper regards Question Answering (QA) as Question-Biased Term Extraction (QBTE). This new QBTE approach liberates QA systems from the heavy burden imposed by question types (or answer types). In conventional approaches, a QA system analyzes a given question and determines the question type, and then it selects answers from among answer candidates that match the question type. Consequently, the output of a QA system is restricted by the design of the question types. The QBTE directly extracts answers as terms biased by the question. To confirm the feasibility of our QBTE approach, we conducted experiments on the CRL QA Data based on 10-fold cross validation, using Maximum Entropy Models (MEMs) as an ML technique. Experimental results showed that the trained system achieved 0.36 in MRR and 0.47 in Top5 accuracy.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/P05-1048",
  "title": "Word Sense Disambiguation vs. Statistical Machine Translation",
  "year": 2005,
  "venue": "ACL",
  "abstract": "We directly investigate a subject of much recent debate: do word sense disambigation models help statistical machine translation quality? We present empirical results casting doubt on this common, but unproved, assumption. Using a state-ofthe-art Chinese word sense disambiguation model to choose translation candidates for a typical IBM statistical MT system, we find that word sense disambiguation does not yield significantly better translation quality than the statistical machine translation system alone. Error analysis suggests several key factors behind this surprising finding, including inherent limitations of current statistical MT architectures.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P05-1067",
  "title": "Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars",
  "year": 2005,
  "venue": "ACL",
  "abstract": "Syntax-based statistical machine translation (MT) aims at applying statistical models to structured data. In this paper, we present a syntax-based statistical machine translation system based on a probabilistic synchronous dependency insertion grammar. Synchronous dependency insertion grammars are a version of synchronous grammars defined on dependency trees. We first introduce our approach to inducing such a grammar from parallel corpora. Second, we describe the graphical model for the machine translation task, which can also be viewed as a stochastic tree-to-tree transducer. We introduce a polynomial time decoding algorithm for the model. We evaluate the outputs of our MT system using the NIST and Bleu automatic MT evaluation software. The result shows that our system outperforms the baseline system based on the IBM models in both translation speed and quality.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P06-1007",
  "title": "A Finite-State Model of Human Sentence Processing",
  "year": 2006,
  "venue": "ACL",
  "abstract": "It has previously been assumed in the psycholinguistic literature that finite-state models of language are crucially limited in their explanatory power by the locality of the probability distribution and the narrow scope of information used by the model. We show that a simple computational model (a bigram part-of-speech tagger based on the design used by Corley and Crocker (2000)) makes correct predictions on processing difficulty observed in a wide range of empirical sentence processing data. We use two modes of evaluation: one that relies on comparison with a control sentence, paralleling practice in human studies; another that measures probability drop in the disambiguating region of the sentence. Both are surprisingly good indicators of the processing difficulty of garden-path sentences. The sentences tested are drawn from published sources and systematically explore five different types of ambiguity: previous studies have been narrower in scope and smaller in scale. We do not deny the limitations of finite-state models, but argue that our results show that their usefulness has been underestimated.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/P06-1098",
  "title": "Left-to-Right Target Generation for Hierarchical Phrase-Based Translation",
  "year": 2006,
  "venue": "ACL",
  "abstract": "We present a hierarchical phrase-based statistical machine translation in which a target sentence is efficiently generated in left-to-right order. The model is a class of synchronous-CFG with a Greibach Normal Form-like structure for the projected production rule: The paired target-side of a production rule takes a phrase prefixed form. The decoder for the targetnormalized form is based on an Earlystyle top down parser on the source side. The target-normalized form coupled with our top down parser implies a left-toright generation of translations which enables us a straightforward integration with ngram language models. Our model was experimented on a Japanese-to-English newswire translation task, and showed statistically significant performance improvements against a phrase-based translation system.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P06-1123",
  "title": "Empirical Lower Bounds on the Complexity of Translational Equivalence",
  "year": 2006,
  "venue": "ACL",
  "abstract": "This paper describes a study of the patterns of translational equivalence exhibited by a variety of bitexts. The study found that the complexity of these patterns in every bitext was higher than suggested in the literature. These findings shed new light on why “syntactic” constraints have not helped to improve statistical translation models, including finitestate phrase-based models, tree-to-string models, and tree-to-tree models. The paper also presents evidence that inversion transduction grammars cannot generate some translational equivalence relations, even in relatively simple real bitexts in syntactically similar languages with rigid word order. Instructions for replicating our experiments are at http://nlp.cs.nyu.edu/GenPar/ACL06",
  "stance": -0.5
 },
 {
  "url": "https://aclanthology.org/P06-1135",
  "title": "Improving QA Accuracy by Question Inversion",
  "year": 2006,
  "venue": "ACL",
  "abstract": "This paper demonstrates a conceptually simple but effective method of increasing the accuracy of QA systems on factoid-style questions. We define the notion of an inverted question, and show that by requiring that the answers to the original and inverted questions be mutually consistent, incorrect answers get demoted in confidence and correct ones promoted. Additionally, we show that lack of validation can be used to assert no-answer (nil) conditions. We demonstrate increases of performance on TREC and other question-sets, and discuss the kinds of future activities that can be particularly beneficial to approaches such as ours.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/P07-1012",
  "title": "Vocabulary Decomposition for Estonian Open Vocabulary Speech Recognition",
  "year": 2007,
  "venue": "ACL",
  "abstract": "Speech recognition in many morphologically rich languages suffers from a very high out-of-vocabulary (OOV) ratio. Earlier work has shown that vocabulary decomposition methods can practically solve this problem for a subset of these languages. This paper compares various vocabulary decomposition approaches to open vocabulary speech recognition, using Estonian speech recognition as a benchmark. Comparisons are performed utilizing large models of 60000 lexical items and smaller vocabularies of 5000 items. A large vocabulary model based on a manually constructed morphological tagger is shown to give the lowest word error rate, while the unsupervised morphology discovery method Morfessor Baseline gives marginally weaker results. Only the Morfessor-based approach is shown to adequately scale to smaller vocabulary sizes.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P07-1034",
  "title": "Instance Weighting for Domain Adaptation in NLP",
  "year": 2007,
  "venue": "ACL",
  "abstract": "Domain adaptation is an important problem in natural language processing (NLP) due to the lack of labeled data in novel domains. In this paper, we study the domain adaptation problem from the instance weighting perspective. We formally analyze and characterize the domain adaptation problem from a distributional view, and show that there are two distinct needs for adaptation, corresponding to the different distributions of instances and classification functions in the source and the target domains. We then propose a general instance weighting framework for domain adaptation. Our empirical results on three NLP tasks show that incorporating and exploiting more information from the target domain through instance weighting is effective.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/P07-1058",
  "title": "Instance-based Evaluation of Entailment Rule Acquisition",
  "year": 2007,
  "venue": "ACL",
  "abstract": "Obtaining large volumes of inference knowledge, such as entailment rules, has become a major factor in achieving robust semantic processing. While there has been substantial research on learning algorithms for such knowledge, their evaluation methodology has been problematic, hindering further research. We propose a novel evaluation methodology for entailment rules which explicitly addresses their semantic properties and yields satisfactory human agreement levels. The methodology is used to compare two state of the art learning algorithms, exposing critical issues for future progress.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/P07-1111",
  "title": "A Re-examination of Machine Learning Approaches for Sentence-Level MT Evaluation",
  "year": 2007,
  "venue": "ACL",
  "abstract": "Recent studies suggest that machine learning can be applied to develop good automatic evaluation metrics for machine translated sentences. This paper further analyzes aspects of learning that impact performance. We argue that previously proposed approaches of training a HumanLikeness classifier is not as well correlated with human judgments of translation quality, but that regression-based learning produces more reliable metrics. We demonstrate the feasibility of regression-based metrics through empirical analysis of learning curves and generalization studies and show that they can achieve higher correlations with human judgments than standard automatic metrics.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/P07-1117",
  "title": "Using Mazurkiewicz Trace Languages for Partition-Based Morphology",
  "year": 2007,
  "venue": "ACL",
  "abstract": "Partition-based morphology is an approach of finite-state morphology where a grammar describes a special kind of regular relations, which split all the strings of a given tuple into the same number of substrings. They are compiled in finite-state machines. In this paper, we address the question of merging grammars using different partitionings into a single finite-state machine. A morphological description may then be obtained by parallel or sequential application of constraints expressed on different partition notions (e.g. morpheme, phoneme, grapheme). The theory of Mazurkiewicz Trace Languages, a well known semantics of parallel systems, provides a way of representing and compiling such a description.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P07-1127",
  "title": "User Requirements Analysis for Meeting Information Retrieval Based on Query Elicitation",
  "year": 2007,
  "venue": "ACL",
  "abstract": "We present a user requirements study for Question Answering on meeting records that assesses the difficulty of users questions in terms of what type of knowledge is required in order to provide the correct answer. We grounded our work on the empirical analysis of elicited user queries. We found that the majority of elicited queries (around 60%) pertain to argumentative processes and outcomes. Our analysis also suggests that standard keyword-based Information Retrieval can only deal successfully with less than 20% of the queries, and that it must be complemented with other types of metadata and inference.",
  "stance": -0.1
 },
 {
  "url": "https://aclanthology.org/P08-1024",
  "title": "A Discriminative Latent Variable Model for Statistical Machine Translation",
  "year": 2008,
  "venue": "ACL",
  "abstract": "Large-scale discriminative machine translation promises to further the state-of-the-art, but has failed to deliver convincing gains over current heuristic frequency count systems. We argue that a principle reason for this failure is not dealing with multiple, equivalent translations. We present a translation model which models derivations as a latent variable, in both training and decoding, and is fully discriminative and globally optimised. Results show that accounting for multiple derivations does indeed improve performance. Additionally, we show that regularisation is essential for maximum conditional likelihood models in order to avoid degenerate solutions.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/P08-1038",
  "title": "A Logical Basis for the D Combinator and Normal Form in CCG",
  "year": 2008,
  "venue": "ACL",
  "abstract": "The standard set of rules defined in Combinatory Categorial Grammar (CCG) fails to provide satisfactory analyses for a number of syntactic structures found in natural languages. These structures can be analyzed elegantly by augmenting CCG with a class of rules based on the combinator D (Curry and Feys, 1958). We show two ways to derive the D rules: one based on unary composition and the other based on a logical characterization of CCG’s rule base (Baldridge, 2002). We also show how Eisner’s (1996) normal form constraints follow from this logic, ensuring that the D rules do not lead to spurious ambiguities.",
  "stance": -0.3
 },
 {
  "url": "https://aclanthology.org/P08-1039",
  "title": "Parsing Noun Phrase Structure with CCG",
  "year": 2008,
  "venue": "ACL",
  "abstract": "Statistical parsing of noun phrase (NP) structure has been hampered by a lack of goldstandard data. This is a significant problem for CCGbank, where binary branching NP derivations are often incorrect, a result of the automatic conversion from the Penn Treebank. We correct these errors in CCGbank using a gold-standard corpus of NP structure, resulting in a much more accurate corpus. We also implement novel NER features that generalise the lexical information needed to parse NPs and provide important semantic information. Finally, evaluating against DepBank demonstrates the effectiveness of our modified corpus and novel features, with an increase in parser performance of 1.51%.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/P08-1063",
  "title": "Robustness and Generalization of Role Sets: PropBank vs. VerbNet",
  "year": 2008,
  "venue": "ACL",
  "abstract": "This paper presents an empirical study on the robustness and generalization of two alternative role sets for semantic role labeling: PropBank numbered roles and VerbNet thematic roles. By testing a state–of–the–art SRL system with the two alternative role annotations, we show that the PropBank role set is more robust to the lack of verb–specific semantic information and generalizes better to infrequent and unseen predicates. Keeping in mind that thematic roles are better for application needs, we also tested the best way to generate VerbNet annotation. We conclude that tagging first PropBank roles and mapping into VerbNet roles is as effective as training and tagging directly on VerbNet, and more robust for domain shifts.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P08-1067",
  "title": "Forest Reranking: Discriminative Parsing with Non-Local Features",
  "year": 2008,
  "venue": "ACL",
  "abstract": "Conventional n-best reranking techniques often suffer from the limited scope of the nbest list, which rules out many potentially good alternatives. We instead propose forest reranking, a method that reranks a packed forest of exponentially many parses. Since exact inference is intractable with non-local features, we present an approximate algorithm inspired by forest rescoring that makes discriminative training practical over the whole Treebank. Our final result, an F-score of 91.7, outperforms both 50-best and 100-best reranking baselines, and is better than any previously reported systems trained on the Treebank.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P08-1071",
  "title": "Assessing Dialog System User Simulation Evaluation Measures Using Human Judges",
  "year": 2008,
  "venue": "ACL",
  "abstract": "Previous studies evaluate simulated dialog corpora using evaluation measures which can be automatically extracted from the dialog systems’ logs. However, the validity of these automatic measures has not been fully proven. In this study, we first recruit human judges to assess the quality of three simulated dialog corpora and then use human judgments as the gold standard to validate the conclusions drawn from the automatic measures. We observe that it is hard for the human judges to reach good agreement when asked to rate the quality of the dialogs from given perspectives. However, the human ratings give consistent ranking of the quality of simulated corpora generated by different simulation models. When building prediction models of human judgments using previously proposed automatic measures, we find that we cannot reliably predict human ratings using a regression model, but we can predict human rankings by a ranking model.",
  "stance": -0.1
 },
 {
  "url": "https://aclanthology.org/P08-1087",
  "title": "Enriching Morphologically Poor Languages for Statistical Machine Translation",
  "year": 2008,
  "venue": "ACL",
  "abstract": "We address the problem of translating from morphologically poor to morphologically rich languages by adding per-word linguistic information to the source language. We use the syntax of the source sentence to extract information for noun cases and verb persons and annotate the corresponding words accordingly. In experiments, we show improved performance for translating from English into Greek and Czech. For English–Greek, we reduce the error on the verb conjugation from 19% to 5.4% and noun case agreement from 9% to 6%.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P08-1100",
  "title": "Analyzing the Errors of Unsupervised Learning",
  "year": 2008,
  "venue": "ACL",
  "abstract": "We identify four types of errors that unsupervised induction systems make and study each one in turn. Our contributions include (1) using a meta-model to analyze the incorrect biases of a model in a systematic way, (2) providing an efficient and robust method of measuring distance between two parameter settings of a model, and (3) showing that local optima issues which typically plague EM can be somewhat alleviated by increasing the number of training examples. We conduct our analyses on three models: the HMM, the PCFG, and a simple dependency model.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/P08-1112",
  "title": "Better Alignments = Better Translations?",
  "year": 2008,
  "venue": "ACL",
  "abstract": "Automatic word alignment is a key step in training statistical machine translation systems. Despite much recent work on word alignment methods, alignment accuracy increases often produce little or no improvements in machine translation quality. In this work we analyze a recently proposed agreement-constrained EM algorithm for unsupervised alignment models. We attempt to tease apart the effects that this simple but effective modification has on alignment precision and recall trade-offs, and how rare and common words are affected across several language pairs. We propose and extensively evaluate a simple method for using alignment models to produce alignments better-suited for phrase-based MT systems, and show significant gains (as measured by BLEU score) in end-to-end translation systems for six languages pairs used in recent MT competitions.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P08-1118",
  "title": "Finding Contradictions in Text",
  "year": 2008,
  "venue": "ACL",
  "abstract": "Detecting conflicting statements is a foundational text understanding task with applications in information analysis. We propose an appropriate definition of contradiction for NLP tasks and develop available corpora, from which we construct a typology of contradictions. We demonstrate that a system for contradiction needs to make more fine-grained distinctions than the common systems for entailment. In particular, we argue for the centrality of event coreference and therefore incorporate such a component based on topicality. We present the first detailed breakdown of performance on this task. Detecting some types of contradiction requires deeper inferential paths than our system is capable of, but we achieve good performance on types arising from negation and antonymy.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/P08-2004",
  "title": "Dimensions of Subjectivity in Natural Language",
  "year": 2008,
  "venue": "ACL",
  "abstract": "Current research in automatic subjectivity analysis deals with various kinds of subjective statements involving human attitudes and emotions. While all of them are related to subjectivity, these statements usually touch on multiple dimensions such as non-objectivity1, uncertainty, vagueness, non-objective measurability, imprecision, and ambiguity, which are inherently different. This paper discusses the differences and relations of six dimensions of subjectivity. Conceptual and linguistic characteristics of each dimension will be demonstrated under different contexts.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P08-2020",
  "title": "Recent Improvements in the CMU Large Scale Chinese-English SMT System",
  "year": 2008,
  "venue": "ACL",
  "abstract": "In this paper we describe recent improvements to components and methods used in our statistical machine translation system for ChineseEnglish used in the January 2008 GALE evaluation. Main improvements are results of consistent data processing, larger statistical models and a POS-based word reordering approach.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P08-2029",
  "title": "Kernels on Linguistic Structures for Answer Extraction",
  "year": 2008,
  "venue": "ACL",
  "abstract": "Natural Language Processing (NLP) for Information Retrieval has always been an interesting and challenging research area. Despite the high expectations, most of the results indicate that successfully using NLP is very complex. In this paper, we show how Support Vector Machines along with kernel functions can effectively represent syntax and semantics. Our experiments on question/answer classification show that the above models highly improve on bag-of-words on a TREC dataset.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/P08-2045",
  "title": "Learning Semantic Links from a Corpus of Parallel Temporal and Causal Relations",
  "year": 2008,
  "venue": "ACL",
  "abstract": "Finding temporal and causal relations is crucial to understanding the semantic structure of a text. Since existing corpora provide no parallel temporal and causal annotations, we annotated 1000 conjoined event pairs, achieving inter-annotator agreement of 81.2% on temporal relations and 77.8% on causal relations. We trained machine learning models using features derived from WordNet and the Google N-gram corpus, and they outperformed a variety of baselines, achieving an F-measure of 49.0 for temporals and 52.4 for causals. Analysis of these models suggests that additional data will improve performance, and that temporal information is crucial to causal relation identification.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P08-2050",
  "title": "Intrinsic vs. Extrinsic Evaluation Measures for Referring Expression Generation",
  "year": 2008,
  "venue": "ACL",
  "abstract": "In this paper we present research in which we apply (i) the kind of intrinsic evaluation metrics that are characteristic of current comparative HLT evaluation, and (ii) extrinsic, human task-performance evaluations more in keeping with NLG traditions, to 15 systems implementing a language generation task. We analyse the evaluation results and find that there are no significant correlations between intrinsic and extrinsic evaluation measures for this task.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/P08-2051",
  "title": "Correlation between ROUGE and Human Evaluation of Extractive Meeting Summaries",
  "year": 2008,
  "venue": "ACL",
  "abstract": "Automatic summarization evaluation is critical to the development of summarization systems. While ROUGE has been shown to correlate well with human evaluation for content match in text summarization, there are many characteristics in multiparty meeting domain, which may pose potential problems to ROUGE. In this paper, we carefully examine how well the ROUGE scores correlate with human evaluation for extractive meeting summarization. Our experiments show that generally the correlation is rather low, but a significantly better correlation can be obtained by accounting for several unique meeting characteristics, such as disfluencies and speaker information, especially when evaluating system-generated summaries.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/P08-2053",
  "title": "Construct State Modification in the Arabic Treebank",
  "year": 2008,
  "venue": "ACL",
  "abstract": "Earlier work in parsing Arabic has speculated that attachment to construct state constructions decreases parsing performance. We make this speculation precise and define the problem of attachment to construct state constructions in the Arabic Treebank. We present the first statistics that quantify the problem. We provide a baseline and the results from a first attempt at a discriminative learning procedure for this task, achieving 80% accuracy.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/P09-1027",
  "title": "Co-Training for Cross-Lingual Sentiment Classification",
  "year": 2009,
  "venue": "ACL",
  "abstract": "The lack of Chinese sentiment corpora limits the research progress on Chinese sentiment classification. However, there are many freely available English sentiment corpora on the Web. This paper focuses on the problem of cross-lingual sentiment classification, which leverages an available English corpus for Chinese sentiment classification by using the English corpus as training data. Machine translation services are used for eliminating the language gap between the training set and test set, and English features and Chinese features are considered as two independent views of the classification problem. We propose a cotraining approach to making use of unlabeled Chinese data. Experimental results show the effectiveness of the proposed approach, which can outperform the standard inductive classifiers and the transductive classifiers.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P09-1035",
  "title": "The Contribution of Linguistic Features to Automatic Machine Translation Evaluation",
  "year": 2009,
  "venue": "ACL",
  "abstract": "A number of approaches to Automatic MT Evaluation based on deep linguistic knowledge have been suggested. However, n-gram based metrics are still today the dominant approach. The main reason is that the advantages of employing deeper linguistic information have not been clarified yet. In this work, we propose a novel approach for meta-evaluation of MT evaluation metrics, since correlation cofficient against human judges do not reveal details about the advantages and disadvantages of particular metrics. We then use this approach to investigate the benefits of introducing linguistic features into evaluation metrics. Overall, our experiments show that (i) both lexical and linguistic metrics present complementary advantages and (ii) combining both kinds of metrics yields the most robust metaevaluation performance.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/P09-1042",
  "title": "Dependency Grammar Induction via Bitext Projection Constraints",
  "year": 2009,
  "venue": "ACL",
  "abstract": "Broad-coverage annotated treebanks necessary to train parsers do not exist for many resource-poor languages. The wide availability of parallel text and accurate parsers in English has opened up the possibility of grammar induction through partial transfer across bitext. We consider generative and discriminative models for dependency grammar induction that use word-level alignments and a source language parser (English) to constrain the space of possible target trees. Unlike previous approaches, our framework does not require full projected parses, allowing partial, approximate transfer through linear expectation constraints on the space of distributions over trees. We consider several types of constraints that range from generic dependency conservation to language-specific annotation rules for auxiliary verb analysis. We evaluate our approach on Bulgarian and Spanish CoNLL shared task data and show that we consistently outperform unsupervised methods and can outperform supervised learning for limited training data.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/P09-1043",
  "title": "Cross-Domain Dependency Parsing Using a Deep Linguistic Grammar",
  "year": 2009,
  "venue": "ACL",
  "abstract": "Pure statistical parsing systems achieves high in-domain accuracy but performs poorly out-domain. In this paper, we propose two different approaches to produce syntactic dependency structures using a large-scale hand-crafted HPSG grammar. The dependency backbone of an HPSG analysis is used to provide general linguistic insights which, when combined with state-of-the-art statistical dependency parsing models, achieves performance improvements on out-domain tests.†",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/P09-1048",
  "title": "Who, What, When, Where, Why? Comparing Multiple Approaches to the Cross-Lingual 5W Task",
  "year": 2009,
  "venue": "ACL",
  "abstract": "Cross-lingual tasks are especially difficult due to the compounding effect of errors in language processing and errors in machine translation (MT). In this paper, we present an error analysis of a new cross-lingual task: the 5W task, a sentence-level understanding task which seeks to return the English 5W's (Who, What, When, Where and Why) corresponding to a Chinese sentence. We analyze systems that we developed, identifying specific problems in language processing and MT that cause errors. The best cross-lingual 5W system was still 19% worse than the best monolingual 5W system, which shows that MT significantly degrades sentence-level understanding. Neither source-language nor targetlanguage analysis was able to circumvent problems in MT, although each approach had advantages relative to the other. A detailed error analysis across multiple systems suggests directions for future research on the problem.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P09-1059",
  "title": "Automatic Adaptation of Annotation Standards: Chinese Word Segmentation and POS Tagging - A Case Study",
  "year": 2009,
  "venue": "ACL",
  "abstract": "Manually annotated corpora are valuable but scarce resources, yet for many annotation tasks such as treebanking and sequence labeling there exist multiple corpora with different and incompatible annotation guidelines or standards. This seems to be a great waste of human efforts, and it would be nice to automatically adapt one annotation standard to another. We present a simple yet effective strategy that transfers knowledge from a differently annotated corpus to the corpus with desired annotation. We test the efficacy of this method in the context of Chinese word segmentation and part-of-speech tagging, where no segmentation and POS tagging standards are widely accepted due to the lack of morphology in Chinese. Experiments show that adaptation from the much larger People’s Daily corpus to the smaller but more popular Penn Chinese Treebank results in significant improvements in both segmentation and tagging accuracies (with error reductions of 30.2% and 14%, respectively), which in turn helps improve Chinese parsing accuracy.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/P09-1063",
  "title": "Improving Tree-to-Tree Translation with Packed Forests",
  "year": 2009,
  "venue": "ACL",
  "abstract": "Current tree-to-tree models suffer from parsing errors as they usually use only 1best parses for rule extraction and decoding. We instead propose a forest-based tree-to-tree model that uses packed forests. The model is based on a probabilistic synchronous tree substitution grammar (STSG), which can be learned from aligned forest pairs automatically. The decoder finds ways of decomposing trees in the source forest into elementary trees using the source projection of STSG while building target forest in parallel. Comparable to the state-of-the-art phrase-based system Moses, using packed forests in tree-to-tree translation results in a significant absolute improvement of 3.6 BLEU points over using 1-best trees.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/P09-2018",
  "title": "Directional Distributional Similarity for Lexical Expansion",
  "year": 2009,
  "venue": "ACL",
  "abstract": "Distributional word similarity is most commonly perceived as a symmetric relation. Yet, one of its major applications is lexical expansion, which is generally asymmetric. This paper investigates the nature of directional (asymmetric) similarity measures, which aim to quantify distributional feature inclusion. We identify desired properties of such measures, specify a particular one based on averaged precision, and demonstrate the empirical benefit of directional measures for expansion.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/P09-2025",
  "title": "Correlating Human and Automatic Evaluation of a German Surface Realiser",
  "year": 2009,
  "venue": "ACL",
  "abstract": "We examine correlations between native speaker judgements on automatically generated German text against automatic evaluation metrics. We look at a number of metrics from the MT and Summarisation communities and find that for a relative ranking task, most automatic metrics perform equally well and have fairly strong correlations to the human judgements. In contrast, on a naturalness judgement task, the General Text Matcher (GTM) tool correlates best overall, although in general, correlation between the human judgements and the automatic metrics was quite weak.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P09-2027",
  "title": "Query-Focused Summaries or Query-Biased Summaries?",
  "year": 2009,
  "venue": "ACL",
  "abstract": "In the context of the Document Understanding Conferences, the task of Query-Focused Multi-Document Summarization is intended to improve agreement in content among humangenerated model summaries. Query-focus also aids the automated summarizers in directing the summary at specific topics, which may result in better agreement with these model summaries. However, while query focus correlates with performance, we show that highperforming automatic systems produce summaries with disproportionally higher query term density than human summarizers do. Experimental evidence suggests that automatic systems heavily rely on query term occurrence and repetition to achieve good performance.",
  "stance": -0.4
 },
 {
  "url": "https://aclanthology.org/P09-2066",
  "title": "From Extractive to Abstractive Meeting Summaries: Can It Be Done by Sentence Compression?",
  "year": 2009,
  "venue": "ACL",
  "abstract": "Most previous studies on meeting summarization have focused on extractive summarization. In this paper, we investigate if we can apply sentence compression to extractive summaries to generate abstractive summaries. We use different compression algorithms, including integer linear programming with an additional step of filler phrase detection, a noisychannel approach using Markovization formulation of grammar rules, as well as human compressed sentences. Our experiments on the ICSI meeting corpus show that when compared to the abstractive summaries, using sentence compression on the extractive summaries improves their ROUGE scores; however, the best performance is still quite low, suggesting the need of language generation for abstractive summarization.",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/P10-1035",
  "title": "Accurate Context-Free Parsing with Combinatory Categorial Grammar",
  "year": 2010,
  "venue": "ACL",
  "abstract": "The definition of combinatory categorial grammar (CCG) in the literature varies quite a bit from author to author. However, the differences between the definitions are important in terms of the language classes of each CCG. We prove that a wide range of CCGs are strongly context-free, including the CCG of CCGbank and of the parser of Clark and Curran (2007). In light of these new results, we train the PCFG parser of Petrov and Klein (2007) on CCGbank and achieve state of the art results in supertagging accuracy, PARSEVAL measures and dependency accuracy.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/P10-1053",
  "title": "On the Computational Complexity of Dominance Links in Grammatical Formalisms",
  "year": 2010,
  "venue": "ACL",
  "abstract": "Dominance links were introduced in grammars to model long distance scrambling phenomena, motivating the definition of multiset-valued linear indexed grammars (MLIGs) by Rambow (1994b), and inspiring quite a few recent formalisms. It turns out that MLIGs have since been rediscovered and reused in a variety of contexts, and that the complexity of their emptiness problem has become the key to several open questions in computer science. We survey complexity results and open issues on MLIGs and related formalisms, and provide new complexity bounds for some linguistically motivated restrictions.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/P10-1111",
  "title": "Hard Constraints for Grammatical Function Labelling",
  "year": 2010,
  "venue": "ACL",
  "abstract": "For languages with (semi-) free word order (such as German), labelling grammatical functions on top of phrase-structural constituent analyses is crucial for making them interpretable. Unfortunately, most statistical classifiers consider only local information for function labelling and fail to capture important restrictions on the distribution of core argument functions such as subject, object etc., namely that there is at most one subject (etc.) per clause. We augment a statistical classifier with an integer linear program imposing hard linguistic constraints on the solution space output by the classifier, capturing global distributional restrictions. We show that this improves labelling quality, in particular for argument grammatical functions, in an intrinsic evaluation, and, importantly, grammar coverage for treebankbased (Lexical-Functional) grammar acquisition and parsing, in an extrinsic evaluation.",
  "stance": 0.1
 },
 {
  "url": "https://aclanthology.org/P10-1127",
  "title": "Generating Image Descriptions Using Dependency Relational Patterns",
  "year": 2010,
  "venue": "ACL",
  "abstract": "This paper presents a novel approach to automatic captioning of geo-tagged images by summarizing multiple webdocuments that contain information related to an image’s location. The summarizer is biased by dependency pattern models towards sentences which contain features typically provided for different scene types such as those of churches, bridges, etc. Our results show that summaries biased by dependency pattern models lead to significantly higher ROUGE scores than both n-gram language models reported in previous work and also Wikipedia baseline summaries. Summaries generated using dependency patterns also lead to more readable summaries than those generated without dependency patterns.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P10-1135",
  "title": "On Learning Subtypes of the Part-Whole Relation: Do Not Mix Your Seeds",
  "year": 2010,
  "venue": "ACL",
  "abstract": "An important relation in information extraction is the part-whole relation. Ontological studies mention several types of this relation. In this paper, we show that the traditional practice of initializing minimally-supervised algorithms with a single set that mixes seeds of different types fails to capture the wide variety of part-whole patterns and tuples. The results obtained with mixed seeds ultimately converge to one of the part-whole relation types. We also demonstrate that all the different types of part-whole relations can still be discovered, regardless of the type characterized by the initializing seeds. We performed our experiments with a state-ofthe-art information extraction algorithm.",
  "stance": -0.9
 },
 {
  "url": "https://aclanthology.org/P10-1144",
  "title": "Coreference Resolution across Corpora: Languages, Coding Schemes, and Preprocessing Information",
  "year": 2010,
  "venue": "ACL",
  "abstract": "This paper explores the effect that different corpus configurations have on the performance of a coreference resolution system, as measured by MUC, B3, and CEAF. By varying separately three parameters (language, annotation scheme, and preprocessing information) and applying the same coreference resolution system, the strong bonds between system and corpus are demonstrated. The experiments reveal problems in coreference resolution evaluation relating to task definition, coding schemes, and features. They also expose systematic biases in the coreference evaluation metrics. We show that system comparison is only possible when corpus parameters are in exact agreement.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/P10-1153",
  "title": "A Generalized-Zero-Preserving Method for Compact Encoding of Concept Lattices",
  "year": 2010,
  "venue": "ACL",
  "abstract": "Constructing an encoding of a concept lattice using short bit vectors allows for efficient computation of join operations on the lattice. Join is the central operation any unification-based parser must support. We extend the traditional bit vector encoding, which represents join failure using the zero vector, to count any vector with less than a fixed number of one bits as failure. This allows non-joinable elements to share bits, resulting in a smaller vector size. A constraint solver is used to construct the encoding, and a variety of techniques are employed to find near-optimal solutions and handle timeouts. An evaluation is provided comparing the extended representation of failure with traditional bit vector",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P10-2012",
  "title": "Cognitively Plausible Models of Human Language Processing",
  "year": 2010,
  "venue": "ACL",
  "abstract": "We pose the development of cognitively plausible models of human language processing as a challenge for computational linguistics. Existing models can only deal with isolated phenomena (e.g., garden paths) on small, specifically selected data sets. The challenge is to build models that integrate multiple aspects of human language processing at the syntactic, semantic, and discourse level. Like human language processing, these models should be incremental, predictive, broad coverage, and robust to noise. This challenge can only be met if standardized data sets and evaluation measures are developed.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/P10-2016",
  "title": "Tackling Sparse Data Issue in Machine Translation Evaluation",
  "year": 2010,
  "venue": "ACL",
  "abstract": "We illustrate and explain problems of n-grams-based machine translation (MT) metrics (e.g. BLEU) when applied to morphologically rich languages such as Czech. A novel metric SemPOS based on the deep-syntactic representation of the sentence tackles the issue and retains the performance for translation to English as well.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/P10-2017",
  "title": "Exemplar-Based Models for Word Meaning in Context",
  "year": 2010,
  "venue": "ACL",
  "abstract": "This paper describes ongoing work on distributional models for word meaning in context. We abandon the usual one-vectorper-word paradigm in favor of an exemplar model that activates only relevant occurrences. On a paraphrasing task, we find that a simple exemplar model outperforms more complex state-of-the-art models.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P10-2024",
  "title": "Complexity Assumptions in Ontology Verbalisation",
  "year": 2010,
  "venue": "ACL",
  "abstract": "We describe the strategy currently pursued for verbalising OWL ontologies by sentences in Controlled Natural Language (i.e., combining generic rules for realising logical patterns with ontology-specific lexicons for realising atomic terms for individuals, classes, and properties) and argue that its success depends on assumptions about the complexity of terms and axioms in the ontology. We then show, through analysis of a corpus of ontologies, that although these assumptions could in principle be violated, they are overwhelmingly respected in practice by ontology develop-",
  "stance": -0.1
 },
 {
  "url": "https://aclanthology.org/P10-2029",
  "title": "Coreference Resolution with Reconcile",
  "year": 2010,
  "venue": "ACL",
  "abstract": "Despite the existence of several noun phrase coreference resolution data sets as well as several formal evaluations on the task, it remains frustratingly difficult to compare results across different coreference resolution systems. This is due to the high cost of implementing a complete end-to-end coreference resolution system, which often forces researchers to substitute available gold-standard information in lieu of implementing a module that would compute that information. Unfortunately, this leads to inconsistent and often unrealistic evaluation scenarios. With the aim to facilitate consistent and realistic experimental evaluations in coreference resolution, we present Reconcile, an infrastructure for the development of learning-based noun phrase (NP) coreference resolution systems. Reconcile is designed to facilitate the rapid creation of coreference resolution systems, easy implementation of new feature sets and approaches to coreference resolution, and empirical evaluation of coreference resolvers across a variety of benchmark data sets and standard scoring metrics. We describe Reconcile and present experimental results showing that Reconcile can be used to create a coreference resolver that achieves performance comparable to state-ofthe-art systems on six benchmark data sets.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/P10-2046",
  "title": "Don't 'Have a Clue'? Unsupervised Co-Learning of Downward-Entailing Operators.",
  "year": 2010,
  "venue": "ACL",
  "abstract": "Researchers in textual entailment have begun to consider inferences involving downward-entailing operators, an interesting and important class of lexical items that change the way inferences are made. Recent work proposed a method for learning English downward-entailing operators that requires access to a high-quality collection of negative polarity items (NPIs). However, English is one of the very few languages for which such a list exists. We propose the first approach that can be applied to the many languages for which there is no pre-existing high-precision database of NPIs. As a case study, we apply our method to Romanian and show that our method yields good results. Also, we perform a cross-linguistic analysis that suggests interesting connections to some findings in linguistic typology.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/P10-2058",
  "title": "Using Speech to Reply to SMS Messages While Driving: An In-Car Simulator User Study",
  "year": 2010,
  "venue": "ACL",
  "abstract": "Speech recognition affords automobile drivers a hands-free, eyes-free method of replying to Short Message Service (SMS) text messages. Although a voice search approach based on template matching has been shown to be more robust to the challenging acoustic environment of automobiles than using dictation, users may have difficulties verifying whether SMS response templates match their intended meaning, especially while driving. Using a high-fidelity driving simulator, we compared dictation for SMS replies versus voice search in increasingly difficult driving conditions. Although the two approaches did not differ in terms of driving performance measures, users made about six times more errors on average using dictation than voice search.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P11-2075",
  "title": "Is Machine Translation Ripe for Cross-Lingual Sentiment Classification?",
  "year": 2011,
  "venue": "ACL",
  "abstract": "Recent advances in Machine Translation (MT) have brought forth a new paradigm for building NLP applications in low-resource scenarios. To build a sentiment classifier for a language with no labeled resources, one can translate labeled data from another language, then train a classifier on the translated text. This can be viewed as a domain adaptation problem, where labeled translations and test data have some mismatch. Various prior work have achieved positive results using this approach. In this opinion piece, we take a step back and make some general statements about crosslingual adaptation problems. First, we claim that domain mismatch is not caused by MT errors, and accuracy degradation will occur even in the case of perfect MT. Second, we argue that the cross-lingual adaptation problem is qualitatively different from other (monolingual) adaptation problems in NLP; thus new adaptation algorithms ought to be considered. This paper will describe a series of carefullydesigned experiments that led us to these conclusions.",
  "stance": -0.4
 },
 {
  "url": "https://aclanthology.org/P11-2107",
  "title": "Predicting Relative Prominence in Noun-Noun Compounds",
  "year": 2011,
  "venue": "ACL",
  "abstract": "There are several theories regarding what influences prominence assignment in English noun-noun compounds. We have developed corpus-driven models for automatically predicting prominence assignment in noun-noun compounds using feature sets based on two such theories: the informativeness theory and the semantic composition theory. The evaluation of the prediction models indicate that though both of these theories are relevant, they account for different types of variability in prominence assignment.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P12-1024",
  "title": "Spectral Learning of Latent-Variable PCFGs",
  "year": 2012,
  "venue": "ACL",
  "abstract": "Jeju, Republic of Korea, 8-14 July 2012. c ©2012 Association for Computational Linguistics Spectral Learning of Latent-Variable PCFGs Shay B. Cohen, Karl Stratos, Michael Collins, Dean P. Foster, and Lyle Ungar Dept. of Computer Science, Columbia University Dept. of Statistics/Dept. of Computer and Information Science, University of Pennsylvania {scohen,stratos,mcollins}@cs.columbia.edu, foster@wharton.upenn.edu, ungar@cis.upenn.edu Abstract",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P12-1070",
  "title": "MIX Is Not a Tree-Adjoining Language",
  "year": 2012,
  "venue": "ACL",
  "abstract": "The language MIX consists of all strings over the three-letter alphabet {a, b, c} that contain an equal number of occurrences of each letter. We prove Joshi’s (1985) conjecture that MIX is not a tree-adjoining language.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P12-1110",
  "title": "Incremental Joint Approach to Word Segmentation, POS Tagging, and Dependency Parsing in Chinese",
  "year": 2012,
  "venue": "ACL",
  "abstract": "We propose the first joint model for word segmentation, POS tagging, and dependency parsing for Chinese. Based on an extension of the incremental joint model for POS tagging and dependency parsing (Hatori et al., 2011), we propose an efficient character-based decoding method that can combine features from state-of-the-art segmentation, POS tagging, and dependency parsing models. We also describe our method to align comparable states in the beam, and how we can combine features of different characteristics in our incremental framework. In experiments using the Chinese Treebank (CTB), we show that the accuracies of the three tasks can be improved significantly over the baseline models, particularly by 0.6% for POS tagging and 2.4% for dependency parsing. We also perform comparison experiments with the partially joint models.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P12-2070",
  "title": "Assessing the Effect of Inconsistent Assessors on Summarization Evaluation",
  "year": 2012,
  "venue": "ACL",
  "abstract": "We investigate the consistency of human assessors involved in summarization evaluation to understand its effect on system ranking and automatic evaluation techniques. Using Text Analysis Conference data, we measure annotator consistency based on human scoring of summaries for Responsiveness, Readability, and Pyramid scoring. We identify inconsistencies in the data and measure to what extent these inconsistencies affect the ranking of automatic summarization systems. Finally, we examine the stability of automatic metrics (ROUGE and CLASSY) with respect to the inconsistent assessments.",
  "stance": -0.6
 },
 {
  "url": "https://aclanthology.org/P13-1036",
  "title": "Scalable Decipherment for Machine Translation via Hash Sampling",
  "year": 2013,
  "venue": "ACL",
  "abstract": "In this paper, we propose a new Bayesian inference method to train statistical machine translation systems using only nonparallel corpora. Following a probabilistic decipherment approach, we first introduce a new framework for decipherment training that is flexible enough to incorporate any number/type of features (besides simple bag-of-words) as side-information used for estimating translation models. In order to perform fast, efficient Bayesian inference in this framework, we then derive a hash sampling strategy that is inspired by the work of Ahmed et al. (2012). The new translation hash sampler enables us to scale elegantly to complex models (for the first time) and large vocabulary/corpora sizes. We show empirical results on the OPUS data—our method yields the best BLEU scores compared to existing approaches, while achieving significant computational speedups (several orders faster). We also report for the first time—BLEU score results for a largescale MT task using only non-parallel data (EMEA corpus).",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P13-1059",
  "title": "Name-aware Machine Translation",
  "year": 2013,
  "venue": "ACL",
  "abstract": "We propose a Name-aware Machine Translation (MT) approach which can tightly integrate name processing into MT model, by jointly annotating parallel corpora, extracting name-aware translation grammar and rules, adding name phrase table and name translation driven decoding. Additionally, we also propose a new MT metric to appropriately evaluate the translation quality of informative words, by assigning different weights to different words according to their importance values in a document. Experiments on Chinese-English translation demonstrated the effectiveness of our approach on enhancing the quality of overall translation, name translation and word alignment over a high-quality MT baseline1.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P13-1071",
  "title": "The Impact of Topic Bias on Quality Flaw Prediction in Wikipedia",
  "year": 2013,
  "venue": "ACL",
  "abstract": "With the increasing amount of user generated reference texts in the web, automatic quality assessment has become a key challenge. However, only a small amount of annotated data is available for training quality assessment systems. Wikipedia contains a large amount of texts annotated with cleanup templates which identify quality flaws. We show that the distribution of these labels is topically biased, since they cannot be applied freely to any arbitrary article. We argue that it is necessary to consider the topical restrictions of each label in order to avoid a sampling bias that results in a skewed classifier and overly optimistic evaluation results. We factor out the topic bias by extracting reliable training instances from the revision history which have a topic distribution similar to the labeled articles. This approach better reflects the situation a classifier would face in a real-life application.",
  "stance": -0.3
 },
 {
  "url": "https://aclanthology.org/P13-1118",
  "title": "DErivBase: Inducing and Evaluating a Derivational Morphology Resource for German",
  "year": 2013,
  "venue": "ACL",
  "abstract": "Derivational models are still an underresearched area in computational morphology. Even for German, a rather resourcerich language, there is a lack of largecoverage derivational knowledge. This paper describes a rule-based framework for inducing derivational families (i.e., clusters of lemmas in derivational relationships) and its application to create a highcoverage German resource, DERIVBASE, mapping over 280k lemmas into more than 17k non-singleton clusters. We focus on the rule component and a qualitative and quantitative evaluation. Our approach achieves up to 93% precision and 71% recall. We attribute the high precision to the fact that our rules are based on information from grammar books.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/P13-1121",
  "title": "Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain",
  "year": 2013,
  "venue": "ACL",
  "abstract": "In automatic summarization, centrality is the notion that a summary should contain the core parts of the source text. Current systems use centrality, along with redundancy avoidance and some sentence compression, to produce mostly extractive summaries. In this paper, we investigate how summarization can advance past this paradigm towards robust abstraction by making greater use of the domain of the source text. We conduct a series of studies comparing human-written model summaries to system summaries at the semantic level of caseframes. We show that model summaries (1) are more abstractive and make use of more sentence aggregation, (2) do not contain as many topical caseframes as system summaries, and (3) cannot be reconstructed solely from the source text, but can be if texts from in-domain documents are added. These results suggest that substantial improvements are unlikely to result from better optimizing centrality-based criteria, but rather more domain knowledge is needed.",
  "stance": -0.4
 },
 {
  "url": "https://aclanthology.org/P13-1139",
  "title": "Models of Translation Competitions",
  "year": 2013,
  "venue": "ACL",
  "abstract": "What do we want to learn from a translation competition and how do we learn it with confidence? We argue that a disproportionate focus on ranking competition participants has led to lots of different rankings, but little insight about which rankings we should trust. In response, we provide the first framework that allows an empirical comparison of different analyses of competition results. We then use this framework to compare several analytical models on data from the Workshop on Machine Translation (WMT).",
  "stance": 0.4
 },
 {
  "url": "https://aclanthology.org/P13-1166",
  "title": "Offspring from Reproduction Problems: What Replication Failure Teaches Us",
  "year": 2013,
  "venue": "ACL",
  "abstract": "Repeating experiments is an important instrument in the scientific toolbox to validate previous work and build upon existing work. We present two concrete use cases involving key techniques in the NLP domain for which we show that reproducing results is still difficult. We show that the deviation that can be found in reproduction efforts leads to questions about how our results should be interpreted. Moreover, investigating these deviations provides new insights and a deeper understanding of the examined techniques. We identify five aspects that can influence the outcomes of experiments that are typically not addressed in research papers. Our use cases show that these aspects may change the answer to research questions leading us to conclude that more care should be taken in interpreting our results and more research involving systematic testing of methods is required in our field.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P13-1167",
  "title": "Evaluating Text Segmentation using Boundary Edit Distance",
  "year": 2013,
  "venue": "ACL",
  "abstract": "This work proposes a new segmentation evaluation metric, named boundary similarity (B), an inter-coder agreement coefficient adaptation, and a confusion-matrix for segmentation that are all based upon an adaptation of the boundary edit distance in Fournier and Inkpen (2012). Existing segmentation metrics such as Pk, WindowDiff, and Segmentation Similarity (S) are all able to award partial credit for near misses between boundaries, but are biased towards segmentations containing few or tightly clustered boundaries. Despite S’s improvements, its normalization also produces cosmetically high values that overestimate agreement & performance, leading this work to propose a solution.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/P13-1168",
  "title": "Crowd Prefers the Middle Path: A New IAA Metric for Crowdsourcing Reveals Turker Biases in Query Segmentation",
  "year": 2013,
  "venue": "ACL",
  "abstract": "Query segmentation, like text chunking, is the first step towards query understanding. In this study, we explore the effectiveness of crowdsourcing for this task. Through carefully designed control experiments and Inter Annotator Agreement metrics for analysis of experimental data, we show that crowdsourcing may not be a suitable approach for query segmentation because the crowd seems to have a very strong bias towards dividing the query into roughly equal (often only two) parts. Similarly, in the case of hierarchical or nested segmentation, turkers have a strong preference towards balanced binary trees.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/P13-2068",
  "title": "Bilingual Lexical Cohesion Trigger Model for Document-Level Machine Translation",
  "year": 2013,
  "venue": "ACL",
  "abstract": "In this paper, we propose a bilingual lexical cohesion trigger model to capture lexical cohesion for document-level machine translation. We integrate the model into hierarchical phrase-based machine translation and achieve an absolute improvement of 0.85 BLEU points on average over the baseline on NIST Chinese-English test sets.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P13-2151",
  "title": "Does Korean defeat phonotactic word segmentation?",
  "year": 2013,
  "venue": "ACL",
  "abstract": "Computational models of infant word segmentation have not been tested on a wide range of languages. This paper applies a phonotactic segmentation model to Korean. In contrast to the undersegmentation pattern previously found in English and Russian, the model exhibited more oversegmentation errors and more errors overall. Despite the high error rate, analysis suggested that lexical acquisition might not be problematic, provided that infants attend only to frequently segmented items.",
  "stance": -0.4
 },
 {
  "url": "https://aclanthology.org/P14-1008",
  "title": "Logical Inference on Dependency-based Compositional Semantics",
  "year": 2014,
  "venue": "ACL",
  "abstract": "Dependency-based Compositional Semantics (DCS) is a framework of natural language semantics with easy-to-process structures as well as strict semantics. In this paper, we equip the DCS framework with logical inference, by defining abstract denotations as an abstraction of the computing process of denotations in original DCS. An inference engine is built to achieve inference on abstract denotations. Furthermore, we propose a way to generate on-the-fly knowledge in logical inference, by combining our framework with the idea of tree transformation. Experiments on FraCaS and PASCAL RTE datasets show promising results.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/P14-1039",
  "title": "That's Not What I Meant! Using Parsers to Avoid Structural Ambiguities in Generated Text",
  "year": 2014,
  "venue": "ACL",
  "abstract": "We investigate whether parsers can be used for self-monitoring in surface realization in order to avoid egregious errors involving “vicious” ambiguities, namely those where the intended interpretation fails to be considerably more likely than alternative ones. Using parse accuracy in a simple reranking strategy for selfmonitoring, we find that with a stateof-the-art averaged perceptron realization ranking model, BLEU scores cannot be improved with any of the well-known Treebank parsers we tested, since these parsers too often make errors that human readers would be unlikely to make. However, by using an SVM ranker to combine the realizer’s model score together with features from multiple parsers, including ones designed to make the ranker more robust to parsing mistakes, we show that significant increases in BLEU scores can be achieved. Moreover, via a targeted manual analysis, we demonstrate that the SVM reranker frequently manages to avoid vicious ambiguities, while its ranking errors tend to affect fluency much more often than adequacy.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/P14-1047",
  "title": "Single-Agent vs. Multi-Agent Techniques for Concurrent Reinforcement Learning of Negotiation Dialogue Policies",
  "year": 2014,
  "venue": "ACL",
  "abstract": "We use single-agent and multi-agent Reinforcement Learning (RL) for learning dialogue policies in a resource allocation negotiation scenario. Two agents learn concurrently by interacting with each other without any need for simulated users (SUs) to train against or corpora to learn from. In particular, we compare the Qlearning, Policy Hill-Climbing (PHC) and Win or Learn Fast Policy Hill-Climbing (PHC-WoLF) algorithms, varying the scenario complexity (state space size), the number of training episodes, the learning rate, and the exploration rate. Our results show that generally Q-learning fails to converge whereas PHC and PHC-WoLF always converge and perform similarly. We also show that very high gradually decreasing exploration rates are required for convergence. We conclude that multiagent RL of dialogue policies is a promising alternative to using single-agent RL and SUs or learning directly from corpora.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P14-1049",
  "title": "Negation Focus Identification with Contextual Discourse Information",
  "year": 2014,
  "venue": "ACL",
  "abstract": "Negative expressions are common in natural language text and play a critical role in information extraction. However, the performances of current systems are far from satisfaction, largely due to its focus on intrasentence information and its failure to consider inter-sentence information. In this paper, we propose a graph model to enrich intrasentence features with inter-sentence features from both lexical and topic perspectives. Evaluation on the *SEM 2012 shared task corpus indicates the usefulness of contextual discourse information in negation focus identification and justifies the effectiveness of our graph model in capturing such global information. *",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P14-1119",
  "title": "Automatic Keyphrase Extraction: A Survey of the State of the Art",
  "year": 2014,
  "venue": "ACL",
  "abstract": "While automatic keyphrase extraction has been examined extensively, state-of-theart performance on this task is still much lower than that on many core natural language processing tasks. We present a survey of the state of the art in automatic keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead.",
  "stance": -0.6
 },
 {
  "url": "https://aclanthology.org/P14-1140",
  "title": "A Recursive Recurrent Neural Network for Statistical Machine Translation",
  "year": 2014,
  "venue": "ACL",
  "abstract": "In this paper, we propose a novel recursive recurrent neural network (R2NN) to model the end-to-end decoding process for statistical machine translation. R2NN is a combination of recursive neural network and recurrent neural network, and in turn integrates their respective capabilities: (1) new information can be used to generate the next hidden state, like recurrent neural networks, so that language model and translation model can be integrated naturally; (2) a tree structure can be built, as recursive neural networks, so as to generate the translation candidates in a bottom up manner. A semi-supervised training approach is proposed to train the parameters, and the phrase pair embedding is explored to model translation confidence directly. Experiments on a Chinese to English translation task show that our proposed R2NN can outperform the stateof-the-art baseline by about 1.5 points in BLEU.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P14-2045",
  "title": "Improving the Recognizability of Syntactic Relations Using Contextualized Examples",
  "year": 2014,
  "venue": "ACL",
  "abstract": "A common task in qualitative data analysis is to characterize the usage of a linguistic entity by issuing queries over syntactic relations between words. Previous interfaces for searching over syntactic structures require programming-style queries. User interface research suggests that it is easier to recognize a pattern than to compose it from scratch; therefore, interfaces for non-experts should show previews of syntactic relations. What these previews should look like is an open question that we explored with a 400-participant Mechanical Turk experiment. We found that syntactic relations are recognized with 34% higher accuracy when contextual examples are shown than a baseline of naming the relations alone. This suggests that user interfaces should display contextual examples of syntactic relations to help users choose between different relations.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/P14-2065",
  "title": "The VerbCorner Project: Findings from Phase 1 of crowd-sourcing a semantic decomposition of verbs",
  "year": 2014,
  "venue": "ACL",
  "abstract": "Any given verb can appear in some syntactic frames (Sally broke the vase, The vase broke) but not others (*Sally broke at the vase, *Sally broke the vase to John). There is now considerable evidence that the syntactic behaviors of some verbs can be predicted by their meanings, and many current theories posit that this is true for most if not all verbs. If true, this fact would have striking implications for theories and models of language acquisition, as well as numerous applications in natural language processing. However, empirical investigations to date have focused on a small number of verbs. We report on early results from VerbCorner, a crowd-sourced project extending this work to a large, representative sample of English verbs.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/P14-2068",
  "title": "Modeling Factuality Judgments in Social Media Text",
  "year": 2014,
  "venue": "ACL",
  "abstract": "How do journalists mark quoted content as certain or uncertain, and how do readers interpret these signals? Predicates such as thinks, claims, and admits offer a range of options for framing quoted content according to the author’s own perceptions of its credibility. We gather a new dataset of direct and indirect quotes from Twitter, and obtain annotations of the perceived certainty of the quoted statements. We then compare the ability of linguistic and extra-linguistic features to predict readers’ assessment of the certainty of quoted content. We see that readers are indeed influenced by such framing devices — and we find no evidence that they consider other factors, such as the source, journalist, or the content itself. In addition, we examine the impact of specific framing devices on perceptions of credibility.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/P14-2083",
  "title": "Linguistically debatable or just plain wrong?",
  "year": 2014,
  "venue": "ACL",
  "abstract": "In linguistic annotation projects, we typically develop annotation guidelines to minimize disagreement. However, in this position paper we question whether we should actually limit the disagreements between annotators, rather than embracing them. We present an empirical analysis of part-of-speech annotated data sets that suggests that disagreements are systematic across domains and to a certain extend also across languages. This points to an underlying ambiguity rather than random errors. Moreover, a quantitative analysis of tag confusions reveals that the majority of disagreements are due to linguistically debatable cases rather than annotation errors. Specifically, we show that even in the absence of annotation guidelines only 2% of annotator choices are linguistically unmotivated.",
  "stance": -0.6
 },
 {
  "url": "https://aclanthology.org/P14-2109",
  "title": "Parser Evaluation Using Derivation Trees: A Complement to evalb",
  "year": 2014,
  "venue": "ACL",
  "abstract": "This paper introduces a new technique for phrase-structure parser analysis, categorizing possible treebank structures by integrating regular expressions into derivation trees. We analyze the performance of the Berkeley parser on OntoNotes WSJ and the English Web Treebank. This provides some insight into the evalb scores, and the problem of domain adaptation with the web data. We also analyze a “test-ontrain” dataset, showing a wide variance in how the parser is generalizing from different structures in the training material.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/P14-2133",
  "title": "How much do word embeddings encode about syntax?",
  "year": 2014,
  "venue": "ACL",
  "abstract": "Do continuous word embeddings encode any useful information for constituency parsing? We isolate three ways in which word embeddings might augment a stateof-the-art statistical parser: by connecting out-of-vocabulary words to known ones, by encouraging common behavior among related in-vocabulary words, and by directly providing features for the lexicon. We test each of these hypotheses with a targeted change to a state-of-the-art baseline. Despite small gains on extremely small supervised training sets, we find that extra information from embeddings appears to make little or no difference to a parser with adequate training data. Our results support an overall hypothesis that word embeddings import syntactic information that is ultimately redundant with distinctions learned from treebanks in other ways.",
  "stance": -0.3
 },
 {
  "url": "https://aclanthology.org/P14-2138",
  "title": "Does the Phonology of L1 Show Up in L2 Texts?",
  "year": 2014,
  "venue": "ACL",
  "abstract": "The relative frequencies of character bigrams appear to contain much information for predicting the first language (L1) of the writer of a text in another language (L2). Tsur and Rappoport (2007) interpret this fact as evidence that word choice is dictated by the phonology of L1. In order to test their hypothesis, we design an algorithm to identify the most discriminative words and the corresponding character bigrams, and perform two experiments to quantify their impact on the L1 identification task. The results strongly suggest an alternative explanation of the effectiveness of character bigrams in identifying the native language of a writer.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/P15-1011",
  "title": "Revisiting Word Embedding for Contrasting Meaning",
  "year": 2015,
  "venue": "ACL",
  "abstract": "Contrasting meaning is a basic aspect of semantics. Recent word-embedding models based on distributional semantics hypothesis are known to be weak for modeling lexical contrast. We present in this paper the embedding models that achieve an F-score of 92% on the widely-used, publicly available dataset, the GRE “most contrasting word” questions (Mohammad et al., 2008). This is the highest performance seen so far on this dataset. Surprisingly at the first glance, unlike what was suggested in most previous work, where relatedness statistics learned from corpora is claimed to yield extra gains over lexicon-based models, we obtained our best result relying solely on lexical resources (Roget’s and WordNet)—corpora statistics did not lead to further improvement. However, this should not be simply taken as that distributional statistics is not useful. We examine several basic concerns in modeling contrasting meaning to provide detailed analysis, with the aim to shed some light on the future directions for this basic semantics modeling problem.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P15-1024",
  "title": "Learning Answer-Entailing Structures for Machine Comprehension",
  "year": 2015,
  "venue": "ACL",
  "abstract": "Understanding open-domain text is one of the primary challenges in NLP. Machine comprehension evaluates the system’s ability to understand text through a series of question-answering tasks on short pieces of text such that the correct answer can be found only in the given text. For this task, we posit that there is a hidden (latent) structure that explains the relation between the question, correct answer, and text. We call this the answer-entailing structure; given the structure, the correctness of the answer is evident. Since the structure is latent, it must be inferred. We present a unified max-margin framework that learns to find these hidden structures (given a corpus of question-answer pairs), and uses what it learns to answer machine comprehension questions on novel texts. We extend this framework to incorporate multi-task learning on the different subtasks that are required to perform machine comprehension. Evaluation on a publicly available dataset shows that our framework outperforms various IR and neuralnetwork baselines, achieving an overall accuracy of 67.8% (vs. 59.9%, the best previously-published result.)",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P15-1074",
  "title": "Vector-space calculation of semantic surprisal for predicting word pronunciation duration",
  "year": 2015,
  "venue": "ACL",
  "abstract": "In order to build psycholinguistic models of processing difficulty and evaluate these models against human data, we need highly accurate language models. Here we specifically consider surprisal, a word’s predictability in context. Existing approaches have mostly used n-gram models or more sophisticated syntax-based parsing models; this largely does not account for effects specific to semantics. We build on the work by Mitchell et al. (2010) and show that the semantic prediction model suggested there can successfully predict spoken word durations in naturalistic conversational data. An interesting finding is that the training data for the semantic model also plays a strong role: the model trained on indomain data, even though a better language model for our data, is not able to predict word durations, while the out-ofdomain trained language model does predict word durations. We argue that this at first counter-intuitive result is due to the out-of-domain model better matching the “language models” of the speakers in our data.",
  "stance": -0.1
 },
 {
  "url": "https://aclanthology.org/P15-1104",
  "title": "Learning Word Representations from Scarce and Noisy Data with Embedding Subspaces",
  "year": 2015,
  "venue": "ACL",
  "abstract": "We investigate a technique to adapt unsupervised word embeddings to specific applications, when only small and noisy labeled datasets are available. Current methods use pre-trained embeddings to initialize model parameters, and then use the labeled data to tailor them for the intended task. However, this approach is prone to overfitting when the training is performed with scarce and noisy data. To overcome this issue, we use the supervised data to find an embedding subspace that fits the task complexity. All the word representations are adapted through a projection into this task-specific subspace, even if they do not occur on the labeled dataset. This approach was recently used in the SemEval 2015 Twitter sentiment analysis challenge, attaining state-of-the-art results. Here we show results improving those of the challenge, as well as additional experiments in a Twitter Part-Of-Speech tagging task.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/P15-1108",
  "title": "Joint Dependency Parsing and Multiword Expression Tokenization",
  "year": 2015,
  "venue": "ACL",
  "abstract": "Complex conjunctions and determiners are often considered as pretokenized units in parsing. This is not always realistic, since they can be ambiguous. We propose a model for joint dependency parsing and multiword expressions identification, in which complex function words are represented as individual tokens linked with morphological dependencies. Our graphbased parser includes standard secondorder features and verbal subcategorization features derived from a syntactic lexicon.We train it on a modified version of the French Treebank enriched with morphological dependencies. It recognizes 81.79% of ADV+que conjunctions with 91.57% precision, and 82.74% of de+DET determiners with 86.70% precision.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P15-1111",
  "title": "Identifying Cascading Errors using Constraints in Dependency Parsing",
  "year": 2015,
  "venue": "ACL",
  "abstract": "Dependency parsers are usually evaluated on attachment accuracy. Whilst easily interpreted, the metric does not illustrate the cascading impact of errors, where the parser chooses an incorrect arc, and is subsequently forced to choose further incorrect arcs elsewhere in the parse. We apply arc-level constraints to MSTparser and ZPar, enforcing the correct analysis of specific error classes, whilst otherwise continuing with decoding. We investigate the direct and indirect impact of applying constraints to the parser. Erroneous NP and punctuation attachments cause the most cascading errors, while incorrect PP and coordination attachments are frequent but less influential. Punctuation is especially challenging, as it has long been ignored in parsing, and serves a variety of disparate syntactic roles.",
  "stance": -0.6
 },
 {
  "url": "https://aclanthology.org/P15-1116",
  "title": "Discontinuous Incremental Shift-reduce Parsing",
  "year": 2015,
  "venue": "ACL",
  "abstract": "We present an extension to incremental shift-reduce parsing that handles discontinuous constituents, using a linear classifier and beam search. We achieve very high parsing speeds (up to 640 sent./sec.) and accurate results (up to 79.52 F1 on TiGer).",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P15-2001",
  "title": "A Framework for the Construction of Monolingual and Cross-lingual Word Similarity Datasets",
  "year": 2015,
  "venue": "ACL",
  "abstract": "Despite being one of the most popular tasks in lexical semantics, word similarity has often been limited to the English language. Other languages, even those that are widely spoken such as Spanish, do not have a reliable word similarity evaluation framework. We put forward robust methodologies for the extension of existing English datasets to other languages, both at monolingual and cross-lingual levels. We propose an automatic standardization for the construction of cross-lingual similarity datasets, and provide an evaluation, demonstrating its reliability and robustness. Based on our procedure and taking the RG-65 word similarity dataset as a reference, we release two high-quality Spanish and Farsi (Persian) monolingual datasets, and fifteen cross-lingual datasets for six languages: English, Spanish, French, German, Portuguese, and Farsi.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P15-2024",
  "title": "Evaluating Machine Translation Systems with Second Language Proficiency Tests",
  "year": 2015,
  "venue": "ACL",
  "abstract": "A lightweight, human-in-the-loop evaluation scheme for machine translation (MT) systems is proposed. It extrinsically evaluates MT systems using human subjects’ scores on second language ability test problems that are machine-translated to the subjects’ native language. A largescale experiment involving 320 subjects revealed that the context-unawareness of the current MT systems severely damages human performance when solving the test problems, while one of the evaluated MT systems performed as good as a human translation produced in a context-unaware condition. An analysis of the experimental results showed that the extrinsic evaluation captured a different dimension of translation quality than that captured by manual and automatic intrinsic evaluation.",
  "stance": 0.4
 },
 {
  "url": "https://aclanthology.org/P15-2026",
  "title": "Exploring the Planet of the APEs: a Comparative Study of State-of-the-art Methods for MT Automatic Post-Editing",
  "year": 2015,
  "venue": "ACL",
  "abstract": "Downstream processing of machine translation (MT) output promises to be a solution to improve translation quality, especially when the MT system’s internal decoding process is not accessible. Both rule-based and statistical automatic postediting (APE) methods have been proposed over the years, but with contrasting results. A missing aspect in previous evaluations is the assessment of different methods: i) under comparable conditions, and ii) on different language pairs featuring variable levels of MT quality. Focusing on statistical APE methods (more portable across languages), we propose the first systematic analysis of two approaches. To understand their potential, we compare them in the same conditions over six language pairs having English as source. Our results evidence consistent improvements on all language pairs, a relation between the extent of the gain and MT output quality, slight but statistically significant performance differences between the two methods, and their possible complementarity.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/P15-2057",
  "title": "Learning Topic Hierarchies for Wikipedia Categories",
  "year": 2015,
  "venue": "ACL",
  "abstract": "Existing studies have utilized Wikipedia for various knowledge acquisition tasks. However, no attempts have been made to explore multi-level topic knowledge contained in Wikipedia articles’",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/P15-2078",
  "title": "Dependency length minimisation effects in short spans: a large-scale analysis of adjective placement in complex noun phrases",
  "year": 2015,
  "venue": "ACL",
  "abstract": "It has been extensively observed that languages minimise the distance between two related words. Dependency length minimisation effects are explained as a means to reduce memory load and for effective communication. In this paper, we ask whether they hold in typically short spans, such as noun phrases, which could be thought of being less subject to efficiency pressure. We demonstrate that minimisation does occur in short spans, but also that it is a complex effect: it is not only the length of the dependency that is at stake, but also the effect of the surrounding dependencies.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P15-2079",
  "title": "Tagging Performance Correlates with Author Age",
  "year": 2015,
  "venue": "ACL",
  "abstract": "Many NLP tools for English and German are based on manually annotated articles from the Wall Street Journal and Frankfurter Rundschau. The average readers of these two newspapers are middle-aged (55 and 47 years old, respectively), and the annotated articles are more than 20 years old by now. This leads us to speculate whether tools induced from these resources (such as part-of-speech taggers) put older language users at an advantage. We show that this is actually the case in both languages, and that the cause goes beyond simple vocabulary differences. In our experiments, we control for gender and region.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/P15-2090",
  "title": "UNRAVEL-A Decipherment Toolkit",
  "year": 2015,
  "venue": "ACL",
  "abstract": "In this paper we present the UNRAVEL toolkit: It implements many of the recently published works on decipherment, including decipherment for deterministic ciphers like e.g. the ZODIAC-408 cipher and Part two of the BEALE ciphers, as well as decipherment of probabilistic ciphers and unsupervised training for machine translation. It also includes data and example configuration files so that the previously published experiments are easy to reproduce.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/P15-2092",
  "title": "What's in a Domain? Analyzing Genre and Topic Differences in Statistical Machine Translation",
  "year": 2015,
  "venue": "ACL",
  "abstract": "Domain adaptation is an active field of research in statistical machine translation (SMT), but so far most work has ignored the distinction between the topic and genre of documents. In this paper we quantify and disentangle the impact of genre and topic differences on translation quality by introducing a new data set that has controlled topic and genre distributions. In addition, we perform a detailed analysis showing that differences across topics only explain to a limited degree translation performance differences across genres, and that genre-specific errors are more attributable to model coverage than to suboptimal scoring of translation candidates.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/P15-2097",
  "title": "Ground Truth for Grammatical Error Correction Metrics",
  "year": 2015,
  "venue": "ACL",
  "abstract": "How do we know which grammatical error correction (GEC) system is best? A number of metrics have been proposed over the years, each motivated by weaknesses of previous metrics; however, the metrics themselves have not been compared to an empirical gold standard grounded in human judgments. We conducted the first human evaluation of GEC system outputs, and show that the rankings produced by metrics such as MaxMatch and I-measure do not correlate well with this ground truth. As a step towards better metrics, we also propose GLEU, a simple variant of BLEU, modified to account for both the source and the reference, and show that it hews much more closely to human judgments.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/P15-2105",
  "title": "Automatic Keyword Extraction on Twitter",
  "year": 2015,
  "venue": "ACL",
  "abstract": "In this paper, we build a corpus of tweets from Twitter annotated with keywords using crowdsourcing methods. We identify key differences between this domain and the work performed on other domains, such as news, which makes existing approaches for automatic keyword extraction not generalize well on Twitter datasets. These datasets include the small amount of content in each tweet, the frequent usage of lexical variants and the high variance of the cardinality of keywords present in each tweet. We propose methods for addressing these issues, which leads to solid improvements on this dataset for this task.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P15-2119",
  "title": "How Well Do Distributional Models Capture Different Types of Semantic Knowledge?",
  "year": 2015,
  "venue": "ACL",
  "abstract": "In recent years, distributional models (DMs) have shown great success in representing lexical semantics. In this work we show that the extent to which DMs represent semantic knowledge is highly dependent on the type of knowledge. We pose the task of predicting properties of concrete nouns in a supervised setting, and compare between learning taxonomic properties (e.g., animacy) and attributive properties (e.g., size, color). We employ four state-of-the-art DMs as sources of feature representation for this task, and show that they all yield poor results when tested on attributive properties, achieving no more than an average F-score of 0.37 in the binary property prediction task, compared to 0.73 on taxonomic properties. Our results suggest that the distributional hypothesis may not be equally applicable to all types of semantic information.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/P16-1060",
  "title": "Which Coreference Evaluation Metric Do You Trust? A Proposal for a Link-based Entity Aware Metric",
  "year": 2016,
  "venue": "ACL",
  "abstract": "Interpretability and discriminative power are the two most basic requirements for an evaluation metric. In this paper, we report the mention identification effect in the B3, CEAF, and BLANC coreference evaluation metrics that makes it impossible to interpret their results properly. The only metric which is insensitive to this flaw is MUC, which, however, is known to be the least discriminative metric. It is a known fact that none of the current metrics are reliable. The common practice for ranking coreference resolvers is to use the average of three different metrics. However, one cannot expect to obtain a reliable score by averaging three unreliable metrics. We propose LEA, a Link-based Entity-Aware evaluation metric that is designed to overcome the shortcomings of the current evaluation metrics. LEA is available as branch LEA-scorer in the reference implementation of the official CoNLL scorer.",
  "stance": -0.4
 },
 {
  "url": "https://aclanthology.org/P16-1080",
  "title": "Analyzing Biases in Human Perception of User Age and Gender from Text",
  "year": 2016,
  "venue": "ACL",
  "abstract": "User traits disclosed through written text, such as age and gender, can be used to personalize applications such as recommender systems or conversational agents. However, human perception of these traits is not perfectly aligned with reality. In this paper, we conduct a large-scale crowdsourcing experiment on guessing age and gender from tweets. We systematically analyze the quality and possible biases of these predictions. We identify the textual cues which lead to miss-assessments of traits or make annotators more or less confident in their choice. Our study demonstrates that differences between real and perceived traits are noteworthy and elucidates inaccurately used stereotypes in human perception.",
  "stance": -0.6
 },
 {
  "url": "https://aclanthology.org/P16-2013",
  "title": "Reference Bias in Monolingual Machine Translation Evaluation",
  "year": 2016,
  "venue": "ACL",
  "abstract": "In the translation industry, human translations are assessed by comparison with the source texts. In the Machine Translation (MT) research community, however, it is a common practice to perform quality assessment using a reference translation instead of the source text. In this paper we show that this practice has a serious issue – annotators are strongly biased by the reference translation provided, and this can have a negative impact on the assessment of MT quality.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P17-1008",
  "title": "The State of the Art in Semantic Representation",
  "year": 2017,
  "venue": "ACL",
  "abstract": "Semantic representation is receiving growing attention in NLP in the past few years, and many proposals for semantic schemes (e.g., AMR, UCCA, GMB, UDS) have been put forth. Yet, little has been done to assess the achievements and the shortcomings of these new contenders, compare them with syntactic schemes, and clarify the general goals of research on semantic representation. We address these gaps by critically surveying the state of the art in the field.",
  "stance": -0.7
 },
 {
  "url": "https://aclanthology.org/P17-1014",
  "title": "Neural AMR: Sequence-to-Sequence Models for Parsing and Generation",
  "year": 2017,
  "venue": "ACL",
  "abstract": "Sequence-to-sequence models have shown strong performance across a broad range of applications. However, their application to parsing and generating text using Abstract Meaning Representation (AMR) has been limited, due to the relatively limited amount of labeled data and the non-sequential nature of the AMR graphs. We present a novel training procedure that can lift this limitation using millions of unlabeled sentences and careful preprocessing of the AMR graphs. For AMR parsing, our model achieves competitive results of 62.1 SMATCH, the current best score reported without significant use of external semantic resources. For AMR generation, our model establishes a new state-of-the-art performance of BLEU 33.8. We present extensive ablative and qualitative analysis including strong evidence that sequence-based AMR models are robust against ordering variations of graph-to-sequence conversions.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P17-1024",
  "title": "FOIL it! Find One mismatch between Image and Language caption",
  "year": 2017,
  "venue": "ACL",
  "abstract": "In this paper, we aim to understand whether current language and vision (LaVi) models truly grasp the interaction between the two modalities. To this end, we propose an extension of the MS-COCO dataset, FOIL-COCO, which associates images with both correct and 'foil' captions, that is, descriptions of the image that are highly similar to the original ones, but contain one single mistake ('foil word'). We show that current LaVi models fall into the traps of this data and perform badly on three tasks: a) caption classification (correct vs. foil); b) foil word detection; c) foil word correction. Humans, in contrast, have near-perfect performance on those tasks. We demonstrate that merely utilising language cues is not enough to model FOIL-COCO and that it challenges the state-of-the-art by requiring a fine-grained understanding of the relation between text and image.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P17-1138",
  "title": "Bandit Structured Prediction for Neural Sequence-to-Sequence Learning",
  "year": 2017,
  "venue": "ACL",
  "abstract": "Bandit structured prediction describes a stochastic optimization framework where learning is performed from partial feedback. This feedback is received in the form of a task loss evaluation to a predicted output structure, without having access to gold standard structures. We advance this framework by lifting linear bandit learning to neural sequence-to-sequence learning problems using attention-based recurrent neural networks. Furthermore, we show how to incorporate control variates into our learning algorithms for variance reduction and improved generalization. We present an evaluation on a neural machine translation task that shows improvements of up to 5.89 BLEU points for domain adaptation from simulated bandit feedback.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/P17-1149",
  "title": "Bridge Text and Knowledge by Learning Multi-Prototype Entity Mention Embedding",
  "year": 2017,
  "venue": "ACL",
  "abstract": "Integrating text and knowledge into a unified semantic space has attracted significant research interests recently. However, the ambiguity in the common space remains a challenge, namely that the same mention phrase usually refers to various entities. In this paper, to deal with the ambiguity of entity mentions, we propose a novel Multi-Prototype Mention Embedding model, which learns multiple sense embeddings for each mention by jointly modeling words from textual contexts and entities derived from a knowledge base. In addition, we further design an efficient language model based approach to disambiguate each mention to a specific sense. In experiments, both qualitative and quantitative analysis demonstrate the high quality of the word, entity and multi-prototype mention embeddings. Using entity linking as a study case, we apply our disambiguation method as well as the multi-prototype mention embeddings on the benchmark dataset, and achieve state-of-the-art performance.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P17-1163",
  "title": "Neural Belief Tracker: Data-Driven Dialogue State Tracking",
  "year": 2017,
  "venue": "ACL",
  "abstract": "One of the core components of modern spoken dialogue systems is the belief tracker, which estimates the user's goal at every step of the dialogue. However, most current approaches have difficulty scaling to larger, more complex dialogue domains. This is due to their dependency on either: a) Spoken Language Understanding models that require large amounts of annotated training data; or b) hand-crafted lexicons for capturing some of the linguistic variation in users' language. We propose a novel Neural Belief Tracking (NBT) framework which overcomes these problems by building on recent advances in representation learning. NBT models reason over pre-trained word vectors, learning to compose them into distributed representations of user utterances and dialogue context. Our evaluation on two datasets shows that this approach surpasses past limitations, matching the performance of state-of-the-art models which rely on hand-crafted semantic lexicons and outperforming them when such lexicons are not provided.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P17-1184",
  "title": "From Characters to Words to in Between: Do We Capture Morphology?",
  "year": 2017,
  "venue": "ACL",
  "abstract": "Words can be represented by composing the representations of subword units such as word segments, characters, and/or character n-grams. While such representations are effective and may capture the morphological regularities of words, they have not been systematically compared, and it is not understood how they interact with different morphological typologies. On a language modeling task, we present experiments that systematically vary (1) the basic unit of representation, (2) the composition of these representations, and (3) the morphological typology of the language modeled. Our results extend previous findings that character representations are effective across typologies, and we find that a previously unstudied combination of character trigram representations composed with bi-LSTMs outperforms most others. But we also find room for improvement: none of the character-level models match the predictive accuracy of a model with access to true morphological analyses, even when learned from an order of magnitude more data.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P17-2011",
  "title": "An Analysis of Action Recognition Datasets for Language and Vision Tasks",
  "year": 2017,
  "venue": "ACL",
  "abstract": "A large amount of recent research has focused on tasks that combine language and vision, resulting in a proliferation of datasets and methods. One such task is action recognition, whose applications include image annotation, scene understanding and image retrieval. In this survey, we categorize the existing approaches based on how they conceptualize this problem and provide a detailed review of existing datasets, highlighting their diversity as well as advantages and disadvantages. We focus on recently developed datasets which link visual information with linguistic resources and provide a fine-grained syntactic and semantic analysis of actions in images.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P17-2012",
  "title": "Learning to Parse and Translate Improves Neural Machine Translation",
  "year": 2017,
  "venue": "ACL",
  "abstract": "There has been relatively little attention to incorporating linguistic prior to neural machine translation. Much of the previous work was further constrained to considering linguistic prior on the source side. In this paper, we propose a hybrid model, called NMT+RNNG, that learns to parse and translate by combining the recurrent neural network grammar into the attention-based neural machine translation. Our approach encourages the neural machine translation model to incorporate linguistic prior during training, and lets it translate on its own afterward. Extensive experiments with four language pairs show the effectiveness of the proposed NMT+RNNG.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P17-2034",
  "title": "A Corpus of Natural Language for Visual Reasoning",
  "year": 2017,
  "venue": "ACL",
  "abstract": "We present a new visual reasoning language dataset, containing 92,244 pairs of examples of natural statements grounded in synthetic images with 3,962 unique sentences. We describe a method of crowdsourcing linguistically-diverse data, and present an analysis of our data. The data demonstrates a broad set of linguistic phenomena, requiring visual and set-theoretic reasoning. We experiment with various models, and show the data presents a strong challenge for future research.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P17-2045",
  "title": "A Neural Architecture for Generating Natural Language Descriptions from Source Code Changes",
  "year": 2017,
  "venue": "ACL",
  "abstract": "We propose a model to automatically describe changes introduced in the source code of a program using natural language. Our method receives as input a set of code commits, which contains both the modifications and message introduced by an user. These two modalities are used to train an encoder-decoder architecture. We evaluated our approach on twelve real world open source projects from four different programming languages. Quantitative and qualitative results showed that the proposed approach can generate feasible and semantically sound descriptions not only in standard in-project settings, but also in a cross-project setting.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P18-1026",
  "title": "Graph-to-Sequence Learning using Gated Graph Neural Networks",
  "year": 2018,
  "venue": "ACL",
  "abstract": "Many NLP applications can be framed as a graph-to-sequence learning problem. Previous work proposing neural architectures on graph-to-sequence obtained promising results compared to grammar-based approaches but still rely on linearisation heuristics and/or standard recurrent networks to achieve the best performance. In this work propose a new model that encodes the full structural information contained in the graph. Our architecture couples the recently proposed Gated Graph Neural Networks with an input transformation that allows nodes and edges to have their own hidden representations, while tackling the parameter explosion problem present in previous work. Experimental results shows that our model outperforms strong baselines in generation from AMR graphs and syntax-based neural machine translation.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P18-1036",
  "title": "Character-Level Models versus Morphology in Semantic Role Labeling",
  "year": 2018,
  "venue": "ACL",
  "abstract": "Character-level models have become a popular approach specially for their accessibility and ability to handle unseen data. However, little is known on their ability to reveal the underlying morphological structure of a word, which is a crucial skill for high-level semantic analysis tasks, such as semantic role labeling (SRL). In this work, we train various types of SRL models that use word, character and morphology level information and analyze how performance of characters compare to words and morphology for several languages. We conduct an in-depth error analysis for each morphological typology and analyze the strengths and limitations of character-level models that relate to out-of-domain data, training data size, long range dependencies and model complexity. Our exhaustive analyses shed light on important characteristics of character-level models and their semantic capability.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P18-1072",
  "title": "On the Limitations of Unsupervised Bilingual Dictionary Induction",
  "year": 2018,
  "venue": "ACL",
  "abstract": "Unsupervised machine translation - i.e., not assuming any cross-lingual supervision signal, whether a dictionary, translations, or comparable corpora - seems impossible, but nevertheless, Lample et al. (2017) recently proposed a fully unsupervised machine translation (MT) model. The model relies heavily on an adversarial, unsupervised cross-lingual word embedding technique for bilingual dictionary induction (Conneau et al., 2017), which we examine here. Our results identify the limitations of current unsupervised MT: unsupervised bilingual dictionary induction performs much worse on morphologically rich languages that are not dependent marking, when monolingual corpora from different domains or different embedding algorithms are used. We show that a simple trick, exploiting a weak supervision signal from identical words, enables more robust induction and establish a near-perfect correlation between unsupervised bilingual dictionary induction performance and a previously unexplored graph similarity metric.",
  "stance": -0.6
 },
 {
  "url": "https://aclanthology.org/P18-1083",
  "title": "No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling",
  "year": 2018,
  "venue": "ACL",
  "abstract": "Though impressive results have been achieved in visual captioning, the task of generating abstract stories from photo streams is still a little-tapped problem. Different from captions, stories have more expressive language styles and contain many imaginary concepts that do not appear in the images. Thus it poses challenges to behavioral cloning algorithms. Furthermore, due to the limitations of automatic metrics on evaluating story quality, reinforcement learning methods with hand-crafted rewards also face difficulties in gaining an overall performance boost. Therefore, we propose an Adversarial REward Learning (AREL) framework to learn an implicit reward function from human demonstrations, and then optimize policy search with the learned reward function. Though automatic evaluation indicates slight performance boost over state-of-the-art (SOTA) methods in cloning expert behaviors, human evaluation shows that our approach achieves significant improvement in generating more human-like stories than SOTA systems.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P18-1098",
  "title": "A Neural Architecture for Automated ICD Coding",
  "year": 2018,
  "venue": "ACL",
  "abstract": "The International Classification of Diseases (ICD) provides a hierarchy of diagnostic codes for classifying diseases. Medical coding - which assigns a subset of ICD codes to a patient visit - is a mandatory process that is crucial for patient care and billing. Manual coding is time-consuming, expensive, and error prone. In this paper, we build a neural architecture for automated coding. It takes the diagnosis descriptions (DDs) of a patient as inputs and selects the most relevant ICD codes. This architecture contains four major ingredients: (1) tree-of-sequences LSTM encoding of code descriptions (CDs), (2) adversarial learning for reconciling the different writing styles of DDs and CDs, (3) isotonic constraints for incorporating the importance order among the assigned codes, and (4) attentional matching for performing many-to-one and one-to-many mappings from DDs to CDs. We demonstrate the effectiveness of the proposed methods on a clinical datasets with 59K patient visits.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P18-1133",
  "title": "Sequicity: Simplifying Task-oriented Dialogue Systems with Single Sequence-to-Sequence Architectures",
  "year": 2018,
  "venue": "ACL",
  "abstract": "Existing solutions to task-oriented dialogue systems follow pipeline designs which introduces architectural complexity and fragility. We propose a novel, holistic, extendable framework based on a single sequence-to-sequence (seq2seq) model which can be optimized with supervised or reinforcement learning. A key contribution is that we design text spans named belief spans to track dialogue believes, allowing task-oriented dialogue systems to be modeled in a seq2seq way. Based on this, we propose a simplistic Two Stage CopyNet instantiation which emonstrates good scalability: significantly reducing model complexity in terms of number of parameters and training time by a magnitude. It significantly outperforms state-of-the-art pipeline-based methods on large datasets and retains a satisfactory entity match rate on out-of-vocabulary (OOV) cases where pipeline-designed competitors totally fail.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P18-1156",
  "title": "DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension",
  "year": 2018,
  "venue": "ACL",
  "abstract": "We propose DuoRC, a novel dataset for Reading Comprehension (RC) that motivates several new challenges for neural approaches in language understanding beyond those offered by existing RC datasets. DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie - one from Wikipedia and the other from IMDb - written by two different authors. We asked crowdsourced workers to create questions from one version of the plot and a different set of workers to extract or synthesize answers from the other version. This unique characteristic of DuoRC where questions and answers are created from different versions of a document narrating the same underlying story, ensures by design, that there is very little lexical overlap between the questions created from one version and the segments containing the answer in the other version. Further, since the two versions have different levels of plot detail, narration style, vocabulary, etc., answering questions from the second version requires deeper language understanding and incorporating external background knowledge. Additionally, the narrative style of passages arising from movie plots (as opposed to typical descriptive passages in existing datasets) exhibits the need to perform complex reasoning over events across multiple sentences. Indeed, we observe that state-of-the-art neural RC models which have achieved near human performance on the SQuAD dataset, even when coupled with traditional NLP techniques to address the challenges presented in DuoRC exhibit very poor performance (F1 score of 37.42% on DuoRC v/s 86% on SQuAD dataset). This opens up several interesting research avenues wherein DuoRC could complement other RC datasets to explore novel neural approaches for studying language understanding.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P18-1176",
  "title": "Did the Model Understand the Question?",
  "year": 2018,
  "venue": "ACL",
  "abstract": "We analyze state-of-the-art deep learning models for three tasks: question answering on (1) images, (2) tables, and (3) passages of text. Using the notion of \"attribution\" (word importance), we find that these deep networks often ignore important question terms. Leveraging such behavior, we perturb questions to craft a variety of adversarial examples. Our strongest attacks drop the accuracy of a visual question answering model from 61.1% to 19%, and that of a tabular question answering model from 33.5% to 3.3%. Additionally, we show how attributions can strengthen attacks proposed by Jia and Liang (2017) on paragraph comprehension models. Our results demonstrate that attributions can augment standard measures of accuracy and empower investigation of model performance. When a model is accurate but for the wrong reasons, attributions can surface erroneous logic in the model that indicates inadequacies in the test data.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P18-2036",
  "title": "Characterizing Departures from Linearity in Word Translation",
  "year": 2018,
  "venue": "ACL",
  "abstract": "We investigate the behavior of maps learned by machine translation methods. The maps translate words by projecting between word embedding spaces of different languages. We locally approximate these maps using linear maps, and find that they vary across the word embedding space. This demonstrates that the underlying maps are non-linear. Importantly, we show that the locally linear maps vary by an amount that is tightly correlated with the distance between the neighborhoods on which they are trained. Our results can be used to test non-linear methods, and to drive the design of more accurate maps for word translation.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P18-2074",
  "title": "Do Neural Network Cross-Modal Mappings Really Bridge Modalities?",
  "year": 2018,
  "venue": "ACL",
  "abstract": "Feed-forward networks are widely used in cross-modal applications to bridge modalities by mapping distributed vectors of one modality to the other, or to a shared space. The predicted vectors are then used to perform e.g., retrieval or labeling. Thus, the success of the whole system relies on the ability of the mapping to make the neighborhood structure (i.e., the pairwise similarities) of the predicted vectors akin to that of the target vectors. However, whether this is achieved has not been investigated yet. Here, we propose a new similarity measure and two ad hoc experiments to shed light on this issue. In three cross-modal benchmarks we learn a large number of language-to-vision and vision-to-language neural network mappings (up to five layers) using a rich diversity of image and text features and loss functions. Our results reveal that, surprisingly, the neighborhood structure of the predicted vectors consistently resembles more that of the input vectors than that of the target vectors. In a second experiment, we further show that untrained nets do not significantly disrupt the neighborhood (i.e., semantic) structure of the input vectors.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/P18-2108",
  "title": "Enhancing Drug-Drug Interaction Extraction from Texts by Molecular Structure Information",
  "year": 2018,
  "venue": "ACL",
  "abstract": "We propose a novel neural method to extract drug-drug interactions (DDIs) from texts using external drug molecular structure information. We encode textual drug pairs with convolutional neural networks and their molecular pairs with graph convolutional networks (GCNs), and then we concatenate the outputs of these two networks. In the experiments, we show that GCNs can predict DDIs from the molecular structures of drugs in high accuracy and the molecular information can enhance text-based DDI extraction by 2.39 percent points in the F-score on the DDIExtraction 2013 shared task data set.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P19-1008",
  "title": "Semantic Expressive Capacity with Bounded Memory",
  "year": 2019,
  "venue": "ACL",
  "abstract": "We investigate the capacity of mechanisms for compositional semantic parsing to describe relations between sentences and semantic representations. We prove that in order to represent certain relations, mechanisms which are syntactically projective must be able to remember an unbounded number of locations in the semantic representations, where nonprojective mechanisms need not. This is the first result of this kind, and has consequences both for grammar-based and for neural systems.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P19-1070",
  "title": "How to (Properly) Evaluate Cross-Lingual Word Embeddings: On Strong Baselines, Comparative Analyses, and Some Misconceptions",
  "year": 2019,
  "venue": "ACL",
  "abstract": "Cross-lingual word embeddings (CLEs) facilitate cross-lingual transfer of NLP models. Despite their ubiquitous downstream usage, increasingly popular projection-based CLE models are almost exclusively evaluated on bilingual lexicon induction (BLI). Even the BLI evaluations vary greatly, hindering our ability to correctly interpret performance and properties of different CLE models. In this work, we take the first step towards a comprehensive evaluation of CLE models: we thoroughly evaluate both supervised and unsupervised CLE models, for a large number of language pairs, on BLI and three downstream tasks, providing new insights concerning the ability of cutting-edge CLE models to support cross-lingual NLP. We empirically demonstrate that the performance of CLE models largely depends on the task at hand and that optimizing CLE models for BLI may hurt downstream performance. We indicate the most robust supervised and unsupervised CLE models and emphasize the need to reassess simple baselines, which still display competitive performance across the board. We hope our work catalyzes further research on CLE evaluation and model analysis.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P19-1075",
  "title": "ChID: A Large-scale Chinese IDiom Dataset for Cloze Test",
  "year": 2019,
  "venue": "ACL",
  "abstract": "Cloze-style reading comprehension in Chinese is still limited due to the lack of various corpora. In this paper we propose a large-scale Chinese cloze test dataset ChID, which studies the comprehension of idiom, a unique language phenomenon in Chinese. In this corpus, the idioms in a passage are replaced by blank symbols and the correct answer needs to be chosen from well-designed candidate idioms. We carefully study how the design of candidate idioms and the representation of idioms affect the performance of state-of-the-art models. Results show that the machine accuracy is substantially worse than that of human, indicating a large space for further research.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P19-1246",
  "title": "You Write like You Eat: Stylistic Variation as a Predictor of Social Stratification",
  "year": 2019,
  "venue": "ACL",
  "abstract": "Inspired by Labov's seminal work on stylisticvariation as a function of social stratification,we develop and compare neural models thatpredict a person's presumed socio-economicstatus, obtained through distant supervision,from their writing style on social media. Thefocus of our work is on identifying the mostimportant stylistic parameters to predict socio-economic group. In particular, we show theeffectiveness of morpho-syntactic features aspredictors of style, in contrast to lexical fea-tures, which are good predictors of topic",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P19-1267",
  "title": "We Need to Talk about Standard Splits",
  "year": 2019,
  "venue": "ACL",
  "abstract": "It is standard practice in speech & language technology to rank systems according to their performance on a test set held out for evaluation. However, few researchers apply statistical tests to determine whether differences in performance are likely to arise by chance, and few examine the stability of system ranking across multiple training-testing splits. We conduct replication and reproduction experiments with nine part-of-speech taggers published between 2000 and 2018, each of which claimed state-of-the-art performance on a widely-used \"standard split\". While we replicate results on the standard split, we fail to reliably reproduce some rankings when we repeat this analysis with randomly generated training-testing splits. We argue that randomly generated splits should be used in system evaluation.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P19-1286",
  "title": "Domain Adaptation of Neural Machine Translation by Lexicon Induction",
  "year": 2019,
  "venue": "ACL",
  "abstract": "It has been previously noted that neural machine translation (NMT) is very sensitive to domain shift. In this paper, we argue that this is a dual effect of the highly lexicalized nature of NMT, resulting in failure for sentences with large numbers of unknown words, and lack of supervision for domain-specific words. To remedy this problem, we propose an unsupervised adaptation method which fine-tunes a pre-trained out-of-domain NMT model using a pseudo-in-domain corpus. Specifically, we perform lexicon induction to extract an in-domain lexicon, and construct a pseudo-parallel in-domain corpus by performing word-for-word back-translation of monolingual in-domain target sentences. In five domains over twenty pairwise adaptation settings and two model architectures, our method achieves consistent improvements without using any in-domain parallel sentences, improving up to 14 BLEU over unadapted models, and up to 2 BLEU over strong back-translation baselines.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P19-1334",
  "title": "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference",
  "year": 2019,
  "venue": "ACL",
  "abstract": "A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P19-1376",
  "title": "Are we there yet? Encoder-decoder neural networks as cognitive models of English past tense inflection",
  "year": 2019,
  "venue": "ACL",
  "abstract": "The cognitive mechanisms needed to account for the English past tense have long been a subject of debate in linguistics and cognitive science. Neural network models were proposed early on, but were shown to have clear flaws. Recently, however, Kirov and Cotterell (2018) showed that modern encoder-decoder (ED) models overcome many of these flaws. They also presented evidence that ED models demonstrate humanlike performance in a nonce-word task. Here, we look more closely at the behaviour of their model in this task. We find that (1) the model exhibits instability across multiple simulations in terms of its correlation with human data, and (2) even when results are aggregated across simulations (treating each simulation as an individual human participant), the fit to the human data is not strong-worse than an older rule-based model. These findings hold up through several alternative training regimes and evaluation measures. Although other neural architectures might do better, we conclude that there is still insufficient evidence to claim that neural nets are a good cognitive model for this task.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P19-1380",
  "title": "Miss Tools and Mr Fruit: Emergent Communication in Agents Learning about Object Affordances",
  "year": 2019,
  "venue": "ACL",
  "abstract": "Recent research studies communication emergence in communities of deep network agents assigned a joint task, hoping to gain insights on human language evolution. We propose here a new task capturing crucial aspects of the human environment, such as natural object affordances, and of human conversation, such as full symmetry among the participants. By conducting a thorough pragmatic and semantic analysis of the emergent protocol, we show that the agents solve the shared task through genuine bilateral, referential communication. However, the agents develop multiple idiolects, which makes us conclude that full symmetry is not a sufficient condition for a common language to emerge.",
  "stance": -0.4
 },
 {
  "url": "https://aclanthology.org/P19-1381",
  "title": "CNNs found to jump around more skillfully than RNNs: Compositional Generalization in Seq2seq Convolutional Networks",
  "year": 2019,
  "venue": "ACL",
  "abstract": "Lake and Baroni (2018) introduced the SCAN dataset probing the ability of seq2seq models to capture compositional generalizations, such as inferring the meaning of \"jump around\" 0-shot from the component words. Recurrent networks (RNNs) were found to completely fail the most challenging generalization cases. We test here a convolutional network (CNN) on these tasks, reporting hugely improved performance with respect to RNNs. Despite the big improvement, the CNN has however not induced systematic rules, suggesting that the difference between compositional and non-compositional behaviour is not clear-cut.",
  "stance": -0.7
 },
 {
  "url": "https://aclanthology.org/P19-1412",
  "title": "Do You Know That Florence Is Packed with Visitors? Evaluating State-of-the-art Models of Speaker Commitment",
  "year": 2019,
  "venue": "ACL",
  "abstract": "When a speaker, Mary, asks \"Do you know that Florence is packed with visitors?\", we take her to believe that Florence is packed with visitors, but not if she asks \"Do you think that Florence is packed with visitors?\". Inferring speaker commitment (aka event factuality) is crucial for information extraction and question answering. Here, we explore the hypothesis that linguistic deficits drive the error patterns of existing speaker commitment models by analyzing the linguistic correlates of model error on a challenging naturalistic dataset. We evaluate two state-of-the-art speaker commitment models on the CommitmentBank, an English dataset of naturally occurring discourses. The CommitmentBank is annotated with speaker commitment towards the content of the complement (\"Florence is packed with visitors\" in our example) of clause-embedding verbs (\"know\", \"think\") under four entailment-canceling environments (negation, modal, question, conditional). A breakdown of items by linguistic features reveals asymmetrical error patterns: while the models achieve good performance on some classes (e.g., negation), they fail to generalize to the diverse linguistic constructions (e.g., conditionals) in natural language, highlighting directions for improvement.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/P19-1459",
  "title": "Probing Neural Network Comprehension of Natural Language Arguments",
  "year": 2019,
  "venue": "ACL",
  "abstract": "We are surprised to find that BERT's peak performance of 77% on the Argument Reasoning Comprehension Task reaches just three points below the average untrained human baseline. However, we show that this result is entirely accounted for by exploitation of spurious statistical cues in the dataset. We analyze the nature of these cues and demonstrate that a range of models all exploit them. This analysis informs the construction of an adversarial dataset on which all models achieve random accuracy. Our adversarial dataset provides a more robust assessment of argument comprehension and should be adopted as the standard in future work.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P19-1472",
  "title": "HellaSwag: Can a Machine Really Finish Your Sentence?",
  "year": 2019,
  "venue": "ACL",
  "abstract": "Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as \"A woman sits at a piano,\" a machine must select the most likely followup: \"She sets her fingers on the keys.\" With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference? In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans (>95% accuracy), state-of-the-art models struggle (<48%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical 'Goldilocks' zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models. Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/P19-1492",
  "title": "Analyzing the Limitations of Cross-lingual Word Embedding Mappings",
  "year": 2019,
  "venue": "ACL",
  "abstract": "Recent research in cross-lingual word embeddings has almost exclusively focused on offline methods, which independently train word embeddings in different languages and map them to a shared space through linear transformations. While several authors have questioned the underlying isomorphism assumption, which states that word embeddings in different languages have approximately the same structure, it is not clear whether this is an inherent limitation of mapping approaches or a more general issue when learning cross-lingual embeddings. So as to answer this question, we experiment with parallel corpora, which allows us to compare offline mapping to an extension of skip-gram that jointly learns both embedding spaces. We observe that, under these ideal conditions, joint learning yields to more isomorphic embeddings, is less sensitive to hubness, and obtains stronger results in bilingual lexicon induction. We thus conclude that current mapping methods do have strong limitations, calling for further research to jointly learn cross-lingual embeddings with a weaker cross-lingual signal.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P19-1554",
  "title": "Misleading Failures of Partial-input Baselines",
  "year": 2019,
  "venue": "ACL",
  "abstract": "Recent work establishes dataset difficulty and removes annotation artifacts via partial-input baselines (e.g., hypothesis-only model for SNLI or question-only model for VQA). A successful partial-input baseline indicates that the dataset is cheatable. But the converse is not necessarily true: failures of partial-input baselines do not mean the dataset is free of artifacts. We first design artificial datasets to illustrate how the trivial patterns that are only visible in the full input can evade any partial-input baseline. Next, we identify such artifacts in the SNLI dataset-a hypothesis-only model augmented with trivial patterns in the premise can solve 15% of previously-thought \"hard\" examples. Our work provides a caveat for the use and creation of partial-input baselines for datasets.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P19-1652",
  "title": "Bridging by Word: Image Grounded Vocabulary Construction for Visual Captioning",
  "year": 2019,
  "venue": "ACL",
  "abstract": "Image Captioning aims at generating a short description for an image. Existing research usually employs the architecture of CNN-RNN that views the generation as a sequential decision-making process and the entire dataset vocabulary is used as decoding space. They suffer from generating high frequent n-gram with irrelevant words. To tackle this problem, we propose to construct an image-grounded vocabulary, based on which, captions are generated with limitation and guidance. In specific, a novel hierarchical structure is proposed to construct the vocabulary incorporating both visual information and relations among words. For generation, we propose a word-aware RNN cell incorporating vocabulary information into the decoding process directly. Reinforce algorithm is employed to train the generator using constraint vocabulary as action space. Experimental results on MS COCO and Flickr30k show the effectiveness of our framework compared to some state-of-the-art models.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.118",
  "title": "iSarcasm: A Dataset of Intended Sarcasm",
  "year": 2020,
  "venue": "ACL",
  "abstract": "We consider the distinction between intended and perceived sarcasm in the context of textual sarcasm detection. The former occurs when an utterance is sarcastic from the perspective of its author, while the latter occurs when the utterance is interpreted as sarcastic by the audience. We show the limitations of previous labelling methods in capturing intended sarcasm and introduce the iSarcasm dataset of tweets labeled for sarcasm directly by their authors. Examining the state-of-the-art sarcasm detection models on our dataset showed low performance compared to previously studied datasets, which indicates that these datasets might be biased or obvious and sarcasm could be a phenomenon under-studied computationally thus far. By providing the iSarcasm dataset, we aim to encourage future NLP research to develop methods for detecting sarcasm in text as intended by the authors of the text, not as labeled under assumptions that we demonstrate to be sub-optimal.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.158",
  "title": "A Systematic Assessment of Syntactic Generalization in Neural Language Models",
  "year": 2020,
  "venue": "ACL",
  "abstract": "While state-of-the-art neural network models continue to achieve lower perplexity scores on language modeling benchmarks, it remains unknown whether optimizing for broad-coverage predictive performance leads to human-like syntactic knowledge. Furthermore, existing work has not provided a clear picture about the model properties required to produce proper syntactic generalizations. We present a systematic evaluation of the syntactic knowledge of neural language models, testing 20 combinations of model types and data sizes on a set of 34 English-language syntactic test suites. We find substantial differences in syntactic generalization performance by model architecture, with sequential models underperforming other architectures. Factorially manipulating model architecture and training dataset size (1M-40M words), we find that variability in syntactic generalization performance is substantially greater by architecture than by dataset size for the corpora tested in our experiments. Our results also reveal a dissociation between perplexity and syntactic generalization performance.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.159",
  "title": "Inflecting When There's No Majority: Limitations of Encoder-Decoder Neural Networks as Cognitive Models for German Plurals",
  "year": 2020,
  "venue": "ACL",
  "abstract": "Can artificial neural networks learn to represent inflectional morphology and generalize to new words as human speakers do? Kirov and Cotterell (2018) argue that the answer is yes: modern Encoder-Decoder (ED) architectures learn human-like behavior when inflecting English verbs, such as extending the regular past tense form /-(e)d/ to novel words. However, their work does not address the criticism raised by Marcus et al. (1995): that neural models may learn to extend not the regular, but the most frequent class - and thus fail on tasks like German number inflection, where infrequent suffixes like /-s/ can still be productively generalized. To investigate this question, we first collect a new dataset from German speakers (production and ratings of plural forms for novel nouns) that is designed to avoid sources of information unavailable to the ED model. The speaker data show high variability, and two suffixes evince 'regular' behavior, appearing more often with phonologically atypical inputs. Encoder-decoder models do generalize the most frequently produced plural class, but do not show human-like variability or 'regular' extension of these other plural markers. We conclude that modern neural models may still struggle with minority-class generalization.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.179",
  "title": "Recurrent Neural Network Language Models Always Learn English-Like Relative Clause Attachment",
  "year": 2020,
  "venue": "ACL",
  "abstract": "A standard approach to evaluating language models analyzes how models assign probabilities to valid versus invalid syntactic constructions (i.e. is a grammatical sentence more probable than an ungrammatical sentence). Our work uses ambiguous relative clause attachment to extend such evaluations to cases of multiple simultaneous valid interpretations, where stark grammaticality differences are absent. We compare model performance in English and Spanish to show that non-linguistic biases in RNN LMs advantageously overlap with syntactic structure in English but not Spanish. Thus, English models may appear to acquire human-like syntactic preferences, while models trained on Spanish fail to acquire comparable human-like preferences. We conclude by relating these results to broader concerns about the relationship between comprehension (i.e. typical language model use cases) and production (which generates the training data for language models), suggesting that necessary linguistic biases are not present in the training signal at all.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.196",
  "title": "On Importance Sampling-Based Evaluation of Latent Language Models",
  "year": 2020,
  "venue": "ACL",
  "abstract": "Language models that use additional latent structures (e.g., syntax trees, coreference chains, knowledge graph links) provide several advantages over traditional language models. However, likelihood-based evaluation of these models is often intractable as it requires marginalizing over the latent space. Existing works avoid this issue by using importance sampling. Although this approach has asymptotic guarantees, analysis is rarely conducted on the effect of decisions such as sample size and choice of proposal distribution on the reported estimates. In this paper, we carry out this analysis for three models: RNNG, EntityNLM, and KGLM. In addition, we elucidate subtle differences in how importance sampling is applied in these works that can have substantial effects on the final estimates, as well as provide theoretical results which reinforce the validity of this technique.",
  "stance": -0.1
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.234",
  "title": "What is Learned in Visually Grounded Neural Syntax Acquisition",
  "year": 2020,
  "venue": "ACL",
  "abstract": "Visual features are a promising signal for learning bootstrap textual models. However, blackbox learning models make it difficult to isolate the specific contribution of visual components. In this analysis, we consider the case study of the Visually Grounded Neural Syntax Learner (Shi et al., 2019), a recent approach for learning syntax from a visual training signal. By constructing simplified versions of the model, we isolate the core factors that yield the model's strong performance. Contrary to what the model might be capable of learning, we find significantly less expressive versions produce similar predictions and perform just as well, or even better. We also find that a simple lexical signal of noun concreteness plays the main role in the model's predictions as opposed to more complex syntactic reasoning.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.337",
  "title": "Investigating Word-Class Distributions in Word Vector Spaces",
  "year": 2020,
  "venue": "ACL",
  "abstract": "This paper presents an investigation on the distribution of word vectors belonging to a certain word class in a pre-trained word vector space. To this end, we made several assumptions about the distribution, modeled the distribution accordingly, and validated each assumption by comparing the goodness of each model. Specifically, we considered two types of word classes - the semantic class of direct objects of a verb and the semantic class in a thesaurus - and tried to build models that properly estimate how likely it is that a word in the vector space is a member of a given word class. Our results on selectional preference and WordNet datasets show that the centroid-based model will fail to achieve good enough performance, the geometry of the distribution and the existence of subgroups will have limited impact, and also the negative instances need to be considered for adequate modeling of the distribution. We further investigated the relationship between the scores calculated by each model and the degree of membership and found that discriminative learning-based models are best in finding the boundaries of a class, while models based on the offset between positive and negative instances perform best in determining the degree of membership.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.36",
  "title": "Jointly Masked Sequence-to-Sequence Model for Non-Autoregressive Neural Machine Translation",
  "year": 2020,
  "venue": "ACL",
  "abstract": "The masked language model has received remarkable attention due to its effectiveness on various natural language processing tasks. However, few works have adopted this technique in the sequence-to-sequence models. In this work, we introduce a jointly masked sequence-to-sequence model and explore its application on non-autoregressive neural machine translation~(NAT). Specifically, we first empirically study the functionalities of the encoder and the decoder in NAT models, and find that the encoder takes a more important role than the decoder regarding the translation quality. Therefore, we propose to train the encoder more rigorously by masking the encoder input while training. As for the decoder, we propose to train it based on the consecutive masking of the decoder input with an $n$-gram loss function to alleviate the problem of translating duplicate words. The two types of masks are applied to the model jointly at the training stage. We conduct experiments on five benchmark machine translation tasks, and our model can achieve 27.69/32.24 BLEU scores on WMT14 English-German/German-English tasks with $5+$ times speed up compared with an autoregressive model.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.375",
  "title": "Do Neural Language Models Show Preferences for Syntactic Formalisms?",
  "year": 2020,
  "venue": "ACL",
  "abstract": "Recent work on the interpretability of deep neural language models has concluded that many properties of natural language syntax are encoded in their representational spaces. However, such studies often suffer from limited scope by focusing on a single language and a single linguistic formalism. In this study, we aim to investigate the extent to which the semblance of syntactic structure captured by language models adheres to a surface-syntactic or deep syntactic style of analysis, and whether the patterns are consistent across different languages. We apply a probe for extracting directed dependency trees to BERT and ELMo models trained on 13 different languages, probing for two different syntactic annotation styles: Universal Dependencies (UD), prioritizing deep syntactic relations, and Surface-Syntactic Universal Dependencies (SUD), focusing on surface structure. We find that both models exhibit a preference for UD over SUD - with interesting variations across languages and layers - and that the strength of this preference is correlated with differences in tree shape.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.384",
  "title": "Probing for Referential Information in Language Models",
  "year": 2020,
  "venue": "ACL",
  "abstract": "Language models keep track of complex information about the preceding context - including, e.g., syntactic relations in a sentence. We investigate whether they also capture information beneficial for resolving pronominal anaphora in English. We analyze two state of the art models with LSTM and Transformer architectures, via probe tasks and analysis on a coreference annotated corpus. The Transformer outperforms the LSTM in all analyses. Our results suggest that language models are more successful at learning grammatical constraints than they are at learning truly referential information, in the sense of capturing the fact that we use language to refer to entities in the world. However, we find traces of the latter aspect, too.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.408",
  "title": "ERASER: A Benchmark to Evaluate Rationalized NLP Models",
  "year": 2020,
  "venue": "ACL",
  "abstract": "State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more interpretable deep models for NLP that reveal the 'reasoning' behind model outputs. But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. We propose the Evaluating Rationales And Simple English Reasoning (ERASER a benchmark to advance research on interpretable models in NLP. This benchmark comprises multiple datasets and tasks for which human annotations of \"rationales\" (supporting evidence) have been collected. We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how faithful these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions). Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems. The benchmark, code, and documentation are available at https://www.eraserbenchmark.com/",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.424",
  "title": "ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations",
  "year": 2020,
  "venue": "ACL",
  "abstract": "In order to simplify a sentence, human editors perform multiple rewriting transformations: they split it into several shorter sentences, paraphrase words (i.e. replacing complex words or phrases by simpler synonyms), reorder components, and/or delete information deemed unnecessary. Despite these varied range of possible text alterations, current models for automatic sentence simplification are evaluated using datasets that are focused on a single transformation, such as lexical paraphrasing or splitting. This makes it impossible to understand the ability of simplification models in more realistic settings. To alleviate this limitation, this paper introduces ASSET, a new dataset for assessing sentence simplification in English. ASSET is a crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations. Through quantitative and qualitative experiments, we show that simplifications in ASSET are better at capturing characteristics of simplicity when compared to other standard evaluation datasets for the task. Furthermore, we motivate the need for developing better methods for automatic evaluation using ASSET, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.431",
  "title": "Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings",
  "year": 2020,
  "venue": "ACL",
  "abstract": "Contextualized representations (e.g. ELMo, BERT) have become the default pretrained representations for downstream NLP applications. In some settings, this transition has rendered their static embedding predecessors (e.g. Word2Vec, GloVe) obsolete. As a side-effect, we observe that older interpretability methods for static embeddings - while more diverse and mature than those available for their dynamic counterparts - are underutilized in studying newer contextualized representations. Consequently, we introduce simple and fully general methods for converting from contextualized representations to static lookup-table embeddings which we apply to 5 popular pretrained models and 9 sets of pretrained weights. Our analysis of the resulting static embeddings notably reveals that pooling over many contexts significantly improves representational quality under intrinsic evaluation. Complementary to analyzing representational quality, we consider social biases encoded in pretrained representations with respect to gender, race/ethnicity, and religion and find that bias is encoded disparately across pretrained models and internal layers even for models with the same training data. Concerningly, we find dramatic inconsistencies between social bias estimators for word embeddings.",
  "stance": -0.9
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.434",
  "title": "Spying on Your Neighbors: Fine-grained Probing of Contextual Embeddings for Information about Surrounding Words",
  "year": 2020,
  "venue": "ACL",
  "abstract": "Although models using contextual word embeddings have achieved state-of-the-art results on a host of NLP tasks, little is known about exactly what information these embeddings encode about the context words that they are understood to reflect. To address this question, we introduce a suite of probing tasks that enable fine-grained testing of contextual embeddings for encoding of information about surrounding words. We apply these tasks to examine the popular BERT, ELMo and GPT contextual encoders, and find that each of our tested information types is indeed encoded as contextual information across tokens, often with near-perfect recoverability-but the encoders vary in which features they distribute to which tokens, how nuanced their distributions are, and how robust the encoding of each feature is to distance. We discuss implications of these results for how different types of models break down and prioritize word-level context information when constructing token embeddings.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.441",
  "title": "Adversarial NLI: A New Benchmark for Natural Language Understanding",
  "year": 2020,
  "venue": "ACL",
  "abstract": "We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate.",
  "stance": -0.5
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.442",
  "title": "Beyond Accuracy: Behavioral Testing of NLP Models with CheckList",
  "year": 2020,
  "venue": "ACL",
  "abstract": "Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.465",
  "title": "How Can We Accelerate Progress Towards Human-like Linguistic Generalization?",
  "year": 2020,
  "venue": "ACL",
  "abstract": "This position paper describes and critiques the Pretraining-Agnostic Identically Distributed (PAID) evaluation paradigm, which has become a central tool for measuring progress in natural language understanding. This paradigm consists of three stages: (1) pre-training of a word prediction model on a corpus of arbitrary size; (2) fine-tuning (transfer learning) on a training set representing a classification task; (3) evaluation on a test set drawn from the same distribution as that training set. This paradigm favors simple, low-bias architectures, which, first, can be scaled to process vast amounts of data, and second, can capture the fine-grained statistical properties of a particular data set, regardless of whether those properties are likely to generalize to examples of the task outside the data set. This contrasts with humans, who learn language from several orders of magnitude less data than the systems favored by this evaluation paradigm, and generalize to new tasks in a consistent way. We advocate for supplementing or replacing PAID with paradigms that reward architectures that generalize as quickly and robustly as humans.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.467",
  "title": "Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?",
  "year": 2020,
  "venue": "ACL",
  "abstract": "While pretrained models such as BERT have shown large gains across natural language understanding tasks, their performance can be improved by further training the model on a data-rich intermediate task, before fine-tuning it on a target task. However, it is still poorly understood when and why intermediate-task training is beneficial for a given target task. To investigate this, we perform a large-scale study on the pretrained RoBERTa model with 110 intermediate-target task combinations. We further evaluate all trained models with 25 probing tasks meant to reveal the specific skills that drive transfer. We observe that intermediate tasks requiring high-level inference and reasoning abilities tend to work best. We also observe that target task performance is strongly correlated with higher-level abilities such as coreference resolution. However, we fail to observe more granular correlations between probing and target task performance, highlighting the need for further work on broad-coverage probing benchmarks. We also observe evidence that the forgetting of knowledge learned during pretraining may limit our analysis, highlighting the need for further work on transfer learning methods in these settings.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.485",
  "title": "Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP",
  "year": 2020,
  "venue": "ACL",
  "abstract": "We survey 146 papers analyzing \"bias\" in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing \"bias\" is an inherently normative process. We further find that these papers' proposed quantitative techniques for measuring or mitigating \"bias\" are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing \"bias\" in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of \"bias\"---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements-and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.486",
  "title": "Social Bias Frames: Reasoning about Social and Power Implications of Language",
  "year": 2020,
  "venue": "ACL",
  "abstract": "To address the limitation, in this paper, we propose a novel framework of initiative defense to degrade the performance of facial manipulation models controlled by malicious users. The basic idea is to actively inject imperceptible venom into target facial data before manipulation. To this end, we first imitate the target manipulation model with a surrogate model, and then devise a poison perturbation generator to obtain the desired venom. An alternating training strategy are further leveraged to train both the surrogate model and the pertur- bation generator. Two typical facial manipulation tasks: face attribute editing and face reenactment, are considered in our initiative defense framework. Extensive experiments demon- strate the effectiveness and robustness of our framework in different settings. Finally, we hope this work can shed some light on initiative countermeasures against more adversarial scenarios.",
  "stance": -0.3
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.495",
  "title": "Obtaining Faithful Interpretations from Compositional Neural Networks",
  "year": 2020,
  "venue": "ACL",
  "abstract": "Neural module networks (NMNs) are a popular approach for modeling compositionality: they achieve high accuracy when applied to problems in language and vision, while reflecting the compositional structure of the problem in the network architecture. However, prior work implicitly assumed that the structure of the network modules, describing the abstract reasoning process, provides a faithful explanation of the model's reasoning; that is, that all modules perform their intended behaviour. In this work, we propose and conduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2 and DROP, two datasets which require composing multiple reasoning steps. We find that the intermediate outputs differ from the expected output, illustrating that the network structure does not provide a faithful explanation of model behaviour. To remedy that, we train the model with auxiliary supervision and propose particular choices for module architecture that yield much better faithfulness, at a minimal cost to accuracy.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.586",
  "title": "Words Aren't Enough, Their Order Matters: On the Robustness of Grounding Visual Referring Expressions",
  "year": 2020,
  "venue": "ACL",
  "abstract": "Visual referring expression recognition is a challenging task that requires natural language understanding in the context of an image. We critically examine RefCOCOg, a standard benchmark for this task, using a human study and show that 83.7% of test instances do not require reasoning on linguistic structure, i.e., words are enough to identify the target object, the word order doesn't matter. To measure the true progress of existing models, we split the test set into two sets, one which requires reasoning on linguistic structure and the other which doesn't. Additionally, we create an out-of-distribution dataset Ref-Adv by asking crowdworkers to perturb in-domain examples such that the target object changes. Using these datasets, we empirically show that existing methods fail to exploit linguistic structure and are 12% to 23% lower in performance than the established progress for this task. We also propose two methods, one based on contrastive learning and the other based on multi-task learning, to increase the robustness of ViLBERT, the current state-of-the-art model for this task. Our datasets are publicly available at https://github.com/aws/aws-refcocog-adv.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.608",
  "title": "Unsupervised Dual Paraphrasing for Two-stage Semantic Parsing",
  "year": 2020,
  "venue": "ACL",
  "abstract": "One daunting problem for semantic parsing is the scarcity of annotation. Aiming to reduce nontrivial human labor, we propose a two-stage semantic parsing framework, where the first stage utilizes an unsupervised paraphrase model to convert an unlabeled natural language utterance into the canonical utterance. The downstream naive semantic parser accepts the intermediate output and returns the target logical form. Furthermore, the entire training process is split into two phases: pre-training and cycle learning. Three tailored self-supervised tasks are introduced throughout training to activate the unsupervised paraphrase model. Experimental results on benchmarks Overnight and GeoGranno demonstrate that our framework is effective and compatible with supervised training.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.633",
  "title": "Out of the Echo Chamber: Detecting Countering Debate Speeches",
  "year": 2020,
  "venue": "ACL",
  "abstract": "An educated and informed consumption of media content has become a challenge in modern times. With the shift from traditional news outlets to social media and similar venues, a major concern is that readers are becoming encapsulated in \"echo chambers\" and may fall prey to fake news and disinformation, lacking easy access to dissenting views. We suggest a novel task aiming to alleviate some of these concerns - that of detecting articles that most effectively counter the arguments - and not just the stance - made in a given text. We study this problem in the context of debate speeches. Given such a speech, we aim to identify, from among a set of speeches on the same topic and with an opposing stance, the ones that directly counter it. We provide a large dataset of 3,685 such speeches (in English), annotated for this relation, which hopefully would be of general interest to the NLP community. We explore several algorithms addressing this task, and while some are successful, all fall short of expert human performance, suggesting room for further research. All data collected during this work is freely available for research.",
  "stance": -0.5
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.635",
  "title": "KdConv: A Chinese Multi-domain Dialogue Dataset Towards Multi-turn Knowledge-driven Conversation",
  "year": 2020,
  "venue": "ACL",
  "abstract": "The research of knowledge-driven conversational systems is largely limited due to the lack of dialog data which consists of multi-turn conversations on multiple topics and with knowledge annotations. In this paper, we propose a Chinese multi-domain knowledge-driven conversation dataset, KdConv, which grounds the topics in multi-turn conversations to knowledge graphs. Our corpus contains 4.5K conversations from three domains (film, music, and travel), and 86K utterances with an average turn number of 19.0. These conversations contain in-depth discussions on related topics and natural transition between multiple topics. To facilitate the following research on this corpus, we provide several benchmark models. Comparative results show that the models can be enhanced by introducing background knowledge, yet there is still a large space for leveraging knowledge to model multi-turn conversations for further research. Results also show that there are obvious performance differences between different domains, indicating that it is worth further explore transfer learning and domain adaptation. The corpus and benchmark models are publicly available.",
  "stance": -0.5
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.650",
  "title": "Semi-supervised Contextual Historical Text Normalization",
  "year": 2020,
  "venue": "ACL",
  "abstract": "Historical text normalization, the task of mapping historical word forms to their modern counterparts, has recently attracted a lot of interest (Bollmann, 2019; Tang et al., 2018; Lusetti et al., 2018; Bollmann et al., 2018;Robertson and Goldwater, 2018; Bollmannet al., 2017; Korchagina, 2017). Yet, virtually all approaches suffer from the two limitations: 1) They consider a fully supervised setup, often with impractically large manually normalized datasets; 2) Normalization happens on words in isolation. By utilizing a simple generative normalization model and obtaining powerful contextualization from the target-side language model, we train accurate models with unlabeled historical data. In realistic training scenarios, our approach often leads to reduction in manually normalized data at the same accuracy levels.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.658",
  "title": "A Call for More Rigor in Unsupervised Cross-lingual Learning",
  "year": 2020,
  "venue": "ACL",
  "abstract": "We review motivations, definition, approaches, and methodology for unsupervised cross-lingual learning and call for a more rigorous position in each of them. An existing rationale for such research is based on the lack of parallel data for many of the world's languages. However, we argue that a scenario without any parallel data and abundant monolingual data is unrealistic in practice. We also discuss different training signals that have been used in previous work, which depart from the pure unsupervised setting. We then describe common methodological issues in tuning and evaluation of unsupervised cross-lingual models and present best practices. Finally, we provide a unified outlook for different types of research in this area (i.e., cross-lingual word embeddings, deep multilingual pretraining, and unsupervised machine translation) and argue for comparable evaluation of these models.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.671",
  "title": "Contrastive Self-Supervised Learning for Commonsense Reasoning",
  "year": 2020,
  "venue": "ACL",
  "abstract": "We propose a self-supervised method to solve Pronoun Disambiguation and Winograd Schema Challenge problems. Our approach exploits the characteristic structure of training corpora related to so-called \"trigger\" words, which are responsible for flipping the answer in pronoun disambiguation. We achieve such commonsense reasoning by constructing pair-wise contrastive auxiliary predictions. To this end, we leverage a mutual exclusive loss regularized by a contrastive margin. Our architecture is based on the recently introduced transformer networks, BERT, that exhibits strong performance on many NLP benchmarks. Empirical results show that our method alleviates the limitation of current supervised approaches for commonsense reasoning. This study opens up avenues for exploiting inexpensive self-supervision to achieve performance gain in commonsense reasoning tasks.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.698",
  "title": "Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly",
  "year": 2020,
  "venue": "ACL",
  "abstract": "Building on Petroni et al. 2019, we propose two new probing tasks analyzing factual knowledge stored in Pretrained Language Models (PLMs). (1) Negation. We find that PLMs do not distinguish between negated (''Birds cannot [MASK]\") and non-negated (''Birds can [MASK]\") cloze questions. (2) Mispriming. Inspired by priming methods in human psychology, we add \"misprimes\" to cloze questions (''Talk? Birds can [MASK]\"). We find that PLMs are easily distracted by misprimes. These results suggest that PLMs still have a long way to go to adequately learn human-like factual knowledge.",
  "stance": -0.9
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.730",
  "title": "TVQA+: Spatio-Temporal Grounding for Video Question Answering",
  "year": 2020,
  "venue": "ACL",
  "abstract": "We present the task of Spatio-Temporal Video Question Answering, which requires intelligent systems to simultaneously retrieve relevant moments and detect referenced visual concepts (people and objects) to answer natural language questions about videos. We first augment the TVQA dataset with 310.8K bounding boxes, linking depicted objects to visual concepts in questions and answers. We name this augmented version as TVQA+. We then propose Spatio-Temporal Answerer with Grounded Evidence (STAGE), a unified framework that grounds evidence in both spatial and temporal domains to answer questions about videos. Comprehensive experiments and analyses demonstrate the effectiveness of our framework and how the rich annotations in our TVQA+ dataset can contribute to the question answering task. Moreover, by performing this joint task, our model is able to produce insightful and interpretable spatio-temporal attention visualizations.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.766",
  "title": "Should All Cross-Lingual Embeddings Speak English?",
  "year": 2020,
  "venue": "ACL",
  "abstract": "Most of recent work in cross-lingual word embeddings is severely Anglocentric. The vast majority of lexicon induction evaluation dictionaries are between English and another language, and the English embedding space is selected by default as the hub when learning in a multilingual setting. With this work, however, we challenge these practices. First, we show that the choice of hub language can significantly impact downstream lexicon induction zero-shot POS tagging performance. Second, we both expand a standard English-centered evaluation dictionary collection to include all language pairs using triangulation, and create new dictionaries for under-represented languages. Evaluating established methods over all these language pairs sheds light into their suitability for aligning embeddings from distant languages and presents new challenges for the field. Finally, in our analysis we identify general guidelines for strong cross-lingual embedding baselines, that extend to language pairs that do not include English.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/2020.acl-main.768",
  "title": "Are Natural Language Inference Models IMPPRESsive? Learning IMPlicature and PRESupposition",
  "year": 2020,
  "venue": "ACL",
  "abstract": "Natural language inference (NLI) is an increasingly important task for natural language understanding, which requires one to infer whether a sentence entails another. However, the ability of NLI models to make pragmatic inferences remains understudied. We create an IMPlicature and PRESupposition diagnostic dataset (IMPPRES), consisting of 32K semi-automatically generated sentence pairs illustrating well-studied pragmatic inference types. We use IMPPRES to evaluate whether BERT, InferSent, and BOW NLI models trained on MultiNLI (Williams et al., 2018) learn to make pragmatic inferences. Although MultiNLI appears to contain very few pairs illustrating these inference types, we find that BERT learns to draw pragmatic inferences. It reliably treats scalar implicatures triggered by \"some\" as entailments. For some presupposition triggers like \"only\", BERT reliably recognizes the presupposition as an entailment, even when the trigger is embedded under an entailment canceling operator like negation. BOW and InferSent show weaker evidence of pragmatic reasoning. We conclude that NLI training encourages models to learn some, but not all, pragmatic inferences.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/J00-3001",
  "title": "Extracting the lowest-frequency words: pitfalls and possibilities",
  "year": 2000,
  "venue": "CL",
  "abstract": "In a medical information extraction system, we use common word association techniques to extract side-effect-related terms. Many of these terms have afrequency of less than five. Standard word-association-based applications disregard the lowest-frequency words, and hence disregard useful information. We therefore devised an extraction system for the full word frequency range. This system computes the significance of association by the log-likelihood ratio and Fisher's exact test. The output of the system shows a recurrent, corpus-independent pattern in both recall and the number of significant words. We will explain these patterns by the statistical behavior of the lowest-frequency words. We used Dutch verb-particle combinations as a second and independent collocation extraction application to illustrate the generality of the observed phenomena. We will conclude that a) word-association-based extraction systems can be enhanced by also considering the lowest-frequency words, b) significance levels should not befixed but adjusted for the optimal window size, c) hapax legomena, words occurring only once, should be disregarded a priori in the statistical analysis, and d) the distribution of the targets to extract should be considered in combination with the extraction method.",
  "stance": 0.1
 },
 {
  "url": "https://aclanthology.org/J01-3001",
  "title": "The Interaction of Knowledge Sources in Word Sense Disambiguation",
  "year": 2001,
  "venue": "CL",
  "abstract": "Word sense disambiguation (WSD) is a computational linguistics task likely to benefit from the tradition of combining different knowledge sources in artificial intelligence research. An important step in the exploration of this hypothesis is to determine which linguistic knowledge sources are most useful and whether their combination leads to improved results. We present a sense tagger which uses several knowledge sources. Tested accuracy exceeds 94% on our evaluation corpus. Our system attempts to disambiguate all content words in running text rather than limiting itself to treating a restricted vocabulary of words. It is argued that this approach is more likely to assist the creation of practical systems.",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/J07-1001",
  "title": "Letter to the Editor",
  "year": 2007,
  "venue": "CL",
  "abstract": "In her review of our book Memory-based Language Processing, which appeared in Computational Linguistics, Issue 32(4), Sandra Kübler comments on the illegibility of the phonetic font. Unfortunately, she had an early copy; the book was reprinted as soon as this problem was noticed, and Cambridge University Press has offered to replace any remaining misprinted copies with reprints upon request. This applies to copies with incorrectly overlapping phonetic symbols on pages 28, 59–63, and 108. They may be sent to Helen Barton, 3rd Floor, The Edinburgh Building, Cambridge University Press, Shaftesbury Road, Cambridge, UK, CB2 2RU. Postage will also be reimbursed if a receipt for the cost is provided.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J08-4010",
  "title": "Erratum: Dependency Parsing of Turkish",
  "year": 2008,
  "venue": "CL",
  "abstract": "In Section 5 of the article “Dependency Parsing of Turkish” by Gülşen Eryiğit, Joakim Nivre, and Kemal Oflazer (September 2008, Vol. 34, No. 3: 357–389), some abbreviations were misinterpreted during the copyediting process. The third sentence of Section 5.2 should be as follows: “We use an unlexicalized feature model where the parser uses only the minor part-of-speech category (as POS) and dependency type of tokens (as DEP) and compare the results with the probabilistic parser.” The first sentence of the second paragraph of Section 5.2.1 should start as follows: “We take the minor part-of-speech category. . . .” The “POS” abbreviation used on page 20 should be read as “minor part-ofspeech,” and the “POS” abbreviations on pages 21, 26, 27, and 28 should be read as “part-of-speech.”",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/J09-1005",
  "title": "Unsupervised Type and Token Identification of Idiomatic Expressions",
  "year": 2009,
  "venue": "CL",
  "abstract": "Idiomatic expressions are plentiful in everyday language, yet they remain mysterious, as it is not clear exactly how people learn and understand them. They are of special interest to linguists, psycholinguists, and lexicographers, mainly because of their syntactic and semantic idiosyncrasies as well as their unclear lexical status. Despite a great deal of research on the properties of idioms in the linguistics literature, there is not much agreement on which properties are characteristic of these expressions. Because of their peculiarities, idiomatic expressions have mostly been overlooked by researchers in computational linguistics. In this article, we look into the usefulness of some of the identified linguistic properties of idioms for their automatic recognition. Specifically, we develop statistical measures that each model a specific property of idiomatic expressions by looking at their actual usage patterns in text. We use these statistical measures in a type-based classification task where we automatically separate idiomatic expressions (expressions with a possible idiomatic interpretation) from similar-on-the-surface literal phrases (for which no idiomatic interpretation is possible). In addition, we use some of the measures in a token identification task where we distinguish idiomatic and literal usages of potentially idiomatic expressions in context.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/J09-4008",
  "title": "An Investigation into the Validity of Some Metrics for Automatically Evaluating Natural Language Generation Systems",
  "year": 2009,
  "venue": "CL",
  "abstract": "There is growing interest in using automatically computed corpus-based evaluation metrics to evaluate Natural Language Generation (NLG) systems, because these are often considerably cheaper than the human-based evaluations which have traditionally been used in NLG. We review previous work on NLG evaluation and on validation of automatic metrics in NLP, and then present the results of two studies of how well some metrics which are popular in other areas of NLP (notably BLEU and ROUGE) correlate with human judgments in the domain of computer-generated weather forecasts. Our results suggest that, at least in this domain, metrics may provide a useful measure of language quality, although the evidence for this is not as strong as we would ideally like to see; however, they do not provide a useful measure of content quality. We also discuss a number of caveats which must be kept in mind when interpreting this and other validation studies.",
  "stance": -0.5
 },
 {
  "url": "https://aclanthology.org/J12-1006",
  "title": "Computational Generation of Referring Expressions: A Survey",
  "year": 2012,
  "venue": "CL",
  "abstract": "This article offers a survey of computational research on referring expression generation (REG). It introduces the REG problem and describes early work in this area, discussing what basic assumptions lie behind it, and showing how its remit has widened in recent years. We discuss computational frameworks underlying REG, and demonstrate a recent trend that seeks to link REG algorithms with well-established Knowledge Representation techniques. Considerable attention is given to recent efforts at evaluating REG algorithms and the lessons that they allow us to learn. The article concludes with a discussion of the way forward in REG, focusing on references in larger and more realistic settings.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J12-2001",
  "title": "Modality and Negation: An Introduction to the Special Issue",
  "year": 2012,
  "venue": "CL",
  "abstract": "objects can be assertions, beliefs, facts, or eventualities. Discourse connectives and their arguments are assigned attribution-related features (Prasad et al. 2006) such as SOURCE (writer, other, arbitrary), TYPE (reflecting the nature of the relation between the agent and the abstract object), SCOPAL POLARITY of attribution, and DETERMINACY (indicating the presence of contexts canceling the entailment of attribution). The text spans signaling the attribution are also marked. Prasad et al. (2006) report that 34% of the discourse relations have some non-writer agent. SCOPAL POLARITY is annotated to identify cases when verbs of attribution (say, think, ...) are negated syntactically",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J12-2003",
  "title": "Did It Happen? The Pragmatic Complexity of Veridicality Assessment",
  "year": 2012,
  "venue": "CL",
  "abstract": "Natural language understanding depends heavily on assessing veridicality—whether events mentioned in a text are viewed as happening or not—but little consideration is given to this property in current relation and event extraction systems. Furthermore, the work that has been done has generally assumed that veridicality can be captured by lexical semantic properties whereas we show that context and world knowledge play a significant role in shaping veridicality. We extend the FactBank corpus, which contains semantically driven veridicality annotations, with pragmatically informed ones. Our annotations are more complex than the lexical assumption predicts but systematic enough to be included in computational work on textual understanding. They also indicate that veridicality judgments are not always categorical, and should therefore be modeled as distributions. We build a classifier to automatically assign event veridicality distributions based on our new annotations. The classifier relies not only on lexical features like hedges or negations, but also on structural features and approximations of world knowledge, thereby providing a nuanced picture of the diverse factors that shape veridicality.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/J12-2006",
  "title": "Modality and Negation in SIMT Use of Modality and Negation in Semantically-Informed Syntactic MT",
  "year": 2012,
  "venue": "CL",
  "abstract": "∗ U.S. Department of Defense, 9800 Savage Rd., Suite 6811, Fort Meade, MD 20755. E-mail: kathrynlb@gmail.com. ∗∗ Center for Advanced Study of Language, University of Maryland, 7005 52nd Avenue, College Park, MD 20742. E-mail: meb@umd.edu. † Department of Computer Science and UMIACS, University of Maryland, AV Williams Building 3153, College Park, MD 20742. E-mail: bonnie@umiacs.umd.edu. ‡ Center for Language and Speech Processing, Johns Hopkins University, 3400 N. Charles Street, Hackerman Hall 320, Baltimore MD 21218. E-mail: {ccb,nwf}@cs.jhu.edu. § Applied Physics Laboratory, Johns Hopkins University, 11000 Johns Hopkins Rd., Laurel, MD 20723. E-mail: christine.piatko@jhuapl.edu. || Carnegie Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213. E-mail: lsl@cs.cmu.edu. # BNN Technologies, 10 Moulton Street, Cambridge, MA 02138. E-mail: smiller@bbn.com.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J12-3006",
  "title": "Tree-Adjoining Grammars Are Not Closed Under Strong Lexicalization",
  "year": 2012,
  "venue": "CL",
  "abstract": "A lexicalized tree-adjoining grammar is a tree-adjoining grammar where each elementary tree contains some overt lexical item. Such grammars are being used to give lexical accounts of syntactic phenomena, where an elementary tree defines the domain of locality of the syntactic and semantic dependencies of its lexical items. It has been claimed in the literature that for every tree-adjoining grammar, one can construct a strongly equivalent lexicalized version. We show that such a procedure does not exist: Tree-adjoining grammars are not closed under strong lexicalization.",
  "stance": -0.1
 },
 {
  "url": "https://aclanthology.org/J12-3007",
  "title": "A Scalable Distributed Syntactic, Semantic, and Lexical Language Model",
  "year": 2012,
  "venue": "CL",
  "abstract": "This paper presents an attempt at building a large scale distributed composite language model that is formed by seamlessly integrating an n-gram model, a structured language model, and probabilistic latent semantic analysis under a directed Markov random field paradigm to simultaneously account for local word lexical information, mid-range sentence syntactic structure, and long-span document semantic content. The composite language model has been trained by performing a convergent N-best list approximate EM algorithm and a follow-up EM algorithm to improve word prediction power on corpora with up to a billion tokens and stored on a supercomputer. The large scale distributed composite language model gives drastic perplexity reduction over n-grams and achieves significantly better translation quality measured by the Bleu score and “readability” of translations when applied to the task of re-ranking the N-best list from a state-of-the-art parsing-based machine translation system.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/J14-3007",
  "title": "Improved Estimation of Entropy for Evaluation of Word Sense Induction",
  "year": 2014,
  "venue": "CL",
  "abstract": "Information-theoretic measures are among the most standard techniques for evaluation of clustering methods including word sense induction (WSI) systems. Such measures rely on sample-based estimates of the entropy. However, the standard maximum likelihood estimates of the entropy are heavily biased with the bias dependent on, among other things, the number of clusters and the sample size. This makes the measures unreliable and unfair when the number of clusters produced by different systems vary and the sample size is not exceedingly large. This corresponds exactly to the setting of WSI evaluation where a ground-truth cluster sense number arguably does not exist and the standard evaluation scenarios use a small number of instances of each word to compute the score. We describe more accurate entropy estimators and analyze their performance both in simulations and on evaluation of WSI systems.",
  "stance": 0.1
 },
 {
  "url": "https://aclanthology.org/J15-4002",
  "title": "Design and Evaluation of Metaphor Processing Systems",
  "year": 2015,
  "venue": "CL",
  "abstract": "ness–concreteness algorithm. However, the evaluation was done on only five",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/J16-2001",
  "title": "Surveys: A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena",
  "year": 2016,
  "venue": "CL",
  "abstract": "Word reordering is one of the most difficult aspects of statistical machine translation (SMT), and an important factor of its quality and efficiency. Despite the vast amount of research published to date, the interest of the community in this problem has not decreased, and no single method appears to be strongly dominant across language pairs. Instead, the choice of the optimal approach for a new translation task still seems to be mostly driven by empirical trials. To orient the reader in this vast and complex research area, we present a comprehensive survey of word reordering viewed as a statistical modeling challenge and as a natural language phenomenon. The survey describes in detail how word reordering is modeled within different string-based and tree-based SMT frameworks and as a stand-alone task, including systematic overviews of the literature in advanced reordering modeling. We then question why some approaches are more successful than others in different language pairs. We argue that besides measuring the amount of reordering, it is important to understand which kinds of reordering occur in a given language pair. To this end, we conduct a qualitative analysis of word reordering phenomena in a diverse sample of language pairs, based on a large collection of linguistic knowledge. Empirical results in the SMT literature are shown to support the hypothesis that a few linguistic facts can be very useful to anticipate the reordering characteristics of a language pair and to select the SMT framework that best suits them.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J16-4013",
  "title": "Reviewers for Volume 42",
  "year": 2016,
  "venue": "CL",
  "abstract": "This journal has a knowledgeable and hard-working editorial board, listed on the journal’s Web site, but for most submissions we also enlist the aid of specialist reviewers outside the editorial board. The Editor of Computational Linguistics would like to express her gratitude to the external reviewers listed here, who anonymously reviewed papers for the journal during the preparation of this volume (Volume 42). Their generosity, judicious judgment, and prompt response substantially helped us to publish a journal that both is timely and maintains exacting scientific standards; it is a genuine pleasure to thank them collectively for their dedicated service.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J18-3002",
  "title": "A Structured Review of the Validity of BLEU",
  "year": 2018,
  "venue": "CL",
  "abstract": "The BLEU metric has been widely used in NLP for over 15 years to evaluate NLP systems, especially in machine translation and natural language generation. I present a structured review of the evidence on whether BLEU is a valid evaluation technique-in other words, whether BLEU scores correlate with real-world utility and user-satisfaction of NLP systems; this review covers 284 correlations reported in 34 papers. Overall, the evidence supports using BLEU for diagnostic evaluation of MT systems (which is what it was originally proposed for), but does not support using BLEU outside of MT, for evaluation of individual texts, or for scientific hypothesis testing.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/2020.cl-2.1",
  "title": "Multilingual and Interlingual Semantic Representations for Natural Language Processing: A Brief Introduction",
  "year": 2020,
  "venue": "CL",
  "abstract": "We introduce the Computational Linguistics special issue on Multilingual and Interlingual Semantic Representations for Natural Language Processing. We situate the special issue's five articles in the context of our fast-changing field, explaining our motivation for this project. We offer a brief summary of the work in the issue, which includes developments on lexical and sentential semantic representations, from symbolic and neural perspectives.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C16-1052",
  "title": "Product Classification in E-Commerce using Distributional Semantics",
  "year": 2016,
  "venue": "COLING",
  "abstract": "Product classification is the task of automatically predicting a taxonomy path for a product in a predefined taxonomy hierarchy given a textual product description or title. For efficient product classification we require a suitable representation for a document (the textual description of a product) feature vector and efficient and fast algorithms for prediction.To address the above challenges, we propose a new distributional semantics representation for document vector formation. We also develop a new two-level ensemble approach utilising (with respect to the taxonomy tree) path-wise, node-wise and depth-wise classifiers to reduce error in the final product classification task. Our experiments show the effectiveness of the distributional representation and the ensemble approach on data sets from a leading e-commerce platform and achieve improved results on various evaluation metrics compared to earlier approaches.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C16-1104",
  "title": "A Hybrid Approach to Generation of Missing Abstracts in Biomedical Literature",
  "year": 2016,
  "venue": "COLING",
  "abstract": "Readers usually rely on abstracts to identify relevant medical information from scientific articles. Abstracts are also essential to advanced information retrieval methods. More than 50 thousand scientific publications in PubMed lack author-generated abstracts, and the relevancy judgements for these papers have to be based on their titles alone. In this paper, we propose a hybrid summarization technique that aims to select the most pertinent sentences from articles to generate an extractive summary in lieu of a missing abstract. We combine i) health outcome detection, ii) keyphrase extraction, and iii) textual entailment recognition between sentences. We evaluate our hybrid approach and analyze the improvements of multi-factor summarization over techniques that rely on a single method, using a collection of 295 manually generated reference summaries. The obtained results show that the hybrid approach outperforms the baseline techniques with an improvement of 13% in recall and 4% in F1 score.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/C16-1171",
  "title": "A Distribution-based Model to Learn Bilingual Word Embeddings",
  "year": 2016,
  "venue": "COLING",
  "abstract": "We introduce a distribution based model to learn bilingual word embeddings from monolingual data. It is simple, effective and does not require any parallel data or any seed lexicon. We take advantage of the fact that word embeddings are usually in form of dense real-valued low-dimensional vector and therefore the distribution of them can be accurately estimated. A novel cross-lingual learning objective is proposed which directly matches the distributions of word embeddings in one language with that in the other language. During the joint learning process, we dynamically estimate the distributions of word embeddings in two languages respectively and minimize the dissimilarity between them through standard back propagation algorithm. Our learned bilingual word embeddings allow to group each word and its translations together in the shared vector space. We demonstrate the utility of the learned embeddings on the task of finding word-to-word translations from monolingual corpora. Our model achieved encouraging performance on data in both related languages and substantially different languages.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C16-1182",
  "title": "Representation and Learning of Temporal Relations",
  "year": 2016,
  "venue": "COLING",
  "abstract": "Determining the relative order of events and times described in text is an important problem in natural language processing. It is also a difficult one: general state-of-the-art performance has been stuck at a relatively low ceiling for years. We investigate the representation of temporal relations, and empirically evaluate the effect that various temporal relation representations have on machine learning performance. While machine learning performance decreases with increased representational expressiveness, not all representation simplifications have equal impact.",
  "stance": -0.6
 },
 {
  "url": "https://aclanthology.org/C16-1277",
  "title": "Keyphrase Annotation with Graph Co-Ranking",
  "year": 2016,
  "venue": "COLING",
  "abstract": "Keyphrase annotation is the task of identifying textual units that represent the main content of a document. Keyphrase annotation is either carried out by extracting the most important phrases from a document, keyphrase extraction, or by assigning entries from a controlled domain-specific vocabulary, keyphrase assignment. Assignment methods are generally more reliable. They provide better-formed keyphrases, as well as keyphrases that do not occur in the document. But they are often silent on the contrary of extraction methods that do not depend on manually built resources. This paper proposes a new method to perform both keyphrase extraction and keyphrase assignment in an integrated and mutual reinforcing manner. Experiments have been carried out on datasets covering different domains of humanities and social sciences. They show statistically significant improvements compared to both keyphrase extraction and keyphrase assignment state-of-the art methods.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C18-1116",
  "title": "Deep Neural Networks at the Service of Multilingual Parallel Sentence Extraction",
  "year": 2018,
  "venue": "COLING",
  "abstract": "Wikipedia provides an invaluable source of parallel multilingual data, which are in high demand for various sorts of linguistic inquiry, including both theoretical and practical studies. We introduce a novel end-to-end neural model for large-scale parallel data harvesting from Wikipedia. Our model is language-independent, robust, and highly scalable. We use our system for collecting parallel German-English, French-English and Persian-English sentences. Human evaluations at the end show the strong performance of this model in collecting high-quality parallel data. We also propose a statistical framework which extends the results of our human evaluation to other language pairs. Our model also obtained a state-of-the-art result on the German-English dataset of BUCC 2017 shared task on parallel sentence extraction from comparable corpora.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C18-1138",
  "title": "Relation Induction in Word Embeddings Revisited",
  "year": 2018,
  "venue": "COLING",
  "abstract": "Given a set of instances of some relation, the relation induction task is to predict which other word pairs are likely to be related in the same way. While it is natural to use word embeddings for this task, standard approaches based on vector translations turn out to perform poorly. To address this issue, we propose two probabilistic relation induction models. The first model is based on translations, but uses Gaussians to explicitly model the variability of these translations and to encode soft constraints on the source and target words that may be chosen. In the second model, we use Bayesian linear regression to encode the assumption that there is a linear relationship between the vector representations of related words, which is considerably weaker than the assumption underlying translation based models.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/C18-1150",
  "title": "A Reinforcement Learning Framework for Natural Question Generation using Bi-discriminators",
  "year": 2018,
  "venue": "COLING",
  "abstract": "Visual Question Generation (VQG) aims to ask natural questions about an image automatically. Existing research focus on training model to fit the annotated data set that makes it indifferent from other language generation tasks. We argue that natural questions need to have two specific attributes from the perspectives of content and linguistic respectively, namely, natural and human-written. Inspired by the setting of discriminator in adversarial learning, we propose two discriminators, one for each attribute, to enhance the training. We then use the reinforcement learning framework to incorporate scores from the two discriminators as the reward to guide the training of the question generator. Experimental results on a benchmark VQG dataset show the effectiveness and robustness of our model compared to some state-of-the-art models in terms of both automatic and human evaluation metrics.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/C18-1157",
  "title": "Recognizing Humour using Word Associations and Humour Anchor Extraction",
  "year": 2018,
  "venue": "COLING",
  "abstract": "This paper attempts to marry the interpretability of statistical machine learning approaches with the more robust models of joke structure and joke semantics capable of being learned by neural models. Specifically, we explore the use of semantic relatedness features based on word associations, rather than the more common Word2Vec similarity, on a binary humour identification task and identify several factors that make word associations a better fit for humour. We also explore the effects of using joke structure, in the form of humour anchors (Yang et al., 2015), for improving the performance of semantic features and show that, while an intriguing idea, humour anchors contain several pitfalls that can hurt performance.",
  "stance": -0.3
 },
 {
  "url": "https://aclanthology.org/C18-1192",
  "title": "Exploratory Neural Relation Classification for Domain Knowledge Acquisition",
  "year": 2018,
  "venue": "COLING",
  "abstract": "The state-of-the-art methods for relation classification are primarily based on deep neural net- works. This kind of supervised learning method suffers from not only limited training data, but also the large number of low-frequency relations in specific domains. In this paper, we propose the task of exploratory relation classification for domain knowledge harvesting. The goal is to learn a classifier on pre-defined relations and discover new relations expressed in texts. A dynamically structured neural network is introduced to classify entity pairs to a continuously expanded relation set. We further propose the similarity sensitive Chinese restaurant process to discover new relations. Experiments conducted on a large corpus show the effectiveness of our neural network, while new relations are discovered with high precision and recall.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/C18-1217",
  "title": "Urdu Word Segmentation using Conditional Random Fields (CRFs)",
  "year": 2018,
  "venue": "COLING",
  "abstract": "State-of-the-art Natural Language Processing algorithms rely heavily on efficient word segmentation. Urdu is amongst languages for which word segmentation is a complex task as it exhibits space omission as well as space insertion issues. This is partly due to the Arabic script which although cursive in nature, consists of characters that have inherent joining and non-joining attributes regardless of word boundary. This paper presents a word segmentation system for Urdu which uses a Conditional Random Field sequence modeler with orthographic, linguistic and morphological features. Our proposed model automatically learns to predict white space as word boundary as well as Zero Width Non-Joiner (ZWNJ) as sub-word boundary. Using a manually annotated corpus, our model achieves F1 score of 0.97 for word boundary identification and 0.85 for sub-word boundary identification tasks. We have made our code and corpus publicly available to make our results reproducible.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.coling-main.117",
  "title": "What Can We Learn from Noun Substitutions in Revision Histories?",
  "year": 2020,
  "venue": "COLING",
  "abstract": "In community-edited resources such as wikiHow, sentences are subject to revisions on a daily basis. Recent work has shown that resulting improvements over time can be modelled computationally, assuming that each revision contributes to the improvement. We take a closer look at a subset of such revisions, for which we attempt to improve a computational model and validate in how far the assumption that 'revised means better' actually holds. The subset of revisions considered here are noun substitutions, which often involve interesting semantic relations, including synonymy, antonymy and hypernymy. Despite the high semantic relatedness, we find that a supervised classifier can distinguish the revised version of a sentence from an original version with an accuracy close to 70%, when taking context into account. In a human annotation study, we observe that annotators identify the revised sentence as the 'better version' with similar performance. Our analysis reveals a fair agreement among annotators when a revision improves fluency. In contrast, noun substitutions that involve other lexical-semantic relationships are often perceived as being equally good or tend to cause disagreements. While these findings are also reflected in classification scores, a comparison of results shows that our model fails in cases where humans can resort to factual knowledge or intuitions about the required level of specificity.",
  "stance": 0.1
 },
 {
  "url": "https://aclanthology.org/2020.coling-main.501",
  "title": "Metrics also Disagree in the Low Scoring Range: Revisiting Summarization Evaluation Metrics",
  "year": 2020,
  "venue": "COLING",
  "abstract": "In text summarization, evaluating the efficacy of automatic metrics without human judgments has become recently popular. One exemplar work (Peyrard, 2019) concludes that automatic metrics strongly disagree when ranking high-scoring summaries. In this paper, we revisit their experiments and find that their observations stem from the fact that metrics disagree in ranking summaries from any narrow scoring range. We hypothesize that this may be because summaries are similar to each other in a narrow scoring range and are thus, difficult to rank. Apart from the width of the scoring range of summaries, we analyze three other properties that impact inter-metric agreement - Ease of Summarization, Abstractiveness, and Coverage.",
  "stance": -0.5
 },
 {
  "url": "https://aclanthology.org/2020.semeval-1.175",
  "title": "NITS-Hinglish-SentiMix at SemEval-2020 Task 9: Sentiment Analysis for Code-Mixed Social Media Text Using an Ensemble Model",
  "year": 2020,
  "venue": "COLING",
  "abstract": "Sentiment Analysis refers to the process of interpreting what a sentence emotes and classifying them as positive, negative, or neutral. The widespread popularity of social media has led to the generation of a lot of text data and specifically, in the Indian social media scenario, the code-mixed Hinglish text i.e, the words of Hindi language, written in the Roman script along with other English words is a common sight. The ability to effectively understand the sentiments in these texts is much needed. This paper proposes a system titled NITS-Hinglish to effectively carry out the sentiment analysis of such code-mixed Hinglish text. The system has fared well with a final F-Score of 0.617 on the test data.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.semeval-1.198",
  "title": "AdelaideCyC at SemEval-2020 Task 12: Ensemble of Classifiers for Offensive Language Detection in Social Media",
  "year": 2020,
  "venue": "COLING",
  "abstract": "This paper describes the systems our team (AdelaideCyC) has developed for SemEval Task 12 (OffensEval 2020) to detect offensive language in social media. The challenge focuses on three subtasks - offensive language identification (subtask A), offense type identification (subtask B), and offense target identification (subtask C). Our team has participated in all the three subtasks. We have developed machine learning and deep learning-based ensembles of models. We have achieved F1-scores of 0.906, 0.552, and 0.623 in subtask A, B, and C respectively. While our performance scores are promising for subtask A, the results demonstrate that subtask B and C still remain challenging to classify.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/2020.semeval-1.218",
  "title": "LAST at SemEval-2020 Task 10: Finding Tokens to Emphasise in Short Written Texts with Precomputed Embedding Models and LightGBM",
  "year": 2020,
  "venue": "COLING",
  "abstract": "To select tokens to be emphasised in short texts, a system mainly based on precomputed embedding models, such as BERT and ELMo, and LightGBM is proposed. Its performance is low. Additional analyzes suggest that its effectiveness is poor at predicting the highest emphasis scores while they are the most important for the challenge and that it is very sensitive to the specific instances provided during learning.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/2020.semeval-1.266",
  "title": "JCT at SemEval-2020 Task 12: Offensive Language Detection in Tweets Using Preprocessing Methods, Character and Word N-grams",
  "year": 2020,
  "venue": "COLING",
  "abstract": "In this paper, we describe our submissions to SemEval-2020 contest. We tackled subtask 12 - \"Multilingual Offensive Language Identification in Social Media\". We developed different models for four languages: Arabic, Danish, Greek, and Turkish. We applied three supervised machine learning methods using various combinations of character and word n-gram features. In addition, we applied various combinations of basic preprocessing methods. Our best submission was a model we built for offensive language identification in Danish using Random Forest. This model was ranked at the 6 position out of 39 submissions. Our result is lower by only 0.0025 than the result of the team that won the 4 place using entirely non-neural methods. Our experiments indicate that char ngram features are more helpful than word ngram features. This phenomenon probably occurs because tweets are more characterized by characters than by words, tweets are short, and contain various special sequences of characters, e.g., hashtags, shortcuts, slang words, and typos.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/W00-0716",
  "title": "Generating Synthetic Speech Prosody with Lazy Learning in Tree Structures",
  "year": 2000,
  "venue": "CoNLL",
  "abstract": "We present ongoing work on prosody prediction for speech synthesis. This approach considers sentences as tree structures and infers the prosody from a corpus of such structures using machine learning techniques. The prediction is achieved from the prosody of the closest sentence of the corpus through tree similarity measurements, using either the nearest neighbour algorithm or an analogy-based approach. We introduce two different tree structure representations, the tree similarity metrics considered, and then we discuss the different prediction methods. Experiments are currently under process to qualify this approach.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/W00-0729",
  "title": "Chunking with Maximum Entropy Models",
  "year": 2000,
  "venue": "CoNLL",
  "abstract": "Maximum Entropy (MaxEnt) models (Jaynes, 1957) are exponential models that implement the intuition that if there is no evidence to favour one alternative solution above another, both alternatives should be equally likely. In order to accomplish this, as much information as possible about the process you want to model must be collected. This information consists of frequencies of events relevant to the process. The frequencies of relevant events are considered to be properties of the process. When building a model we have to constrain our attention to models with these properties. In most cases the process is only partially described. The MaxEnt framework now demands that from all the models that satisfy these constraints, we choose the model with the flattest probability distribution. This is the model with the highest entropy (given the fact that the constraints are met). When we are looking for a conditional model P(w]h), the MaxEnt solution has the form:",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/W00-0730",
  "title": "Use of Support Vector Learning for Chunk Identification",
  "year": 2000,
  "venue": "CoNLL",
  "abstract": "Support Vector Machines (SVMs), first introduced by Vapnik (Cortes and Vapnik, 1995; Vapnik, 1995), are relatively new learning approaches for solving two-class pat tern recognition problems. SVMs are well-known for their good generalization performance, and have been applied to many pat tern recognition problems. In the field of natural language processing, SVMs are applied to text categorization, and are reported to have achieved high accuracy without falling into over-fitting even with a large number of words taken as the features (Joachims, 1998; Taira and Haruno, 1999) First of all, let us define the training data which belongs to either positive or negative class as follows:",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/W00-0735",
  "title": "Single-Classifier Memory-Based Phrase Chunking",
  "year": 2000,
  "venue": "CoNLL",
  "abstract": "In the shared task for CoNLL-2000, words and tags form the basic multi-valued features for predicting a rich phrase segmentation code. While the tag features, containing WSJ paxt-ofspeech tags (Marcus et al., 1993), have about 45 values, the word features have more than 10,000 values. In our study we have looked at how memory-based learning, as implemented in the TiMBL software system (Daelemans et al., 2000), can handle such features. We have limited our search to single classifiers, thereby explicitly ignoring the possibility to build a metalearning classifier architecture that could be expected to improve accuracy. Given this restriction we have explored the following:",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/W01-0712",
  "title": "Learning Computational Grammars",
  "year": 2001,
  "venue": "CoNLL",
  "abstract": "This paper reports on the LEARNING COMPUTATIONAL GRAMMARS (LCG) project, a postdoc network devoted to studying the application of machine learning techniques to grammars suitable for computational use. We were interested in a more systematic survey to understand the relevance of many factors to the success of learning, esp. the availability of annotated data, the kind of dependencies in the data, and the availability of knowledge bases (grammars). We focused on syntax, esp. noun phrase (NP) syntax.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/W05-0623",
  "title": "A Joint Model for Semantic Role Labeling",
  "year": 2005,
  "venue": "CoNLL",
  "abstract": "We present a semantic role labeling system submitted to the closed track of the CoNLL-2005 shared task. The system, introduced in (Toutanova et al., 2005), implements a joint model that captures dependencies among arguments of a predicate using log-linear models in a discriminative re-ranking framework. We also describe experiments aimed at increasing the robustness of the system in the presence of syntactic parse errors. Our final system achieves F1-Measures of 76.68 and 78.45 on the development and the WSJ portion of the test set, respectively.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/W06-2901",
  "title": "A Mission for Computational Natural Language Learning",
  "year": 2006,
  "venue": "CoNLL",
  "abstract": "In this presentation, I will look back at 10 years of CoNLL conferences and the state of the art of machine learning of language that is evident from this decade of research. My conclusion, intended to provoke discussion, will be that we currently lack a clear motivation or “mission” to survive as a discipline. I will suggest that a new mission for the field could be found in a renewed interest for theoretical work (which learning algorithms have a bias that matches the properties of language?, what is the psycholinguistic relevance of learner design issues?), in more sophisticated comparative methodology, and in solving the problem of transfer, reusability, and adaptation of learned knowledge.",
  "stance": -0.3
 },
 {
  "url": "https://aclanthology.org/W06-2907",
  "title": "Investigating Lexical Substitution Scoring for Subtitle Generation",
  "year": 2006,
  "venue": "CoNLL",
  "abstract": "This paper investigates an isolated setting of the lexical substitution task of replacing words with their synonyms. In particular, we examine this problem in the setting of subtitle generation and evaluate state of the art scoring methods that predict the validity of a given substitution. The paper evaluates two context independent models and two contextual models. The major findings suggest that distributional similarity provides a useful complementary estimate for the likelihood that two Wordnet synonyms are indeed substitutable, while proper modeling of contextual constraints is still a challenging task for future research.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/W06-2923",
  "title": "LingPars, a Linguistically Inspired, Language-Independent Machine Learner for Dependency Treebanks",
  "year": 2006,
  "venue": "CoNLL",
  "abstract": "This paper presents a Constraint Grammarinspired machine learner and parser, Ling­ Pars, that assigns dependencies to morpho­ logically annotated treebanks in a functioncentred way. The system not only bases at­ tachment probabilities for PoS, case, mood, lemma on those features' function probabili­ ties, but also uses topological features like function/PoS n-grams, barrier tags and daughter-sequences. In the CoNLL shared task, performance was below average on at­ tachment scores, but a relatively higher score for function tags/deprels in isolation suggests that the system's strengths were not fully exploited in the current architecture.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D07-1008",
  "title": "Large Margin Synchronous Generation and its Application to Sentence Compression",
  "year": 2007,
  "venue": "CoNLL",
  "abstract": "This paper presents a tree-to-tree transduction method for text rewriting. Our model is based on synchronous tree substitution grammar, a formalism that allows local distortion of the tree topology and can thus naturally capture structural mismatches. We describe an algorithm for decoding in this framework and show how the model can be trained discriminatively within a large margin framework. Experimental results on sentence compression bring significant improvements over a state-of-the-art model.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D07-1011",
  "title": "Incremental Generation of Plural Descriptions: Similarity and Partitioning",
  "year": 2007,
  "venue": "CoNLL",
  "abstract": "Approaches to plural reference generation emphasise descriptive brevity, but often lack empirical backing. This paper describes a corpus-based study of plural descriptions, and proposes a psycholinguisticallymotivated algorithm for plural reference generation. The descriptive strategy is based on partitioning and incorporates corpusderived heuristics. An exhaustive evaluation shows that the output closely matches human data.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/D07-1030",
  "title": "Using RBMT Systems to Produce Bilingual Corpus for SMT",
  "year": 2007,
  "venue": "CoNLL",
  "abstract": "This paper proposes a method using the existing Rule-based Machine Translation (RBMT) system as a black box to produce synthetic bilingual corpus, which will be used as training data for the Statistical Machine Translation (SMT) system. We use the existing RBMT system to translate the monolingual corpus into synthetic bilingual corpus. With the synthetic bilingual corpus, we can build an SMT system even if there is no real bilingual corpus. In our experiments using BLEU as a metric, the system achieves a relative improvement of 11.7% over the best RBMT system that is used to produce the synthetic bilingual corpora. We also interpolate the model trained on a real bilingual corpus and the models trained on the synthetic bilingual corpora. The interpolated model achieves an absolute improvement of 0.0245 BLEU score (13.1% relative) as compared with the individual model trained on the real bilingual corpus.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D07-1091",
  "title": "Factored Translation Models",
  "year": 2007,
  "venue": "CoNLL",
  "abstract": "We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level — may it be linguistic markup or automatically generated word classes. In a number of experiments we show that factored translation models lead to better translation performance, both in terms of automatic scores, as well as more grammatical coherence.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D07-1102",
  "title": "Log-Linear Models of Non-Projective Trees, $k$-best MST Parsing and Tree-Ranking",
  "year": 2007,
  "venue": "CoNLL",
  "abstract": "We present our system used in the CoNLL 2007 shared task on multilingual parsing. The system is composed of three components: a k-best maximum spanning tree (MST) parser, a tree labeler, and a reranker that orders the k-best labeled trees. We present two techniques for training the MST parser: tree-normalized and graphnormalized conditional training. The treebased reranking model allows us to explicitly model global syntactic phenomena. We describe the reranker features which include non-projective edge attributes. We provide an analysis of the errors made by our system and suggest changes to the models and features that might rectify the current system.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D07-1112",
  "title": "Frustratingly Hard Domain Adaptation for Dependency Parsing",
  "year": 2007,
  "venue": "CoNLL",
  "abstract": "We describe some challenges of adaptation in the 2007 CoNLL Shared Task on Domain Adaptation. Our error analysis for this task suggests that a primary source of error is differences in annotation guidelines between treebanks. Our suspicions are supported by the observation that no team was able to improve target domain performance substantially over a state of the art baseline.",
  "stance": -0.1
 },
 {
  "url": "https://aclanthology.org/W08-2108",
  "title": "Fast Mapping in Word Learning: What Probabilities Tell Us",
  "year": 2008,
  "venue": "CoNLL",
  "abstract": "Children can determine the meaning of a new word from hearing it used in a familiar context—an ability often referred to as fast mapping. In this paper, we study fast mapping in the context of a general probabilistic model of word learning. We use our model to simulate fast mapping experiments on children, such as referent selection and retention. The word learning model can perform these tasks through an inductive interpretation of the acquired probabilities. Our results suggest that fast mapping occurs as a natural consequence of learning more words, and provides explanations for the (occasionally contradictory) child experimental data.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/W08-2125",
  "title": "Collective Semantic Role Labelling with Markov Logic",
  "year": 2008,
  "venue": "CoNLL",
  "abstract": "This paper presents our system for the Open Track of the CoNLL 2008 Shared Task (Surdeanu et al., 2008) in Joint Dependency Parsing1 and Semantic Role Labelling. We use Markov Logic to define a joint SRL model and achieve a semantic F-score of 74.59%, the second best in the Open Track.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/W08-2130",
  "title": "Discriminative Learning of Syntactic and Semantic Dependencies",
  "year": 2008,
  "venue": "CoNLL",
  "abstract": "A Maximum Entropy Model based system for discriminative learning of syntactic and semantic dependencies submitted to the CoNLL-2008 shared task (Surdeanu, et al., 2008) is presented in this paper. The system converts the dependency learning task to classification issues and reconstructs the dependent relations based on classification results. Finally F1 scores of 86.69, 69.95 and 78.35 are obtained for syntactic dependencies, semantic dependencies and the whole system respectively in closed challenge. For open challenge the corresponding F1 scores are 86.69, 68.99 and 77.84.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/W08-2140",
  "title": "Applying Sentence Simplification to the CoNLL-2008 Shared Task",
  "year": 2008,
  "venue": "CoNLL",
  "abstract": "Our submission to the CoNLL-2008 shared task (Surdeanu et al., 2008) focused on applying a novel method for semantic role labeling to the shared task. Our system first simplifies each sentence to be labeled using a set of hand-constructed rules; the weights of the system are trained on semantic role labeling data to generate simplifications which are as useful as possible for semantic role labeling. Our system is only a semantic role labeling system, and thus did not receive a score for Syntactic Dependencies (or, by extension, a score for the complete problem). Unlike most systems in the shared task, our system took constituency parses as input. On the subtask of semantic dependencies, our system obtained an F1 score of 76.17, the highest in the open task. In this paper we give a high-level overview of the sentence simplification system, and discuss and analyze the modifications to this system required for the CoNLL-2008 shared task.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/W09-1101",
  "title": "Joint Inference for Natural Language Processing",
  "year": 2009,
  "venue": "CoNLL",
  "abstract": "of the Invited Talk In recent decades, researchers in natural language processing have made great progress on welldefined subproblems such as part-of-speech tagging, phrase chunking, syntactic parsing, named-entity recognition, coreference and semantic-role labeling. Better models, features, and learning algorithms have allowed systems to perform many of these tasks with 90% accuracy or better. However, success in integrated, end-to-end natural language understanding",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/W09-1110",
  "title": "Interactive Feature Space Construction using Semantic Information",
  "year": 2009,
  "venue": "CoNLL",
  "abstract": "Specifying an appropriate feature space is an important aspect of achieving good performance when designing systems based upon learned classifiers. Effectively incorporating information regarding semantically related words into the feature space is known to produce robust, accurate classifiers and is one apparent motivation for efforts to automatically generate such resources. However, naive incorporation of this semantic information may result in poor performance due to increased ambiguity. To overcome this limitation, we introduce the interactive feature space construction protocol, where the learner identifies inadequate regions of the feature space and in coordination with a domain expert adds descriptiveness through existing semantic resources. We demonstrate effectiveness on an entity and relation extraction system including both performance improvements and robustness to reductions in annotated data.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/W09-1115",
  "title": "Investigating Automatic Alignment Methods for Slide Generation from Academic Papers",
  "year": 2009,
  "venue": "CoNLL",
  "abstract": "In this paper we investigate the task of automatic generation of slide presentations from academic papers, focusing initially on slide to paper alignment. We compare and evaluate four different alignment systems which utilize various combinations of methods used widely in other alignment and question answering approaches, such as TF-IDF term weighting and query expansion. Our best aligner achieves an accuracy of 75% and our findings show that for this application, average TF-IDF scoring performs more poorly than a simpler method based on the number of matched terms, and query expansion degrades aligner performance.",
  "stance": -0.4
 },
 {
  "url": "https://aclanthology.org/W09-1122",
  "title": "Lexical Patterns or Dependency Patterns: Which Is Better for Hypernym Extraction?",
  "year": 2009,
  "venue": "CoNLL",
  "abstract": "We compare two different types of extraction patterns for automatically deriving semantic information from text: lexical patterns, built from words and word class information, and dependency patterns with syntactic information obtained from a full parser. We are particularly interested in whether the richer linguistic information provided by a parser allows for a better performance of subsequent information extraction work. We evaluate automatic extraction of hypernym information from text and conclude that the application of dependency patterns does not lead to substantially higher precision and recall scores than using lexical patterns.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/W10-2909",
  "title": "Improved Unsupervised POS Induction Using Intrinsic Clustering Quality and a Zipfian Constraint",
  "year": 2010,
  "venue": "CoNLL",
  "abstract": "Modern unsupervised POS taggers usually apply an optimization procedure to a nonconvex function, and tend to converge to local maxima that are sensitive to starting conditions. The quality of the tagging induced by such algorithms is thus highly variable, and researchers report average results over several random initializations. Consequently, applications are not guaranteed to use an induced tagging of the quality reported for the algorithm. In this paper we address this issue using an unsupervised test for intrinsic clustering quality. We run a base tagger with different random initializations, and select the best tagging using the quality test. As a base tagger, we modify a leading unsupervised POS tagger (Clark, 2003) to constrain the distributions of word types across clusters to be Zipfian, allowing us to utilize a perplexity-based quality test. We show that the correlation between our quality test and gold standard-based tagging quality measures is high. Our results are better in most evaluation measures than all results reported in the literature for this task, and are always better than the Clark average results.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/W10-2927",
  "title": "Inspecting the Structural Biases of Dependency Parsing Algorithms",
  "year": 2010,
  "venue": "CoNLL",
  "abstract": "We propose the notion of a structural bias inherent in a parsing system with respect to the language it is aiming to parse. This structural bias characterizes the behaviour of a parsing system in terms of structures it tends to underand overproduce. We propose a Boosting-based method for uncovering some of the structural bias inherent in parsing systems. We then apply our method to four English dependency parsers (an Arc-Eager and Arc-Standard transition-based parsers, and firstand second-order graph-based parsers). We show that all four parsers are biased with respect to the kind of annotation they are trained to parse. We present a detailed analysis of the biases that highlights specific differences and commonalities between the parsing systems, and improves our understanding of their strengths and weaknesses.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/W11-0310",
  "title": "Gender Attribution: Tracing Stylometric Evidence Beyond Topic and Genre",
  "year": 2011,
  "venue": "CoNLL",
  "abstract": "Sociolinguistic theories (e.g., Lakoff (1973)) postulate that women’s language styles differ from that of men. In this paper, we explore statistical techniques that can learn to identify the gender of authors in modern English text, such as web blogs and scientific papers. Although recent work has shown the efficacy of statistical approaches to gender attribution, we conjecture that the reported performance might be overly optimistic due to non-stylistic factors such as topic bias in gender that can make the gender detection task easier. Our work is the first that consciously avoids gender bias in topics, thereby providing stronger evidence to gender-specific styles in language beyond topic. In addition, our comparative study provides new insights into robustness of various stylometric techniques across topic and genre.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D12-1043",
  "title": "An \"AI readability\" Formula for French as a Foreign Language",
  "year": 2012,
  "venue": "CoNLL",
  "abstract": "This paper present a new readability formula for French as a foreign language (FFL), which relies on 46 textual features representative of the lexical, syntactic, and semantic levels as well as some of the specificities of the FFL context. We report comparisons between several techniques for feature selection and various learning algorithms. Our best model, based on support vector machines (SVM), significantly outperforms previous FFL formulas. We also found that semantic features behave poorly in our case, in contrast with some previous readability studies on English as a first language.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D12-1116",
  "title": "Entity based Q&A Retrieval",
  "year": 2012,
  "venue": "CoNLL",
  "abstract": "Bridging the lexical gap between the user’s question and the question-answer pairs in the Q&A archives has been a major challenge for Q&A retrieval. State-of-the-art approaches address this issue by implicitly expanding the queries with additional words using statistical translation models. While useful, the effectiveness of these models is highly dependant on the availability of quality corpus in the absence of which they are troubled by noise issues. Moreover these models perform word based expansion in a context agnostic manner resulting in translation that might be mixed and fairly general. This results in degraded retrieval performance. In this work we address the above issues by extending the lexical word based translation model to incorporate semantic concepts (entities). We explore strategies to learn the translation probabilities between words and the concepts using the Q&A archives and a popular entity catalog. Experiments conducted on a large scale real data show that the proposed techniques are promising.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/W13-3502",
  "title": "Analysis of Stopping Active Learning based on Stabilizing Predictions",
  "year": 2013,
  "venue": "CoNLL",
  "abstract": "Within the natural language processing (NLP) community, active learning has been widely investigated and applied in order to alleviate the annotation bottleneck faced by developers of new NLP systems and technologies. This paper presents the first theoretical analysis of stopping active learning based on stabilizing predictions (SP). The analysis has revealed three elements that are central to the success of the SP method: (1) bounds on Cohen’s Kappa agreement between successively trained models impose bounds on differences in F-measure performance of the models; (2) since the stop set does not have to be labeled, it can be made large in practice, helping to guarantee that the results transfer to previously unseen streams of examples at test/application time; and (3) good (low variance) sample estimates of Kappa between successive models can be obtained. Proofs of relationships between the level of Kappa agreement and the difference in performance between consecutive models are presented. Specifically, if the Kappa agreement between two models exceeds a threshold T (where T > 0), then the difference in F-measure performance between those models is bounded above by 4(1−T ) T in all cases. If precision of the positive conjunction of the models is assumed to be p, then the bound can be tightened to 4(1−T ) (p+1)T .",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/W13-3511",
  "title": "Hidden Markov tree models for semantic class induction",
  "year": 2013,
  "venue": "CoNLL",
  "abstract": "In this paper, we propose a new method for semantic class induction. First, we introduce a generative model of sentences, based on dependency trees and which takes into account homonymy. Our model can thus be seen as a generalization of Brown clustering. Second, we describe an efficient algorithm to perform inference and learning in this model. Third, we apply our proposed method on two large datasets (108 tokens, 105 words types), and demonstrate that classes induced by our algorithm improve performance over Brown clustering on the task of semisupervised supersense tagging and named entity recognition.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/W14-1601",
  "title": "What's in a p-value in NLP?",
  "year": 2014,
  "venue": "CoNLL",
  "abstract": "In NLP, we need to document that our proposed methods perform significantly better with respect to standard metrics than previous approaches, typically by reporting p-values obtained by rankor randomization-based tests. We show that significance results following current research standards are unreliable and, in addition, very sensitive to sample size, covariates such as sentence length, as well as to the existence of multiple metrics. We estimate that under the assumption of perfect metrics and unbiased data, we need a significance cut-off at ⇠0.0025 to reduce the risk of false positive results to <5%. Since in practice we often have considerable selection bias and poor metrics, this, however, will not do alone.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/K15-1033",
  "title": "Do dependency parsing metrics correlate with human judgments?",
  "year": 2015,
  "venue": "CoNLL",
  "abstract": "Using automatic measures such as labeled and unlabeled attachment scores is common practice in dependency parser evaluation. In this paper, we examine whether these measures correlate with human judgments of overall parse quality. We ask linguists with experience in dependency annotation to judge system outputs. We measure the correlation between their judgments and a range of parse evaluation metrics across five languages. The humanmetric correlation is lower for dependency parsing than for other NLP tasks. Also, inter-annotator agreement is sometimes higher than the agreement between judgments and metrics, indicating that the standard metrics fail to capture certain aspects of parse quality, such as the relevance of root attachment or the relative importance of the different parts of speech.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/K15-1037",
  "title": "One Million Sense-Tagged Instances for Word Sense Disambiguation and Induction",
  "year": 2015,
  "venue": "CoNLL",
  "abstract": "Supervised word sense disambiguation (WSD) systems are usually the best performing systems when evaluated on standard benchmarks. However, these systems need annotated training data to function properly. While there are some publicly available open source WSD systems, very few large annotated datasets are available to the research community. The two main goals of this paper are to extract and annotate a large number of samples and release them for public use, and also to evaluate this dataset against some word sense disambiguation and induction tasks. We show that the open source IMS WSD system trained on our dataset achieves stateof-the-art results in standard disambiguation tasks and a recent word sense induction task, outperforming several task submissions and strong baselines.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/K16-1025",
  "title": "Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation",
  "year": 2016,
  "venue": "CoNLL",
  "abstract": "Named Entity Disambiguation (NED) refers to the task of resolving multiple named entity mentions in a document to their correct references in a knowledge base (KB) (e.g., Wikipedia). In this paper, we propose a novel embedding method specifically designed for NED. The proposed method jointly maps words and entities into the same continuous vector space. We extend the skip-gram model by using two models. The KB graph model learns the relatedness of entities using the link structure of the KB, whereas the anchor context model aims to align vectors such that similar words and entities occur close to one another in the vector space by leveraging KB anchors and their context words. By combining contexts based on the proposed embedding with standard NED features, we achieved state-of-theart accuracy of 93.1% on the standard CoNLL dataset and 85.2% on the TAC 2010 dataset.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/K17-1023",
  "title": "Robust Coreference Resolution and Entity Linking on Dialogues: Character Identification on TV Show Transcripts",
  "year": 2017,
  "venue": "CoNLL",
  "abstract": "This paper presents a novel approach to character identification, that is an entity linking task that maps mentions to characters in dialogues from TV show transcripts. We first augment and correct several cases of annotation errors in an existing corpus so the corpus is clearer and cleaner for statistical learning. We also introduce the agglomerative convolutional neural network that takes groups of features and learns mention and mention-pair embeddings for coreference resolution. We then propose another neural model that employs the embeddings learned and creates cluster embeddings for entity linking. Our coreference resolution model shows comparable results to other state-of-the-art systems. Our entity linking model significantly outperforms the previous work, showing the F1 score of 86.76% and the accuracy of 95.30% for character identification.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/K18-1038",
  "title": "Vectorial Semantic Spaces Do Not Encode Human Judgments of Intervention Similarity",
  "year": 2018,
  "venue": "CoNLL",
  "abstract": "Despite their practical success and impressive performances, neural-network-based and distributed semantics techniques have often been criticized as they remain fundamentally opaque and difficult to interpret. In a vein similar to recent pieces of work investigating the linguistic abilities of these representations, we study another core, defining property of language: the property of long-distance dependencies. Human languages exhibit the ability to interpret discontinuous elements distant from each other in the string as if they were adjacent. This ability is blocked if a similar, but extraneous, element intervenes between the discontinuous components. We present results that show, under exhaustive and precise conditions, that one kind of word embeddings and the similarity spaces they define do not encode the properties of intervention similarity in long-distance dependencies, and that therefore they fail to represent this core linguistic notion.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/K19-1062",
  "title": "Deep Structured Neural Network for Event Temporal Relation Extraction",
  "year": 2019,
  "venue": "CoNLL",
  "abstract": "We propose a novel deep structured learning framework for event temporal relation extraction. The model consists of 1) a recurrent neural network (RNN) to learn scoring functions for pair-wise relations, and 2) a structured support vector machine (SSVM) to make joint predictions. The neural network automatically learns representations that account for long-term contexts to provide robust features for the structured model, while the SSVM incorporates domain knowledge such as transitive closure of temporal relations as constraints to make better globally consistent decisions. By jointly training the two components, our model combines the benefits of both data-driven learning and knowledge exploitation. Experimental results on three high-quality event temporal relation datasets (TCR, MATRES, and TB-Dense) demonstrate that incorporated with pre-trained contextualized embeddings, the proposed model achieves significantly better performances than the state-of-the-art methods on all three datasets. We also provide thorough ablation studies to investigate our model.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.conll-1.18",
  "title": "Processing effort is a poor predictor of cross-linguistic word order frequency",
  "year": 2020,
  "venue": "CoNLL",
  "abstract": "Some have argued that word orders which are more difficult to process should be rarer cross-linguistically. Our current study fails to replicate the results of Maurits, Navarro, and Perfors (2010), who used an entropy-based Uniform Information Density (UID) measure to moderately predict the Greenbergian typology of transitive word orders. We additionally report an inability of three measures of processing difficulty - entropy-based UID, surprisal-based UID, and pointwise mutual information - to correctly predict the correct typological distribution, using transitive constructions from 20 languages in the Universal Dependencies project (version 2.5). However, our conclusions are limited by data sparsity.",
  "stance": -0.7
 },
 {
  "url": "https://aclanthology.org/2020.conll-1.26",
  "title": "\"LazImpa\": Lazy and Impatient neural agents learn to communicate efficiently",
  "year": 2020,
  "venue": "CoNLL",
  "abstract": "Previous work has shown that artificial neural agents naturally develop surprisingly non-efficient codes. This is illustrated by the fact that in a referential game involving a speaker and a listener neural networks optimizing accurate transmission over a discrete channel, the emergent messages fail to achieve an optimal length. Furthermore, frequent messages tend to be longer than infrequent ones, a pattern contrary to the Zipf Law of Abbreviation (ZLA) observed in all natural languages. Here, we show that near-optimal and ZLA-compatible messages can emerge, but only if both the speaker and the listener are modified. We hence introduce a new communication system, \"LazImpa\", where the speaker is made increasingly lazy, i.e., avoids long messages, and the listener impatient, i.e., seeks to guess the intended content as soon as possible.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.conll-1.37",
  "title": "On the Computational Power of Transformers and Its Implications in Sequence Modeling",
  "year": 2020,
  "venue": "CoNLL",
  "abstract": "Transformers are being used extensively across several sequence modeling tasks. Significant research effort has been devoted to experimentally probe the inner workings of Transformers. However, our conceptual and theoretical understanding of their power and inherent limitations is still nascent. In particular, the roles of various components in Transformers such as positional encodings, attention heads, residual connections, and feedforward networks, are not clear. In this paper, we take a step towards answering these questions. We analyze the computational power as captured by Turing-completeness. We first provide an alternate and simpler proof to show that vanilla Transformers are Turing-complete and then we prove that Transformers with only positional masking and without any positional encoding are also Turing-complete. We further analyze the necessity of each component for the Turing-completeness of the network; interestingly, we find that a particular type of residual connection is necessary. We demonstrate the practical implications of our results via experiments on machine translation and synthetic tasks.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/2020.conll-1.39",
  "title": "Filler-gaps that neural networks fail to generalize",
  "year": 2020,
  "venue": "CoNLL",
  "abstract": "It can be difficult to separate abstract linguistic knowledge in recurrent neural networks (RNNs) from surface heuristics. In this work, we probe for highly abstract syntactic constraints that have been claimed to govern the behavior of filler-gap dependencies across different surface constructions. For models to generalize abstract patterns in expected ways to unseen data, they must share representational features in predictable ways. We use cumulative priming to test for representational overlap between disparate filler-gap constructions in English and find evidence that the models learn a general representation for the existence of filler-gap dependencies. However, we find no evidence that the models learn any of the shared underlying grammatical constraints we tested. Our work raises questions about the degree to which RNN language models learn abstract linguistic representations.",
  "stance": -0.9
 },
 {
  "url": "https://aclanthology.org/W01-0518",
  "title": "Probabilistic Context-Free Grammars for Syllabification and Grapheme-to-Phoneme Conversion",
  "year": 2001,
  "venue": "EMNLP",
  "abstract": "We investigated the applicability of probabilistic context-free grammars to syllabi cation and grapheme-to-phoneme conversion. The results show that the standard probability model of context-free grammars performs very well in predicting syllable boundaries. However, our results indicate that the standard probability model does not solve grapheme-to-phoneme conversion su ciently although, we varied all free parameters of the probabilistic reestimation procedure.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/W02-1025",
  "title": "A Method for Open-Vocabulary Speech-Driven Text Retrieval",
  "year": 2002,
  "venue": "EMNLP",
  "abstract": "While recent retrieval techniques do not limit the number of",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/W04-3221",
  "title": "Attribute-Based and Value-Based Clustering: An Evaluation",
  "year": 2004,
  "venue": "EMNLP",
  "abstract": "In most research on concept acquisition from corpora, concepts are modeled as vectors of relations extracted from syntactic structures. In the case of modifiers, these relations often specify values of attributes, as in (attr red); this is unlike what typically proposed in theories of knowledge representation, where concepts are typically defined in terms of their attributes (e.g., color). We compared models of concepts based on values with models based on attributes, using lexical clustering as the basis for comparison. We find that attribute-based models work better than value-based ones, and result in shorter descriptions; but that mixed models including both the best attributes and the best values work best of all.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/H05-1038",
  "title": "Using Question Series to Evaluate Question Answering System Effectiveness",
  "year": 2005,
  "venue": "EMNLP",
  "abstract": "The original motivation for using question series in the TREC 2004 question answering track was the desire to model aspects of dialogue processing in an evaluation task that included different question types. The structure introduced by the series also proved to have an important additional benefit: the series is at an appropriate level of granularity for aggregating scores for an effective evaluation. The series is small enough to be meaningful at the task level since it represents a single user interaction, yet it is large enough to avoid the highly skewed score distributions exhibited by single questions. An analysis of the reliability of the per-series evaluation shows the evaluation is stable for differences in scores seen in the track.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/H05-1051",
  "title": "Differentiating Homonymy and Polysemy in Information Retrieval",
  "year": 2005,
  "venue": "EMNLP",
  "abstract": "Recent studies into Web retrieval have shown that word sense disambiguation can increase retrieval effectiveness. However, it remains unclear as to the minimum disambiguation accuracy required and the granularity with which one must define word sense in order to maximize these benefits. This study answers these questions using a simulation of the effects of ambiguity on information retrieval. It goes beyond previous studies by differentiating between homonymy and polysemy. Results show that retrieval is more sensitive to polysemy than homonymy and that, when resolving polysemy, accuracy as low as 55% can potentially lead to increased performance.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/D08-1002",
  "title": "It's a Contradiction - no, it's not: A Case Study using Functional Relations",
  "year": 2008,
  "venue": "EMNLP",
  "abstract": "Contradiction Detection (CD) in text is a difficult NLP task. We investigate CD over functions (e.g., BornIn(Person)=Place), and present a domain-independent algorithm that automatically discovers phrases denoting functions with high precision. Previous work on CD has investigated hand-chosen sentence pairs. In contrast, we automatically harvested from the Web pairs of sentences that appear contradictory, but were surprised to find that most pairs are in fact consistent. For example, “Mozart was born in Salzburg” does not contradict “Mozart was born in Austria” despite the functional nature of the phrase “was born in”. We show that background knowledge about meronyms (e.g., Salzburg is in Austria), synonyms, functions, and more is essential for success in the CD task.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D08-1043",
  "title": "Bridging Lexical Gaps between Queries and Questions on Large Online Q&A Collections with Compact Translation Models",
  "year": 2008,
  "venue": "EMNLP",
  "abstract": "Lexical gaps between queries and questions (documents) have been a major issue in question retrieval on large online question and answer (Q&A) collections. Previous studies address the issue by implicitly expanding queries with the help of translation models pre-constructed using statistical techniques. However, since it is possible for unimportant words (e.g., non-topical words, common words) to be included in the translation models, a lack of noise control on the models can cause degradation of retrieval performance. This paper investigates a number of empirical methods for eliminating unimportant words in order to construct compact translation models for retrieval purposes. Experiments conducted on a real world Q&A collection show that substantial improvements in retrieval performance can be achieved by using compact translation models.",
  "stance": 0.4
 },
 {
  "url": "https://aclanthology.org/D08-1048",
  "title": "Automatic induction of FrameNet lexical units",
  "year": 2008,
  "venue": "EMNLP",
  "abstract": "Most attempts to integrate FrameNet in NLP systems have so far failed because of its limited coverage. In this paper, we investigate the applicability of distributional and WordNetbased models on the task of lexical unit induction, i.e. the expansion of FrameNet with new lexical units. Experimental results show that our distributional and WordNet-based models achieve good level of accuracy and coverage, especially when combined.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/D08-1050",
  "title": "Adapting a Lexicalized-Grammar Parser to Contrasting Domains",
  "year": 2008,
  "venue": "EMNLP",
  "abstract": "Most state-of-the-art wide-coverage parsers are trained on newspaper text and suffer a loss of accuracy in other domains, making parser adaptation a pressing issue. In this paper we demonstrate that a CCG parser can be adapted to two new domains, biomedical text and questions for a QA system, by using manually-annotated training data at the POS and lexical category levels only. This approach achieves parser accuracy comparable to that on newspaper data without the need for annotated parse trees in the new domain. We find that retraining at the lexical category level yields a larger performance increase for questions than for biomedical text and analyze the two datasets to investigate why different domains might behave differently for parser adaptation.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/D08-1052",
  "title": "LTAG Dependency Parsing with Bidirectional Incremental Construction",
  "year": 2008,
  "venue": "EMNLP",
  "abstract": "In this paper, we first introduce a new architecture for parsing, bidirectional incremental parsing. We propose a novel algorithm for incremental construction, which can be applied to many structure learning problems in NLP. We apply this algorithm to LTAG dependency parsing, and achieve significant improvement on accuracy over the previous best result on the same data set.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D08-1064",
  "title": "Decomposability of Translation Metrics for Improved Evaluation and Efficient Algorithms",
  "year": 2008,
  "venue": "EMNLP",
  "abstract": "BLEU is the de facto standard for evaluation and development of statistical machine translation systems. We describe three real-world situations involving comparisons between different versions of the same systems where one can obtain improvements in BLEU scores that are questionable or even absurd. These situations arise because BLEU lacks the property of decomposability, a property which is also computationally convenient for various applications. We propose a very conservative modification to BLEU and a cross between BLEU and word error rate that address these issues while improving correlation with human judgments.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/D08-1082",
  "title": "A Generative Model for Parsing Natural Language to Meaning Representations",
  "year": 2008,
  "venue": "EMNLP",
  "abstract": "In this paper, we present an algorithm for learning a generative model of natural language sentences together with their formal meaning representations with hierarchical structures. The model is applied to the task of mapping sentences to hierarchical representations of their underlying meaning. We introduce dynamic programming techniques for efficient training and decoding. In experiments, we demonstrate that the model, when coupled with a discriminative reranking technique, achieves state-of-the-art performance when tested on two publicly available corpora. The generative model degrades robustly when presented with instances that are different from those seen in training. This allows a notable improvement in recall compared to previous models.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D08-1087",
  "title": "N-gram Weighting: Reducing Training Data Mismatch in Cross-Domain Language Model Estimation",
  "year": 2008,
  "venue": "EMNLP",
  "abstract": "In domains with insufficient matched training data, language models are often constructed by interpolating component models trained from partially matched corpora. Since the ngrams from such corpora may not be of equal relevance to the target domain, we propose an n-gram weighting technique to adjust the component n-gram probabilities based on features derived from readily available segmentation and metadata information for each corpus. Using a log-linear combination of such features, the resulting model achieves up to a 1.2% absolute word error rate reduction over a linearly interpolated baseline language model on a lecture transcription task.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D08-1089",
  "title": "A Simple and Effective Hierarchical Phrase Reordering Model",
  "year": 2008,
  "venue": "EMNLP",
  "abstract": "While phrase-based statistical machine translation systems currently deliver state-of-theart performance, they remain weak on word order changes. Current phrase reordering models can properly handle swaps between adjacent phrases, but they typically lack the ability to perform the kind of long-distance reorderings possible with syntax-based systems. In this paper, we present a novel hierarchical phrase reordering model aimed at improving non-local reorderings, which seamlessly integrates with a standard phrase-based system with little loss of computational efficiency. We show that this model can successfully handle the key examples often used to motivate syntax-based systems, such as the rotation of a prepositional phrase around a noun phrase. We contrast our model with reordering models commonly used in phrase-based systems, and show that our approach provides statistically significant BLEU point gains for two language pairs: Chinese-English (+0.53 on MT05 and +0.71 on MT08) and Arabic-English (+0.55 on MT05).",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/D08-1105",
  "title": "Word Sense Disambiguation Using OntoNotes: An Empirical Study",
  "year": 2008,
  "venue": "EMNLP",
  "abstract": "The accuracy of current word sense disambiguation (WSD) systems is affected by the fine-grained sense inventory of WordNet as well as a lack of training examples. Using the WSD examples provided through OntoNotes, we conduct the first large-scale WSD evaluation involving hundreds of word types and tens of thousands of sense-tagged examples, while adopting a coarse-grained sense inventory. We show that though WSD systems trained with a large number of examples can obtain a high level of accuracy, they nevertheless suffer a substantial drop in accuracy when applied to a different domain. To address this issue, we propose combining a domain adaptation technique using feature augmentation with active learning. Our results show that this approach is effective in reducing the annotation effort required to adapt a WSD system to a new domain. Finally, we propose that one can maximize the dual benefits of reducing the annotation effort while ensuring an increase in WSD accuracy, by only performing active learning on the set of most frequently occurring word types.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/D09-1006",
  "title": "Feasibility of Human-in-the-loop Minimum Error Rate Training",
  "year": 2009,
  "venue": "EMNLP",
  "abstract": "Minimum error rate training (MERT) involves choosing parameter values for a machine translation (MT) system that maximize performance on a tuning set as measured by an automatic evaluation metric, such as BLEU. The method is best when the system will eventually be evaluated using the same metric, but in reality, most MT evaluations have a human-based component. Although performing MERT with a human-based metric seems like a daunting task, we describe a new metric, RYPT, which takes human judgments into account, but only requires human input to build a database that can be reused over and over again, hence eliminating the need for human input at tuning time. In this investigative study, we analyze the diversity (or lack thereof) of the candidates produced during MERT, we describe how this redundancy can be used to our advantage, and show that RYPT is a better predictor of translation quality than BLEU.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D09-1008",
  "title": "Effective Use of Linguistic and Contextual Information for Statistical Machine Translation",
  "year": 2009,
  "venue": "EMNLP",
  "abstract": "Current methods of using lexical features in machine translation have difficulty in scaling up to realistic MT tasks due to a prohibitively large number of parameters involved. In this paper, we propose methods of using new linguistic and contextual features that do not suffer from this problem and apply them in a state-ofthe-art hierarchical MT system. The features used in this work are non-terminal labels, non-terminal length distribution, source string context and source dependency LM scores. The effectiveness of our techniques is demonstrated by significant improvements over a strong baseline. On Arabic-to-English translation, improvements in lower-cased BLEU are 2.0 on NIST MT06 and 1.7 on MT08 newswire data on decoding output. On Chinese-to-English translation, the improvements are 1.0 on MT06 and 0.8 on MT08 newswire data.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/D09-1035",
  "title": "It's Not You, it's Me: Detecting Flirting and its Misperception in Speed-Dates",
  "year": 2009,
  "venue": "EMNLP",
  "abstract": "Automatically detecting human social intentions from spoken conversation is an important task for dialogue understanding. Since the social intentions of the speaker may differ from what is perceived by the hearer, systems that analyze human conversations need to be able to extract both the perceived and the intended social meaning. We investigate this difference between intention and perception by using a spoken corpus of speed-dates in which both the speaker and the listener rated the speaker on flirtatiousness. Our flirtationdetection system uses prosodic, dialogue, and lexical features to detect a speaker’s intent to flirt with up to 71.5% accuracy, significantly outperforming the baseline, but also outperforming the human interlocuters. Our system addresses lexical feature sparsity given the small amount of training data by using an autoencoder network to map sparse lexical feature vectors into 30 compressed features. Our analysis shows that humans are very poor perceivers of intended flirtatiousness, instead often projecting their own intended behavior onto their interlocutors.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/D09-1041",
  "title": "A Comparison of Model Free versus Model Intensive Approaches to Sentence Compression",
  "year": 2009,
  "venue": "EMNLP",
  "abstract": "This work introduces a model free approach to sentence compression, which grew out of ideas from Nomoto (2008), and examines how it compares to a state-of-art model intensive approach known as Tree-to-Tree Transducer, or T3 (Cohn and Lapata, 2008). It is found that a model free approach significantly outperforms T3 on the particular data we created from the Internet. We also discuss what might have caused T3’s poor performance.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/D09-1055",
  "title": "Mining Search Engine Clickthrough Log for Matching N-gram Features",
  "year": 2009,
  "venue": "EMNLP",
  "abstract": "User clicks on a URL in response to a query are extremely useful predictors of the URL’s relevance to that query. Exact match click features tend to suffer from severe data sparsity issues in web ranking. Such sparsity is particularly pronounced for new URLs or long queries where each distinct query-url pair will rarely occur. To remedy this, we present a set of straightforward yet informative query-url n-gram features that allows for generalization of limited user click data to large amounts of unseen query-url pairs. The method is motivated by techniques leveraged in the NLP community for dealing with unseen words. We find that there are interesting regularities across queries and their preferred destination URLs; for example, queries containing “form” tend to lead to clicks on URLs containing “pdf”. We evaluate our set of new query-url features on a web search ranking task and obtain improvements that are statistically significant at a p-value < 0.0001 level over a strong baseline with exact match clickthrough features.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/D09-1065",
  "title": "EEG responds to conceptual stimuli and corpus semantics",
  "year": 2009,
  "venue": "EMNLP",
  "abstract": "Mitchell et al. (2008) demonstrated that corpus-extracted models of semantic knowledge can predict neural activation patterns recorded using fMRI. This could be a very powerful technique for evaluating conceptual models extracted from corpora; however, fMRI is expensive and imposes strong constraints on data collection. Following on experiments that demonstrated that EEG activation patterns encode enough information to discriminate broad conceptual categories, we show that corpus-based semantic representations can predict EEG activation patterns with significant accuracy, and we evaluate the relative performance of different corpus-models on this task.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/D09-1090",
  "title": "Bilingual dictionary generation for low-resourced language pairs",
  "year": 2009,
  "venue": "EMNLP",
  "abstract": "Bilingual dictionaries are vital resources in many areas of natural language processing. Numerous methods of machine translation require bilingual dictionaries with large coverage, but less-frequent language pairs rarely have any digitalized resources. Since the need for these resources is increasing, but the human resources are scarce for less represented languages, efficient automatized methods are needed. This paper introduces a fully automated, robust pivot language based bilingual dictionary generation method that uses the WordNet of the pivot language to build a new bilingual dictionary. We propose the usage of WordNet in order to increase accuracy; we also introduce a bidirectional selection method with a flexible threshold to maximize recall. Our evaluations showed 79% accuracy and 51% weighted recall, outperforming representative pivot language based methods. A dictionary generated with this method will still need manual post-editing, but the improved recall and precision decrease the work of human correctors.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/D09-1113",
  "title": "Empirical Exploitation of Click Data for Task Specific Ranking",
  "year": 2009,
  "venue": "EMNLP",
  "abstract": "There have been increasing needs for task specific rankings in web search such as rankings for specific query segments like long queries, time-sensitive queries, navigational queries, etc; or rankings for specific domains/contents like answers, blogs, news, etc. In the spirit of ”divide-andconquer”, task specific ranking may have potential advantages over generic ranking since different tasks have task-specific features, data distributions, as well as featuregrade correlations. A critical problem for the task-specific ranking is training data insufficiency, which may be solved by using the data extracted from click log. This paper empirically studies how to appropriately exploit click data to improve rank function learning in task-specific ranking. The main contributions are 1) the exploration on the utilities of two promising approaches for click pair extraction; 2) the analysis of the role played by the noise information which inevitably appears in click data extraction; 3) the appropriate strategy for combining training data and click data; 4) the comparison of click data which are consistent and inconsistent with baseline function.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/D09-1149",
  "title": "Semi-Supervised Learning for Semantic Relation Classification using Stratified Sampling Strategy",
  "year": 2009,
  "venue": "EMNLP",
  "abstract": "This paper presents a new approach to selecting the initial seed set using stratified sampling strategy in bootstrapping-based semi-supervised learning for semantic relation classification. First, the training data is partitioned into several strata according to relation types/subtypes, then relation instances are randomly sampled from each stratum to form the initial seed set. We also investigate different augmentation strategies in iteratively adding reliable instances to the labeled set, and find that the bootstrapping procedure may stop at a reasonable point to significantly decrease the training time without degrading too much in performance. Experiments on the ACE RDC 2003 and 2004 corpora show the stratified sampling strategy contributes more than the bootstrapping procedure itself. This suggests that a proper sampling strategy is critical in semi-supervised learning.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/D09-1159",
  "title": "Phrase Dependency Parsing for Opinion Mining",
  "year": 2009,
  "venue": "EMNLP",
  "abstract": "In this paper, we present a novel approach for mining opinions from product reviews, where it converts opinion mining task to identify product features, expressions of opinions and relations between them. By taking advantage of the observation that a lot of product features are phrases, a concept of phrase dependency parsing is introduced, which extends traditional dependency parsing to phrase level. This concept is then implemented for extracting relations between product features and expressions of opinions. Experimental evaluations show that the mining task can benefit from phrase dependency parsing.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D10-1022",
  "title": "Negative Training Data Can be Harmful to Text Classification",
  "year": 2010,
  "venue": "EMNLP",
  "abstract": "This paper studies the effects of training data on binary text classification and postulates that negative training data is not needed and may even be harmful for the task. Traditional binary classification involves building a classifier using labeled positive and negative training examples. The classifier is then applied to classify test instances into positive and negative classes. A fundamental assumption is that the training and test data are identically distributed. However, this assumption may not hold in practice. In this paper, we study a particular problem where the positive data is identically distributed but the negative data may or may not be so. Many practical text classification and retrieval applications fit this model. We argue that in this setting negative training data should not be used, and that PU learning can be employed to solve the problem. Empirical evaluation has been conducted to support our claim. This result is important as it may fundamentally change the current binary classification paradigm.",
  "stance": -0.1
 },
 {
  "url": "https://aclanthology.org/D10-1024",
  "title": "Evaluating Models of Latent Document Semantics in the Presence of OCR Errors",
  "year": 2010,
  "venue": "EMNLP",
  "abstract": "Models of latent document semantics such as the mixture of multinomials model and Latent Dirichlet Allocation have received substantial attention for their ability to discover topical semantics in large collections of text. In an effort to apply such models to noisy optical character recognition (OCR) text output, we endeavor to understand the effect that character-level noise can have on unsupervised topic modeling. We show the effects both with document-level topic analysis (document clustering) and with word-level topic analysis (LDA) on both synthetic and real-world OCR data. As expected, experimental results show that performance declines as word error rates increase. Common techniques for alleviating these problems, such as filtering low-frequency words, are successful in enhancing model quality, but exhibit failure trends similar to models trained on unprocessed OCR output in the case of LDA. To our knowledge, this study is the first of its kind.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/D10-1055",
  "title": "Further Meta-Evaluation of Broad-Coverage Surface Realization",
  "year": 2010,
  "venue": "EMNLP",
  "abstract": "We present the first evaluation of the utility of automatic evaluation metrics on surface realizations of Penn Treebank data. Using outputs of the OpenCCG and XLE realizers, along with ranked WordNet synonym substitutions, we collected a corpus of generated surface realizations. These outputs were then rated and post-edited by human annotators. We evaluated the realizations using seven automatic metrics, and analyzed correlations obtained between the human judgments and the automatic scores. In contrast to previous NLG meta-evaluations, we find that several of the metrics correlate moderately well with human judgments of both adequacy and fluency, with the TER family performing best overall. We also find that all of the metrics correctly predict more than half of the significant systemlevel differences, though none are correct in all cases. We conclude with a discussion of the implications for the utility of such metrics in evaluating generation in the presence of variation. A further result of our research is a corpus of post-edited realizations, which will be made available to the research community.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/D10-1080",
  "title": "Lessons Learned in Part-of-Speech Tagging of Conversational Speech",
  "year": 2010,
  "venue": "EMNLP",
  "abstract": "This paper examines tagging models for spontaneous English speech transcripts. We analyze the performance of state-of-the-art tagging models, either generative or discriminative, left-to-right or bidirectional, with or without latent annotations, together with the use of ToBI break indexes and several methods for segmenting the speech transcripts (i.e., conversation side, speaker turn, or humanannotated sentence). Based on these studies, we observe that: (1) bidirectional models tend to achieve better accuracy levels than left-toright models, (2) generative models seem to perform somewhat better than discriminative models on this task, and (3) prosody improves tagging performance of models on conversation sides, but has much less impact on smaller segments. We conclude that, although the use of break indexes can indeed significantly improve performance over baseline models without them on conversation sides, tagging accuracy improves more by using smaller segments, for which the impact of the break indexes is marginal.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/D10-1118",
  "title": "It Depends on the Translation: Unsupervised Dependency Parsing via Word Alignment",
  "year": 2010,
  "venue": "EMNLP",
  "abstract": "We reveal a previously unnoticed connection between dependency parsing and statistical machine translation (SMT), by formulating the dependency parsing task as a problem of word alignment. Furthermore, we show that two well known models for these respective tasks (DMV and the IBM models) share common modeling assumptions. This motivates us to develop an alignment-based framework for unsupervised dependency parsing. The framework (which will be made publicly available) is flexible, modular and easy to extend. Using this framework, we implement several algorithms based on the IBM alignment models, which prove surprisingly effective on the dependency parsing task, and demonstrate the potential of the alignment-based approach.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D11-1012",
  "title": "A Joint Model for Extended Semantic Role Labeling",
  "year": 2011,
  "venue": "EMNLP",
  "abstract": "This paper presents a model that extends semantic role labeling. Existing approaches independently analyze relations expressed by verb predicates or those expressed as nominalizations. However, sentences express relations via other linguistic phenomena as well. Furthermore, these phenomena interact with each other, thus restricting the structures they articulate. In this paper, we use this intuition to define a joint inference model that captures the inter-dependencies between verb semantic role labeling and relations expressed using prepositions. The scarcity of jointly labeled data presents a crucial technical challenge for learning a joint model. The key strength of our model is that we use existing structure predictors as black boxes. By enforcing consistency constraints between their predictions, we show improvements in the performance of both tasks without retraining the individual models.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D11-1044",
  "title": "Quasi-Synchronous Phrase Dependency Grammars for Machine Translation",
  "year": 2011,
  "venue": "EMNLP",
  "abstract": "We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009). This formulation allows us to combine structural components of phrase-based and syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D11-1124",
  "title": "Summarize What You Are Interested In: An Optimization Framework for Interactive Personalized Summarization",
  "year": 2011,
  "venue": "EMNLP",
  "abstract": "Most traditional summarization methods treat their outputs as static and plain texts, which fail to capture user interests during summarization because the generated summaries are the same for different users. However, users have individual preferences on a particular source document collection and obviously a universal summary for all users might not always be satisfactory. Hence we investigate an important and challenging problem in summary generation, i.e., Interactive Personalized Summarization (IPS), which generates summaries in an interactive and personalized manner. Given the source documents, IPS captures user interests by enabling interactive clicks and incorporates personalization by modeling captured reader preference. We develop experimental systems to compare 5 rival algorithms on 4 instinctively different datasets which amount to 5197 documents. Evaluation results in ROUGE metrics indicate the comparable performance between IPS and the best competing system but IPS produces summaries with much more user satisfaction according to evaluator ratings. Besides, low ROUGE consistency among these user preferred summaries indicates the existence of personalization.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/D11-1149",
  "title": "A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions",
  "year": 2011,
  "venue": "EMNLP",
  "abstract": "This paper describes a novel probabilistic approach for generating natural language sentences from their underlying semantics in the form of typed lambda calculus. The approach is built on top of a novel reduction-based weighted synchronous context free grammar formalism, which facilitates the transformation process from typed lambda calculus into natural language sentences. Sentences can then be generated based on such grammar rules with a log-linear model. To acquire such grammar rules automatically in an unsupervised manner, we also propose a novel approach with a generative model, which maps from sub-expressions of logical forms to word sequences in natural language sentences. Experiments on benchmark datasets for both English and Chinese generation tasks yield significant improvements over results obtained by two state-of-the-art machine translation models, in terms of both automatic metrics and human evaluation.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D13-1020",
  "title": "MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text",
  "year": 2013,
  "venue": "EMNLP",
  "abstract": "We present MCTest, a freely available set of stories and associated questions intended for research on the machine comprehension of text. Previous work on machine comprehension (e.g., semantic modeling) has made great strides, but primarily focuses either on limited-domain datasets, or on solving a more restricted goal (e.g., open-domain relation extraction). In contrast, MCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension. Reading comprehension can test advanced abilities such as causal reasoning and understanding the world, yet, by being multiple-choice, still provide a clear metric. By being fictional, the answer typically can be found only in the story itself. The stories and questions are also carefully limited to those a young child would understand, reducing the world knowledge that is required for the task. We present the scalable crowd-sourcing methods that allow us to cheaply construct a dataset of 500 stories and 2000 questions. By screening workers (with grammar tests) and stories (with grading), we have ensured that the data is the same quality as another set that we manually edited, but at one tenth the editing cost. By being open-domain, yet carefully restricted, we hope MCTest will serve to encourage research and provide a clear metric for advancement on the machine comprehension of text.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/D13-1072",
  "title": "Exploiting Language Models for Visual Recognition",
  "year": 2013,
  "venue": "EMNLP",
  "abstract": "The problem of learning language models from large text corpora has been widely studied within the computational linguistic community. However, little is known about the performance of these language models when applied to the computer vision domain. In this work, we compare representative models: a window-based model, a topic model, a distributional memory and a commonsense knowledge database, ConceptNet, in two visual recognition scenarios: human action recognition and object prediction. We examine whether the knowledge extracted from texts through these models are compatible to the knowledge represented in images. We determine the usefulness of different language models in aiding the two visual recognition tasks. The study shows that the language models built from general text corpora can be used instead of expensive annotated images and even outperform the image model when testing on a big general dataset.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/D13-1079",
  "title": "Rule-Based Information Extraction is Dead! Long Live Rule-Based Information Extraction Systems!",
  "year": 2013,
  "venue": "EMNLP",
  "abstract": "The rise of “Big Data” analytics over unstructured text has led to renewed interest in information extraction (IE). We surveyed the landscape of IE technologies and identified a major disconnect between industry and academia: while rule-based IE dominates the commercial world, it is widely regarded as dead-end technology by the academia. We believe the disconnect stems from the way in which the two communities measure the benefits and costs of IE, as well as academia’s perception that rulebased IE is devoid of research challenges. We make a case for the importance of rule-based IE to industry practitioners. We then lay out a research agenda in advancing the state-of-theart in rule-based IE systems which we believe has the potential to bridge the gap between academic research and industry practice.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/D13-1202",
  "title": "Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts",
  "year": 2013,
  "venue": "EMNLP",
  "abstract": "Traditional distributional semantic models extract word meaning representations from cooccurrence patterns of words in text corpora. Recently, the distributional approach has been extended to models that record the cooccurrence of words with visual features in image collections. These image-based models should be complementary to text-based ones, providing a more cognitively plausible view of meaning grounded in visual perception. In this study, we test whether image-based models capture the semantic patterns that emerge from fMRI recordings of the neural signal. Our results indicate that, indeed, there is a significant correlation between image-based and brain-based semantic similarities, and that image-based models complement text-based ones, so that the best correlations are achieved when the two modalities are combined. Despite some unsatisfactory, but explained outcomes (in particular, failure to detect differential association of models with brain areas), the results show, on the one hand, that imagebased distributional semantic models can be a precious new tool to explore semantic representation in the brain, and, on the other, that neural data can be used as the ultimate test set to validate artificial semantic models in terms of their cognitive plausibility.",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/D14-1005",
  "title": "Learning Image Embeddings using Convolutional Neural Networks for Improved Multi-Modal Semantics",
  "year": 2014,
  "venue": "EMNLP",
  "abstract": "We construct multi-modal concept representations by concatenating a skip-gram linguistic representation vector with a visual concept representation vector computed using the feature extraction layers of a deep convolutional neural network (CNN) trained on a large labeled object recognition dataset. This transfer learning approach brings a clear performance gain over features based on the traditional bag-of-visual-word approach. Experimental results are reported on the WordSim353 and MEN semantic relatedness evaluation tasks. We use visual features computed using either ImageNet or ESP Game images.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/D14-1020",
  "title": "Testing for Significance of Increased Correlation with Human Judgment",
  "year": 2014,
  "venue": "EMNLP",
  "abstract": "Automatic metrics are widely used in machine translation as a substitute for human assessment. With the introduction of any new metric comes the question of just how well that metric mimics human assessment of translation quality. This is often measured by correlation with human judgment. Significance tests are generally not used to establish whether improvements over existing methods such as BLEU are statistically significant or have occurred simply by chance, however. In this paper, we introduce a significance test for comparing correlations of two metrics, along with an open-source implementation of the test. When applied to a range of metrics across seven language pairs, tests show that for a high proportion of metrics, there is insufficient evidence to conclude significant improvement over BLEU.",
  "stance": -0.4
 },
 {
  "url": "https://aclanthology.org/D14-1215",
  "title": "Brighter than Gold: Figurative Language in User Generated Comparisons",
  "year": 2014,
  "venue": "EMNLP",
  "abstract": "Comparisons are common linguistic devices used to indicate the likeness of two things. Often, this likeness is not meant in the literal sense—for example, “I slept like a log” does not imply that logs actually sleep. In this paper we propose a computational study of figurative comparisons, or similes. Our starting point is a new large dataset of comparisons extracted from product reviews and annotated for figurativeness. We use this dataset to characterize figurative language in naturally occurring comparisons and reveal linguistic patterns indicative of this phenomenon. We operationalize these insights and apply them to a new task with high relevance to text understanding: distinguishing between figurative and literal comparisons. Finally, we apply this framework to explore the social context in which figurative language is produced, showing that similes are more likely to accompany opinions showing extreme sentiment, and that they are uncommon in reviews deemed helpful.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/D15-1013",
  "title": "Re-evaluating Automatic Summarization with BLEU and 192 Shades of ROUGE",
  "year": 2015,
  "venue": "EMNLP",
  "abstract": "We provide an analysis of current evaluation methodologies applied to summarization metrics and identify the following areas of concern: (1) movement away from evaluation by correlation with human assessment; (2) omission of important components of human assessment from evaluations, in addition to large numbers of metric variants; (3) absence of methods of significance testing improvements over a baseline. We outline an evaluation methodology that overcomes all such challenges, providing the first method of significance testing suitable for evaluation of summarization metrics. Our evaluation reveals for the first time which metric variants significantly outperform others, optimal metric variants distinct from current recommended best variants, as well as machine translation metric BLEU to have performance on-par with ROUGE for the purpose of evaluation of summarization systems. We subsequently replicate a recent large-scale evaluation that relied on, what we now know to be, suboptimal ROUGE variants revealing distinct conclusions about the relative performance of state-of-the-art summarization systems.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/D15-1038",
  "title": "Traversing Knowledge Graphs in Vector Space",
  "year": 2015,
  "venue": "EMNLP",
  "abstract": "Path queries on a knowledge graph can be used to answer compositional questions such as “What languages are spoken by people living in Lisbon?”. However, knowledge graphs often have missing facts (edges) which disrupts path queries. Recent models for knowledge base completion impute missing facts by embedding knowledge graphs in vector spaces. We show that these models can be recursively applied to answer path queries, but that they suffer from cascading errors. This motivates a new “compositional” training objective, which dramatically improves all models’ ability to answer path queries, in some cases more than doubling accuracy. On a standard knowledge base completion task, we also demonstrate that compositional training acts as a novel form of structural regularization, reliably improving performance across all base models (reducing errors by up to 43%) and achieving new state-of-the-art results.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D15-1052",
  "title": "Human Evaluation of Grammatical Error Correction Systems",
  "year": 2015,
  "venue": "EMNLP",
  "abstract": "The paper presents the results of the first large-scale human evaluation of automatic grammatical error correction (GEC) systems. Twelve participating systems and the unchanged input of the CoNLL-2014 shared task have been reassessed in a WMT-inspired human evaluation procedure. Methods introduced for the Workshop of Machine Translation evaluation campaigns have been adapted to GEC and extended where necessary. The produced rankings are used to evaluate standard metrics for grammatical error correction in terms of correlation with human judgment.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/D15-1071",
  "title": "Detecting Risks in the Banking System by Sentiment Analysis",
  "year": 2015,
  "venue": "EMNLP",
  "abstract": "In November 2014, the European Central Bank (ECB) started to directly supervise the largest banks in the Eurozone via the Single Supervisory Mechanism (SSM). While supervisory risk assessments are usually based on quantitative data and surveys, this work explores whether sentiment analysis is capable of measuring a bank’s attitude and opinions towards risk by analyzing text data. For realizing this study, a collection consisting of more than 500 CEO letters and outlook sections extracted from bank annual reports is built up. Based on these data, two distinct experiments are conducted. The evaluations find promising opportunities, but also limitations for risk sentiment analysis in banking supervision. At the level of individual banks, predictions are relatively inaccurate. In contrast, the analysis of aggregated figures revealed strong and significant correlations between uncertainty or negativity in textual disclosures and the quantitative risk indicator’s future evolution. Risk sentiment analysis should therefore rather be used for macroprudential analyses than for assessments of individual banks.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/D15-1148",
  "title": "Detecting Content-Heavy Sentences: A Cross-Language Case Study",
  "year": 2015,
  "venue": "EMNLP",
  "abstract": "The information conveyed by some sentences would be more easily understood by a reader if it were expressed in multiple sentences. We call such sentences content heavy: these are possibly grammatical but difficult to comprehend, cumbersome sentences. In this paper we introduce the task of detecting content-heavy sentences in cross-lingual context. Specifically we develop methods to identify sentences in Chinese for which English speakers would prefer translations consisting of more than one sentence. We base our analysis and definitions on evidence from multiple human translations and reader preferences on flow and understandability. We show that machine translation quality when translating content heavy sentences is markedly worse than overall quality and that this type of sentence are fairly common in Chinese news. We demonstrate that sentence length and punctuation usage in Chinese are not sufficient clues for accurately detecting heavy sentences and present a richer classification model that accurately identifies these sentences.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D15-1243",
  "title": "Evaluation of Word Vector Representations by Subspace Alignment",
  "year": 2015,
  "venue": "EMNLP",
  "abstract": "Unsupervisedly learned word vectors have proven to provide exceptionally effective features in many NLP tasks. Most common intrinsic evaluations of vector quality measure correlation with similarity judgments. However, these often correlate poorly with how well the learned representations perform as features in downstream evaluation tasks. We present QVEC—a computationally inexpensive intrinsic evaluation measure of the quality of word embeddings based on alignment to a matrix of features extracted from manually crafted lexical resources—that obtains strong correlation with performance of the vectors in a battery of downstream semantic evaluation tasks.1",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/D15-1285",
  "title": "Topic Identification and Discovery on Text and Speech",
  "year": 2015,
  "venue": "EMNLP",
  "abstract": "We compare the multinomial i-vector framework from the speech community with LDA, SAGE, and LSA as feature learners for topic ID on multinomial speech and text data. We also compare the learned representations in their ability to discover topics, quantified by distributional similarity to gold-standard topics and by human interpretability. We find that topic ID and topic discovery are competing objectives. We argue that LSA and i-vectors should be more widely considered by the text processing community as pre-processing steps for downstream tasks, and also speculate about speech processing tasks that could benefit from more interpretable representations like SAGE.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/D16-1010",
  "title": "Comparing Computational Cognitive Models of Generalization in a Language Acquisition Task",
  "year": 2016,
  "venue": "EMNLP",
  "abstract": "Natural language acquisition relies on appropriate generalization: the ability to produce novel sentences, while learning to restrict productions to acceptable forms in the language. Psycholinguists have proposed various properties that might play a role in guiding appropriate generalizations, looking at learning of verb alternations as a testbed. Several computational cognitive models have explored aspects of this phenomenon, but their results are hard to compare given the high variability in the linguistic properties represented in their input. In this paper, we directly compare two recent approaches, a Bayesian model and a connectionist model, in their ability to replicate human judgments of appropriate generalizations. We find that the Bayesian model more accurately mimics the judgments due to its richer learning mechanism that can exploit distributional properties of the input in a manner consistent with human behaviour.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/D16-1015",
  "title": "Improving Semantic Parsing via Answer Type Inference",
  "year": 2016,
  "venue": "EMNLP",
  "abstract": "In this work, we show the possibility of inferring the answer type before solving a factoid question and leveraging the type information to improve semantic parsing. By replacing the topic entity in a question with its type, we are able to generate an abstract form of the question, whose answer corresponds to the answer type of the original question. A bidirectional LSTM model is built to train over the abstract form of questions and infer their answer types. It is also observed that if we convert a question into a statement form, our LSTM model achieves better accuracy. Using the predicted type information to rerank the logical forms returned by AgendaIL, one of the leading semantic parsers, we are able to improve the F1-score from 49.7% to 52.6% on the WEBQUESTIONS data.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D16-1183",
  "title": "Neural Shift-Reduce CCG Semantic Parsing",
  "year": 2016,
  "venue": "EMNLP",
  "abstract": "We present a shift-reduce CCG semantic parser. Our parser uses a neural network architecture that balances model capacity and computational cost. We train by transferring a model from a computationally expensive loglinear CKY parser. Our learner addresses two challenges: selecting the best parse for learning when the CKY parser generates multiple correct trees, and learning from partial derivations when the CKY parser fails to parse. We evaluate on AMR parsing. Our parser performs comparably to the CKY parser, while doing significantly fewer operations. We also present results for greedy semantic parsing with a relatively small drop in performance.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D16-1217",
  "title": "Does 'well-being' translate on Twitter?",
  "year": 2016,
  "venue": "EMNLP",
  "abstract": "We investigate whether psychological wellbeing translates across English and Spanish Twitter, by building and comparing source language and automatically translated weighted lexica in English and Spanish. We find that the source language models perform substantially better than the machine translated versions. Moreover, manually correcting translation errors does not improve model performance, suggesting that meaningful cultural information is being lost in translation. Further work is needed to clarify when automatic translation of well-being lexica is effective and how it can be improved for crosscultural analysis.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/D16-1218",
  "title": "Beyond Canonical Texts: A Computational Analysis of Fanfiction",
  "year": 2016,
  "venue": "EMNLP",
  "abstract": "While much computational work on fiction has focused on works in the literary canon, user-created fanfiction presents a unique opportunity to study an ecosystem of literary production and consumption, embodying qualities both of large-scale literary data (55 billion tokens) and also a social network (with over 2 million users). We present several empirical analyses of this data in order to illustrate the range of affordances it presents to research in NLP, computational social science and the digital humanities. We find that fanfiction deprioritizes main protagonists in comparison to canonical texts, has a statistically significant difference in attention allocated to female characters, and offers a framework for developing models of reader reactions to stories.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/D17-1123",
  "title": "A Short Survey on Taxonomy Learning from Text Corpora: Issues, Resources and Recent Advances",
  "year": 2017,
  "venue": "EMNLP",
  "abstract": "A taxonomy is a semantic hierarchy, consisting of concepts linked by is-a relations. While a large number of taxonomies have been constructed from human-compiled resources (e.g., Wikipedia), learning taxonomies from text corpora has received a growing interest and is essential for long-tailed and domain-specific knowledge acquisition. In this paper, we overview recent advances on taxonomy construction from free texts, reorganizing relevant subtasks into a complete framework. We also overview resources for evaluation and discuss challenges for future research.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/D17-1167",
  "title": "A Question Answering Approach for Emotion Cause Extraction",
  "year": 2017,
  "venue": "EMNLP",
  "abstract": "Emotion cause extraction aims to identify the reasons behind a certain emotion expressed in text. It is a much more difficult task compared to emotion classification. Inspired by recent advances in using deep memory networks for question answering (QA), we propose a new approach which considers emotion cause identification as a reading comprehension task in QA. Inspired by convolutional neural networks, we propose a new mechanism to store relevant context in different memory slots to model context information. Our proposed approach can extract both word level sequence features and lexical features. Performance evaluation shows that our method achieves the state-of-the-art performance on a recently released emotion cause dataset, outperforming a number of competitive baselines by at least 3.01% in F-measure.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D17-1215",
  "title": "Adversarial Examples for Evaluating Reading Comprehension Systems",
  "year": 2017,
  "venue": "EMNLP",
  "abstract": "Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of 75% F1 score to 36%; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7%. We hope our insights will motivate the development of new models that understand language more precisely.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/D17-1239",
  "title": "Challenges in Data-to-Document Generation",
  "year": 2017,
  "venue": "EMNLP",
  "abstract": "Recent neural models have shown significant progress on the problem of generating short descriptive texts conditioned on a small number of database records. In this work, we suggest a slightly more difficult data-to-text generation task, and investigate how effective current approaches are on this task. In particular, we introduce a new, large-scale corpus of data records paired with descriptive documents, propose a series of extractive evaluation methods for analyzing performance, and obtain baseline results using current neural generation methods. Experiments show that these models produce fluent text, but fail to convincingly approximate human-generated documents. Moreover, even templated baselines exceed the performance of these neural models on some metrics, though copy- and reconstruction-based extensions lead to noticeable improvements.",
  "stance": -0.9
 },
 {
  "url": "https://aclanthology.org/D18-1029",
  "title": "On the Relation between Linguistic Typology and (Limitations of) Multilingual Language Modeling",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "A key challenge in cross-lingual NLP is developing general language-independent architectures that are equally applicable to any language. However, this ambition is largely hampered by the variation in structural and semantic properties, i.e. the typological profiles of the world's languages. In this work, we analyse the implications of this variation on the language modeling (LM) task. We present a large-scale study of state-of-the art n-gram based and neural language models on 50 typologically diverse languages covering a wide variety of morphological systems. Operating in the full vocabulary LM setup focused on word-level prediction, we demonstrate that a coarse typology of morphological systems is predictive of absolute LM performance. Moreover, fine-grained typological features such as exponence, flexivity, fusion, and inflectional synthesis are borne out to be responsible for the proliferation of low-frequency phenomena which are organically difficult to model by statistical architectures, or for the meaning ambiguity of character n-grams. Our study strongly suggests that these features have to be taken into consideration during the construction of next-level language-agnostic LM architectures, capable of handling morphologically complex languages such as Tamil or Korean.",
  "stance": -0.4
 },
 {
  "url": "https://aclanthology.org/D18-1040",
  "title": "Back-Translation Sampling by Targeting Difficult Words in Neural Machine Translation",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "Neural Machine Translation has achieved state-of-the-art performance for several language pairs using a combination of parallel and synthetic data. Synthetic data is often generated by back-translating sentences randomly sampled from monolingual data using a reverse translation model. While back-translation has been shown to be very effective in many cases, it is not entirely clear why. In this work, we explore different aspects of back-translation, and show that words with high prediction loss during training benefit most from the addition of synthetic data. We introduce several variations of sampling strategies targeting difficult-to-predict words using prediction losses and frequencies of words. In addition, we also target the contexts of difficult words and sample sentences that are similar in context. Experimental results for the WMT news translation task show that our method improves translation quality by up to 1.7 and 1.2 Bleu points over back-translation using random sampling for German-English and English-German, respectively.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/D18-1128",
  "title": "Do explanations make VQA models more predictable to a human?",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "A rich line of research attempts to make deep neural networks more transparent by generating human-interpretable 'explanations' of their decision process, especially for interactive tasks like Visual Question Answering (VQA). In this work, we analyze if existing explanations indeed make a VQA model - its responses as well as failures - more predictable to a human. Surprisingly, we find that they do not. On the other hand, we find that human-in-the-loop approaches that treat the model as a black-box do.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/D18-1174",
  "title": "Disambiguated skip-gram model",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "We present disambiguated skip-gram: a neural-probabilistic model for learning multi-sense distributed representations of words. Disambiguated skip-gram jointly estimates a skip-gram-like context word prediction model and a word sense disambiguation model. Unlike previous probabilistic models for learning multi-sense word embeddings, disambiguated skip-gram is end-to-end differentiable and can be interpreted as a simple feed-forward neural network. We also introduce an effective pruning strategy for the embeddings learned by disambiguated skip-gram. This allows us to control the granularity of representations learned by our model. In experimental evaluation disambiguated skip-gram improves state-of-the are results in several word sense induction benchmarks.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D18-1175",
  "title": "Picking Apart Story Salads",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "During natural disasters and conflicts, information about what happened is often confusing and messy, and distributed across many sources. We would like to be able to automatically identify relevant information and assemble it into coherent narratives of what happened. To make this task accessible to neural models, we introduce Story Salads, mixtures of multiple documents that can be generated at scale. By exploiting the Wikipedia hierarchy, we can generate salads that exhibit challenging inference problems. Story salads give rise to a novel, challenging clustering task, where the objective is to group sentences from the same narratives. We demonstrate that simple bag-of-words similarity clustering falls short on this task, and that it is necessary to take into account global context and coherence.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D18-1209",
  "title": "Improved Semantic-Aware Network Embedding with Fine-Grained Word Alignment",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "Network embeddings, which learns low-dimensional representations for each vertex in a large-scale network, have received considerable attention in recent years. For a wide range of applications, vertices in a network are typically accompanied by rich textual information such as user profiles, paper abstracts, etc. In this paper, we propose to incorporate semantic features into network embeddings by matching important words between text sequences for all pairs of vertices. We introduce an word-by-word alignment framework that measures the compatibility of embeddings between word pairs, and then adaptively accumulates these alignment features with a simple yet effective aggregation function. In experiments, we evaluate the proposed framework on three real-world benchmarks for downstream tasks, including link prediction and multi-label vertex classification. The experimental results demonstrate that our model outperforms state-of-the-art network embedding methods by a large margin.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D18-1306",
  "title": "Marginal Likelihood Training of BiLSTM-CRF for Biomedical Named Entity Recognition from Disjoint Label Sets",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "Extracting typed entity mentions from text is a fundamental component to language understanding and reasoning. While there exist substantial labeled text datasets for multiple subsets of biomedical entity types-such as genes and proteins, or chemicals and diseases-it is rare to find large labeled datasets containing labels for all desired entity types together. This paper presents a method for training a single CRF extractor from multiple datasets with disjoint or partially overlapping sets of entity types. Our approach employs marginal likelihood training to insist on labels that are present in the data, while filling in \"missing labels\". This allows us to leverage all the available data within a single model. In experimental results on the Biocreative V CDR (chemicals/diseases), Biocreative VI ChemProt (chemicals/proteins) and MedMentions (19 entity types) datasets, we show that joint training on multiple datasets improves NER F1 over training in isolation, and our methods achieve state-of-the-art results.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D18-1313",
  "title": "The Lazy Encoder: A Fine-Grained Analysis of the Role of Morphology in Neural Machine Translation",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "Neural sequence-to-sequence models have proven very effective for machine translation, but at the expense of model interpretability. To shed more light into the role played by linguistic structure in the process of neural machine translation, we perform a fine-grained analysis of how various source-side morphological features are captured at different levels of the NMT encoder while varying the target language. Differently from previous work, we find no correlation between the accuracy of source morphology encoding and translation quality. We do find that morphological features are only captured in context and only to the extent that they are directly transferable to the target words.",
  "stance": -0.5
 },
 {
  "url": "https://aclanthology.org/D18-1324",
  "title": "The Importance of Generation Order in Language Modeling",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "Neural language models are a critical component of state-of-the-art systems for machine translation, summarization, audio transcription, and other tasks. These language models are almost universally autoregressive in nature, generating sentences one token at a time from left to right. This paper studies the influence of token generation order on model quality via a novel two-pass language model that produces partially-filled sentence \"templates\" and then fills in missing tokens. We compare various strategies for structuring these two passes and observe a surprisingly large variation in model quality. We find the most effective strategy generates function words in the first pass followed by content words in the second. We believe these experimental results justify a more extensive investigation of the generation order for neural language models.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/D18-1364",
  "title": "Implicational Universals in Stochastic Constraint-Based Phonology",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "This paper focuses on the most basic implicational universals in phonological theory, called T-orders after Anttila and Andrus (2006). It shows that the T-orders predicted by stochastic (and partial order) Optimality Theory coincide with those predicted by categorical OT. Analogously, the T-orders predicted by stochastic Harmonic Grammar coincide with those predicted by categorical HG. In other words, these stochastic constraint-based frameworks do not tamper with the typological structure induced by the original categorical frameworks.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/D18-1425",
  "title": "Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "We present Spider, a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 college students. It consists of 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables covering 138 different domains. We define a new complex and cross-domain semantic parsing and text-to-SQL task so that different complicated SQL queries and databases appear in train and test sets. In this way, the task requires the model to generalize well to both new SQL queries and new database schemas. Therefore, Spider is distinct from most of the previous semantic parsing tasks because they all use a single database and have the exact same program in the train set and the test set. We experiment with various state-of-the-art models and the best model achieves only 9.7% exact matching accuracy on a database split setting. This shows that Spider presents a strong challenge for future research. Our dataset and task with the most recent updates are publicly available at https://yale-lily.github.io/seq2sql/spider.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D18-1437",
  "title": "Object Hallucination in Image Captioning",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "Despite continuously improving performance, contemporary image captioning models are prone to \"hallucinating\" objects that are not actually in a scene. One problem is that standard metrics only measure similarity to ground truth captions and may not fully capture image relevance. In this work, we propose a new image relevance metric to evaluate current models with veridical visual labels and assess their rate of object hallucination. We analyze how captioning model architectures and learning objectives contribute to object hallucination, explore when hallucination is likely due to image misclassification or language priors, and assess how well current sentence metrics capture object hallucination. We investigate these questions on the standard image captioning benchmark, MSCOCO, using a diverse set of models. Our analysis yields several interesting findings, including that models which score best on standard sentence metrics do not always have lower hallucination and that models which hallucinate more tend to make errors driven by language priors.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/D18-1442",
  "title": "Iterative Document Representation Learning Towards Summarization with Polishing",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "In this paper, we introduce Iterative Text Summarization (ITS), an iteration-based model for supervised extractive text summarization, inspired by the observation that it is often necessary for a human to read an article multiple times in order to fully understand and summarize its contents. Current summarization approaches read through a document only once to generate a document representation, resulting in a sub-optimal representation. To address this issue we introduce a model which iteratively polishes the document representation on many passes through the document. As part of our model, we also introduce a selective reading mechanism that decides more accurately the extent to which each sentence in the model should be updated. Experimental results on the CNN/DailyMail and DUC2002 datasets demonstrate that our model significantly outperforms state-of-the-art extractive systems when evaluated by machines and by humans.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D18-1467",
  "title": "Making \"fetch\" happen: The influence of social and linguistic context on nonstandard word growth and decline",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "In an online community, new words come and go: today's \"haha\" may be replaced by tomorrow's \"lol.\" Changes in online writing are usually studied as a social process, with innovations diffusing through a network of individuals in a speech community. But unlike other types of innovation, language change is shaped and constrained by the grammatical system in which it takes part. To investigate the role of social and structural factors in language change, we undertake a large-scale analysis of the frequencies of non-standard words in Reddit. Dissemination across many linguistic contexts is a predictor of success: words that appear in more linguistic contexts grow faster and survive longer. Furthermore, social dissemination plays a less important role in explaining word growth and decline than previously hypothesized.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/D18-1500",
  "title": "Understanding Deep Learning Performance through an Examination of Test Set Difficulty: A Psychometric Case Study",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "Interpreting the performance of deep learning models beyond test set accuracy is challenging. Characteristics of individual data points are often not considered during evaluation, and each data point is treated equally. In this work we examine the impact of a test set question's difficulty to determine if there is a relationship between difficulty and performance. We model difficulty using well-studied psychometric methods on human response patterns. Experiments on Natural Language Inference (NLI) and Sentiment Analysis (SA) show that the likelihood of answering a question correctly is impacted by the question's difficulty. In addition, as DNNs are trained on larger datasets easy questions start to have a higher probability of being answered correctly than harder questions.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/D18-1512",
  "title": "Has Machine Translation Achieved Human Parity? A Case for Document-level Evaluation",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese-English news translation task. We empirically test this claim with alternative evaluation protocols, contrasting the evaluation of single sentences and entire documents. In a pairwise ranking experiment, human raters assessing adequacy and fluency show a stronger preference for human over machine translation when evaluating documents as compared to isolated sentences. Our findings emphasise the need to shift towards document-level evaluation as machine translation improves to the degree that errors which are hard or impossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/D18-1513",
  "title": "Automatic Reference-Based Evaluation of Pronoun Translation Misses the Point",
  "year": 2018,
  "venue": "EMNLP",
  "abstract": "We compare the performance of the APT and AutoPRF metrics for pronoun translation against a manually annotated dataset comprising human judgements as to the correctness of translations of the PROTEST test suite. Although there is some correlation with the human judgements, a range of issues limit the performance of the automated metrics. Instead, we recommend the use of semi-automatic metrics and test suites in place of fully automatic metrics.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/D19-1107",
  "title": "Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets",
  "year": 2019,
  "venue": "EMNLP",
  "abstract": "Crowdsourcing has been the prevalent paradigm for creating natural language understanding datasets in recent years. A common crowdsourcing practice is to recruit a small number of high-quality workers, and have them massively generate examples. Having only a few workers generate the majority of examples raises concerns about data diversity, especially when workers freely generate sentences. In this paper, we perform a series of experiments showing these concerns are evident in three recent NLP datasets. We show that model performance improves when training with annotator identifiers as features, and that models are able to recognize the most productive annotators. Moreover, we show that often models do not generalize well to examples from annotators that did not contribute to the training set. Our findings suggest that annotator bias should be monitored during dataset creation, and that test set annotators should be disjoint from training set annotators.",
  "stance": -0.7
 },
 {
  "url": "https://aclanthology.org/D19-1133",
  "title": "uniblock: Scoring and Filtering Corpus with Unicode Block Information",
  "year": 2019,
  "venue": "EMNLP",
  "abstract": "The preprocessing pipelines in Natural Language Processing usually involve a step of removing sentences consisted of illegal characters. The definition of illegal characters and the specific removal strategy depend on the task, language, domain, etc, which often lead to tiresome and repetitive scripting of rules. In this paper, we introduce a simple statistical method, uniblock, to overcome this problem. For each sentence, uniblock generates a fixed-size feature vector using Unicode block information of the characters. A Gaussian mixture model is then estimated on some clean corpus using variational inference. The learned model can then be used to score sentences and filter corpus. We present experimental results on Sentiment Analysis, Language Modeling and Machine Translation, and show the simplicity and effectiveness of our method.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D19-1222",
  "title": "To Annotate or Not? Predicting Performance Drop under Domain Shift",
  "year": 2019,
  "venue": "EMNLP",
  "abstract": "Performance drop due to domain-shift is an endemic problem for NLP models in production. This problem creates an urge to continuously annotate evaluation datasets to measure the expected drop in the model performance which can be prohibitively expensive and slow. In this paper, we study the problem of predicting the performance drop of modern NLP models under domain-shift, in the absence of any target domain labels. We investigate three families of methods ($H$-divergence, reverse classification accuracy and confidence measures), show how they can be used to predict the performance drop and study their robustness to adversarial domain-shifts. Our results on sentiment classification and sequence labelling show that our method is able to predict performance drops with an error rate as low as 2.15% and 0.89% for sentiment analysis and POS tagging respectively.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D19-1228",
  "title": "How well do NLI models capture verb veridicality?",
  "year": 2019,
  "venue": "EMNLP",
  "abstract": "In natural language inference (NLI), contexts are considered veridical if they allow us to infer that their underlying propositions make true claims about the real world. We investigate whether a state-of-the-art natural language inference model (BERT) learns to make correct inferences about veridicality in verb-complement constructions. We introduce an NLI dataset for veridicality evaluation consisting of 1,500 sentence pairs, covering 137 unique verbs. We find that both human and model inferences generally follow theoretical patterns, but exhibit a systematic bias towards assuming that verbs are veridical-a bias which is amplified in BERT. We further show that, encouragingly, BERT's inferences are sensitive not only to the presence of individual verb types, but also to the syntactic role of the verb, the form of the complement clause (to- vs. that-complements), and negation.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/D19-1269",
  "title": "Collaborative Policy Learning for Open Knowledge Graph Reasoning",
  "year": 2019,
  "venue": "EMNLP",
  "abstract": "In recent years, there has been a surge of interests in interpretable graph reasoning methods. However, these models often suffer from limited performance when working on sparse and incomplete graphs, due to the lack of evidential paths that can reach target entities. Here we study open knowledge graph reasoning-a task that aims to reason for missing facts over a graph augmented by a background text corpus. A key challenge of the task is to filter out \"irrelevant\" facts extracted from corpus, in order to maintain an effective search space during path inference. We propose a novel reinforcement learning framework to train two collaborative agents jointly, i.e., a multi-hop graph reasoner and a fact extractor. The fact extraction agent generates fact triples from corpora to enrich the graph on the fly; while the reasoning agent provides feedback to the fact extractor and guides it towards promoting facts that are helpful for the interpretable reasoning. Experiments on two public datasets demonstrate the effectiveness of the proposed approach.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/D19-1285",
  "title": "Is the Red Square Big? MALeViC: Modeling Adjectives Leveraging Visual Contexts",
  "year": 2019,
  "venue": "EMNLP",
  "abstract": "This work aims at modeling how the meaning of gradable adjectives of size ('big', 'small') can be learned from visually-grounded contexts. Inspired by cognitive and linguistic evidence showing that the use of these expressions relies on setting a threshold that is dependent on a specific context, we investigate the ability of multi-modal models in assessing whether an object is 'big' or 'small' in a given visual scene. In contrast with the standard computational approach that simplistically treats gradable adjectives as 'fixed' attributes, we pose the problem as relational: to be successful, a model has to consider the full visual context. By means of four main tasks, we show that state-of-the-art models (but not a relatively strong baseline) can learn the function subtending the meaning of size adjectives, though their performance is found to decrease while moving from simple to more complex tasks. Crucially, models fail in developing abstract representations of gradable adjectives that can be used compositionally.",
  "stance": -0.4
 },
 {
  "url": "https://aclanthology.org/D19-1287",
  "title": "Representation of Constituents in Neural Language Models: Coordination Phrase as a Case Study",
  "year": 2019,
  "venue": "EMNLP",
  "abstract": "Neural language models have achieved state-of-the-art performances on many NLP tasks, and recently have been shown to learn a number of hierarchically-sensitive syntactic dependencies between individual words. However, equally important for language processing is the ability to combine words into phrasal constituents, and use constituent-level features to drive downstream expectations. Here we investigate neural models' ability to represent constituent-level features, using coordinated noun phrases as a case study. We assess whether different neural language models trained on English and French represent phrase-level number and gender features, and use those features to drive downstream expectations. Our results suggest that models use a linear combination of NP constituent number to drive CoordNP/verb number agreement. This behavior is highly regular and even sensitive to local syntactic context, however it differs crucially from observed human behavior. Models have less success with gender agreement. Models trained on large corpora perform best, and there is no obvious advantage for models trained using explicit syntactic supervision.",
  "stance": -0.3
 },
 {
  "url": "https://aclanthology.org/D19-1296",
  "title": "Weakly Supervised Multilingual Causality Extraction from Wikipedia",
  "year": 2019,
  "venue": "EMNLP",
  "abstract": "We present a method for extracting causality knowledge from Wikipedia, such as Protectionism -> Trade war, where the cause and effect entities correspond to Wikipedia articles. Such causality knowledge is easy to verify by reading corresponding Wikipedia articles, to translate to multiple languages through Wikidata, and to connect to knowledge bases derived from Wikipedia. Our method exploits Wikipedia article sections that describe causality and the redundancy stemming from the multilinguality of Wikipedia. Experiments showed that our method achieved precision and recall above 98% and 64%, respectively. In particular, it could extract causalities whose cause and effect were written distantly in a Wikipedia article. We have released the code and data for further research.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D19-1310",
  "title": "Table-to-Text Generation with Effective Hierarchical Encoder on Three Dimensions (Row, Column and Time)",
  "year": 2019,
  "venue": "EMNLP",
  "abstract": "Although Seq2Seq models for table-to-text generation have achieved remarkable progress, modeling table representation in one dimension is inadequate. This is because (1) the table consists of multiple rows and columns, which means that encoding a table should not depend only on one dimensional sequence or set of records and (2) most of the tables are time series data (e.g. NBA game data, stock market data), which means that the description of the current table may be affected by its historical data. To address aforementioned problems, not only do we model each table cell considering other records in the same row, we also enrich table's representation by modeling each table cell in context of other cells in the same column or with historical (time dimension) data respectively. In addition, we develop a table cell fusion gate to combine representations from row, column and time dimension into one dense vector according to the saliency of each dimension's representation. We evaluated our methods on ROTOWIRE, a benchmark dataset of NBA basketball games. Both automatic and human evaluation results demonstrate the effectiveness of our model with improvement of 2.66 in BLEU over the strong baseline and outperformance of state-of-the-art model.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D19-1331",
  "title": "On NMT Search Errors and Model Errors: Cat Got Your Tongue?",
  "year": 2019,
  "venue": "EMNLP",
  "abstract": "We report on search errors and model errors in neural machine translation (NMT). We present an exact inference procedure for neural sequence models based on a combination of beam search and depth-first search. We use our exact search to find the global best model scores under a Transformer base model for the entire WMT15 English-German test set. Surprisingly, beam search fails to find these global best model scores in most cases, even with a very large beam size of 100. For more than 50% of the sentences, the model in fact assigns its global best score to the empty translation, revealing a massive failure of neural models in properly accounting for adequacy. We show by constraining search with a minimum translation length that at the root of the problem of empty translations lies an inherent bias towards shorter translations. We conclude that vanilla NMT in its current form requires just the right amount of beam search errors, which, from a modelling perspective, is a highly unsatisfactory conclusion indeed, as the model often prefers an empty translation.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/D19-1381",
  "title": "A Search-based Neural Model for Biomedical Nested and Overlapping Event Detection",
  "year": 2019,
  "venue": "EMNLP",
  "abstract": "We tackle the nested and overlapping event detection task and propose a novel search-based neural network (SBNN) structured prediction model that treats the task as a search problem on a relation graph of trigger-argument structures. Unlike existing structured prediction tasks such as dependency parsing, the task targets to detect DAG structures, which constitute events, from the relation graph. We define actions to construct events and use all the beams in a beam search to detect all event structures that may be overlapping and nested. The search process constructs events in a bottom-up manner while modelling the global properties for nested and overlapping structures simultaneously using neural networks. We show that the model achieves performance comparable to the state-of-the-art model Turku Event Extraction System (TEES) on the BioNLP Cancer Genetics (CG) Shared Task 2013 without the use of any syntactic and hand-engineered features. Further analyses on the development set show that our model is more computationally efficient while yielding higher F1-score performance.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/D19-1414",
  "title": "Nearly-Unsupervised Hashcode Representations for Biomedical Relation Extraction",
  "year": 2019,
  "venue": "EMNLP",
  "abstract": "Recently, kernelized locality sensitive hashcodes have been successfully employed as representations of natural language text, especially showing high relevance to biomedical relation extraction tasks. In this paper, we propose to optimize the hashcode representations in a nearly unsupervised manner, in which we only use data points, but not their class labels, for learning. The optimized hashcode representations are then fed to a supervised classifi er following the prior work. This nearly unsupervised approach allows fine-grained optimization of each hash function, which is particularly suitable for building hashcode representations generalizing from a training set to a test set. We empirically evaluate the proposed approach for biomedical relation extraction tasks, obtaining significant accuracy improvements w.r.t. state-of-the-art supervised and semi-supervised approaches.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D19-1563",
  "title": "A Knowledge Regularized Hierarchical Approach for Emotion Cause Analysis",
  "year": 2019,
  "venue": "EMNLP",
  "abstract": "Emotion cause analysis, which aims to identify the reasons behind emotions, is a key topic in sentiment analysis. A variety of neural network models have been proposed recently, however, these previous models mostly focus on the learning architecture with local textual information, ignoring the discourse and prior knowledge, which play crucial roles in human text comprehension. In this paper, we propose a new method to extract emotion cause with a hierarchical neural model and knowledge-based regularizations, which aims to incorporate discourse context information and restrain the parameters by sentiment lexicon and common knowledge. The experimental results demonstrate that our proposed method achieves the state-of-the-art performance on two public datasets in different languages (Chinese and English), outperforming a number of competitive baselines by at least 2.08% in F-measure.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/D19-1607",
  "title": "Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model",
  "year": 2019,
  "venue": "EMNLP",
  "abstract": "Because it is not feasible to collect training data for every language, there is a growing interest in cross-lingual transfer learning. In this paper, we systematically explore zero-shot cross-lingual transfer learning on reading comprehension tasks with language representation model pre-trained on multi-lingual corpus. The experimental results show that with pre-trained language representation zero-shot learning is feasible, and translating the source data into the target language is not necessary and even degrades the performance. We further explore what does the model learn in zero-shot setting.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/D19-1662",
  "title": "Adversarial Removal of Demographic Attributes Revisited",
  "year": 2019,
  "venue": "EMNLP",
  "abstract": "Elazar and Goldberg (2018) showed that protected attributes can be extracted from the representations of a debiased neural network for mention detection at above-chance levels, by evaluating a diagnostic classifier on a held-out subsample of the data it was trained on. We revisit their experiments and conduct a series of follow-up experiments showing that, in fact, the diagnostic classifier generalizes poorly to both new in-domain samples and new domains, indicating that it relies on correlations specific to their particular data sample. We further show that a diagnostic classifier trained on the biased baseline neural network also does not generalize to new samples. In other words, the biases detected in Elazar and Goldberg (2018) seem restricted to their particular data sample, and would therefore not bias the decisions of the model on new samples, whether in-domain or out-of-domain. In light of this, we discuss better methodologies for detecting bias in our models.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.127",
  "title": "Double Graph Based Reasoning for Document-level Relation Extraction",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Document-level relation extraction aims to extract relations among entities within a document. Different from sentence-level relation extraction, it requires reasoning over multiple sentences across paragraphs. In this paper, we propose Graph Aggregation-and-Inference Network (GAIN), a method to recognize such relations for long paragraphs. GAIN constructs two graphs, a heterogeneous mention-level graph (MG) and an entity-level graph (EG). The former captures complex interaction among different mentions and the latter aggregates mentions underlying for the same entities. Based on the graphs we propose a novel path reasoning mechanism to infer relations between entities. Experiments on the public dataset, DocRED, show GAIN achieves a significant performance improvement (2.85 on F1) over the previous state-of-the-art. Our code is available at https://github.com/PKUnlp-icler/GAIN.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.147",
  "title": "GraphDialog: Integrating Graph Knowledge into End-to-End Task-Oriented Dialogue Systems",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "End-to-end task-oriented dialogue systems aim to generate system responses directly from plain text inputs. There are two challenges for such systems: one is how to effectively incorporate external knowledge bases (KBs) into the learning framework; the other is how to accurately capture the semantics of dialogue history. In this paper, we address these two challenges by exploiting the graph structural information in the knowledge base and in the dependency parsing tree of the dialogue. To effectively leverage the structural information in dialogue history, we propose a new recurrent cell architecture which allows representation learning on graphs. To exploit the relations between entities in KBs, the model combines multi-hop reasoning ability based on the graph structure. Experimental results show that the proposed model achieves consistent improvement over state-of-the-art models on two different task-oriented dialogue datasets.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.156",
  "title": "RNNs can generate bounded hierarchical languages with optimal memory",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Recurrent neural networks empirically generate natural language with high syntactic fidelity. However, their success is not well-understood theoretically. We provide theoretical insight into this success, proving in a finite-precision setting that RNNs can efficiently generate bounded hierarchical languages that reflect the scaffolding of natural language syntax. We introduce Dyck-$(k,m)$, the language of well-nested brackets (of $k$ types) and $m$-bounded nesting depth, reflecting the bounded memory needs and long-distance dependencies of natural language syntax. The best known results use $O(k^{\\fracm2})$ memory (hidden units) to generate these languages. We prove that an RNN with $O(m \\log k)$ hidden units suffices, an exponential reduction in memory, by an explicit construction. Finally, we show that no algorithm, even with unbounded computation, can suffice with $o(m \\log k)$ hidden units.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.26",
  "title": "Incremental Processing in the Age of Non-Incremental Encoders: An Empirical Assessment of Bidirectional Models for Incremental NLU",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "While humans process language incrementally, the best language encoders currently used in NLP do not. Both bidirectional LSTMs and Transformers assume that the sequence that is to be encoded is available in full, to be processed either forwards and backwards (BiLSTMs) or as a whole (Transformers). We investigate how they behave under incremental interfaces, when partial output must be provided based on partial input seen up to a certain time step, which may happen in interactive systems. We test five models on various NLU datasets and compare their performance using three incremental evaluation metrics. The results support the possibility of using bidirectional encoders in incremental mode while retaining most of their non-incremental quality. The \"omni-directional\" BERT model, which achieves better non-incremental performance, is impacted more by the incremental access. This can be alleviated by adapting the training regime (truncated training), or the testing procedure, by delaying the output until some right context is available or by incorporating hypothetical right contexts generated by a language model like GPT-2.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.260",
  "title": "On the weak link between importance and prunability of attention heads",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Given the success of Transformer-based models, two directions of study have emerged: interpreting role of individual attention heads and down-sizing the models for efficiency. Our work straddles these two streams: We analyse the importance of basing pruning strategies on the interpreted role of the attention heads. We evaluate this on Transformer and BERT models on multiple NLP tasks. Firstly, we find that a large fraction of the attention heads can be randomly pruned with limited effect on accuracy. Secondly, for Transformers, we find no advantage in pruning attention heads identified to be important based on existing studies that relate importance to the location of a head. On the BERT model too we find no preference for top or bottom layers, though the latter are reported to have higher importance. However, strategies that avoid pruning middle layers and consecutive layers perform better. Finally, during fine-tuning the compensation for pruned attention heads is roughly equally distributed across the un-pruned heads. Our results thus suggest that interpretation of attention heads does not strongly inform pruning.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.33",
  "title": "What Have We Achieved on Text Summarization?",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Deep learning has led to significant improvement in text summarization with various methods investigated and improved ROUGE scores reported over the years. However, gaps still exist between summaries produced by automatic summarizers and human professionals. Aiming to gain more understanding of summarization systems with respect to their strengths and limits on a fine-grained syntactic and semantic level, we consult the Multidimensional Quality Metric (MQM) and quantify 8 major sources of errors on 10 representative summarization models manually. Primarily, we find that 1) under similar settings, extractive summarizers are in general better than their abstractive counterparts thanks to strength in faithfulness and factual-consistency; 2) milestone techniques such as copy, coverage and hybrid extractive/abstractive methods do bring specific improvements but also demonstrate limitations; 3) pre-training techniques, and in particular sequence-to-sequence pre-training, are highly effective for improving text summarization, with BART giving the best results.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.349",
  "title": "PlotMachines: Outline-Conditioned Generation with Dynamic Plot State Tracking",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "We propose the task of outline-conditioned story generation: given an outline as a set of phrases that describe key characters and events to appear in a story, the task is to generate a coherent narrative that is consistent with the provided outline. This task is challenging as the input only provides a rough sketch of the plot, and thus, models need to generate a story by interweaving the key points provided in the outline. This requires the model to keep track of the dynamic states of the latent plot, conditioning on the input outline while generating the full story. We present PlotMachines, a neural narrative model that learns to transform an outline into a coherent story by tracking the dynamic plot states. In addition, we enrich PlotMachines with high-level discourse structure so that the model can learn different writing styles corresponding to different parts of the narrative. Comprehensive experiments over three fiction and non-fiction datasets demonstrate that large-scale language models, such as GPT-2 and Grover, despite their impressive generation performance, are not sufficient in generating coherent narratives for the given outline, and dynamic plot state tracking is important for composing narratives with tighter, more consistent plots.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.350",
  "title": "Do sequence-to-sequence VAEs learn global features of sentences?",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Autoregressive language models are powerful and relatively easy to train. However, these models are usually trained without explicit conditioning labels and do not offer easy ways to control global aspects such as sentiment or topic during generation. Bowman & al. 2016 adapted the Variational Autoencoder (VAE) for natural language with the sequence-to-sequence architecture and claimed that the latent vector was able to capture such global features in an unsupervised manner. We question this claim. We measure which words benefit most from the latent information by decomposing the reconstruction loss per position in the sentence. Using this method, we find that VAEs are prone to memorizing the first words and the sentence length, producing local features of limited usefulness. To alleviate this, we investigate alternative architectures based on bag-of-words assumptions and language model pretraining. These variants learn latent variables that are more global, i.e., more predictive of topic or sentiment labels. Moreover, using reconstructions, we observe that they decrease memorization: the first word and the sentence length are not recovered as accurately than with the baselines, consequently yielding more diverse reconstructions.",
  "stance": -0.5
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.362",
  "title": "Do Explicit Alignments Robustly Improve Multilingual Encoders?",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Multilingual BERT (mBERT), XLM-RoBERTa (XLMR) and other unsupervised multilingual encoders can effectively learn cross-lingual representation. Explicit alignment objectives based on bitexts like Europarl or MultiUN have been shown to further improve these representations. However, word-level alignments are often suboptimal and such bitexts are unavailable for many languages. In this paper, we propose a new contrastive alignment objective that can better utilize such signal, and examine whether these previous alignment methods can be adapted to noisier sources of aligned data: a randomly sampled 1 million pair subset of the OPUS collection. Additionally, rather than report results on a single dataset with a single model run, we report the mean and standard derivation of multiple runs with different seeds, on four datasets and tasks. Our more extensive analysis finds that, while our new objective outperforms previous work, overall these methods do not improve performance with a more robust evaluation framework. Furthermore, the gains from using a better underlying model eclipse any benefits from alignment training. These negative results dictate more care in evaluating these methods and suggest limitations in applying explicit alignment objectives.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.363",
  "title": "From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Massively multilingual transformers (MMTs) pretrained via language modeling (e.g., mBERT, XLM-R) have become a default paradigm for zero-shot language transfer in NLP, offering unmatched transfer performance. Current evaluations, however, verify their efficacy in transfers (a) to languages with sufficiently large pretraining corpora, and (b) between close languages. In this work, we analyze the limitations of downstream language transfer with MMTs, showing that, much like cross-lingual word embeddings, they are substantially less effective in resource-lean scenarios and for distant languages. Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER) and two high-level tasks (NLI, QA), empirically correlate transfer performance with linguistic proximity between source and target languages, but also with the size of target language corpora used in MMT pretraining. Most importantly, we demonstrate that the inexpensive few-shot transfer (i.e., additional fine-tuning on a few target-language instances) is surprisingly effective across the board, warranting more research efforts reaching beyond the limiting zero-shot conditions.",
  "stance": -0.3
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.394",
  "title": "An Empirical Investigation Towards Efficient Multi-Domain Language Model Pre-training",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Pre-training large language models has become a standard in the natural language processing community. Such models are pre-trained on generic data (e.g. BookCorpus and English Wikipedia) and often fine-tuned on tasks in the same domain. However, in order to achieve state-of-the-art performance on out of domain tasks such as clinical named entity recognition and relation extraction, additional in domain pre-training is required. In practice, staged multi-domain pre-training presents performance deterioration in the form of catastrophic forgetting (CF) when evaluated on a generic benchmark such as GLUE. In this paper we conduct an empirical investigation into known methods to mitigate CF. We find that elastic weight consolidation provides best overall scores yielding only a 0.33% drop in performance across seven generic tasks while remaining competitive in bio-medical tasks. Furthermore, we explore gradient and latent clustering based data selection techniques to improve coverage when using elastic weight consolidation and experience replay methods.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.40",
  "title": "Don't Use English Dev: On the Zero-Shot Cross-Lingual Evaluation of Contextual Embeddings",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Multilingual contextual embeddings have demonstrated state-of-the-art performance in zero-shot cross-lingual transfer learning, where multilingual BERT is fine-tuned on one source language and evaluated on a different target language. However, published results for mBERT zero-shot accuracy vary as much as 17 points on the MLDoc classification task across four papers. We show that the standard practice of using English dev accuracy for model selection in the zero-shot setting makes it difficult to obtain reproducible results on the MLDoc and XNLI tasks. English dev accuracy is often uncorrelated (or even anti-correlated) with target language accuracy, and zero-shot performance varies greatly at different points in the same fine-tuning run and between different fine-tuning runs. These reproducibility issues are also present for other tasks with different pre-trained embeddings (e.g., MLQA with XLM-R). We recommend providing oracle scores alongside zero-shot results: still fine-tune using English data, but choose a checkpoint with the target dev set. Reporting this upper bound makes results more consistent by avoiding arbitrarily bad checkpoints.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.429",
  "title": "Writing Strategies for Science Communication: Data and Computational Analysis",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Communicating complex scientific ideas without misleading or overwhelming the public is challenging. While science communication guides exist, they rarely offer empirical evidence for how their strategies are used in practice. Writing strategies that can be automatically recognized could greatly support science communication efforts by enabling tools to detect and suggest strategies for writers. We compile a set of writing strategies drawn from a wide range of prescriptive sources and develop an annotation scheme allowing humans to recognize them. We collect a corpus of 128k science writing documents in English and annotate a subset of this corpus. We use the annotations to train transformer-based classifiers and measure the strategies' use in the larger corpus. We find that the use of strategies, such as storytelling and emphasizing the most important findings, varies significantly across publications with different reader audiences.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.446",
  "title": "Imitation Attacks and Defenses for Black-box Machine Translation Systems",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Adversaries may look to steal or attack black-box NLP systems, either for financial gain or to exploit model errors. One setting of particular interest is machine translation (MT), where models have high commercial value and errors can be costly. We investigate possible exploitations of black-box MT systems and explore a preliminary defense against such threats. We first show that MT systems can be stolen by querying them with monolingual sentences and training models to imitate their outputs. Using simulated experiments, we demonstrate that MT model stealing is possible even when imitation models have different input data or architectures than their target models. Applying these ideas, we train imitation models that reach within 0.6 BLEU of three production MT systems on both high-resource and low-resource language pairs. We then leverage the similarity of our imitation models to transfer adversarial examples to the production systems. We use gradient-based attacks that expose inputs which lead to semantically-incorrect translations, dropped content, and vulgar model outputs. To mitigate these vulnerabilities, we propose a defense that modifies translation outputs in order to misdirect the optimization of imitation models. This defense degrades the adversary's BLEU score and attack success rate at some cost in the defender's BLEU and inference speed.",
  "stance": -0.5
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.45",
  "title": "Condolence and Empathy in Online Communities",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Offering condolence is a natural reaction to hearing someone's distress. Individuals frequently express distress in social media, where some communities can provide support. However, not all condolence is equal-trite responses offer little actual support despite their good intentions. Here, we develop computational tools to create a massive dataset of 11.4M expressions of distress and 2.8M corresponding offerings of condolence in order to examine the dynamics of condolence online. Our study reveals widespread disparity in what types of distress receive supportive condolence rather than just engagement. Building on studies from social psychology, we analyze the language of condolence and develop a new dataset for quantifying the empathy in a condolence using appraisal theory. Finally, we demonstrate that the features of condolence individuals find most helpful online differ substantially in their features from those seen in interpersonal settings.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.477",
  "title": "LAReQA: Language-Agnostic Answer Retrieval from a Multilingual Pool",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "We present LAReQA, a challenging new benchmark for language-agnostic answer retrieval from a multilingual candidate pool. Unlike previous cross-lingual tasks, LAReQA tests for \"strong\" cross-lingual alignment, requiring semantically related cross-language pairs to be closer in representation space than unrelated same-language pairs. This level of alignment is important for the practical task of cross-lingual information retrieval. Building on multilingual BERT (mBERT), we study different strategies for achieving strong alignment. We find that augmenting training data via machine translation is effective, and improves significantly over using mBERT out-of-the-box. Interestingly, model performance on zero-shot variants of our task that only target \"weak\" alignment is not predictive of performance on LAReQA. This finding underscores our claim that language-agnostic retrieval is a substantively new kind of cross-lingual evaluation, and suggests that measuring both weak and strong alignment will be important for improving cross-lingual systems going forward. We release our dataset and evaluation code at https://github.com/google-research-datasets/lareqa.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.573",
  "title": "Compositional and Lexical Semantics in RoBERTa, BERT and DistilBERT: A Case Study on CoQA",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Many NLP tasks have benefited from transferring knowledge from contextualized word embeddings, however the picture of what type of knowledge is transferred is incomplete. This paper studies the types of linguistic phenomena accounted for by language models in the context of a Conversational Question Answering (CoQA) task. We identify the problematic areas for the finetuned RoBERTa, BERT and DistilBERT models through systematic error analysis - basic arithmetic (counting phrases), compositional semantics (negation and Semantic Role Labeling), and lexical semantics (surprisal and antonymy). When enhanced with the relevant linguistic knowledge through multitask learning, the models improve in performance. Ensembles of the enhanced models yield a boost between 2.2 and 2.7 points in F1 score overall, and up to 42.1 points in F1 on the hardest question classes. The results show differences in ability to represent compositional and lexical information between RoBERTa, BERT and DistilBERT.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.576",
  "title": "On the Ability and Limitations of Transformers to Recognize Formal Languages",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Transformers have supplanted recurrent models in a large number of NLP tasks. However, the differences in their abilities to model different syntactic properties remain largely unknown. Past works suggest that LSTMs generalize very well on regular languages and have close connections with counter languages. In this work, we systematically study the ability of Transformers to model such languages as well as the role of its individual components in doing so. We first provide a construction of Transformers for a subclass of counter languages, including well-studied languages such as n-ary Boolean Expressions, Dyck-1, and its generalizations. In experiments, we find that Transformers do well on this subclass, and their learned mechanism strongly correlates with our construction. Perhaps surprisingly, in contrast to LSTMs, Transformers do well only on a subset of regular languages with degrading performance as we make languages more complex according to a well-known measure of complexity. Our analysis also provides insights on the role of self-attention mechanism in modeling certain behaviors and the influence of positional encoding schemes on the learning and generalization abilities of the model.",
  "stance": -0.3
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.616",
  "title": "Detecting Word Sense Disambiguation Biases in Machine Translation for Model-Agnostic Adversarial Attacks",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Word sense disambiguation is a well-known source of translation errors in NMT. We posit that some of the incorrect disambiguation choices are due to models' over-reliance on dataset artifacts found in training data, specifically superficial word co-occurrences, rather than a deeper understanding of the source text. We introduce a method for the prediction of disambiguation errors based on statistical data properties, demonstrating its effectiveness across several domains and model types. Moreover, we develop a simple adversarial attack strategy that minimally perturbs sentences in order to elicit disambiguation errors to further probe the robustness of translation models. Our findings indicate that disambiguation robustness varies substantially between domains and that different models trained on the same data are vulnerable to different attacks.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.693",
  "title": "Knowledge-guided Open Attribute Value Extraction with Reinforcement Learning",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Open attribute value extraction for emerging entities is an important but challenging task. A lot of previous works formulate the problem as a question-answering (QA) task. While the collections of articles from web corpus provide updated information about the emerging entities, the retrieved texts can be noisy, irrelevant, thus leading to inaccurate answers. Effectively filtering out noisy articles as well as bad answers is the key to improve extraction accuracy. Knowledge graph (KG), which contains rich, well organized information about entities, provides a good resource to address the challenge. In this work, we propose a knowledge-guided reinforcement learning (RL) framework for open attribute value extraction. Informed by relevant knowledge in KG, we trained a deep Q-network to sequentially compare extracted answers to improve extraction accuracy. The proposed framework is applicable to different information extraction system. Our experimental results show that our method outperforms the baselines by 16.5 - 27.8%.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.emnlp-main.696",
  "title": "Improving Low Compute Language Modeling with In-Domain Embedding Initialisation",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Many NLP applications, such as biomedical data and technical support, have 10-100 million tokens of in-domain data and limited computational resources for learning from it. How should we train a language model in this scenario? Most language modeling research considers either a small dataset with a closed vocabulary (like the standard 1 million token Penn Treebank), or the whole web with byte-pair encoding. We show that for our target setting in English, initialising and freezing input embeddings using in-domain data can improve language model performance by providing a useful representation of rare words, and this pattern holds across several different domains. In the process, we show that the standard convention of tying input and output embeddings does not improve perplexity when initializing with embeddings trained on in-domain data.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/2020.findings-emnlp.117",
  "title": "Evaluating Models' Local Decision Boundaries via Contrast Sets",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Standard test sets for supervised learning evaluate in-distribution generalization. Unfortunately, when a dataset has systematic gaps (e.g., annotation artifacts), these evaluations are misleading: a model can learn simple decision rules that perform well on the test set but do not capture the abilities a dataset is intended to test. We propose a more rigorous annotation paradigm for NLP that helps to close systematic gaps in the test data. In particular, after a dataset is constructed, we recommend that the dataset authors manually perturb the test instances in small but meaningful ways that (typically) change the gold label, creating contrast sets. Contrast sets provide a local view of a model's decision boundary, which can be used to more accurately evaluate a model's true linguistic capabilities. We demonstrate the efficacy of contrast sets by creating them for 10 diverse NLP datasets (e.g., DROP reading comprehension, UD parsing, and IMDb sentiment analysis). Although our contrast sets are not explicitly adversarial, model performance is significantly lower on them than on the original test sets-up to 25% in some cases. We release our contrast sets as new evaluation benchmarks and encourage future dataset construction efforts to follow similar annotation processes.",
  "stance": -0.1
 },
 {
  "url": "https://aclanthology.org/2020.findings-emnlp.123",
  "title": "TextHide: Tackling Data Privacy in Language Understanding Tasks",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "An unsolved challenge in distributed or federated learning is to effectively mitigate privacy risks without slowing down training or reducing accuracy. In this paper, we propose TextHide aiming at addressing this challenge for natural language understanding tasks. It requires all participants to add a simple encryption step to prevent an eavesdropping attacker from recovering private text data. Such an encryption step is efficient and only affects the task performance slightly. In addition, TextHide fits well with the popular framework of fine-tuning pre-trained language models (e.g., BERT) for any sentence or sentence-pair task. We evaluate TextHide on the GLUE benchmark, and our experiments show that TextHide can effectively defend attacks on shared gradients or representations and the averaged accuracy reduction is only 1.9%. We also present an analysis of the security of TextHide using a conjecture about the computational intractability of a mathematical problem.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.findings-emnlp.27",
  "title": "Understanding tables with intermediate pre-training",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Table entailment, the binary classification task of finding if a sentence is supported or refuted by the content of a table, requires parsing language and table structure as well as numerical and discrete reasoning. While there is extensive work on textual entailment, table entailment is less well studied. We adapt TAPAS (Herzig et al., 2020), a table-based BERT model, to recognize entailment. Motivated by the benefits of data augmentation, we create a balanced dataset of millions of automatically created training examples which are learned in an intermediate step prior to fine-tuning. This new data is not only useful for table entailment, but also for SQA (Iyyer et al., 2017), a sequential table QA task. To be able to use long examples as input of BERT models, we evaluate table pruning techniques as a pre-processing step to drastically improve the training and prediction efficiency at a moderate drop in accuracy. The different methods set the new state-of-the-art on the TabFact (Chen et al., 2020) and SQA datasets.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.findings-emnlp.295",
  "title": "WER we are and WER we think we are",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Natural language processing of conversational speech requires the availability of high-quality transcripts. In this paper, we express our skepticism towards the recent reports of very low Word Error Rates (WERs) achieved by modern Automatic Speech Recognition (ASR) systems on benchmark datasets. We outline several problems with popular benchmarks and compare three state-of-the-art commercial ASR systems on an internal dataset of real-life spontaneous human conversations and HUB'05 public benchmark. We show that WERs are significantly higher than the best reported results. We formulate a set of guidelines which may aid in the creation of real-life, multi-domain datasets with high quality annotations for training and testing of robust ASR systems.",
  "stance": -0.9
 },
 {
  "url": "https://aclanthology.org/2020.findings-emnlp.319",
  "title": "Improving Target-side Lexical Transfer in Multilingual Neural Machine Translation",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "To improve the performance of Neural Machine Translation (NMT) for low-resource languages (LRL), one effective strategy is to leverage parallel data from a related high-resource language (HRL). However, multilingual data has been found more beneficial for NMT models that translate from the LRL to a target language than the ones that translate into the LRLs. In this paper, we aim to improve the effectiveness of multilingual transfer for NMT models that translate into the LRL, by designing a better decoder word embedding. Extending upon a general-purpose multilingual encoding method Soft Decoupled Encoding (Wang et al., 2019), we propose DecSDE, an efficient character n-gram based embedding specifically designed for the NMT decoder. Our experiments show that DecSDE leads to consistent gains of up to 1.8 BLEU on translation from English to four different languages.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/2020.findings-emnlp.358",
  "title": "Effects of Naturalistic Variation in Goal-Oriented Dialog",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Existing benchmarks used to evaluate the performance of end-to-end neural dialog systems lack a key component: natural variation present in human conversations. Most datasets are constructed through crowdsourcing, where the crowd workers follow a fixed template of instructions while enacting the role of a user/agent. This results in straight-forward, somewhat routine, and mostly trouble-free conversations, as crowd workers do not think to represent the full range of actions that occur naturally with real users. In this work, we investigate the impact of naturalistic variation on two goal-oriented datasets: bAbI dialog task and Stanford Multi-Domain Dataset (SMD). We also propose new and more effective testbeds for both datasets, by introducing naturalistic variation by the user. We observe that there is a significant drop in performance (more than 60% in Ent. F1 on SMD and 85% in per-dialog accuracy on bAbI task) of recent state-of-the-art end-to-end neural methods such as BossNet and GLMP on both datasets.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/2020.findings-emnlp.389",
  "title": "What's so special about BERT's layers? A closer look at the NLP pipeline in monolingual and multilingual models",
  "year": 2020,
  "venue": "EMNLP",
  "abstract": "Peeking into the inner workings of BERT has shown that its layers resemble the classical NLP pipeline, with progressively more complex tasks being concentrated in later layers. To investigate to what extent these results also hold for a language other than English, we probe a Dutch BERT-based model and the multilingual BERT model for Dutch NLP tasks. In addition, through a deeper analysis of part-of-speech tagging, we show that also within a given task, information is spread over different parts of the network and the pipeline might not be as neat as it seems. Each layer has different specialisations, so that it may be more useful to combine information from different layers, instead of selecting a single one based on the best overall performance.",
  "stance": -0.3
 },
 {
  "url": "https://aclanthology.org/A00-1035",
  "title": "Spelling and Grammar Correction for Danish in SCARRIE",
  "year": 2000,
  "venue": "NAACL",
  "abstract": "This paper reports on work carried out to develop a spelling and grammar corrector for Danish, addressing in particular the issue of how a form of shallow parsing is combined with error detection and correction for the treatment of context-dependent spelling errors. The syntactic grammar for Danish used by the system has been developed with the aim of dealing with the most frequent error types found in a parallel corpus of unedited and proofread texts specifically collected by the project's end users. By focussing on certain grammatical constructions and certain error types, it has been possible to exploit the linguistic 'intelligence' provided by syntactic parsing and yet keep the system robust and efficient. The system described is thus superior to other existing spelling checkers for Danish in its ability to deal with contextdependent errors.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/A00-2002",
  "title": "The Automatic Translation of Discourse Structures",
  "year": 2000,
  "venue": "NAACL",
  "abstract": "We empirically show that there are significant differences between the discourse structure of Japanese texts and the discourse structure of their corresponding English translations. To improve translation quality, we propose a computational model for rewriting discourse structures. When we train our model on a parallel corpus of manually built Japanese and English discourse structure trees, we learn to rewrite Japanese trees as trees that are closer to the natural English rendering than the original ones.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/A00-2029",
  "title": "Predicting Automatic Speech Recognition Performance Using Prosodic Cues",
  "year": 2000,
  "venue": "NAACL",
  "abstract": "In spoken dialogue systems, it is important for a system to know how likely a speech recognition hypothesis is to be correct, so it can reprompt for fresh input, or, in cases where many errors have occurred, change its interaction strategy or switch the caller to a human attendant. We have discovered prosodic features which more accurately predict when a recognition hypothesis contains a word error than the acoustic confidence score thresholds traditionally used in automatic speech recognition. We present analytic results indicating that there are significant prosodic differences between correctly and incorrectly recognized turns in the T O O T train information corpus. We then present machine learning results showing how the use of prosodic features to automatically predict correct versus incorrectly recognized turns improves over the use of acoustic confidence scores alone. ",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/A00-2037",
  "title": "Acknowledgments in Human-Computer Interaction",
  "year": 2000,
  "venue": "NAACL",
  "abstract": "Acknowledgments are relatively rare in humancomputer interaction. Are people unwilling to use this human convention when talking to a machine, or is their scarcity due to the way that spoken-language interfaces are designed? We found that, given a simple spoken-language interface that provided opportunities for and responded to acknowle d g m e n t s , abou t h a l f of our sub j ec t s u s e d acknowledgments at least once and nearly 30% used them extensively during the interaction.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/N03-2020",
  "title": "A Robust Retrieval Engine for Proximal and Structural Search",
  "year": 2003,
  "venue": "NAACL",
  "abstract": "Katsuya Masuda† Takashi Ninomiya†‡ Yusuke Miyao† Tomoko Ohta†‡ Jun’ichi Tsujii†‡ † Department of Computer Science, Graduate School of Information Science and Technology, University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033, Japan ‡ CREST, JST (Japan Science and Technology Corporation) Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012, Japan {kmasuda,ninomi,yusuke,okap,tsujii}@is.s.u-tokyo.ac.jp",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/N06-1009",
  "title": "Role of Local Context in Automatic Deidentification of Ungrammatical, Fragmented Text",
  "year": 2006,
  "venue": "NAACL",
  "abstract": "Deidentification of clinical records is a crucial step before these records can be distributed to non-hospital researchers. Most approaches to deidentification rely heavily on dictionaries and heuristic rules; these approaches fail to remove most personal health information (PHI) that cannot be found in dictionaries. They also can fail to remove PHI that is ambiguous between PHI and non-PHI. Named entity recognition (NER) technologies can be used for deidentification. Some of these technologies exploit both local and global context of a word to identify its entity type. When documents are grammatically written, global context can improve NER. In this paper, we show that we can deidentify medical discharge summaries using support vector machines that rely on a statistical representation of local context. We compare our approach with three different systems. Comparison with a rulebased approach shows that a statistical representation of local context contributes more to deidentification than dictionaries and hand-tailored heuristics. Comparison with two well-known systems, SNoW and IdentiFinder, shows that when the language of documents is fragmented, local context contributes more to deidentification than global context.",
  "stance": 0.4
 },
 {
  "url": "https://aclanthology.org/N06-1050",
  "title": "Creating a Test Collection for Citation-based IR Experiments",
  "year": 2006,
  "venue": "NAACL",
  "abstract": "We present an approach to building a test collection of research papers. The approach is based on the Cranfield 2 tests but uses as its vehicle a current conference; research questions and relevance judgements of all cited papers are elicited from conference authors. The resultant test collection is different from TREC’s in that it comprises scientific articles rather than newspaper text and, thus, allows for IR experiments that include citation information. The test collection currently consists of 170 queries with relevance judgements; the document collection is the ACL Anthology. We describe properties of our queries and relevance judgements, and demonstrate the use of the test collection in an experimental setup. One potentially problematic property of our collection is that queries have a low number of relevant documents; we discuss ways of alleviating this.",
  "stance": 0.4
 },
 {
  "url": "https://aclanthology.org/N06-1055",
  "title": "Semantic role labeling of nominalized predicates in Chinese",
  "year": 2006,
  "venue": "NAACL",
  "abstract": "Recent work on semantic role labeling (SRL) has focused almost exclusively on the analysis of the predicate-argument structure of verbs, largely due to the lack of human-annotated resources for other types of predicates that can serve as training and test data for the semantic role labeling systems. However, it is wellknown that verbs are not the only type of predicates that can take arguments. Most notably, nouns that are nominalized forms of verbs and relational nouns generally are also considered to have their own predicate-argument structure. In this paper we report results of SRL experiments on nominalized predicates in Chinese, using a newly completed corpus, the Chinese Nombank. We also discuss the impact of using publicly available manually annotated verb data to improve the SRL accuracy of nouns, exploiting a widely-held assumption that verbs and their nominalizations share the same predicate-argument structure. Finally, we discuss the results of applying reranking techniques to improve SRL accuracy for nominalized predicates, which showed insignificant improvement.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/N06-1062",
  "title": "Unlimited vocabulary speech recognition for agglutinative languages",
  "year": 2006,
  "venue": "NAACL",
  "abstract": "It is practically impossible to build a word-based lexicon for speech recognition in agglutinative languages that would cover all the relevant words. The problem is that words are generally built by concatenating several prefixes and suffixes to the word roots. Together with compounding and inflections this leads to millions of different, but still frequent word forms. Due to inflections, ambiguity and other phenomena, it is also not trivial to automatically split the words into meaningful parts. Rule-based morphological analyzers can perform this splitting, but due to the handcrafted rules, they also suffer from an out-of-vocabulary problem. In this paper we apply a recently proposed fully automatic and rather language and vocabulary independent way to build subword lexica for three different agglutinative languages. We demonstrate the language portability as well by building a successful large vocabulary speech recognizer for each language and show superior recognition performance compared to the corresponding word-based reference systems.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/N06-2022",
  "title": "Automatic Recognition of Personality in Conversation",
  "year": 2006,
  "venue": "NAACL",
  "abstract": "The identification of personality by automatic analysis of conversation has many applications in natural language processing, from leader identification in meetings to partner matching on dating websites. We automatically train models of the main five personality dimensions, on a corpus of conversation extracts and personality ratings. Results show that the models perform better than the baseline, and their analysis confirms previous findings linking language and personality, while revealing many new linguistic and prosodic markers.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/N07-1001",
  "title": "Exploiting Acoustic and Syntactic Features for Prosody Labeling in a Maximum Entropy Framework",
  "year": 2007,
  "venue": "NAACL",
  "abstract": "In this paper we describe an automatic prosody labeling framework that exploits both language and speech information. We model the syntactic-prosodic information with a maximum entropy model that achieves an accuracy of 85.2% and 91.5% for pitch accent and boundary tone labeling on the Boston University Radio News corpus. We model the acousticprosodic stream with two different models, one a maximum entropy model and the other a traditional HMM. We finally couple the syntactic-prosodic and acousticprosodic components to achieve significantly improved pitch accent and boundary tone classification accuracies of 86.0% and 93.1% respectively. Similar experimental results are also reported on Boston Directions corpus.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N07-1030",
  "title": "Joint Determination of Anaphoricity and Coreference Resolution using Integer Programming",
  "year": 2007,
  "venue": "NAACL",
  "abstract": "Standard pairwise coreference resolution systems are subject to errors resulting from their performing anaphora identification as an implicit part of coreference resolution. In this paper, we propose an integer linear programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task, such that each local model informs the other for the final assignments. This joint ILP formulation provides f score improvements of 3.7-5.3% over a base coreference classifier on the ACE datasets.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N07-1046",
  "title": "A Log-Linear Block Transliteration Model based on Bi-Stream HMMs",
  "year": 2007,
  "venue": "NAACL",
  "abstract": "We propose a novel HMM-based framework to accurately transliterate unseen named entities. The framework leverages features in letteralignment and letter n-gram pairs learned from available bilingual dictionaries. Letter-classes, such as vowels/non-vowels, are integrated to further improve transliteration accuracy. The proposed transliteration system is applied to out-of-vocabulary named-entities in statistical machine translation (SMT), and a significant improvement over traditional transliteration approach is obtained. Furthermore, by incorporating an automatic spell-checker based on statistics collected from web search engines, transliteration accuracy is further improved. The proposed system is implemented within our SMT system and applied to a real translation scenario from Arabic to English.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N07-1048",
  "title": "Analysis of Morph-Based Speech Recognition and the Modeling of Out-of-Vocabulary Words Across Languages",
  "year": 2007,
  "venue": "NAACL",
  "abstract": "We analyze subword-based language models (LMs) in large-vocabulary continuous speech recognition across four “morphologically rich” languages: Finnish, Estonian, Turkish, and Egyptian Colloquial Arabic. By estimating n-gram LMs over sequences of morphs instead of words, better vocabulary coverage and reduced data sparsity is obtained. Standard word LMs suffer from high out-of-vocabulary (OOV) rates, whereas the morph LMs can recognize previously unseen word forms by concatenating morphs. We show that the morph LMs generally outperform the word LMs and that they perform fairly well on OOVs without compromising the accuracy obtained for in-vocabulary words.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/N07-1069",
  "title": "Can Semantic Roles Generalize Across Genres?",
  "year": 2007,
  "venue": "NAACL",
  "abstract": "PropBank has been widely used as training data for Semantic Role Labeling. However, because this training data is taken from the WSJ, the resulting machine learning models tend to overfit on idiosyncrasies of that text’s style, and do not port well to other genres. In addition, since PropBank was designed on a verb-by-verb basis, the argument labels Arg2 Arg5 get used for very diverse argument roles with inconsistent training instances. For example, the verb “make” uses Arg2 for the “Material” argument; but the verb “multiply” uses Arg2 for the “Extent” argument. As a result, it can be difficult for automatic classifiers to learn to distinguish arguments Arg2-Arg5. We have created a mapping between PropBank and VerbNet that provides a VerbNet thematic role label for each verb-specific PropBank label. Since VerbNet uses argument labels that are more consistent across verbs, we are able to demonstrate that these new labels are easier to learn.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/N07-2040",
  "title": "Are Some Speech Recognition Errors Easier to Detect than Others?",
  "year": 2007,
  "venue": "NAACL",
  "abstract": "This study investigates whether some speech recognition (SR) errors are easier to detect and what patterns can be identified from those errors. Specifically, SR errors were examined from both nonlinguistic and linguistic perspectives. The analyses of non-linguistic properties revealed that high error ratios and consecutive errors lowered the ease of error detection. The analyses of linguistic properties showed that ease of error detection was associated with changing parts-of-speech of reference words in SR errors. Additionally, syntactic relations themselves and the change of syntactic relations had impact on the ease of error detection.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/N07-2041",
  "title": "Simultaneous Identification of Biomedical Named-Entity and Functional Relation Using Statistical Parsing Techniques",
  "year": 2007,
  "venue": "NAACL",
  "abstract": "In this paper we propose a statistical parsing technique that simultaneously identifies biomedical named-entities (NEs) and extracts subcellular localization relations for bacterial proteins from the text in MEDLINE articles. We build a parser that derives both syntactic and domain-dependent semantic information and achieves an F-score of 48.4% for the relation extraction task. We then propose a semi-supervised approach that incorporates noisy automatically labeled data to improve the F-score of our parser to 83.2%. Our key contributions are: learning from noisy data, and building an annotated corpus that can benefit relation extraction research.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N07-2055",
  "title": "A Semi-Automatic Evaluation Scheme: Automated Nuggetization for Manual Annotation",
  "year": 2007,
  "venue": "NAACL",
  "abstract": "In this paper we describe automatic information nuggetization and its application to text comparison. More specifically, we take a close look at how machine-generated nuggets can be used to create evaluation material. A semiautomatic annotation scheme is designed to produce gold-standard data with exceptionally high inter-human agreement.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/N09-1004",
  "title": "A Fully Unsupervised Word Sense Disambiguation Method Using Dependency Knowledge",
  "year": 2009,
  "venue": "NAACL",
  "abstract": "Word sense disambiguation is the process of determining which sense of a word is used in a given context. Due to its importance in understanding semantics of natural languages, word sense disambiguation has been extensively studied in Computational Linguistics. However, existing methods either are brittle and narrowly focus on specific topics or words, or provide only mediocre performance in real-world settings. Broad coverage and disambiguation quality are critical for a word sense disambiguation system. In this paper we present a fully unsupervised word sense disambiguation method that requires only a dictionary and unannotated text as input. Such an automatic approach overcomes the problem of brittleness suffered in many existing methods and makes broad-coverage word sense disambiguation feasible in practice. We evaluated our approach using SemEval 2007 Task 7 (Coarse-grained English All-words Task), and our system significantly outperformed the best unsupervised system participating in SemEval 2007 and achieved the performance approaching top-performing supervised systems. Although our method was only tested with coarse-grained sense disambiguation, it can be directly applied to fine-grained sense disambiguation.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/N09-1017",
  "title": "The Role of Implicit Argumentation in Nominal SRL",
  "year": 2009,
  "venue": "NAACL",
  "abstract": "Nominals frequently surface without overtly expressed arguments. In order to measure the potential benefit of nominal SRL for downstream processes, such nominals must be accounted for. In this paper, we show that a state-of-the-art nominal SRL system with an overall argument F1 of 0.76 suffers a performance loss of more than 9% when nominals with implicit arguments are included in the evaluation. We then develop a system that takes implicit argumentation into account, improving overall performance by nearly 5%. Our results indicate that the degree of implicit argumentation varies widely across nominals, making automated detection of implicit argumentation an important step for nominal SRL.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/N09-1043",
  "title": "Assessing and Improving the Performance of Speech Recognition for Incremental Systems",
  "year": 2009,
  "venue": "NAACL",
  "abstract": "In incremental spoken dialogue systems, partial hypotheses about what was said are required even while the utterance is still ongoing. We define measures for evaluating the quality of incremental ASR components with respect to the relative correctness of the partial hypotheses compared to hypotheses that can optimize over the complete input, the timing of hypothesis formation relative to the portion of the input they are about, and hypothesis stability, defined as the number of times they are revised. We show that simple incremental post-processing can improve stability dramatically, at the cost of timeliness (from 90% of edits of hypotheses being spurious down to 10% at a lag of 320ms). The measures are not independent, and we show how system designers can find a desired operating point for their ASR. To our knowledge, we are the first to suggest and examine a variety of measures for assessing incremental ASR and improve performance on this basis.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N09-1055",
  "title": "An Iterative Reinforcement Approach for Fine-Grained Opinion Mining",
  "year": 2009,
  "venue": "NAACL",
  "abstract": "With the in-depth study of sentiment analysis research, finer-grained opinion mining, which aims to detect opinions on different review features as opposed to the whole review level, has been receiving more and more attention in the sentiment analysis research community recently. Most of existing approaches rely mainly on the template extraction to identify the explicit relatedness between product feature and opinion terms, which is insufficient to detect the implicit review features and mine the hidden sentiment association in reviews, which satisfies (1) the review features are not appear explicit in the review sentences; (2) it can be deduced by the opinion words in its context. From an information theoretic point of view, this paper proposed an iterative reinforcement framework based on the improved information bottleneck algorithm to address such problem. More specifically, the approach clusters product features and opinion words simultaneously and iteratively by fusing both their semantic information and co-occurrence information. The experimental results demonstrate that our approach outperforms the template extraction based approaches.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/N09-1074",
  "title": "Improved Syntactic Models for Parsing Speech with Repairs",
  "year": 2009,
  "venue": "NAACL",
  "abstract": "This paper introduces three new syntactic models for representing speech with repairs. These models are developed to test the intuition that the erroneous parts of speech repairs (reparanda) are not generated or recognized as such while occurring, but only after they have been corrected. Thus, they are designed to minimize the differences in grammar rule applications between fluent and disfluent speech containing similar structure. The three models considered in this paper are also designed to isolate the mechanism of impact, by systematically exploring different variables.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N09-2026",
  "title": "Multi-scale Personalization for Voice Search Applications",
  "year": 2009,
  "venue": "NAACL",
  "abstract": "Voice Search applications provide a very convenient and direct access to a broad variety of services and information. However, due to the vast amount of information available and the open nature of the spoken queries, these applications still suffer from recognition errors. This paper explores the utilization of personalization features for the post-processing of recognition results in the form of n-best lists. Personalization is carried out from three different angles: short-term, long-term and Web-based, and a large variety of features are proposed for use in a log-linear classification framework. Experimental results on data obtained from a commercially deployed Voice Search system show that the combination of the proposed features leads to a substantial sentence error rate reduction. In addition, it is shown that personalization features which are very different in nature can successfully complement each other.",
  "stance": 0.4
 },
 {
  "url": "https://aclanthology.org/N09-2042",
  "title": "Search Engine Adaptation by Feedback Control Adjustment for Time-sensitive Query",
  "year": 2009,
  "venue": "NAACL",
  "abstract": "We propose a new method to rank a special category of time-sensitive queries that are year qualified. The method adjusts the retrieval scores of a base ranking function according to time-stamps of web documents so that the freshest documents are ranked higher. Our method, which is based on feedback control theory, uses ranking errors to adjust the search engine behavior. For this purpose, we use a simple but effective method to extract year qualified queries by mining query logs and a time-stamp recognition method that considers titles and urls of web documents. Our method was tested on a commercial search engine. The experiments show that our approach can significantly improve relevance ranking for year qualified queries even if all the existing methods for comparison failed.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N09-2049",
  "title": "Analysing Recognition Errors in Unlimited-Vocabulary Speech Recognition",
  "year": 2009,
  "venue": "NAACL",
  "abstract": "We analyze the recognition errors made by a morph-based continuous speech recognition system, which practically allows an unlimited vocabulary. Examining the role of the acoustic and language models in erroneous regions shows how speaker adaptive training (SAT) and discriminative training with minimum phone frame error (MPFE) criterion decrease errors in different error classes. Analyzing the errors with respect to word frequencies and manually classified error types reveals the most potential areas for improving the system.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/N09-2050",
  "title": "The independence of dimensions in multidimensional dialogue act annotation",
  "year": 2009,
  "venue": "NAACL",
  "abstract": "This paper presents empirical evidence for the orthogonality of the DIT multidimensional dialogue act annotation scheme, showing that the ten dimensions of communication which underlie this scheme are addressed independently in natural dialogue.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N10-1003",
  "title": "Products of Random Latent Variable Grammars",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "We show that the automatically induced latent variable grammars of Petrov et al. (2006) vary widely in their underlying representations, depending on their EM initialization point. We use this to our advantage, combining multiple automatically learned grammars into an unweighted product model, which gives significantly improved performance over state-ofthe-art individual grammars. In our model, the probability of a constituent is estimated as a product of posteriors obtained from multiple grammars that differ only in the random seed used for initialization, without any learning or tuning of combination weights. Despite its simplicity, a product of eight automatically learned grammars improves parsing accuracy from 90.2% to 91.8% on English, and from 80.3% to 84.5% on German.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N10-1032",
  "title": "Testing a Grammar Customization System with Sahaptin",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "I briefly describe a system for automatically creating an implemented grammar of a natural language based on answers to a web-based questionnaire, then present a grammar of Sahaptin, a language of the Pacific Northwest with complex argument-marking and agreement patterns, that was developed to test the system. The development of this grammar has proved useful in three ways: (1) verifying the correct functioning of the grammar customization system, (2) motivating the addition of a new pattern of agreement to the system, and (3) making detailed predictions that uncovered gaps in the linguistic descriptions of Sahaptin.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N10-1034",
  "title": "Fast Query for Large Treebanks",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "A variety of query systems have been developed for interrogating parsed corpora, or treebanks. With the arrival of efficient, widecoverage parsers, it is feasible to create very large databases of trees. However, existing approaches that use in-memory search, or relational or XML database technologies, do not scale up. We describe a method for storage, indexing, and query of treebanks that uses an information retrieval engine. Several experiments with a large treebank demonstrate excellent scaling characteristics for a wide range of query types. This work facilitates the curation of much larger treebanks, and enables them to be used effectively in a variety of scientific and engineering tasks.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N10-1035",
  "title": "Efficient Parsing of Well-Nested Linear Context-Free Rewriting Systems",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "The use of well-nested linear context-free rewriting systems has been empirically motivated for modeling of the syntax of languages with discontinuous constituents or relatively free word order. We present a chart-based parsing algorithm that asymptotically improves the known running time upper bound for this class of rewriting systems. Our result is obtained through a linear space construction of a binary normal form for the grammar at hand.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N10-1037",
  "title": "Evaluation Metrics for the Lexical Substitution Task",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "We identify some problems of the evaluation metrics used for the English Lexical Substitution Task of SemEval-2007, and propose alternative metrics that avoid these problems, which we hope will better guide the future development of lexical substitution systems.",
  "stance": -0.4
 },
 {
  "url": "https://aclanthology.org/N10-1038",
  "title": "Movie Reviews and Revenues: An Experiment in Text Regression",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "We consider the problem of predicting a movie’s opening weekend revenue. Previous work on this problem has used metadata about a movie—e.g., its genre, MPAA rating, and cast—with very limited work making use of text about the movie. In this paper, we use the text of film critics’ reviews from several sources to predict opening weekend revenue. We describe a new dataset pairing movie reviews with metadata and revenue data, and show that review text can substitute for metadata, and even improve over it, for prediction.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/N10-1040",
  "title": "Improving Phrase-Based Translation with Prototypes of Short Phrases",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "We investigate methods of generating additional bilingual phrase pairs for a phrasebased decoder by translating short sequences of source text. Because our translation task is more constrained, we can use a model that employs more linguistically rich features than a traditional decoder. We have implemented an example of this approach. Experimental results suggest that the phrase pairs produced by our method are useful to the decoder, and lead to improved sentence translations.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N10-1045",
  "title": "Towards Cross-Lingual Textual Entailment",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "This paper investigates cross-lingual textual entailment as a semantic relation between two text portions in different languages, and proposes a prospective research direction. We argue that cross-lingual textual entailment (CLTE) can be a core technology for several cross-lingual NLP applications and tasks. Through preliminary experiments, we aim at proving the feasibility of the task, and providing a reliable baseline. We also introduce new applications for CLTE that will be explored in future work.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/N10-1052",
  "title": "Generalizing Hierarchical Phrase-based Translation using Rules with Adjacent Nonterminals",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "Hierarchical phrase-based translation (Hiero, (Chiang, 2005)) provides an attractive framework within which both shortand longdistance reorderings can be addressed consistently and ef ciently. However, Hiero is generally implemented with a constraint preventing the creation of rules with adjacent nonterminals, because such rules introduce computational and modeling challenges. We introduce methods to address these challenges, and demonstrate that rules with adjacent nonterminals can improve Hiero's generalization power and lead to signi cant performance gains in Chinese-English translation.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/N10-1053",
  "title": "The Effect of Ambiguity on the Automated Acquisition of WSD Examples",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "Several methods for automatically generating labeled examples that can be used as training data for WSD systems have been proposed, including a semisupervised approach based on relevance feedback (Stevenson et al., 2008a). This approach was shown to generate examples that improved the performance of a WSD system for a set of ambiguous terms from the biomedical domain. However, we find that this approach does not perform as well on other data sets. The levels of ambiguity in these data sets are analysed and we suggest this is the reason for this negative result.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/N10-1056",
  "title": "For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "We report on work in progress on extracting lexical simplifications (e.g., “collaborate” → “work together”), focusing on utilizing edit histories in Simple English Wikipedia for this task. We consider two main approaches: (1) deriving simplification probabilities via an edit model that accounts for a mixture of different operations, and (2) using metadata to focus on edits that are more likely to be simplification operations. We find our methods to outperform a reasonable baseline and yield many high-quality lexical simplifications not included in an independently-created manually prepared list.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N10-1060",
  "title": "\"cba to check the spelling\": Investigating Parser Performance on Discussion Forum Posts",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "We evaluate the Berkeley parser on text from an online discussion forum. We evaluate the parser output with and without gold tokens and spellings (using Sparseval and Parseval), and we compile a list of problematic phenomena for this domain. The Parseval f-score for a small development set is 77.56. This increases to 80.27 when we apply a set of simple transformations to the input sentences and to the Wall Street Journal (WSJ) training sections.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/N10-1072",
  "title": "Learning to Link Entities with Knowledge Base",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "This paper address the problem of entity linking. Specifically, given an entity mentioned in unstructured texts, the task is to link this entity with an entry stored in the existing knowledge base. This is an important task for information extraction. It can serve as a convenient gateway to encyclopedic information, and can greatly improve the web users’ experience. Previous learning based solutions mainly focus on classification framework. However, it’s more suitable to consider it as a ranking problem. In this paper, we propose a learning to rank algorithm for entity linking. It effectively utilizes the relationship information among the candidates when ranking. The experiment results on the TAC 20091 dataset demonstrate the effectiveness of our proposed framework. The proposed method achieves 18.5% improvement in terms of accuracy over the classification models for those entities which have corresponding entries in the Knowledge Base. The overall performance of the system is also better than that of the state-of-the-art methods.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N10-1075",
  "title": "Subword Variation in Text Message Classification",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "For millions of people in less resourced regions of the world, text messages (SMS) provide the only regular contact with their doctor. Classifying messages by medical labels supports rapid responses to emergencies, the early identification of epidemics and everyday administration, but challenges include textbrevity, rich morphology, phonological variation, and limited training data. We present a novel system that addresses these, working with a clinic in rural Malawi and texts in the Chichewa language. We show that modeling morphological and phonological variation leads to a substantial average gain of F=0.206 and an error reduction of up to 63.8% for specific labels, relative to a baseline system optimized over word-sequences. By comparison, there is no significant gain when applying the same system to the English translations of the same texts/labels, emphasizing the need for subword modeling in many languages. Language independent morphological models perform as accurately as language specific models, indicating a broad deployment potential.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N10-1080",
  "title": "The Best Lexical Metric for Phrase-Based Statistical MT System Optimization",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "Translation systems are generally trained to optimize BLEU, but many alternative metrics are available. We explore how optimizing toward various automatic evaluation metrics (BLEU, METEOR, NIST, TER) affects the resulting model. We train a state-of-the-art MT system using MERT on many parameterizations of each metric and evaluate the resulting models on the other metrics and also using human judges. In accordance with popular wisdom, we find that it’s important to train on the same metric used in testing. However, we also find that training to a newer metric is only useful to the extent that the MT model’s structure and features allow it to take advantage of the metric. Contrasting with TER’s good correlation with human judgments, we show that people tend to prefer BLEU and NIST trained models to those trained on edit distance based metrics like TER or WER. Human preferences for METEOR trained models varies depending on the source language. Since using BLEU or NIST produces models that are more robust to evaluation by other metrics and perform well in human judgments, we conclude they are still the best choice for training.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/N10-1089",
  "title": "Can Recognising Multiword Expressions Improve Shallow Parsing?",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "There is significant evidence in the literature that integrating knowledge about multiword expressions can improve shallow parsing accuracy. We present an experimental study to quantify this improvement, focusing on compound nominals, proper names and adjectivenoun constructions. The evaluation set of multiword expressions is derived from WordNet and the textual data are downloaded from the web. We use a classification method to aid human annotation of output parses. This method allows us to conduct experiments on a large dataset of unannotated data. Experiments show that knowledge about multiword expressions leads to an increase of between 7.5% and 9.5% in accuracy of shallow parsing in sentences containing these multiword expressions.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N10-1091",
  "title": "Ensemble Models for Dependency Parsing: Cheap and Good?",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "Previous work on dependency parsing used various kinds of combination models but a systematic analysis and comparison of these approaches is lacking. In this paper we implemented such a study for English dependency parsing and find several non-obvious facts: (a) the diversity of base parsers is more important than complex models for learning (e.g., stacking, supervised meta-classification), (b) approximate, linear-time re-parsing algorithms guarantee well-formed dependency trees without significant performance loss, and (c) the simplest scoring model for re-parsing (unweighted voting) performs essentially as well as other more complex models. This study proves that fast and accurate ensemble parsers can be built with minimal effort.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/N10-1098",
  "title": "Engaging learning groups using Social Interaction Strategies",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "Conversational Agents have been shown to be effective tutors in a wide range of educational domains. However, these agents are often ignored and abused in collaborative learning scenarios involving multiple students. In our work presented here, we design and evaluate interaction strategies motivated from prior research in small group communication. We will discuss how such strategies can be implemented in agents. As a first step towards evaluating agents that can interact socially, we report results showing that human tutors employing these strategies are able to cover more concepts with the students besides being rated as better integrated, likeable and friendlier.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/N10-1110",
  "title": "Investigations into the Crandem Approach to Word Recognition",
  "year": 2010,
  "venue": "NAACL",
  "abstract": "We suggest improvements to a previously proposed framework for integrating Conditional Random Fields and Hidden Markov Models, dubbed a Crandem system (2009). The previous authors’ work suggested that local label posteriors derived from the CRF were too low-entropy for use in word-level automatic speech recognition. As an alternative to the log posterior representation used in their system, we explore frame-level representations derived from the CRF feature functions. We also describe a weight normalization transformation that leads to increased entropy of the CRF posteriors. We report significant gains over the previous Crandem system on the Wall Street Journal word recognition task.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/N12-1001",
  "title": "Multiple Narrative Disentanglement: Unraveling Infinite Jest",
  "year": 2012,
  "venue": "NAACL",
  "abstract": "Many works (of both fiction and non-fiction) span multiple, intersecting narratives, each of which constitutes a story in its own right. In this work I introduce the task of multiple narrative disentanglement (MND), in which the aim is to tease these narratives apart by assigning passages from a text to the sub-narratives to which they belong. The motivating example I use is David Foster Wallace’s fictional text Infinite Jest. I selected this book because it contains multiple, interweaving narratives within its sprawling 1,000-plus pages. I propose and evaluate a novel unsupervised approach to MND that is motivated by the theory of narratology. This method achieves strong empirical results, successfully disentangling the threads in Infinite Jest and significantly outperforming baseline strategies in doing so.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N12-1018",
  "title": "Apples to Oranges: Evaluating Image Annotations from Natural Language Processing Systems",
  "year": 2012,
  "venue": "NAACL",
  "abstract": "We examine evaluation methods for systems that automatically annotate images using cooccurring text. We compare previous datasets for this task using a series of baseline measures inspired by those used in information retrieval, computer vision, and extractive summarization. Some of our baselines match or exceed the best published scores for those datasets. These results illuminate incorrect assumptions and improper practices regarding preprocessing, evaluation metrics, and the collection of gold image annotations. We conclude with a list of recommended practices for future research combining language and vision processing techniques.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/N12-1022",
  "title": "Topical Segmentation: a Study of Human Performance and a New Measure of Quality.",
  "year": 2012,
  "venue": "NAACL",
  "abstract": "In a large-scale study of how people find topical shifts in written text, 27 annotators were asked to mark topically continuous segments in 20 chapters of a novel. We analyze the resulting corpus for inter-annotator agreement and examine disagreement patterns. The results suggest that, while the overall agreement is relatively low, the annotators show high agreement on a subset of topical breaks – places where most prominent topic shifts occur. We recommend taking into account the prominence of topical shifts when evaluating topical segmentation, effectively penalizing more severely the errors on more important breaks. We propose to account for this in a simple modification of the windowDiff metric. We discuss the experimental results of evaluating several topical segmenters with and without considering the importance of the individual breaks, and emphasize the more insightful nature of the latter analysis.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/N12-1026",
  "title": "Optimized Online Rank Learning for Machine Translation",
  "year": 2012,
  "venue": "NAACL",
  "abstract": "We present an online learning algorithm for statistical machine translation (SMT) based on stochastic gradient descent (SGD). Under the online setting of rank learning, a corpus-wise loss has to be approximated by a batch local loss when optimizing for evaluation measures that cannot be linearly decomposed into a sentence-wise loss, such as BLEU. We propose a variant of SGD with a larger batch size in which the parameter update in each iteration is further optimized by a passive-aggressive algorithm. Learning is efficiently parallelized and line search is performed in each round when merging parameters across parallel jobs. Experiments on the NIST Chinese-to-English Open MT task indicate significantly better translation results.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N12-1030",
  "title": "The Challenges of Parsing Chinese with Combinatory Categorial Grammar",
  "year": 2012,
  "venue": "NAACL",
  "abstract": "We apply Combinatory Categorial Grammar to wide-coverage parsing in Chinese with the new Chinese CCGbank, bringing a formalism capable of transparently recovering non-local dependencies to a language in which they are particularly frequent. We train two state-of-the-art English  parsers: the parser of Petrov and Klein (P&K), and the Clark and Curran (C&C) parser, uncovering a surprising performance gap between them not observed in English — 72.73 (P&K) and 67.09 (C&C) F -score on  6. We explore the challenges of Chinese  parsing through three novel ideas: developing corpus variants rather than treating the corpus as fixed; controlling noun/verb and other  ambiguities; and quantifying the impact of constructions like pro-drop.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N12-1043",
  "title": "A Comparative Investigation of Morphological Language Modeling for the Languages of the European Union",
  "year": 2012,
  "venue": "NAACL",
  "abstract": "We investigate a language model that combines morphological and shape features with a Kneser-Ney model and test it in a large crosslingual study of European languages. Even though the model is generic and we use the same architecture and features for all languages, the model achieves reductions in perplexity for all 21 languages represented in the Europarl corpus, ranging from 3% to 11%. We show that almost all of this perplexity reduction can be achieved by identifying suffixes by frequency.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N12-1062",
  "title": "Tuning as Linear Regression",
  "year": 2012,
  "venue": "NAACL",
  "abstract": "We propose a tuning method for statistical machine translation, based on the pairwise ranking approach. Hopkins and May (2011) presented a method that uses a binary classifier. In this work, we use linear regression and show that our approach is as effective as using a binary classifier and converges faster.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N12-1063",
  "title": "Ranking-based readability assessment for early primary children's literature",
  "year": 2012,
  "venue": "NAACL",
  "abstract": "Determining the reading level of children’s literature is an important task for providing educators and parents with an appropriate reading trajectory through a curriculum. Automating this process has been a challenge addressed before in the computational linguistics literature, with most studies attempting to predict the particular grade level of a text. However, guided reading levels developed by educators operate at a more fine-grained level, with multiple levels corresponding to each grade. We find that ranking performs much better than classification at the fine-grained leveling task, and that features derived from the visual layout of a book are just as predictive as standard text features of level; including both sets of features, we find that we can predict the reading level up to 83% of the time on a small corpus of children’s books.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N12-1064",
  "title": "How Text Segmentation Algorithms Gain from Topic Models",
  "year": 2012,
  "venue": "NAACL",
  "abstract": "This paper introduces a general method to incorporate the LDA Topic Model into text segmentation algorithms. We show that semantic information added by Topic Models significantly improves the performance of two wordbased algorithms, namely TextTiling and C99. Additionally, we introduce the new TopicTiling algorithm that is designed to take better advantage of topic information. We show consistent improvements over word-based methods and achieve state-of-the art performance on a standard dataset.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N12-1068",
  "title": "Are You Sure? Confidence in Prediction of Dependency Tree Edges",
  "year": 2012,
  "venue": "NAACL",
  "abstract": "We describe and evaluate several methods for estimating the confidence in the per-edge correctness of a predicted dependency parse. We show empirically that the confidence is associated with the probability that an edge is selected correctly and that it can be used to detect incorrect edges very efficiently. We evaluate our methods on parsing text in 14 languages.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N12-1079",
  "title": "Why Not Grab a Free Lunch? Mining Large Corpora for Parallel Sentences to Improve Translation Modeling",
  "year": 2012,
  "venue": "NAACL",
  "abstract": "It is well known that the output quality of statistical machine translation (SMT) systems increases with more training data. To obtain more parallel text for translation modeling, researchers have turned to the web to mine parallel sentences, but most previous approaches have avoided the difficult problem of pairwise similarity on cross-lingual documents and instead rely on heuristics. In contrast, we confront this challenge head on using the MapReduce framework. On a modest cluster, our scalable end-to-end processing pipeline was able to automatically gather 5.8m parallel sentence pairs from English and German Wikipedia. Augmenting existing bitext with these data yielded significant improvements over a state-of-the-art baseline (2.39 BLEU points in the best case).",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N13-1018",
  "title": "Towards Topic Labeling with Phrase Entailment and Aggregation",
  "year": 2013,
  "venue": "NAACL",
  "abstract": "We propose a novel framework for topic labeling that assigns the most representative phrases for a given set of sentences covering the same topic. We build an entailment graph over phrases that are extracted from the sentences, and use the entailment relations to identify and select the most relevant phrases. We then aggregate those selected phrases by means of phrase generalization and merging. We motivate our approach by applying over conversational data, and show that our framework improves performance significantly over baseline algorithms.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N13-1019",
  "title": "Topic Segmentation with a Structured Topic Model",
  "year": 2013,
  "venue": "NAACL",
  "abstract": "We present a new hierarchical Bayesian model for unsupervised topic segmentation. This new model integrates a point-wise boundary sampling algorithm used in Bayesian segmentation into a structured topic model that can capture a simple hierarchical topic structure latent in documents. We develop an MCMC inference algorithm to split/merge segment(s). Experimental results show that our model outperforms previous unsupervised segmentation methods using only lexical information on Choi’s datasets and two meeting transcripts and has performance comparable to those previous methods on two written datasets.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N13-1020",
  "title": "Text Alignment for Real-Time Crowd Captioning",
  "year": 2013,
  "venue": "NAACL",
  "abstract": "The primary way of providing real-time captioning for deaf and hard of hearing people is to employ expensive professional stenographers who can type as fast as natural speaking rates. Recent work has shown that a feasible alternative is to combine the partial captions of ordinary typists, each of whom types part of what they hear. In this paper, we describe an improved method for combining partial captions into a final output based on weighted A search and multiple sequence alignment (MSA). In contrast to prior work, our method allows the tradeoff between accuracy and speed to be tuned, and provides formal error bounds. Our method outperforms the current state-of-the-art on Word Error Rate (WER) (29.6%), BLEU Score (41.4%), and F-measure (36.9%). The end goal is for these captions to be used by people, and so we also compare how these metrics correlate with the judgments of 50 study participants, which may assist others looking to make further progress on this problem.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N13-1021",
  "title": "Discriminative Joint Modeling of Lexical Variation and Acoustic Confusion for Automated Narrative Retelling Assessment",
  "year": 2013,
  "venue": "NAACL",
  "abstract": "Automatically assessing the fidelity of a retelling to the original narrative – a task of growing clinical importance – is challenging, given extensive paraphrasing during retelling along with cascading automatic speech recognition (ASR) errors. We present a word tagging approach using conditional random fields (CRFs) that allows a diversity of features to be considered during inference, including some capturing acoustic confusions encoded in word confusion networks. We evaluate the approach under several scenarios, including both supervised and unsupervised training, the latter achieved by training on the output of a baseline automatic word-alignment model. We also adapt the ASR models to the domain, and evaluate the impact of error rate on performance. We find strong robustness to ASR errors, even using just the 1-best system output. A hybrid approach making use of both automatic alignment and CRFs trained tagging models achieves the best performance, yielding strong improvements over using either approach alone.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N13-1023",
  "title": "Segmentation Strategies for Streaming Speech Translation",
  "year": 2013,
  "venue": "NAACL",
  "abstract": "The study presented in this work is a first effort at real-time speech translation of TED talks, a compendium of public talks with different speakers addressing a variety of topics. We address the goal of achieving a system that balances translation accuracy and latency. In order to improve ASR performance for our diverse data set, adaptation techniques such as constrained model adaptation and vocal tract length normalization are found to be useful. In order to improve machine translation (MT) performance, techniques that could be employed in real-time such as monotonic and partial translation retention are found to be of use. We also experiment with inserting text segmenters of various types between ASR and MT in a series of real-time translation experiments. Among other results, our experiments demonstrate that a good segmentation is useful, and a novel conjunction-based segmentation strategy improves translation quality nearly as much as other strategies such as comma-based segmentation. It was also found to be important to synchronize various pipeline components in order to minimize latency.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N13-1032",
  "title": "Improving reordering performance using higher order and structural features",
  "year": 2013,
  "venue": "NAACL",
  "abstract": "Recent work has shown that word aligned data can be used to learn a model for reordering source sentences to match the target order. This model learns the cost of putting a word immediately before another word and finds the best reordering by solving an instance of the Traveling Salesman Problem (TSP). However, for efficiently solving the TSP, the model is restricted to pairwise features which examine only a pair of words and their neighborhood. In this work, we go beyond these pairwise features and learn a model to rerank the n-best reorderings produced by the TSP model using higher order and structural features which help in capturing longer range dependencies. In addition to using a more informative set of source side features, we also capture target side features indirectly by using the translation score assigned to a reordering. Our experiments, involving Urdu-English, show that the proposed approach outperforms a state-of-theart PBSMT system which uses the TSP model for reordering by 1.3 BLEU points, and a publicly available state-of-the-art MT system, Hiero, by 3 BLEU points.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N13-1039",
  "title": "Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters",
  "year": 2013,
  "venue": "NAACL",
  "abstract": "We consider the problem of part-of-speech tagging for informal, online conversational text. We systematically evaluate the use of large-scale unsupervised word clustering and new lexical features to improve tagging accuracy. With these features, our system achieves state-of-the-art tagging results on both Twitter and IRC POS tagging tasks; Twitter tagging is improved from 90% to 93% accuracy (more than 3% absolute). Qualitative analysis of these word clusters yields insights about NLP and linguistic phenomena in this genre. Additionally, we contribute the first POS annotation guidelines for such text and release a new dataset of English language tweets annotated using these guidelines. Tagging software, annotation guidelines, and large-scale word clusters are available at: http://www.ark.cs.cmu.edu/TweetNLP This paper describes release 0.3 of the “CMU Twitter Part-of-Speech Tagger” and annotated data.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N13-1053",
  "title": "Negative Deceptive Opinion Spam",
  "year": 2013,
  "venue": "NAACL",
  "abstract": "The rising influence of user-generated online reviews (Cone, 2011) has led to growing incentive for businesses to solicit and manufacture DECEPTIVE OPINION SPAM—fictitious reviews that have been deliberately written to sound authentic and deceive the reader. Recently, Ott et al. (2011) have introduced an opinion spam dataset containing gold standard deceptive positive hotel reviews. However, the complementary problem of negative deceptive opinion spam, intended to slander competitive offerings, remains largely unstudied. Following an approach similar to Ott et al. (2011), in this work we create and study the first dataset of deceptive opinion spam with negative sentiment reviews. Based on this dataset, we find that standard n-gram text categorization techniques can detect negative deceptive opinion spam with performance far surpassing that of human judges. Finally, in conjunction with the aforementioned positive review dataset, we consider the possible interactions between sentiment and deception, and present initial results that encourage further exploration of this relationship.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/N13-1054",
  "title": "Improving speech synthesis quality by reducing pitch peaks in the source recordings",
  "year": 2013,
  "venue": "NAACL",
  "abstract": "We present a method for improving the perceived naturalness of corpus-based speech synthesizers. It consists in removing pronounced pitch peaks in the original recordings, which typically lead to noticeable discontinuities in the synthesized speech. We perceptually evaluated this method using two concatenative and two HMM-based synthesis systems, and found that using it on the source recordings managed to improve the naturalness of the synthesizers and had no effect on their intelligibility.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N13-1055",
  "title": "Robust Systems for Preposition Error Correction Using Wikipedia Revisions",
  "year": 2013,
  "venue": "NAACL",
  "abstract": "We show that existing methods for training preposition error correction systems, whether using well-edited text or error-annotated corpora, do not generalize across very different test sets. We present a new, large errorannotated corpus and use it to train systems that generalize across three different test sets, each from a different domain and with different error characteristics. This new corpus is automatically extracted from Wikipedia revisions and contains over one million instances of preposition corrections.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/N13-1069",
  "title": "Systematic Comparison of Professional and Crowdsourced Reference Translations for Machine Translation",
  "year": 2013,
  "venue": "NAACL",
  "abstract": "We present a systematic study of the effect of crowdsourced translations on Machine Translation performance. We compare Machine Translation systems trained on the same data but with translations obtained using Amazon’s Mechanical Turk vs. professional translations, and show that the same performance is obtained from Mechanical Turk translations at 1/5th the cost. We also show that adding a Mechanical Turk reference translation of the development set improves parameter tuning and output evaluation.",
  "stance": 0.1
 },
 {
  "url": "https://aclanthology.org/N13-1080",
  "title": "What's in a Domain? Multi-Domain Learning for Multi-Attribute Data",
  "year": 2013,
  "venue": "NAACL",
  "abstract": "Multi-Domain learning assumes that a single metadata attribute is used in order to divide the data into so-called domains. However, real-world datasets often have multiple metadata attributes that can divide the data into domains. It is not always apparent which single attribute will lead to the best domains, and more than one attribute might impact classification. We propose extensions to two multi-domain learning techniques for our multi-attribute setting, enabling them to simultaneously learn from several metadata attributes. Experimentally, they outperform the multi-domain learning baseline, even when it selects the single “best” attribute.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N13-1122",
  "title": "To Link or Not to Link? A Study on End-to-End Tweet Entity Linking",
  "year": 2013,
  "venue": "NAACL",
  "abstract": "Information extraction from microblog posts is an important task, as today microblogs capture an unprecedented amount of information and provide a view into the pulse of the world. As the core component of information extraction, we consider the task of Twitter entity linking in this paper. In the current entity linking literature, mention detection and entity disambiguation are frequently cast as equally important but distinct problems. However, in our task, we find that mention detection is often the performance bottleneck. The reason is that messages on micro-blogs are short, noisy and informal texts with little context, and often contain phrases with ambiguous meanings. To rigorously address the Twitter entity linking problem, we propose a structural SVM algorithm for entity linking that jointly optimizes mention detection and entity disambiguation as a single end-to-end task. By combining structural learning and a variety of firstorder, second-order, and context-sensitive features, our system is able to outperform existing state-of-the art entity linking systems by 15% F1.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N15-1010",
  "title": "Not All Character N-grams Are Created Equal: A Study in Authorship Attribution",
  "year": 2015,
  "venue": "NAACL",
  "abstract": "Character n-grams have been identified as the most successful feature in both singledomain and cross-domain Authorship Attribution (AA), but the reasons for their discriminative value were not fully understood. We identify subgroups of character n-grams that correspond to linguistic aspects commonly claimed to be covered by these features: morphosyntax, thematic content and style. We evaluate the predictiveness of each of these groups in two AA settings: a single domain setting and a cross-domain setting where multiple topics are present. We demonstrate that character ngrams that capture information about affixes and punctuation account for almost all of the power of character n-grams as features. Our study contributes new insights into the use of n-grams for future AA work and other classification tasks.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N15-1015",
  "title": "What's Cookin'? Interpreting Cooking Videos using Text, Speech and Vision",
  "year": 2015,
  "venue": "NAACL",
  "abstract": "We present a novel method for aligning a sequence of instructions to a video of someone carrying out a task. In particular, we focus on the cooking domain, where the instructions correspond to the recipe. Our technique relies on an HMM to align the recipe steps to the (automatically generated) speech transcript. We then refine this alignment using a state-of-the-art visual food detector, based on a deep convolutional neural network. We show that our technique outperforms simpler techniques based on keyword spotting. It also enables interesting applications, such as automatically illustrating recipes with keyframes, and searching within a video for events of interest.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N15-1026",
  "title": "Personalized Page Rank for Named Entity Disambiguation",
  "year": 2015,
  "venue": "NAACL",
  "abstract": "The task of Named Entity Disambiguation is to map entity mentions in the document to their correct entries in some knowledge base. We present a novel graph-based disambiguation approach based on Personalized PageRank (PPR) that combines local and global evidence for disambiguation and effectively filters out noise introduced by incorrect candidates. Experiments show that our method outperforms state-of-the-art approaches by achieving 91.7% in microand 89.9% in macroaccuracy on a dataset of 27.8K named entity mentions.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N15-1027",
  "title": "When and why are log-linear models self-normalizing?",
  "year": 2015,
  "venue": "NAACL",
  "abstract": "Several techniques have recently been proposed for training “self-normalized” discriminative models. These attempt to find parameter settings for which unnormalized model scores approximate the true label probability. However, the theoretical properties of such techniques (and of self-normalization generally) have not been investigated. This paper examines the conditions under which we can expect self-normalization to work. We characterize a general class of distributions that admit self-normalization, and prove generalization bounds for procedures that minimize empirical normalizer variance. Motivated by these results, we describe a novel variant of an established procedure for training self-normalized models. The new procedure avoids computing normalizers for most training examples, and decreases training time by as much as factor of ten while preserving model quality.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N15-1029",
  "title": "Disfluency Detection with a Semi-Markov Model and Prosodic Features",
  "year": 2015,
  "venue": "NAACL",
  "abstract": "We present a discriminative model for detecting disfluencies in spoken language transcripts. Structurally, our model is a semiMarkov conditional random field with features targeting characteristics unique to speech repairs. This gives a significant performance improvement over standard chain-structured CRFs that have been employed in past work. We then incorporate prosodic features over silences and relative word duration into our semi-CRF model, resulting in further performance gains; moreover, these features are not easily replaced by discrete prosodic indicators such as ToBI breaks. Our final system, the semi-CRF with prosodic information, achieves an F-score of 85.4, which is 1.3 F1 better than the best prior reported F-score on this dataset.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N15-1034",
  "title": "Sign constraints on feature weights improve a joint model of word segmentation and phonology",
  "year": 2015,
  "venue": "NAACL",
  "abstract": "This paper describes a joint model of word segmentation and phonological alternations, which takes unsegmented utterances as input and infers word segmentations and underlying phonological representations. The model is a Maximum Entropy or log-linear model, which can express a probabilistic version of Optimality Theory (OT; Prince and Smolensky (2004)), a standard phonological framework. The features in our model are inspired by OT’s Markedness and Faithfulness constraints. Following the OT principle that such features indicate “violations”, we require their weights to be non-positive. We apply our model to a modified version of the Buckeye corpus (Pitt et al., 2007) in which the only phonological alternations are deletions of word-final /d/ and /t/ segments. The model sets a new state-ofthe-art for this corpus for word segmentation, identification of underlying forms, and identification of /d/ and /t/ deletions. We also show that the OT-inspired sign constraints on feature weights are crucial for accurate identification of deleted /d/s; without them our model posits approximately 10 times more deleted underlying /d/s than appear in the manually annotated data.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N15-1035",
  "title": "Semi-Supervised Word Sense Disambiguation Using Word Embeddings in General and Specific Domains",
  "year": 2015,
  "venue": "NAACL",
  "abstract": "One of the weaknesses of current supervised word sense disambiguation (WSD) systems is that they only treat a word as a discrete entity. However, a continuous-space representation of words (word embeddings) can provide valuable information and thus improve generalization accuracy. Since word embeddings are typically obtained from unlabeled data using unsupervised methods, this method can be seen as a semi-supervised word sense disambiguation approach. This paper investigates two ways of incorporating word embeddings in a word sense disambiguation setting and evaluates these two methods on some SensEval/SemEval lexical sample and all-words tasks and also a domain-specific lexical sample task. The obtained results show that such representations consistently improve the accuracy of the selected supervised WSD system. Moreover, our experiments on a domainspecific dataset show that our supervised baseline system beats the best knowledge-based systems by a large margin.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/N15-1054",
  "title": "Inferring Missing Entity Type Instances for Knowledge Base Completion: New Dataset and Methods",
  "year": 2015,
  "venue": "NAACL",
  "abstract": "Most of previous work in knowledge base (KB) completion has focused on the problem of relation extraction. In this work, we focus on the task of inferring missing entity type instances in a KB, a fundamental task for KB competition yet receives little attention. Due to the novelty of this task, we construct a large-scale dataset and design an automatic evaluation methodology. Our knowledge base completion method uses information within the existing KB and external information from Wikipedia. We show that individual methods trained with a global objective that considers unobserved cells from both the entity and the type side gives consistently higher quality predictions compared to baseline methods. We also perform manual evaluation on a small subset of the data to verify the effectiveness of our knowledge base completion methods and the correctness of our proposed automatic evaluation method.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/N15-1062",
  "title": "Constraint-Based Models of Lexical Borrowing",
  "year": 2015,
  "venue": "NAACL",
  "abstract": "Linguistic borrowing is the phenomenon of transferring linguistic constructions (lexical, phonological, morphological, and syntactic) from a “donor” language to a “recipient” language as a result of contacts between communities speaking different languages. Borrowed words are found in all languages, and—in contrast to cognate relationships—borrowing relationships may exist across unrelated languages (for example, about 40% of Swahili’s vocabulary is borrowed from Arabic). In this paper, we develop a model of morpho-phonological transformations across languages with features based on universal constraints from Optimality Theory (OT). Compared to several standard— but linguistically naïve—baselines, our OTinspired model obtains good performance with only a few dozen training examples, making this a cost-effective strategy for sharing lexical information across languages.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N15-1117",
  "title": "Removing the Training Wheels: A Coreference Dataset that Entertains Humans and Challenges Computers",
  "year": 2015,
  "venue": "NAACL",
  "abstract": "Coreference is a core nlp problem. However, newswire data, the primary source of existing coreference data, lack the richness necessary to truly solve coreference. We present a new domain with denser references—quiz bowl questions—that is challenging and enjoyable to humans, and we use the quiz bowl community to develop a new coreference dataset, together with an annotation framework that can tag any text data with coreferences and named entities. We also successfully integrate active learning into this annotation pipeline to collect documents maximally useful to coreference models. State-of-the-art coreference systems underperform a simple classifier on our new dataset, motivating non-newswire data for future coreference research.",
  "stance": -0.3
 },
 {
  "url": "https://aclanthology.org/N15-1124",
  "title": "Accurate Evaluation of Segment-level Machine Translation Metrics",
  "year": 2015,
  "venue": "NAACL",
  "abstract": "Evaluation of segment-level machine translation metrics is currently hampered by: (1) low inter-annotator agreement levels in human assessments; (2) lack of an effective mechanism for evaluation of translations of equal quality; and (3) lack of methods of significance testing improvements over a baseline. In this paper, we provide solutions to each of these challenges and outline a new human evaluation methodology aimed specifically at assessment of segment-level metrics. We replicate the human evaluation component of WMT-13 and reveal that the current state-of-the-art performance of segment-level metrics is better than previously believed. Three segment-level metrics — METEOR, NLEPOR and SENTBLEUMOSES — are found to correlate with human assessment at a level not significantly outperformed by any other metric in both the individual language pair assessment for Spanish-toEnglish and the aggregated set of 9 language pairs.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/N15-1126",
  "title": "Why Read if You Can Scan? Trigger Scoping Strategy for Biographical Fact Extraction",
  "year": 2015,
  "venue": "NAACL",
  "abstract": "The rapid growth of information sources brings a unique challenge to biographical information extraction: how to find specific facts without having to read all the words. An effective solution is to follow the human scanning strategy which keeps a specific keyword in mind and searches within a specific scope. In this paper, we mimic a scanning process to extract biographical facts. We use event and relation triggers as keywords, identify their scopes and apply type constraints to extract answers within the scope of a trigger. Experiments demonstrate that our approach outperforms state-of-the-art methods up to 26% absolute gain in F-score without using any syntactic analysis or external knowledge bases.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N15-1132",
  "title": "Unsupervised Most Frequent Sense Detection using Word Embeddings",
  "year": 2015,
  "venue": "NAACL",
  "abstract": "An acid test for any new Word Sense Disambiguation (WSD) algorithm is its performance against the Most Frequent Sense (MFS). The field of WSD has found the MFS baseline very hard to beat. Clearly, if WSD researchers had access to MFS values, their striving to better this heuristic will push the WSD frontier. However, getting MFS values requires sense annotated corpus in enormous amounts, which is out of bounds for most languages, even if their WordNets are available. In this paper, we propose an unsupervised method for MFS detection from the untagged corpora, which exploits word embeddings. We compare the word embedding of a word with all its sense embeddings and obtain the predominant sense with the highest similarity. We observe significant performance gain for Hindi WSD over the WordNet First Sense (WFS) baseline. As for English, the SemCor baseline is bettered for those words whose frequency is greater than 2. Our approach is language and domain independent.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/N15-1160",
  "title": "Large-Scale Native Language Identification with Cross-Corpus Evaluation",
  "year": 2015,
  "venue": "NAACL",
  "abstract": "We present a large-scale Native Language Identification (NLI) experiment on new data, with a focus on cross-corpus evaluation to identify corpusand genre-independent language transfer features. We test a new corpus and show it is comparable to other NLI corpora and suitable for this task. Cross-corpus evaluation on two large corpora achieves good accuracy and evidences the existence of reliable language transfer features, but lower performance also suggests that NLI models are not completely portable across corpora. Finally, we present a brief case study of features distinguishing Japanese learners’ English writing, demonstrating the presence of cross-corpus and cross-genre language transfer features that are highly applicable to SLA and ESL research.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/N15-1161",
  "title": "Unediting: Detecting Disfluencies Without Careful Transcripts",
  "year": 2015,
  "venue": "NAACL",
  "abstract": "Speech transcripts often only capture semantic content, omitting disfluencies that can be useful for analyzing social dynamics of a discussion. This work describes steps in building a model that can recover a large fraction of locations where disfluencies were present, by transforming carefully annotated text to match the standard transcription style, introducing a two-stage model for handling different types of disfluencies, and applying semi-supervised learning. Experiments show improvement in disfluency detection on Supreme Court oral arguments, nearly 23% improvement in F1.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N15-1179",
  "title": "Do We Really Need Lexical Information? Towards a Top-down Approach to Sentiment Analysis of Product Reviews",
  "year": 2015,
  "venue": "NAACL",
  "abstract": "Most of the current approaches to sentiment analysis of product reviews are dependent on lexical sentiment information and proceed in a bottom-up way, adding new layers of features to lexical data. In this paper, we maintain that a typical product review is not a bag of sentiments, but a narrative with an underlying structure and reoccurring patterns, which allows us to predict its sentiments knowing only its general polarity and discourse cues that occur in it. We hypothesize that knowing only the review’s score and its discourse patterns would allow us to accurately predict the sentiments of its individual sentences. The experiments we conducted prove this hypothesis and show a substantial improvement over the lexical baseline.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/N16-1005",
  "title": "Controlling Politeness in Neural Machine Translation via Side Constraints",
  "year": 2016,
  "venue": "NAACL",
  "abstract": "Many languages use honorifics to express politeness, social distance, or the relative social status between the speaker and their addressee(s). In machine translation from a language without honorifics such as English, it is difficult to predict the appropriate honorific, but users may want to control the level of politeness in the output. In this paper, we perform a pilot study to control honorifics in neural machine translation (NMT) via side constraints, focusing on English→German. We show that by marking up the (English) source side of the training data with a feature that encodes the use of honorifics on the (German) target side, we can control the honorifics produced at test time. Experiments show that the choice of honorifics has a big impact on translation quality as measured by BLEU, and oracle experiments show that substantial improvements are possible by constraining the translation to the desired level of politeness.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N16-1016",
  "title": "A Long Short-Term Memory Framework for Predicting Humor in Dialogues",
  "year": 2016,
  "venue": "NAACL",
  "abstract": "We propose a first-ever attempt to employ a Long Short-Term memory based framework to predict humor in dialogues. We analyze data from a popular TV-sitcom, whose canned laughters give an indication of when the audience would react. We model the setuppunchline relation of conversational humor with a Long Short-Term Memory, with utterance encodings obtained from a Convolutional Neural Network. Out neural network framework is able to improve the F-score of 8% over a Conditional Random Field baseline. We show how the LSTM effectively models the setup-punchline relation reducing the number of false positives and increasing the recall. We aim to employ our humor prediction model to build effective empathetic machine able to understand jokes.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N16-1018",
  "title": "Counter-fitting Word Vectors to Linguistic Constraints",
  "year": 2016,
  "venue": "NAACL",
  "abstract": "In this work, we present a novel counter-fitting method which injects antonymy and synonymy constraints into vector space representations in order to improve the vectors’ capability for judging semantic similarity. Applying this method to publicly available pre-trained word vectors leads to a new state of the art performance on the SimLex-999 dataset. We also show how the method can be used to tailor the word vector space for the downstream task of dialogue state tracking, resulting in robust improvements across different dialogue domains.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N16-1038",
  "title": "Questioning Arbitrariness in Language: a Data-Driven Study of Conventional Iconicity",
  "year": 2016,
  "venue": "NAACL",
  "abstract": "This paper presents a data-driven investigation of phonesthemes, phonetic units said to carry meaning associations, thus challenging the traditionally assumed arbitrariness of language. Phonesthemes have received a substantial amount of attention within the cognitive science literature on sound iconicity, but nevertheless remain a controversial and understudied phenomenon. Here we employ NLP techniques to address two main questions: How can the existence of phonesthemes be tested at a large scale with quantitative methods? And how can the meaning arguably carried by a phonestheme be induced automatically from word embeddings? We develop novel methods to make progress on these fronts and compare our results to previous work, obtaining substantial improvements.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N16-1086",
  "title": "What to talk about and how? Selective Generation using LSTMs with Coarse-to-Fine Alignment",
  "year": 2016,
  "venue": "NAACL",
  "abstract": "We propose an end-to-end, domainindependent neural encoder-aligner-decoder model for selective generation, i.e., the joint task of content selection and surface realization. Our model first encodes a full set of over-determined database event records via an LSTM-based recurrent neural network, then utilizes a novel coarse-to-fine aligner to identify the small subset of salient records to talk about, and finally employs a decoder to generate free-form descriptions of the aligned, selected records. Our model achieves the best selection and generation results reported to-date (with 59% relative improvement in generation) on the benchmark WEATHERGOV dataset, despite using no specialized features or linguistic resources. Using an improved k-nearest neighbor beam filter helps further. We also perform a series of ablations and visualizations to elucidate the contributions of our key model components. Lastly, we evaluate the generalizability of our model on the ROBOCUP dataset, and get results that are competitive with or better than the state-of-the-art, despite being severely data-starved.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N16-1098",
  "title": "A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories",
  "year": 2016,
  "venue": "NAACL",
  "abstract": "Representation and learning of commonsense knowledge is one of the foundational problems in the quest to enable deep language understanding. This issue is particularly challenging for understanding casual and correlational relationships between events. While this topic has received a lot of interest in the NLP community, research has been hindered by the lack of a proper evaluation framework. This paper attempts to address this problem with a new framework for evaluating story understanding and script learning: the ‘Story Cloze Test’. This test requires a system to choose the correct ending to a four-sentence story. We created a new corpus of 50k five-sentence commonsense stories, ROCStories, to enable this evaluation. This corpus is unique in two ways: (1) it captures a rich set of causal and temporal commonsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation. Experimental evaluation shows that a host of baselines and state-of-the-art models based on shallow language understanding struggle to achieve a high score on the Story Cloze Test. We discuss these implications for script and story learning, and offer suggestions for deeper language understanding.",
  "stance": -0.7
 },
 {
  "url": "https://aclanthology.org/N18-1007",
  "title": "Parsing Speech: a Neural Approach to Integrating Lexical and Acoustic-Prosodic Information",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "In conversational speech, the acoustic signal provides cues that help listeners disambiguate difficult parses. For automatically parsing spoken utterances, we introduce a model that integrates transcribed text and acoustic-prosodic features using a convolutional neural network over energy and pitch trajectories coupled with an attention-based recurrent neural network that accepts text and prosodic features. We find that different types of acoustic-prosodic features are individually helpful, and together give statistically significant improvements in parse and disfluency detection F1 scores over a strong text-only baseline. For this study with known sentence boundaries, error analyses show that the main benefit of acoustic-prosodic features is in sentences with disfluencies, attachment decisions are most improved, and transcription errors obscure gains from prosody.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N18-1008",
  "title": "Tied Multitask Learning for Neural Speech Translation",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "We explore multitask models for neural translation of speech, augmenting them in order to reflect two intuitive notions. First, we introduce a model where the second task decoder receives information from the decoder of the first task, since higher-level intermediate representations should provide useful information. Second, we apply regularization that encourages transitivity and invertibility. We show that the application of these notions on jointly trained models improves performance on the tasks of low-resource speech transcription and translation. It also leads to better performance when using attention information for word discovery over unsegmented input.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N18-1010",
  "title": "Attentive Interaction Model: Modeling Changes in View in Argumentation",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "We present a neural architecture for modeling argumentative dialogue that explicitly models the interplay between an Opinion Holder's (OH's) reasoning and a challenger's argument, with the goal of predicting if the argument successfully changes the OH's view. The model has two components: (1) vulnerable region detection, an attention model that identifies parts of the OH's reasoning that are amenable to change, and (2) interaction encoding, which identifies the relationship between the content of the OH's reasoning and that of the challenger's argument. Based on evaluation on discussions from the Change My View forum on Reddit, the two components work together to predict an OH's change in view, outperforming several baselines. A posthoc analysis suggests that sentences picked out by the attention model are addressed more frequently by successful arguments than by unsuccessful ones.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N18-1011",
  "title": "Automatic Focus Annotation: Bringing Formal Pragmatics Alive in Analyzing the Information Structure of Authentic Data",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "Analyzing language in context, both from a theoretical and from a computational perspective, is receiving increased interest. Complementing the research in linguistics on discourse and information structure, in computational linguistics identifying discourse concepts was also shown to improve the performance of certain applications, for example, Short Answer Assessment systems (Ziai and Meurers, 2014). Building on the research that established detailed annotation guidelines for manual annotation of information structural concepts for written (Dipper et al., 2007; Ziai and Meurers, 2014) and spoken language data (Calhoun et al., 2010), this paper presents the first approach automating the analysis of focus in authentic written data. Our classification approach combines a range of lexical, syntactic, and semantic features to achieve an accuracy of 78.1% for identifying focus.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N18-1023",
  "title": "Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "We present a reading comprehension challenge in which questions can only be answered by taking into account information from multiple sentences. We solicit and verify questions and answers for this challenge through a 4-step crowdsourcing experiment. Our challenge dataset contains 6,500+ questions for 1000+ paragraphs across 7 different domains (elementary school science, news, travel guides, fiction stories, etc) bringing in linguistic diversity to the texts and to the questions wordings. On a subset of our dataset, we found human solvers to achieve an F1-score of 88.1%. We analyze a range of baselines, including a recent state-of-art reading comprehension system, and demonstrate the difficulty of this challenge, despite a high human performance. The dataset is the first to study multi-sentence inference at scale, with an open-ended set of question types that requires reasoning skills.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N18-1025",
  "title": "QuickEdit: Editing Text & Translations by Crossing Words Out",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "We propose a framework for computer-assisted text editing. It applies to translation post-editing and to paraphrasing. Our proposal relies on very simple interactions: a human editor modifies a sentence by marking tokens they would like the system to change. Our model then generates a new sentence which reformulates the initial sentence by avoiding marked words. The approach builds upon neural sequence-to-sequence modeling and introduces a neural network which takes as input a sentence along with change markers. Our model is trained on translation bitext by simulating post-edits. We demonstrate the advantage of our approach for translation post-editing through simulated post-edits. We also evaluate our model for paraphrasing through a user study.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/N18-1086",
  "title": "Neural Syntactic Generative Models with Exact Marginalization",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "We present neural syntactic generative models with exact marginalization that support both dependency parsing and language modeling. Exact marginalization is made tractable through dynamic programming over shift-reduce parsing and minimal RNN-based feature sets. Our algorithms complement previous approaches by supporting batched training and enabling online computation of next word probabilities. For supervised dependency parsing, our model achieves a state-of-the-art result among generative approaches. We also report empirical results on unsupervised syntactic models and their role in language modeling. We find that our model formulation of latent dependencies with exact marginalization do not lead to better intrinsic language modeling performance than vanilla RNNs, and that parsing accuracy is not correlated with language modeling perplexity in stack-based models.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/N18-1175",
  "title": "The Argument Reasoning Comprehension Task: Identification and Reconstruction of Implicit Warrants",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "Reasoning is a crucial part of natural language argumentation. To comprehend an argument, one must analyze its warrant, which explains why its claim follows from its premises. As arguments are highly contextualized, warrants are usually presupposed and left implicit. Thus, the comprehension does not only require language understanding and logic skills, but also depends on common sense. In this paper we develop a methodology for reconstructing warrants systematically. We operationalize it in a scalable crowdsourcing process, resulting in a freely licensed dataset with warrants for 2k authentic arguments from news comments. On this basis, we present a new challenging task, the argument reasoning comprehension task. Given an argument with a claim and a premise, the goal is to choose the correct implicit warrant from two options. Both warrants are plausible and lexically close, but lead to contradicting claims. A solution to this task will define a substantial step towards automatic warrant reconstruction. However, experiments with several neural attention and language models reveal that current approaches do not suffice.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/N18-1195",
  "title": "Towards Understanding Text Factors in Oral Reading",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "Using a case study, we show that variation in oral reading rate across passages for professional narrators is consistent across readers and much of it can be explained using features of the texts being read. While text complexity is a poor predictor of the reading rate, a substantial share of variability can be explained by timing and story-based factors with performance reaching r=0.75 for unseen passages and narrator.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/N18-1200",
  "title": "Speaker Naming in Movies",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "We propose a new model for speaker naming in movies that leverages visual, textual, and acoustic modalities in an unified optimization framework. To evaluate the performance of our model, we introduce a new dataset consisting of six episodes of the Big Bang Theory TV show and eighteen full movies covering different genres. Our experiments show that our multimodal model significantly outperforms several competitive baselines on the average weighted F-score metric. To demonstrate the effectiveness of our framework, we design an end-to-end memory network model that leverages our speaker naming model and achieves state-of-the-art results on the subtitles task of the MovieQA 2017 Challenge.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N18-2005",
  "title": "Is Something Better than Nothing? Automatically Predicting Stance-based Arguments Using Deep Learning and Small Labelled Dataset",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "Online reviews have become a popular portal among customers making decisions about purchasing products. A number of corpora of reviews have been widely investigated in NLP in general, and, in particular, in argument mining. This is a subset of NLP that deals with extracting arguments and the relations among them from user-based content. A major problem faced by argument mining research is the lack of human-annotated data. In this paper, we investigate the use of weakly supervised and semi-supervised methods for automatically annotating data, and thus providing large annotated datasets. We do this by building on previous work that explores the classification of opinions present in reviews based whether the stance is expressed explicitly or implicitly. In the work described here, we automatically annotate stance as implicit or explicit and our results show that the datasets we generate, although noisy, can be used to learn better models for implicit/explicit opinion classification.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/N18-2015",
  "title": "A Simple and Effective Approach to the Story Cloze Test",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "In the Story Cloze Test, a system is presented with a 4-sentence prompt to a story, and must determine which one of two potential endings is the 'right' ending to the story. Previous work has shown that ignoring the training set and training a model on the validation set can achieve high accuracy on this task due to stylistic differences between the story endings in the training set and validation and test sets. Following this approach, we present a simpler fully-neural approach to the Story Cloze Test using skip-thought embeddings of the stories in a feed-forward network that achieves close to state-of-the-art performance on this task without any feature engineering. We also find that considering just the last sentence of the prompt instead of the whole prompt yields higher accuracy with our approach.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N18-2021",
  "title": "Training Structured Prediction Energy Networks with Indirect Supervision",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "This paper introduces rank-based training of structured prediction energy networks (SPENs). Our method samples from output structures using gradient descent and minimizes the ranking violation of the sampled structures with respect to a scalar scoring function defined with domain knowledge. We have successfully trained SPEN for citation field extraction without any labeled data instances, where the only source of supervision is a simple human-written scoring function. Such scoring functions are often easy to provide; the SPEN then furnishes an efficient structured prediction inference procedure.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N18-2022",
  "title": "Si O No, Que Penses? Catalonian Independence and Linguistic Identity on Social Media",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "Political identity is often manifested in language variation, but the relationship between the two is still relatively unexplored from a quantitative perspective. This study examines the use of Catalan, a language local to the semi-autonomous region of Catalonia in Spain, on Twitter in discourse related to the 2017 independence referendum. We corroborate prior findings that pro-independence tweets are more likely to include the local language than anti-independence tweets. We also find that Catalan is used more often in referendum-related discourse than in other contexts, contrary to prior findings on language variation. This suggests a strong role for the Catalan language in the expression of Catalonian political identity.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/N18-2041",
  "title": "Knowledge-Enriched Two-Layered Attention Network for Sentiment Analysis",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "We propose a novel two-layered attention network based on Bidirectional Long Short-Term Memory for sentiment analysis. The novel two-layered attention network takes advantage of the external knowledge bases to improve the sentiment prediction. It uses the Knowledge Graph Embedding generated using the WordNet. We build our model by combining the two-layered attention network with the supervised model based on Support Vector Regression using a Multilayer Perceptron network for sentiment analysis. We evaluate our model on the benchmark dataset of SemEval 2017 Task 5. Experimental results show that the proposed model surpasses the top system of SemEval 2017 Task 5. The model performs significantly better by improving the state-of-the-art system at SemEval 2017 Task 5 by 1.7 and 3.7 points for sub-tracks 1 and 2 respectively.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N18-2085",
  "title": "Are All Languages Equally Hard to Language-Model?",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "For general modeling methods applied to diverse languages, a natural question is: how well should we expect our models to work on languages with differing typological profiles? In this work, we develop an evaluation framework for fair cross-linguistic comparison of language models, using translated text so that all models are asked to predict approximately the same information. We then conduct a study on 21 languages, demonstrating that in some languages, the textual expression of the information is harder to predict with both n-gram and LSTM language models. We show complex inflectional morphology to be a cause of performance differences among languages.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N18-2113",
  "title": "Evaluating Historical Text Normalization Systems: How Well Do They Generalize?",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "We highlight several issues in the evaluation of historical text normalization systems that make it hard to tell how well these systems would actually work in practice-i.e., for new datasets or languages; in comparison to more naïve systems; or as a preprocessing step for downstream NLP tools. We illustrate these issues and exemplify our proposed evaluation practices by comparing two neural models against a naïve baseline system. We show that the neural models generalize well to unseen words in tests on five languages; nevertheless, they provide no clear benefit over the naïve baseline for downstream POS tagging of an English historical collection. We conclude that future work should include more rigorous evaluation, including both intrinsic and extrinsic measures where possible.",
  "stance": -0.7
 },
 {
  "url": "https://aclanthology.org/N18-2118",
  "title": "Slot-Gated Modeling for Joint Slot Filling and Intent Prediction",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "Attention-based recurrent neural network models for joint intent detection and slot filling have achieved the state-of-the-art performance, while they have independent attention weights. Considering that slot and intent have the strong relationship, this paper proposes a slot gate that focuses on learning the relationship between intent and slot attention vectors in order to obtain better semantic frame results by the global optimization. The experiments show that our proposed model significantly improves sentence-level semantic frame accuracy with 4.2% and 1.9% relative improvement compared to the attentional model on benchmark ATIS and Snips datasets respectively",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N18-2123",
  "title": "Visual Referring Expression Recognition: What Do Systems Actually Learn?",
  "year": 2018,
  "venue": "NAACL",
  "abstract": "We present an empirical analysis of state-of-the-art systems for referring expression recognition - the task of identifying the object in an image referred to by a natural language expression - with the goal of gaining insight into how these systems reason about language and vision. Surprisingly, we find strong evidence that even sophisticated and linguistically-motivated models for this task may ignore linguistic structure, instead relying on shallow correlations introduced by unintended biases in the data selection and annotation process. For example, we show that a system trained and tested on the input image without the input referring expression can achieve a precision of 71.2% in top-2 predictions. Furthermore, a system that predicts only the object category given the input can achieve a precision of 84.2% in top-2 predictions. These surprisingly positive results for what should be deficient prediction scenarios suggest that careful analysis of what our models are learning - and further, how our data is constructed - is critical as we seek to make substantive progress on grounded language tasks.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/N19-1069",
  "title": "Adversarial Training for Satire Detection: Controlling for Confounding Variables",
  "year": 2019,
  "venue": "NAACL",
  "abstract": "The automatic detection of satire vs. regular news is relevant for downstream applications (for instance, knowledge base population) and to improve the understanding of linguistic characteristics of satire. Recent approaches build upon corpora which have been labeled automatically based on article sources. We hypothesize that this encourages the models to learn characteristics for different publication sources (e.g., \"The Onion\" vs. \"The Guardian\") rather than characteristics of satire, leading to poor generalization performance to unseen publication sources. We therefore propose a novel model for satire detection with an adversarial component to control for the confounding variable of publication source. On a large novel data set collected from German news (which we make available to the research community), we observe comparable satire classification performance and, as desired, a considerable drop in publication classification performance with adversarial training. Our analysis shows that the adversarial component is crucial for the model to learn to pay attention to linguistic properties of satire.",
  "stance": -0.3
 },
 {
  "url": "https://aclanthology.org/N19-1070",
  "title": "Keyphrase Generation: A Text Summarization Struggle",
  "year": 2019,
  "venue": "NAACL",
  "abstract": "Authors' keyphrases assigned to scientific articles are essential for recognizing content and topic aspects. Most of the proposed supervised and unsupervised methods for keyphrase generation are unable to produce terms that are valuable but do not appear in the text. In this paper, we explore the possibility of considering the keyphrase string as an abstractive summary of the title and the abstract. First, we collect, process and release a large dataset of scientific paper metadata that contains 2.2 million records. Then we experiment with popular text summarization neural architectures. Despite using advanced deep learning models, large quantities of data and many days of computation, our systematic evaluation on four test datasets reveals that the explored text summarization methods could not produce better keyphrases than the simpler unsupervised methods, or the existing supervised ones.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/N19-1132",
  "title": "Cross-Corpora Evaluation and Analysis of Grammatical Error Correction Models - Is Single-Corpus Evaluation Enough?",
  "year": 2019,
  "venue": "NAACL",
  "abstract": "This study explores the necessity of performing cross-corpora evaluation for grammatical error correction (GEC) models. GEC models have been previously evaluated based on a single commonly applied corpus: the CoNLL-2014 benchmark. However, the evaluation remains incomplete because the task difficulty varies depending on the test corpus and conditions such as the proficiency levels of the writers and essay topics. To overcome this limitation, we evaluate the performance of several GEC models, including NMT-based (LSTM, CNN, and transformer) and an SMT-based model, against various learner corpora (CoNLL-2013, CoNLL-2014, FCE, JFLEG, ICNALE, and KJ). Evaluation results reveal that the models' rankings considerably vary depending on the corpus, indicating that single-corpus evaluation is insufficient for GEC models.",
  "stance": -0.5
 },
 {
  "url": "https://aclanthology.org/N19-1143",
  "title": "Vector of Locally Aggregated Embeddings for Text Representation",
  "year": 2019,
  "venue": "NAACL",
  "abstract": "We present Vector of Locally Aggregated Embeddings (VLAE) for effective and, ultimately, lossless representation of textual content. Our model encodes each input text by effectively identifying and integrating the representations of its semantically-relevant parts. The proposed model generates high quality representation of textual content and improves the classification performance of current state-of-the-art deep averaging networks across several text classification tasks.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N19-1145",
  "title": "Biomedical Event Extraction based on Knowledge-driven Tree-LSTM",
  "year": 2019,
  "venue": "NAACL",
  "abstract": "Event extraction for the biomedical domain is more challenging than that in the general news domain since it requires broader acquisition of domain-specific knowledge and deeper understanding of complex contexts. To better encode contextual information and external background knowledge, we propose a novel knowledge base (KB)-driven tree-structured long short-term memory networks (Tree-LSTM) framework, incorporating two new types of features: (1) dependency structures to capture wide contexts; (2) entity properties (types and category descriptions) from external ontologies via entity linking. We evaluate our approach on the BioNLP shared task with Genia dataset and achieve a new state-of-the-art result. In addition, both quantitative and qualitative studies demonstrate the advancement of the Tree-LSTM and the external knowledge representation for biomedical event extraction.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N19-1165",
  "title": "Text Processing Like Humans Do: Visually Attacking and Shielding NLP Systems",
  "year": 2019,
  "venue": "NAACL",
  "abstract": "Visual modifications to text are often used to obfuscate offensive comments in social media (e.g., \"!d10t\") or as a writing style (\"1337\" in \"leet speak\"), among other scenarios. We consider this as a new type of adversarial attack in NLP, a setting to which humans are very robust, as our experiments with both simple and more difficult visual perturbations demonstrate. We investigate the impact of visual adversarial attacks on current NLP systems on character-, word-, and sentence-level tasks, showing that both neural and non-neural models are, in contrast to humans, extremely sensitive to such attacks, suffering performance decreases of up to 82%. We then explore three shielding methods-visual character embeddings, adversarial training, and rule-based recovery-which substantially improve the robustness of the models. However, the shielding methods still fall behind performances achieved in non-attack scenarios, which demonstrates the difficulty of dealing with visual attacks.",
  "stance": -0.3
 },
 {
  "url": "https://aclanthology.org/N19-1233",
  "title": "Evaluating Text GANs as Language Models",
  "year": 2019,
  "venue": "NAACL",
  "abstract": "Generative Adversarial Networks (GANs) are a promising approach for text generation that, unlike traditional language models (LM), does not suffer from the problem of \"exposure bias\". However, A major hurdle for understanding the potential of GANs for text generation is the lack of a clear evaluation metric. In this work, we propose to approximate the distribution of text generated by a GAN, which permits evaluating them with traditional probability-based LM metrics. We apply our approximation procedure on several GAN-based models and show that they currently perform substantially worse than state-of-the-art LMs. Our evaluation procedure promotes better understanding of the relation between GANs and LMs, and can accelerate progress in GAN-based text generation.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/N19-1237",
  "title": "Evaluating Rewards for Question Generation Models",
  "year": 2019,
  "venue": "NAACL",
  "abstract": "Recent approaches to question generation have used modifications to a Seq2Seq architecture inspired by advances in machine translation. Models are trained using teacher forcing to optimise only the one-step-ahead prediction. However, at test time, the model is asked to generate a whole sequence, causing errors to propagate through the generation process (exposure bias). A number of authors have suggested that optimising for rewards less tightly coupled to the training data might counter this mismatch. We therefore optimise directly for various objectives beyond simply replicating the ground truth questions, including a novel approach using an adversarial discriminator that seeks to generate questions that are indistinguishable from real examples. We confirm that training with policy gradient methods leads to increases in the metrics used as rewards. We perform a human evaluation, and show that although these metrics have previously been assumed to be good proxies for question quality, they are poorly aligned with human judgement and the model simply learns to exploit the weaknesses of the reward source.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/N19-1245",
  "title": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms",
  "year": 2019,
  "venue": "NAACL",
  "abstract": "We introduce a large-scale dataset of math word problems and an interpretable neural math problem solver by learning to map problems to their operation programs. Due to annotation challenges, current datasets in this domain have been either relatively small in scale or did not offer precise operational annotations over diverse problem types. We introduce a new representation language to model operation programs corresponding to each math problem that aim to improve both the performance and the interpretability of the learned models. Using this representation language, we significantly enhance the AQUA-RAT dataset with fully-specified operational programs. We additionally introduce a neural sequence-to-program model with automatic problem categorization. Our experiments show improvements over competitive baselines in our dataset as well as the AQUA-RAT dataset. The results are still lower than human performance indicating that the dataset poses new challenges for future research. Our dataset is available at https://math-qa.github.io/math-QA/",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/N19-1252",
  "title": "A Grounded Unsupervised Universal Part-of-Speech Tagger for Low-Resource Languages",
  "year": 2019,
  "venue": "NAACL",
  "abstract": "Unsupervised part of speech (POS) tagging is often framed as a clustering problem, but practical taggers need to ground their clusters as well. Grounding generally requires reference labeled data, a luxury a low-resource language might not have. In this work, we describe an approach for low-resource unsupervised POS tagging that yields fully grounded output and requires no labeled training data. We find the classic method of Brown et al. (1992) clusters well in our use case and employ a decipherment-based approach to grounding. This approach presumes a sequence of cluster IDs is a 'ciphertext' and seeks a POS tag-to-cluster ID mapping that will reveal the POS sequence. We show intrinsically that, despite the difficulty of the task, we obtain reasonable performance across a variety of languages. We also show extrinsically that incorporating our POS tagger into a name tagger leads to state-of-the-art tagging performance in Sinhalese and Kinyarwanda, two languages with nearly no labeled POS data available. We further demonstrate our tagger's utility by incorporating it into a true 'zero-resource' variant of the MALOPA (Ammar et al., 2016) dependency parser model that removes the current reliance on multilingual resources and gold POS tags for new languages. Experiments show that including our tagger makes up much of the accuracy lost when gold POS tags are unavailable.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/N19-1298",
  "title": "A Richer-but-Smarter Shortest Dependency Path with Attentive Augmentation for Relation Extraction",
  "year": 2019,
  "venue": "NAACL",
  "abstract": "To extract the relationship between two entities in a sentence, two common approaches are (1) using their shortest dependency path (SDP) and (2) using an attention model to capture a context-based representation of the sentence. Each approach suffers from its own disadvantage of either missing or redundant information. In this work, we propose a novel model that combines the advantages of these two approaches. This is based on the basic information in the SDP enhanced with information selected by several attention mechanisms with kernel filters, namely RbSP (Richer-but-Smarter SDP). To exploit the representation behind the RbSP structure effectively, we develop a combined deep neural model with a LSTM network on word sequences and a CNN on RbSP. Experimental results on the SemEval-2010 dataset demonstrate improved performance over competitive baselines. The data and source code are available at https://github.com/catcd/RbSP.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/N19-1320",
  "title": "Reinforcement Learning Based Text Style Transfer without Parallel Training Corpus",
  "year": 2019,
  "venue": "NAACL",
  "abstract": "Text style transfer rephrases a text from a source style (e.g., informal) to a target style (e.g., formal) while keeping its original meaning. Despite the success existing works have achieved using a parallel corpus for the two styles, transferring text style has proven significantly more challenging when there is no parallel training corpus. In this paper, we address this challenge by using a reinforcement-learning-based generator-evaluator architecture. Our generator employs an attention-based encoder-decoder to transfer a sentence from the source style to the target style. Our evaluator is an adversarially trained style discriminator with semantic and syntactic constraints that score the generated sentence for style, meaning preservation, and fluency. Experimental results on two different style transfer tasks-sentiment transfer, and formality transfer-show that our model outperforms state-of-the-art approaches.Furthermore, we perform a manual evaluation that demonstrates the effectiveness of the proposed method using subjective metrics of generated text quality.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/N19-1378",
  "title": "What do Entity-Centric Models Learn? Insights from Entity Linking in Multi-Party Dialogue",
  "year": 2019,
  "venue": "NAACL",
  "abstract": "Humans use language to refer to entities in the external world. Motivated by this, in recent years several models that incorporate a bias towards learning entity representations have been proposed. Such entity-centric models have shown empirical success, but we still know little about why. In this paper we analyze the behavior of two recently proposed entity-centric models in a referential task, Entity Linking in Multi-party Dialogue (SemEval 2018 Task 4). We show that these models outperform the state of the art on this task, and that they do better on lower frequency entities than a counterpart model that is not entity-centric, with the same model size. We argue that making models entity-centric naturally fosters good architectural decisions. However, we also show that these models do not really build entity representations and that they make poor use of linguistic context. These negative results underscore the need for model analysis, to test whether the motivations for particular architectures are borne out in how models behave when deployed.",
  "stance": -0.5
 },
 {
  "url": "https://aclanthology.org/N19-1396",
  "title": "Understanding the Behaviour of Neural Abstractive Summarizers using Contrastive Examples",
  "year": 2019,
  "venue": "NAACL",
  "abstract": "Neural abstractive summarizers generate summary texts using a language model conditioned on the input source text, and have recently achieved high ROUGE scores on benchmark summarization datasets. We investigate how they achieve this performance with respect to human-written gold-standard abstracts, and whether the systems are able to understand deeper syntactic and semantic structures. We generate a set of contrastive summaries which are perturbed, deficient versions of human-written summaries, and test whether existing neural summarizers score them more highly than the human-written summaries. We analyze their performance on different datasets and find that these systems fail to understand the source text, in a majority of the cases.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/N19-1422",
  "title": "Probing the Need for Visual Context in Multimodal Machine Translation",
  "year": 2019,
  "venue": "NAACL",
  "abstract": "Current work on multimodal machine translation (MMT) has suggested that the visual modality is either unnecessary or only marginally beneficial. We posit that this is a consequence of the very simple, short and repetitive sentences used in the only available dataset for the task (Multi30K), rendering the source text sufficient as context. In the general case, however, we believe that it is possible to combine visual and textual information in order to ground translations. In this paper we probe the contribution of the visual modality to state-of-the-art MMT models by conducting a systematic analysis where we partially deprive the models from source-side textual context. Our results show that under limited textual context, models are capable of leveraging the visual input to generate better translations. This contradicts the current belief that MMT models disregard the visual modality because of either the quality of the image features or the way they are integrated into the model.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/S01-1004",
  "title": "English Lexical Sample Task Description",
  "year": 2001,
  "venue": "SemEval",
  "abstract": "The English lexical sample task (adjectives and nouns) for SENSEVAL 2 was set up according to the same principles as for SENSEVAL1, as reported in (Kilgarriff and Rosenzweig, 2000). (Adjectives and nouns only, because the data preparation for the verbs lexical sample was undertaken alongside that for the English all-words task, and is reported in Palmer et al (this volume). All discussion below up to the Results section covers only adjectives and nouns.)",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/S01-1007",
  "title": "The Italian Lexical Sample Task",
  "year": 2001,
  "venue": "SemEval",
  "abstract": "In this paper we give an overall description of the Italian lexical sample task for SENSEV AL-2, together with some general reflections about on the one hand the overall task of lexical-semantic annotation and on the other about the adequacy of existing lexical-semantic reference resources.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S01-1012",
  "title": "The SENSEVAL-2 Panel on Domains, Topics and Senses",
  "year": 2001,
  "venue": "SemEval",
  "abstract": "An important aspect of sense disambiguation is the wider semantic space (domain, topic) in which the ambiguous word occurs. This may be most clearly illustrated by some cross-lingual examples, as they would appear in (machine) translation. Consider for instance the English word housing. In a more general \"sense\", this translates in German into Wohnung. In an engineering setting however it translates into Gehiiuse. Also verbs may be translated differently (i.e. have a different sense) according to the semantic space in which they occur. For instance, English warming up translates into erhitzen in a more general sense, but into aufwiinnen in the sports domain. Because of the apparent relevance then of domains or topics on sense disambiguation, a panel was organized at SENSEV AL-2 to discuss some current and previous work in this area. The paper presents a more extended overview based on the relevant literature, besides giving a summary of the discussion that developed after the panel presentations.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/W04-0814",
  "title": "The University of Amsterdam at Senseval-3: Semantic roles and Logic forms",
  "year": 2004,
  "venue": "SemEval",
  "abstract": "We describe our participation in two of the tasks organized within Senseval-3: Automatic Labeling of Semantic Roles and Identification of Logic Forms in English.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/W04-0863",
  "title": "Joining forces to resolve lexical ambiguity: East meets West in Barcelona",
  "year": 2004,
  "venue": "SemEval",
  "abstract": "This paper describes the component models and combination model built as a joint effort between Swarthmore College, Hong Kong PolyU, and HKUST. Though other models described elsewhere contributed to the final combination model, this paper focuses solely on the joint contributions to the ”Swat-HK” effort.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S07-1011",
  "title": "SemEval-2007 Task 12: Turkish Lexical Sample Task",
  "year": 2007,
  "venue": "SemEval",
  "abstract": "This paper presents the task definition, resources, and the single participant system for Task 12: Turkish Lexical Sample Task (TLST), which was organized in the SemEval-2007 evaluation exercise. The methodology followed for developing the specific linguistic resources necessary for the task has been described in this context. A language-specific feature set was defined for Turkish. TLST consists of three pieces of data: The dictionary, the training data, and the evaluation data. Finally, a single system that utilizes a simple statistical method was submitted for the task and evaluated.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/S07-1033",
  "title": "GYDER: Maxent Metonymy Resolution",
  "year": 2007,
  "venue": "SemEval",
  "abstract": "Though the GYDER system has achieved the highest accuracy scores for the metonymy resolution shared task at SemEval-2007 in all six subtasks, we don’t consider the results (72.80% accuracy for org, 84.36% for loc) particularly impressive, and argue that metonymy resolution needs more features.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/S07-1055",
  "title": "OE: WSD Using Optimal Ensembling (OE) Method",
  "year": 2007,
  "venue": "SemEval",
  "abstract": "Optimal ensembling (OE) is a word sense disambiguation (WSD) method using word-specific training factors (average positive vs negative training per sense, posex and negex) to predict best system (classifier algorithm / applicable feature set) for given target word. Our official entry (OE1) in Senseval-4 Task 17 (coarse-grained English lexical sample task) contained many design flaws and thus failed to show the whole potential of the method, finishing -4.9% behind top system (+0.5 gain over best base system). A fixed system (OE2) finished only -3.4% behind (+2.0% net gain). All our systems were 'closed', i.e. used the official training data only (average 56 training examples per each sense). We also show that the official evaluation measure tends to favor systems that do well with high-trained words.",
  "stance": -0.4
 },
 {
  "url": "https://aclanthology.org/S07-1087",
  "title": "UMND2 : SenseClusters Applied to the Sense Induction Task of Senseval-4",
  "year": 2007,
  "venue": "SemEval",
  "abstract": "SenseClusters is a freely–available open– source system that served as the University of Minnesota, Duluth entry in the SENSEVAL-4 sense induction task. For this task SenseClusters was configured to construct representations of the instances to be clustered using the centroid of word cooccurrence vectors that replace the words in an instance. These instances are then clustered using k–means where the number of clusters is discovered automatically using the Adapted Gap Statistic. In these experiments SenseClusters did not use any information outside of the raw untagged text that was to be clustered, and no tuning of the system was performed using external corpora.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/S07-1100",
  "title": "USYD: WSD and Lexical Substitution using the Web1T corpus",
  "year": 2007,
  "venue": "SemEval",
  "abstract": "This paper describes the University of Sydney’s WSD and Lexical Substitution systems for SemEval-2007. These systems are principally based on evaluating the substitutability of potential synonyms in the context of the target word. Substitutability is measured using Pointwise Mutual Information as obtained from the Web1T corpus. The WSD systems are supervised, while the Lexical Substitution system is unsupervised. The lexical sample sub-task also used syntactic category information given from a CCG-based parse to assist in verb disambiguation, while both WSD tasks also make use of more traditional features. These related systems participated in the Coarse-Grained English All-Words WSD task (task 7), the Lexical Substitution Task (task 10) and the English Lexical Sample WSD sub-task (task 17).",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/S10-1005",
  "title": "SemEval-2010 Task 7: Argument Selection and Coercion",
  "year": 2010,
  "venue": "SemEval",
  "abstract": "We describe the Argument Selection and Coercion task for the SemEval-2010 evaluation exercise. This task involves characterizing the type of compositional operation that exists between a predicate and the arguments it selects. Specifically, the goal is to identify whether the type that a verb selects is satisfied directly by the argument, or whether the argument must change type to satisfy the verb typing. We discuss the problem in detail, describe the data preparation for the task, and analyze the results of the submissions.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/S10-1013",
  "title": "SemEval-2010 Task 17: All-Words Word Sense Disambiguation on a Specific Domain",
  "year": 2010,
  "venue": "SemEval",
  "abstract": "Domain portability and adaptation of NLP components and Word Sense Disambiguation systems present new challenges. The difficulties found by supervised systems to adapt might change the way we assess the strengths and weaknesses of supervised and knowledge-based WSD systems. Unfortunately, all existing evaluation datasets for specific domains are lexical-sample corpora. This task presented all-words datasets on the environment domain for WSD in four languages (Chinese, Dutch, English, Italian). 11 teams participated, with supervised and knowledge-based systems, mainly in the English dataset. The results show that in all languages the participants where able to beat the most frequent sense heuristic as estimated from general corpora. The most successful approaches used some sort of supervision in the form of hand-tagged examples from the domain.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/S10-1018",
  "title": "SUCRE: A Modular System for Coreference Resolution",
  "year": 2010,
  "venue": "SemEval",
  "abstract": "This paper presents SUCRE, a new software tool for coreference resolution and its feature engineering. It is able to separately do noun, pronoun and full coreference resolution. SUCRE introduces a new approach to the feature engineering of coreference resolution based on a relational database model and a regular feature definition language. SUCRE successfully participated in SemEval-2010 Task 1 on Coreference Resolution in Multiple Languages (Recasens et al., 2010) for gold and regular closed annotation tracks of six languages. It obtained the best results in several categories, including the regular closed annotation tracks of English and German.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S10-1044",
  "title": "FBK_NK: A WordNet-Based System for Multi-Way Classification of Semantic Relations",
  "year": 2010,
  "venue": "SemEval",
  "abstract": "We describe a WordNet-based system for the extraction of semantic relations between pairs of nominals appearing in English texts. The system adopts a lightweight approach, based on training a Bayesian Network classifier using large sets of binary features. Our features consider: i) the context surrounding the annotated nominals, and ii) different types of knowledge extracted from WordNet, including direct and explicit relations between the annotated nominals, and more general and implicit evidence (e.g. semantic boundary collocations). The system achieved a Macro-averaged F1 of 68.02% on the “Multi-Way Classification of Semantic Relations Between Pairs of Nominals” task (Task #8) at SemEval-2010.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S10-1067",
  "title": "PKU_HIT: An Event Detection System Based on Instances Expansion and Rich Syntactic Features",
  "year": 2010,
  "venue": "SemEval",
  "abstract": "This paper describes the PKU_HIT system on event detection in the SemEval-2010 Task. We construct three modules for the three sub-tasks of this evaluation. For target verb WSD, we build a Naïve Bayesian classifier which uses additional training instances expanded from an untagged Chinese corpus automatically. For sentence SRL and event detection, we use a feature-based machine learning method which makes combined use of both constituent-based and dependencybased features. Experimental results show that the Macro Accuracy of the WSD module reaches 83.81% and F-Score of the SRL module is 55.71%.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S12-1076",
  "title": "ETS: Discriminative Edit Models for Paraphrase Scoring",
  "year": 2012,
  "venue": "SemEval",
  "abstract": "Many problems in natural language processing can be viewed as variations of the task of measuring the semantic textual similarity between short texts. However, many systems that address these tasks focus on a single task and may or may not generalize well. In this work, we extend an existing machine translation metric, TERp (Snover et al., 2009a), by adding support for more detailed feature types and by implementing a discriminative learning algorithm. These additions facilitate applications of our system, called PERP, to similarity tasks other than machine translation evaluation, such as paraphrase recognition. In the SemEval 2012 Semantic Textual Similarity task, PERP performed competitively, particularly at the two surprise subtasks revealed shortly before the submission deadline.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/S13-1037",
  "title": "More Words and Bigger Pictures",
  "year": 2013,
  "venue": "SemEval",
  "abstract": "Object recognition is a little like translation: a picture (text in a source language) goes in, and a description (text in a target language) comes out. I will use this analogy, which has proven fertile, to describe recent progress in object recognition. We have very good methods to spot some objects in images, but extending these methods to produce descriptions of images remains very difficult. The description might come in the form of a set of words, indicating objects, and boxes or regions spanned by the object. This representation is difficult to work with, because some objects seem to be much more important than others, and because objects interact. An alternative is a sentence or a paragraph describing the picture, and recent work indicates how one might generate rich structures like this. Furthermore, recent work suggests that it is easier and more effective to generate descriptions of images in terms of chunks of meaning (”person on a horse”) rather than just objects (”person”; ”horse”). Finally, if the picture contains objects that are unfamiliar, then we need to generate useful descriptions that will make it possible to interact with them, even though we don’t know what they are. ",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/S13-2018",
  "title": "IIRG: A Naive Approach to Evaluating Phrasal Semantics",
  "year": 2013,
  "venue": "SemEval",
  "abstract": "This paper describes the IIRG 1 system entered in SemEval-2013, the 7th International Workshop on Semantic Evaluation. We participated in Task 5 Evaluating Phrasal Semantics. We have adopted a token-based approach to solve this task using 1) Naı̈ve Bayes methods and 2) Word Overlap methods, both of which rely on the extraction of syntactic features. We found that the word overlap method significantly out-performs the Naı̈ve Bayes methods, achieving our highest overall score with an accuracy of approximately 78%.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S13-2108",
  "title": "UTurku: Drug Named Entity Recognition and Drug-Drug Interaction Extraction Using SVM Classification and Domain Knowledge",
  "year": 2013,
  "venue": "SemEval",
  "abstract": "The DDIExtraction 2013 task in the SemEval conference concerns the detection of drug names and statements of drug-drug interactions (DDI) from text. Extraction of DDIs is important for providing up-to-date knowledge on adverse interactions between coadministered drugs. We apply the machine learning based Turku Event Extraction System to both tasks. We evaluate three feature sets, syntactic features derived from deep parsing, enhanced optionally with features derived from DrugBank or from both DrugBank and MetaMap. TEES achieves F-scores of 60% for the drug name recognition task and 59% for the DDI extraction task.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/S14-1001",
  "title": "More or less supervised supersense tagging of Twitter",
  "year": 2014,
  "venue": "SemEval",
  "abstract": "We present two Twitter datasets annotated with coarse-grained word senses (supersenses), as well as a series of experiments with three learning scenarios for supersense tagging: weakly supervised learning, as well as unsupervised and supervised domain adaptation. We show that (a) off-the-shelf tools perform poorly on Twitter, (b) models augmented with embeddings learned from Twitter data perform much better, and (c) errors can be reduced using type-constrained inference with distant supervision from WordNet.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S15-2153",
  "title": "SemEval 2015 Task 18: Broad-Coverage Semantic Dependency Parsing",
  "year": 2015,
  "venue": "SemEval",
  "abstract": "Task 18 at SemEval 2015 defines BroadCoverage Semantic Dependency Parsing (SDP) as the problem of recovering sentence-internal predicate–argument relationships for all content words, i.e. the semantic structure constituting the relational core of sentence meaning. In this task description, we position the problem in comparison to other language analysis sub-tasks, introduce and compare the semantic dependency target representations used, and summarize the task setup, participating systems, and main results.",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/S15-2162",
  "title": "Lisbon: Evaluating TurboSemanticParser on Multiple Languages and Out-of-Domain Data",
  "year": 2015,
  "venue": "SemEval",
  "abstract": "As part of the SemEval-2015 shared task on Broad-Coverage Semantic Dependency Parsing, we evaluate the performace of our last year’s system (TurboSemanticParser) on multiple languages and out-of-domain data. Our system is characterized by a feature-rich linear model, that includes scores for first and second-order dependencies (arcs, siblings, grandparents and co-parents). For decoding this second-order model, we solve a linear relaxation of that problem using alternating directions dual decomposition (AD). The experiments have shown that, even though the parser’s performance in Chinese and Czech attains around 80% (not too far from English performance), domain shift is a serious issue, suggesting domain adaptation as an interesting avenue for future research.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S16-1005",
  "title": "CUFE at SemEval-2016 Task 4: A Gated Recurrent Model for Sentiment Classification",
  "year": 2016,
  "venue": "SemEval",
  "abstract": "In this paper we describe a deep learning system that has been built for SemEval 2016 Task4 (Subtask A and B). In this work we trained a Gated Recurrent Unit (GRU) neural network model on top of two sets of word embeddings: (a) general word embeddings generated from unsupervised neural language model; and (b) task specific word embeddings generated from supervised neural language model that was trained to classify tweets into positive and negative categories. We also added a method for analyzing and splitting multi-words hashtags and appending them to the tweet body before feeding it to our model. Our models achieved 0.58 F1-measure for Subtask A (ranked 12/34) and 0.679 Recall for Subtask B (ranked 12/19).",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S16-1141",
  "title": "WHUNlp at SemEval-2016 Task DiMSUM: A Pilot Study in Detecting Minimal Semantic Units and their Meanings using Supervised Models",
  "year": 2016,
  "venue": "SemEval",
  "abstract": "This paper describes our approach towards the SemEval-2016 Task 10: Detecting Minimal Semantic Units and their Meanings (DiMSUM). We consider that the two problems are similar to multiword expression detection and supersense tagging, respectively. The former problem is formalized as a sequence labeling problem solved by first-order CRFs, and the latter one is formalized as a classification problem solved by Maximum Entropy Algorithm. To carry out our pilot study quickly, we extract some simple features such as words or part-of-speech tags from the training set, and avoid using external resources such as WordNet or Brown clusters which are allowed in the supervised closed condition. Experimental results show that much further work on feature engineering and model optimization needs to be explored.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/S16-2011",
  "title": "When Hyperparameters Help: Beneficial Parameter Combinations in Distributional Semantic Models",
  "year": 2016,
  "venue": "SemEval",
  "abstract": "Distributional semantic models can predict many linguistic phenomena, including word similarity, lexical ambiguity, and semantic priming, or even to pass TOEFL synonymy and analogy tests (Landauer and Dumais, 1997; Griffiths et al., 2007; Turney and Pantel, 2010). But what does it take to create a competitive distributional model? Levy et al. (2015) argue that the key to success lies in hyperparameter tuning rather than in the model’s architecture. More hyperparameters trivially lead to potential performance gains, but what do they actually do to improve the models? Are individual hyperparameters’ contributions independent of each other? Or are only specific parameter combinations beneficial? To answer these questions, we perform a quantitative and qualitative evaluation of major hyperparameters as identified in previous research.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/S17-1030",
  "title": "Ways of Asking and Replying in Duplicate Question Detection",
  "year": 2017,
  "venue": "SemEval",
  "abstract": "This paper presents the results of systematic experimentation on the impact in duplicate question detection of different types of questions across both a number of established approaches and a novel, superior one used to address this language processing task. This study permits to gain a novel insight on the different levels of robustness of the diverse detection methods with respect to different conditions of their application, including the ones that approximate real usage scenarios.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/S17-2048",
  "title": "FA3L at SemEval-2017 Task 3: A ThRee Embeddings Recurrent Neural Network for Question Answering",
  "year": 2017,
  "venue": "SemEval",
  "abstract": "In this paper we present ThReeNN, a model for Community Question Answering, Task 3, of SemEval-2017. The proposed model exploits both syntactic and semantic information to build a single and meaningful embedding space. Using a dependency parser in combination with word embeddings, the model creates sequences of inputs for a Recurrent Neural Network, which are then used for the ranking purposes of the Task. The score obtained on the official test data shows promising results.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S17-2054",
  "title": "SwissAlps at SemEval-2017 Task 3: Attention-based Convolutional Neural Network for Community Question Answering",
  "year": 2017,
  "venue": "SemEval",
  "abstract": "In this paper we propose a system for reranking answers for a given question. Our method builds on a siamese CNN architecture which is extended by two attention mechanisms. The approach was evaluated on the datasets of the SemEval-2017 competition for Community Question Answering (cQA), where it achieved 7th place obtaining a MAP score of 86:24 points on the Question-Comment Similarity subtask.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S17-2065",
  "title": "DataStories at SemEval-2017 Task 6: Siamese LSTM with Attention for Humorous Text Comparison",
  "year": 2017,
  "venue": "SemEval",
  "abstract": "In this paper we present a deep-learning system that competed at SemEval-2017 Task 6 ''#HashtagWars: Learning a Sense of Humor\". We participated in Subtask A, in which the goal was, given two Twitter messages, to identify which one is funnier. We propose a Siamese architecture with bidirectional Long Short-Term Memory (LSTM) networks, augmented with an attention mechanism. Our system works on the token-level, leveraging word embeddings trained on a big collection of unlabeled Twitter messages. We ranked 2nd in 7 teams. A post-completion improvement of our model, achieves state-of-the-art results on #HashtagWars dataset.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S17-2092",
  "title": "SemEval-2017 Task 11: End-User Development using Natural Language",
  "year": 2017,
  "venue": "SemEval",
  "abstract": "This task proposes a challenge to support the interaction between users and applications, micro-services and software APIs using natural language. The task aims for supporting the evaluation and evolution of the discussions surrounding the natural language processing approaches within the context of end-user natural language programming, under scenarios of high semantic heterogeneity/gap.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S17-2119",
  "title": "YNUDLG at SemEval-2017 Task 4: A GRU-SVM Model for Sentiment Classification and Quantification in Twitter",
  "year": 2017,
  "venue": "SemEval",
  "abstract": "Sentiment analysis is one of the central issues in Natural Language Processing and has become more and more important in many fields. Typical sentiment analysis classifies the sentiment of sentences into several discrete classes (e.g.,positive or negative). In this paper we describe our deep learning system(combining GRU and SVM) to solve both two-, three- and five-tweet polarity classifications. We first trained a gated recurrent neural network using pre-trained word embeddings, then we extracted features from GRU layer and input these features into support vector machine to fulfill both the classification and quantification subtasks. The proposed approach achieved 37th, 19th, and 14rd places in subtasks A, B and C, respectively.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S17-2134",
  "title": "YNU-HPCC at SemEval 2017 Task 4: Using A Multi-Channel CNN-LSTM Model for Sentiment Classification",
  "year": 2017,
  "venue": "SemEval",
  "abstract": "In this paper, we propose a multi-channel convolutional neural network-long short-term memory (CNN-LSTM) model that consists of two parts: multi-channel CNN and LSTM to analyze the sentiments of short English messages from Twitter. Un-like a conventional CNN, the proposed model applies a multi-channel strategy that uses several filters of different length to extract active local n-gram features in different scales. This information is then sequentially composed using LSTM. By combining both CNN and LSTM, we can consider both local information within tweets and long-distance dependency across tweets in the classification process. Officially released results show that our system outperforms the baseline algo-rithm.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S18-1019",
  "title": "AttnConvnet at SemEval-2018 Task 1: Attention-based Convolutional Neural Networks for Multi-label Emotion Classification",
  "year": 2018,
  "venue": "SemEval",
  "abstract": "In this paper, we propose an attention-based classifier that predicts multiple emotions of a given sentence. Our model imitates human's two-step procedure of sentence understanding and it can effectively represent and classify sentences. With emoji-to-meaning preprocessing and extra lexicon utilization, we further improve the model performance. We train and evaluate our model with data provided by SemEval-2018 task 1-5, each sentence of which has several labels among 11 given emotions. Our model achieves 5th/1st rank in English/Spanish respectively.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S18-1045",
  "title": "DeepMiner at SemEval-2018 Task 1: Emotion Intensity Recognition Using Deep Representation Learning",
  "year": 2018,
  "venue": "SemEval",
  "abstract": "In this paper, we propose a regression system to infer the emotion intensity of a tweet. We develop a multi-aspect feature learning mechanism to capture the most discriminative semantic features of a tweet as well as the emotion information conveyed by each word in it. We combine six types of feature groups: (1) a tweet representation learned by an LSTM deep neural network on the training data, (2) a tweet representation learned by an LSTM network on a large corpus of tweets that contain emotion words (a distant supervision corpus), (3) word embeddings trained on the distant supervision corpus and averaged over all words in a tweet, (4) word and character n-grams, (5) features derived from various sentiment and emotion lexicons, and (6) other hand-crafted features. As part of the word embedding training, we also learn the distributed representations of multi-word expressions (MWEs) and negated forms of words. An SVR regressor is then trained over the full set of features. We evaluate the effectiveness of our ensemble feature sets on the SemEval-2018 Task 1 datasets and achieve a Pearson correlation of 72% on the task of tweet emotion intensity prediction.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S18-1075",
  "title": "TAJJEB at SemEval-2018 Task 2: Traditional Approaches Just Do the Job with Emoji Prediction",
  "year": 2018,
  "venue": "SemEval",
  "abstract": "Emojis are widely used on social media andunderstanding their meaning is important forboth practical purposes (e.g. opinion mining,sentiment detection) and theoretical purposes(e.g. how different L1 speakers use them, dothey have some syntax?); this paper presents aset of experiments that aim to predict a singleemoji from a tweet. We built different mod-els and we found that the test results are verydifferent from the validation results.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S18-1128",
  "title": "SIRIUS-LTG-UiO at SemEval-2018 Task 7: Convolutional Neural Networks with Shortest Dependency Paths for Semantic Relation Extraction and Classification in Scientific Papers",
  "year": 2018,
  "venue": "SemEval",
  "abstract": "This article presents the SIRIUS-LTG-UiO system for the SemEval 2018 Task 7 on Semantic Relation Extraction and Classification in Scientific Papers. First we extract the shortest dependency path (sdp) between two entities, then we introduce a convolutional neural network (CNN) which takes the shortest dependency path embeddings as input and performs relation classification with differing objectives for each subtask of the shared task. This approach achieved overall F1 scores of 76.7 and 83.2 for relation classification on clean and noisy data, respectively. Furthermore, for combined relation extraction and classification on clean data, it obtained F1 scores of 37.4 and 33.6 for each phase. Our system ranks 3rd in all three sub-tasks of the shared task.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S18-1139",
  "title": "Talla at SemEval-2018 Task 7: Hybrid Loss Optimization for Relation Classification using Convolutional Neural Networks",
  "year": 2018,
  "venue": "SemEval",
  "abstract": "This paper describes our approach to SemEval-2018 Task 7 - given an entity-tagged text from the ACL Anthology corpus, identify and classify pairs of entities that have one of six possible semantic relationships. Our model consists of a convolutional neural network leveraging pre-trained word embeddings, unlabeled ACL-abstracts, and multiple window sizes to automatically learn useful features from entity-tagged sentences. We also experiment with a hybrid loss function, a combination of cross-entropy loss and ranking loss, to boost the separation in classification scores. Lastly, we include WordNet-based features to further improve the performance of our model. Our best model achieves an F1(macro) score of 74.2 and 84.8 on subtasks 1.1 and 1.2, respectively.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S18-1145",
  "title": "Digital Operatives at SemEval-2018 Task 8: Using dependency features for malware NLP",
  "year": 2018,
  "venue": "SemEval",
  "abstract": "The four sub-tasks of SecureNLP build towards a capability for quickly highlighting critical information from malware reports, such as the specific actions taken by a malware sample. Digital Operatives (DO) submitted to sub-tasks 1 and 2, using standard text analysis technology (text classification for sub-task 1, and a CRF for sub-task 2). Performance is broadly competitive with other submitted systems on sub-task 1 and weak on sub-task 2. The annotation guidelines for the intermediate sub-tasks create a linkage to the final task, which is both an annotation challenge and a potentially useful feature of the task. The methods that DO chose do not attempt to make use of this linkage, which may be a missed opportunity. This motivates a post-hoc error analysis. It appears that the annotation task is very hard, and that in some cases both deep conceptual knowledge and substantial surrounding context are needed in order to correctly classify sentences.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/S18-2010",
  "title": "The Limitations of Cross-language Word Embeddings Evaluation",
  "year": 2018,
  "venue": "SemEval",
  "abstract": "The aim of this work is to explore the possible limitations of existing methods of cross-language word embeddings evaluation, addressing the lack of correlation between intrinsic and extrinsic cross-language evaluation methods. To prove this hypothesis, we construct English-Russian datasets for extrinsic and intrinsic evaluation tasks and compare performances of 5 different cross-language models on them. The results say that the scores even on different intrinsic benchmarks do not correlate to each other. We can conclude that the use of human references as ground truth for cross-language word embeddings is not proper unless one does not understand how do native speakers process semantics in their cognition.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/S18-2021",
  "title": "Deep Affix Features Improve Neural Named Entity Recognizers",
  "year": 2018,
  "venue": "SemEval",
  "abstract": "We propose a practical model for named entity recognition (NER) that combines word and character-level information with a specific learned representation of the prefixes and suffixes of the word. We apply this approach to multilingual and multi-domain NER and show that it achieves state of the art results on the CoNLL 2002 Spanish and Dutch and CoNLL 2003 German NER datasets, consistently achieving 1.5-2.3 percent over the state of the art without relying on any dictionary features. Additionally, we show improvement on SemEval 2013 task 9.1 DrugNER, achieving state of the art results on the MedLine dataset and the second best results overall (-1.3% from state of the art). We also establish a new benchmark on the I2B2 2010 Clinical NER dataset with 84.70 F-score.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S19-1023",
  "title": "Word Embeddings (Also) Encode Human Personality Stereotypes",
  "year": 2019,
  "venue": "SemEval",
  "abstract": "Word representations trained on text reproduce human implicit bias related to gender, race and age. Methods have been developed to remove such bias. Here, we present results that show that human stereotypes exist even for much more nuanced judgments such as personality, for a variety of person identities beyond the typically legally protected attributes and that these are similarly captured in word representations. Specifically, we collected human judgments about a person's Big Five personality traits formed solely from information about the occupation, nationality or a common noun description of a hypothetical person. Analysis of the data reveals a large number of statistically significant stereotypes in people. We then demonstrate the bias captured in lexical representations is statistically significantly correlated with the documented human bias. Our results, showing bias for a large set of person descriptors for such nuanced traits put in doubt the feasibility of broadly and fairly applying debiasing methods and call for the development of new methods for auditing language technology systems and resources.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/S19-1028",
  "title": "On Adversarial Removal of Hypothesis-only Bias in Natural Language Inference",
  "year": 2019,
  "venue": "SemEval",
  "abstract": "Popular Natural Language Inference (NLI) datasets have been shown to be tainted by hypothesis-only biases. Adversarial learning may help models ignore sensitive biases and spurious correlations in data. We evaluate whether adversarial learning can be used in NLI to encourage models to learn representations free of hypothesis-only biases. Our analyses indicate that the representations learned via adversarial learning may be less biased, with only small drops in NLI accuracy.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/S19-2026",
  "title": "CoAStaL at SemEval-2019 Task 3: Affect Classification in Dialogue using Attentive BiLSTMs",
  "year": 2019,
  "venue": "SemEval",
  "abstract": "This work describes the system presented by the CoAStaL Natural Language Processing group at University of Copenhagen. The main system we present uses the same attention mechanism presented in (Yang et al., 2016). Our overall model architecture is also inspired by their hierarchical classification model and adapted to deal with classification in dialogue by encoding information at the turn level. We use different encodings for each turn to create a more expressive representation of dialogue context which is then fed into our classifier.We also define a custom preprocessing step in order to deal with language commonly used in interactions across many social media outlets. Our proposed system achieves a micro F1 score of 0.7340 on the test set and shows significant gains in performance compared to a system using dialogue level encoding.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/S19-2034",
  "title": "EmoSense at SemEval-2019 Task 3: Bidirectional LSTM Network for Contextual Emotion Detection in Textual Conversations",
  "year": 2019,
  "venue": "SemEval",
  "abstract": "In this paper, we describe a deep-learning system for emotion detection in textual conversations that participated in SemEval-2019 Task 3 \"EmoContext\". We designed a specific architecture of bidirectional LSTM which allows not only to learn semantic and sentiment feature representation, but also to capture user-specific conversation features. To fine-tune word embeddings using distant supervision we additionally collected a significant amount of emotional texts. The system achieved 72.59% micro-average F1 score for emotion classes on the test dataset, thereby significantly outperforming the officially-released baseline. Word embeddings and the source code were released for the research community.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S19-2035",
  "title": "EPITA-ADAPT at SemEval-2019 Task 3: Detecting emotions in textual conversations using deep learning models combination",
  "year": 2019,
  "venue": "SemEval",
  "abstract": "Messaging platforms like WhatsApp, Facebook Messenger and Twitter have gained recently much popularity owing to their ability in connecting users in real-time. The content of these textual messages can be a useful resource for text mining to discover and unhide various aspects, including emotions. In this paper we present our submission for SemEval 2019 task 'EmoContext'. The task consists of classifying a given textual dialogue into one of four emotion classes: Angry, Happy, Sad and Others. Our proposed system is based on the combination of different deep neural networks techniques. In particular, we use Recurrent Neural Networks (LSTM, B-LSTM, GRU, B-GRU), Convolutional Neural Network (CNN) and Transfer Learning (TL) methodes. Our final system, achieves an F1 score of 74.51% on the subtask evaluation dataset.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S19-2058",
  "title": "TDBot at SemEval-2019 Task 3: Context Aware Emotion Detection Using A Conditioned Classification Approach",
  "year": 2019,
  "venue": "SemEval",
  "abstract": "With the system description it is shown how to use the context information while detecting the emotion in a dialogue. Some guidelines about how to handle emojis was also laid out. While developing this system I realized the importance of pre-processing in conversational text data, or in general NLP related tasks; it can not be over emphasized.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/S19-2127",
  "title": "nlpUP at SemEval-2019 Task 6: A Deep Neural Language Model for Offensive Language Detection",
  "year": 2019,
  "venue": "SemEval",
  "abstract": "This paper presents our submission for the SemEval shared task 6, sub-task A on the identification of offensive language. Our proposed model, C-BiGRU, combines a Convolutional Neural Network (CNN) with a bidirectional Recurrent Neural Network (RNN). We utilize word2vec to capture the semantic similarities between words. This composition allows us to extract long term dependencies in tweets and distinguish between offensive and non-offensive tweets. In addition, we evaluate our approach on a different dataset and show that our model is capable of detecting online aggressiveness in both English and German tweets. Our model achieved a macro F1-score of 79.40% on the SemEval dataset.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S19-2155",
  "title": "SemEval-2019 Task 12: Toponym Resolution in Scientific Papers",
  "year": 2019,
  "venue": "SemEval",
  "abstract": "We present the SemEval-2019 Task 12 which focuses on toponym resolution in scientific articles. Given an article from PubMed, the task consists of detecting mentions of names of places, or toponyms, and mapping the mentions to their corresponding entries in GeoNames.org, a database of geospatial locations. We proposed three subtasks. In Subtask 1, we asked participants to detect all toponyms in an article. In Subtask 2, given toponym mentions as input, we asked participants to disambiguate them by linking them to entries in GeoNames. In Subtask 3, we asked participants to perform both the detection and the disambiguation steps for all toponyms. A total of 29 teams registered, and 8 teams submitted a system run. We summarize the corpus and the tools created for the challenge. They are freely available at https://competitions.codalab.org/competitions/19948. We also analyze the methods, the results and the errors made by the competing systems with a focus on toponym disambiguation.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/S19-2216",
  "title": "OleNet at SemEval-2019 Task 9: BERT based Multi-Perspective Models for Suggestion Mining",
  "year": 2019,
  "venue": "SemEval",
  "abstract": "This paper describes our system partici- pated in Task 9 of SemEval-2019: the task is focused on suggestion mining and it aims to classify given sentences into sug- gestion and non-suggestion classes in do- main specific and cross domain training setting respectively. We propose a multi- perspective architecture for learning rep- resentations by using different classical models including Convolutional Neural Networks (CNN), Gated Recurrent Units (GRU), Feed Forward Attention (FFA), etc. To leverage the semantics distributed in large amount of unsupervised data, we also have adopted the pre-trained Bidi- rectional Encoder Representations from Transformers (BERT) model as an en- coder to produce sentence and word rep- resentations. The proposed architecture is applied for both sub-tasks, and achieved f1-score of 0.7812 for subtask A, and 0.8579 for subtask B. We won the first and second place for the two tasks respec- tively in the final competition.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/S19-2223",
  "title": "YNU_DYX at SemEval-2019 Task 9: A Stacked BiLSTM for Suggestion Mining Classification",
  "year": 2019,
  "venue": "SemEval",
  "abstract": "In this paper we describe a deep-learning system that competed as SemEval 2019 Task 9-SubTask A: Suggestion Mining from Online Reviews and Forums. We use Word2Vec to learn the distributed representations from sentences. This system is composed of a Stacked Bidirectional Long-Short Memory Network (SBiLSTM) for enriching word representations before and after the sequence relationship with context. We perform an ensemble to improve the effectiveness of our model. Our official submission results achieve an F1-score 0.5659.",
  "stance": 1.0
 }
]