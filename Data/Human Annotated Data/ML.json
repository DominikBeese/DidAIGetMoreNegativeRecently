[
 {
  "url": "https://www.aaai.org/Papers/AAAI/2000/AAAI00-020.pdf",
  "title": "Anchoring Symbols to Sensor Data: Preliminary Report",
  "year": 2000,
  "venue": "AAAI",
  "abstract": "Anchoring is the process of creating and maintaining the correspondence between symbols and percepts that refer to the same physical objects. Although this process must necessarily be present in any physically embedded system that includes a symbolic component (e.g., an autonomous robot), no systematic study of anchoring as a problem per se has been reported in the literature on intelligent systems. In this paper, we propose a domain-independent definition of the anchoring problem, and identify its three basic functionalities: find, reacquire, and track. We illustrate our definition on two systems operating in two different domains: an unmanned airborne vehicle for traffic surveillance; and a mobile robot for office navigation.",
  "stance": 0.5
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2000/AAAI00-064.pdf",
  "title": "Solving Advanced Reasoning Tasks Using Quantified Boolean Formulas",
  "year": 2000,
  "venue": "AAAI",
  "abstract": "We consider the compilation of different reasoning tasks into the evaluation problem of quantified boolean formulas (QBFs) as an approach to develop prototype reasoning systems useful for, e.g., experimental purposes. Such a method is a natural generalization of a similar technique applied to NP-problems and has been recently proposed by other researchers. More specifically, we present translations of several well-known reasoning tasks from the area of nonmonotonic reasoning into QBFs, and compare their implementation in the prototype system QUIP with established NMRprovers. The results show reasonable performance, and document that the QBF approach is an attractive tool for rapid prototyping of experimental knowledge-representation systems.",
  "stance": 0.5
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2000/AAAI00-078.pdf",
  "title": "GeoRep: A Flexible Tool for Spatial Representation of Line Drawings",
  "year": 2000,
  "venue": "AAAI",
  "abstract": "A key problem in diagrammatic reasoning is understanding how people reason about qualitative relationships in diagrams. We claim that progress in diagrammatic reasoning is slowed by two problems: (1) researchers tend to start from scratch, creating new spatial reasoners for each new problem area, and (2) constraints from human visual processing are rarely considered. To address these problems, we created GeoRep, a spatial reasoning engine that generates qualitative spatial descriptions from line drawings. GeoRep has been successfully used in several research projects, including cognitive simulation studies of human vision. In this paper, we outline GeoRep’s architecture, explain the domainindependent and domain-specific aspects of its processing, and motivate the representations it produces. We then survey how GeoRep has been used in three different projects–a model of symmetry, a model of understanding juxtaposition diagrams of physical situations, and a system for reasoning about military courses of action. Introduction: How Diagrams Work Diagrams are ubiquitous. In daily communications, through sketches, maps, and figures, people use diagrams to convey information. Some diagrams depict intrinsically spatial domains, such as bus routes or furniture arrangements. Other diagrams use spatial concepts to compactly show more abstract relations, such as corporate hierarchies or data flow in a computer program. In all such domains, diagrams can be extremely effective. It is also true, however, that there is a keen difference between effective and ineffective diagrams. Small visual differences may distinguish a diagram that elucidates from one that confuses (Tufte, 1990). A key difference between good and bad diagrams is how well they utilize the kinds of qualitative spatial relations most easily perceived by the human visual system. In the best diagrams, these spatial relations support the conceptual relations the reader is meant to infer. For example, in a thermodynamics diagram, an arrow may indicate the direction of heat flow, with thicker arrows to indicate greater flow, or tapering arrows to indicate heat dissipation. Or, in a circuit diagram, wires Copyright © 2000, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. may be drawn so that related wires are adjacent and parallel, so they can be visually grouped. For this reason, to understand how diagrams work, we must show how diagrams use visual characteristics to support particular qualitative inferences. In the system described here, we model this process as an interaction between two representation levels: 1. A low-level, domain-independent representation which involves a representative set of primitive spatial relations. This level models human low-level vision. 2. A high-level, domain-specific representation that models visual skills for a particular domain. This level links lowlevel visual relations to a domain’s conceptual content. These two representation levels form the basis of GeoRep. GeoRep is an engine for building diagrammatic reasoners. GeoRep takes as input a line drawing, given as a set of primitive visual elements. From this drawing, GeoRep creates a predicate calculus representation of the drawing's visual relations. To perform this task, GeoRep, given the drawing, examines the primitive shapes in the figure, looking for a broad set of low-level visual relations. These relations are detected by a library of visual operations (assumed to be domain-independent) which partially cover the set of universal visual routines (Ullman, 1984). Next, GeoRep uses these relations, in combination with domain-dependent rules, to generate the second, domain-specific representation. GeoRep's two-level",
  "stance": 0.4
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2000/AAAI00-157.pdf",
  "title": "Knowledge Representation on the Internet: Achieving Interoperability in a Dynamic, Distributed Environment",
  "year": 2000,
  "venue": "AAAI",
  "abstract": "The Internet’s explosive growth is making it harder and harder to harness its potential. There is so much information available that users are frequently overwhelmed by information overload. Due to limitations in modern natural language processing, an important part of search involves keyword-based techniques, which tend to have poor precision and recall. Some systems use the format of a web page to extract information, but due to the changing nature of these pages, such systems are very fragile. It has been claimed that the Extensible Markup Language (XML) will solve these problems by replacing the presentation-oriented tags of HTML with contentspecific tags. While it is true that XML will be useful for data exchange and separating content from format, once XML is in widespread usage there will be significant interoperability problems. Unless all content providers agree on the same set of tags and the meanings of these tags it will be impossible to automatically integrate their information. The field of knowledge representation has studied techniques for storing, modifying and reasoning with complex information. A growing subfield of KR is the study of ontologies, which are reusable knowledge components. Recent research has shown that semantically marking up web pages using terms from an explicit ontology can greatly improve retrieval, integrate the data of many pages, and enable intelligent internet-based agents as well. However two characteristics of the Internet provide significant challenges for an ontology approach: it is ever changing and it is decentralized. Thus, it must be possible to adapt ontologies to meet existing needs in a timely fashion, but such changes must not have an adverse impact on the objects that depend on the ontology (i.e., those ontologies and web pages that use the ontology to define their terms). My work with SHOE, which stands for Simple HTML Ontology Extensions, has given me a lot of experience with the design and application of semantic markup languages for the Web. SHOE distinguishes between two types of web pages: ontologies and instances. A SHOE ontology describes a domain by defining categories, relations, inference rules, and other elements. Each",
  "stance": -0.3
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2000/AAAI00-169.pdf",
  "title": "Combining Classification and Temporal Learning",
  "year": 2000,
  "venue": "AAAI",
  "abstract": "This introduces TRACA (Temporal Reinforcement-learning and Classification Architecture), a connectionist learning system for solving problems in large state spaces. These types of problems, such as robot control, commonly include the presence of irrelevant attributes and hidden-state. TRACA is capable of dealing with both irrelevant information and hidden-state while addressing two common shortcomings of other learning systems. The first shortcoming is requiring a large number of training examples which is unrealistic for learning in the real world. The second is having to pre-determine or constrain network structure and size.",
  "stance": 0.4
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2000/AAAI00-179.pdf",
  "title": "Interfacing Issues for Information Extraction",
  "year": 2000,
  "venue": "AAAI",
  "abstract": "Traditional approaches to information extraction implicitly assume that many elements of the task are static — the user’s query, and the description of domain and corpus, for example. We believe that in many real situations, however, this assumption does not hold and it is important to consider how the system could best support interaction with the user when the assumption breaks down. Current goals in the information extraction community are for the system to produce accurate results while being easy to retrain and port to a new domain. We seek to extend current approaches to handle dynamic elements of the problem. “Evolving queries”, discussed in the information retrieval (IR) literature, need to be supported by information extraction (IE) systems; IR and IE are both, after all, tools for gathering information from documents in response to a user query. When a casual user — neither an expert in the use of the system, nor in the domain — engages in any information gathering task, there will be an initial phase of investigation and discovery during which the user becomes familiar with the system, the domain, and the documents in the corpus and the user’s query may change over time or evolve. For example, a user may have a query about terrorist activities, asking for the names of perpetrators and the locations of targets; an interim system output prompts the user to refine the query, redefining terrorist activities as involving only a subset of weapons while generalizing to allow for additional (e.g., government) perpetrators. The query is not the only element that may change over time; certainly the domain evolves as additional documents are processed. As well, when the corpus is very large or dynamic (e.g., the Internet), the corpus itself may be seen as evolving — rules for mapping text patterns to query items that apply at one time or for one portion of the corpus no longer apply for another. To provide more robust support for information extraction in a dynamic environment, we consider such issues as:",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2002/AAAI02-051.pdf",
  "title": "The Design of Collectives of Agents to Control Non-Markovian Systems",
  "year": 2002,
  "venue": "AAAI",
  "abstract": "The “Collective Intelligence” (COIN) framework concerns the design of collectives of reinforcement-learning agents such that their interaction causes a provided “world” utility function concerning the entire collective to be maximized. Previously, we applied that framework to scenarios involving Markovian dynamics where no re-evolution of the system from counter-factual initial conditions (an often expensive calculation) is permitted. This approach sets the individual utility function of each agent to be both aligned with the world utility, and at the same time, easy for the associated agents to optimize. Here we extend that approach to systems involving non-Markovian dynamics. In computer simulations, we compare our techniques with each other and with conventional “team games” We show whereas in team games performance often degrades badly with time, it steadily improves when our techniques are used. We also investigate situations where the system’s dimensionality is effectively reduced. We show that this leads to difficulties in the agents’ ability to learn. The implication is that “learning” is a property only of high-enough dimensional systems.",
  "stance": -0.7
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2002/AAAI02-119.pdf",
  "title": "Reviewing the Design of DAML+OIL: An Ontology Language for the Semantic Web",
  "year": 2002,
  "venue": "AAAI",
  "abstract": "In the current “Syntactic Web”, uninterpreted syntactic constructs are given meaning only by private off-line agreements that are inaccessible to computers. In the Semantic Web vision, this is replaced by a web where both data and its semantic definition are accessible and manipulable by computer software. DAML+OIL is an ontology language specifically designed for this use in the Web; it exploits existing Web standards (XML and RDF), adding the familiar ontological primitives of object oriented and frame based systems, and the formal rigor of a very expressive description logic. The definition of DAML+OIL is now over a year old, and the language has been in fairly widespread use. In this paper, we review DAML+OIL’s relation with its key ingredients (XML, RDF, OIL, DAML-ONT, Description Logics), we discuss the design decisions and trade-offs that were the basis for the language definition, and identify a number of implementation challenges posed by the current language. These issues are important for designers of other representation languages for the Semantic Web, be they competitors or successors of DAML+OIL, such as the language currently under definition by W3C.",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2002/AAAI02-166.pdf",
  "title": "Generating Trading Agent Strategies",
  "year": 2002,
  "venue": "AAAI",
  "abstract": "My thesis work concerns the generation of trading agent strategies—automatically, semi-automatically, and manually. Automatic generation of an agent strategy means creating a system that can read the description of some mechanism (i.e., a game) and output a strategy for a participating agent—i.e., a mapping from percepts to actions in the environment defined by the mechanism. To make this more concrete, consider an extremely simple auction mechanism: a two-player first-price sealed-bid auction. This is a game in which two players each have one piece of private information—their valuations for the good being auctioned. Each agent also has a continuum of possible actions—its bid amount. The payoff to an agent is its valuation minus its bid, if its bid is highest, and zero otherwise. My current system can take such a game description and output the optimal strategy, i.e., the Nash equilibrium. (In this case, that strategy is to bid half of your valuation.) Existing game solvers (Gambit and Gala) can only solve games with an enumerated (finite) set of actions, and this limitation makes it impossible to even approximate (i.e., by discretizing) the solution to games with a continuum of actions because the size of the game tree quickly explodes. Of course, the optimal strategy for the first-price sealed-bid auction was computed before game solvers existed; however, my algorithm can automatically solve any of a class of games (with certain caveats) that current solvers can’t. In addition to this algorithm for exact solutions, I have an approximation algorithm using monte carlo simulation that can handle a more general class of games (e.g., arbitrary payoff functions and any number of players) albeit at high computational cost. Both of the above methods are only tractable for quite simple games. For example, almost any mechanism that involves iterated bidding and multiple auctions is likely not to be tractable for strictly game-theoretic analysis, regardless of whether exact or approximate solutions are sought. An example of such a mechanism that we are analyzing is a simultaneous ascending auction for scheduling resources among a group of agents. In this domain, every agent has certain preferences for acquiring time slots (say, for use of a resource in a factory) and simultaneous English auctions are held for every slot until bidding stops and the slots are allocated. Since this mechanism is too complicated for the fully",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2002/AAAI02-169.pdf",
  "title": "Disciple-RKF/COG: Agent Teaching by Subject Matter Experts",
  "year": 2002,
  "venue": "AAAI",
  "abstract": "We are addressing the knowledge acquisition bottleneck in the development of knowledge-based systems by elaborating the Disciple theory and methodology that enables subject matter experts to build such systems by themselves, with limited assistance from knowledge engineers (Tecuci 1998). The investigated solution consists of developing a very capable learning agent shell that can perform many of the functions of a knowledge engineer. As an expert system shell, the learning agent shell includes a general problem solving engine that can be reused for multiple applications. In addition, it includes a multistrategy learning engine for building its knowledge base (KB) which has two main components: an object ontology that defines the concepts from a specific application domain, and a set of task reduction rules expressed with these concepts. The subject matter expert and the agent engage into a mixed-initiative reasoning process during which the expert is teaching the agent his problem solving expertise, and the agent learns from the expert, building, verifying, and improving its KB. Over the years we have developed a series of increasingly more capable learning agent shells from the Disciple family. The most recent family member, DiscipleRKF/COG, represents a significant advancement over its predecessors. It implements a more powerful plausible version space representation that allows all the types of knowledge from the KB (not only the rules, but also the objects and the tasks) to be learned with similar methods. Moreover, the partially learned knowledge pieces are represented at several levels of formalization, from natural language to formal logic, facilitating expert-agent communication, mixed-initiative problem solving, and learning. As a consequence, Disciple-RKF/COG incorporates new tools that allow a subject matter expert to perform additional knowledge engineering tasks, such as scenario specification, modeling of his problem solving process, and task formalization. Disciple-RKF/COG was used and evaluated in several courses at the US Army War College, with very promising results, being made part of their regular syllabi.",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2004/AAAI04-022.pdf",
  "title": "Modeling Choices in Quasigroup Completion: SAT Versus CSP",
  "year": 2004,
  "venue": "AAAI",
  "abstract": "We perform a systematic comparison of SAT and CSP models for a challenging combinatorial problem, quasigroup completion (QCP). Our empirical results clearly indicate the superiority of the 3D SAT encoding (Kautz et al. 2001), with various solvers, over other SAT and CSP models. We propose a partial explanation of the observed performance. Analytically, we focus on the relative conciseness of the 3D model and the pruning power of unit propagation. Empirically, the focus is on the role of the unit-propagation heuristic of the best performing solver, Satz (Li & Anbulagan 1997), which proves crucial to its success, and results in a significant improvement in scalability when imported into the CSP solvers. Our results strongly suggest that SAT encodings of permutation problems (Hnich, Smith, & Walsh 2004) may well prove quite competitive in other domains, in particular when compared with the currently preferred channeling CSP models.",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2004/AAAI04-056.pdf",
  "title": "Learning and Inferring Transportation Routines",
  "year": 2004,
  "venue": "AAAI",
  "abstract": "This paper introduces a hierarchical Markov model that can learn and infer a user’s daily movements through the community. The model uses multiple levels of abstraction in order to bridge the gap between raw GPS sensor measurements and high level information such as a user’s mode of transportation or her goal. We apply Rao-Blackwellised particle filters for efficient inference both at the low level and at the higher levels of the hierarchy. Significant locations such as goals or locations where the user frequently changes mode of transportation are learned from GPS data logs without requiring any manual labeling. We show how to detect abnormal behaviors (e.g. taking a wrong bus) by concurrently tracking his activities with a trained and a prior model. Experiments show that our model is able to accurately predict the goals of a person and to recognize situations in which the user performs unknown activities.",
  "stance": 0.7
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2004/AAAI04-140.pdf",
  "title": "Flexible Decision-Making in Sequential Auctions",
  "year": 2004,
  "venue": "AAAI",
  "abstract": "It is quite common that items are sold sequentially in a series of auctions. For example, in the Seattle Fur Exchange, approximately eighty percent of pelts are auctioned sequentially (Lambson & Thurston 2003). On eBay, there are thousands of identical or similar items that are sold sequentially. A sequential auction is a combination of individual auctions. Each individual auction, which we call a component auction, can be any type of auction, such as a first-price, sealed-bid auction. In this thesis, I assume that there are q items for sale in a sequence of auctions, K. There are n agents, n > q, competing for these q items. Each agent has single-unit demand and will leave if it wins one item. Agents have incomplete information; each agent knows its own true valuation; however, it knows only a distribution function of other agents’ valuations. After each auction, all bidders’ bids are revealed, a practice which is common on eBay and other e-commerce sites. The intention of decision-making in sequential auctions is to find the optimal solutions for agents in the sequence of auctions as a whole. One myopic approach is to treat sequential auctions as a collection of independent auctions. However, as pointed out by Engelbrecht-Wiggans and Weber (Engelbrecht-Wiggans & Weber 1979), this kind of approach may be inappropriate in the general case. A better approach is to model sequential auctions as a game and find the equilibrium strategies. There is a voluminous theoretical literature on finding equilibria in sequential auctions. However, these models lack flexibility and a re-analysis is necessary even for a slightly different model. On the other hand, the vast number of trading opportunities and the increasingly fluid markets bolster the need for automated trading support in the form of trading agents—software programs that participate in electronic markets on behalf of a user. Simple bidding tools, like eSnipe1 and AuctionBlitz2, enable bidders to automate submission of bids. However, these tools lack the sophistication that bidders require when faced with a plethora of auctions possibly occurring in a sequence. The need for economic ef-",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2006/AAAI06-239.pdf",
  "title": "Machine Reading",
  "year": 2006,
  "venue": "AAAI",
  "abstract": "Over the last two decades or so, Natural Language Processing (NLP) has developed powerful methods for low-level syntactic and semantic text processing tasks such as parsing, semantic role labeling, and text categorization. Over the same period, the fields of machine learning and probabilistic reasoning have yielded important breakthroughs as well. It is now time to investigate how to leverage these advances to understand text.",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2006/AAAI06-244.pdf",
  "title": "Turing’s Dream and the Knowledge Challenge",
  "year": 2006,
  "venue": "AAAI",
  "abstract": "There is a set of clear-cut challenges, all centering around knowledge, that have received insufficient attention in AI, and whose solution could bring the realization of Turing’s dream – the dream of a machine we can talk with just like a person, and which is therefore (at least) our intellectual equal. These challenges have to do with the representation of linguistically expressible knowledge, the role of knowledge in language understanding, the use of knowledge for several sorts of commonsense reasoning, and knowledge accumulation. Concerning the last topic, I briefly present preliminary results of some of our recent efforts to extract “shallow” general knowledge about the world from large text corpora. Hi-fidelity Representation of Linguistically Expressible Knowledge “Language is fundamental to our ability to think; it is ‘more or less synonymous with symbolic thought,’...” – Donald Johanson, Sci. Am. 1998, expounding on Ian Tattersall’s Becoming Human Humans acquire large amounts of knowledge about the world from verbal expressions of such knowledge, and in turn, are able to communicate much of their internalized knowledge in language. It stands to reason, therefore, that our ‘mentalese’ must match the expressive resources of language – and that artificial agents competent in language will also require internal representations no less expressive than language. My collaborators and I have argued for many years (e.g., (Schubert & Hwang 1989; 2000; Schubert 2000)) that a representation capable of supporting natural language and commonsense reasoning in machines must allow not only for predication, logical connectives, and ∀/∃-quantifiers – the devices of classical logic – but also the following: • generalized quantifiers such as most or often; e.g., “Most of the trick-or-treaters who came to our door received several candy bars”; though set-theoretic paraphrases are possible, inference is more straightforward if such quantifiers are directly allowed for; • event/situation reference; e.g., (following the previous sentence) “This nearly emptied our bowl of treats by 7:30 Copyright c © 2006, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. pm”; note the anaphoric reference to the collection of ‘receiving events’, and the temporal modification of the ‘nearly-emptying event’; • modification (of predicates and sentences); examples are “nearly” in the previous sentence, verbs like “become” and “pose as” (with a predicate complement), and sentence adverbials such as “perhaps” and “according to reliable sources”; many such examples require an intensional semantics, i.e., one that does not reduce meaning to reference (extension) in the world; • reification (of predicates and sentences); examples are “Domestic dogs evolved from wolves and African wild dogs” (nominal predicate reification), “Composing music requires problem solving, but is deeply gratifying” (VP predicate reification), “That Turing was brilliant is beyond doubt” (proposition reification), “For Turing to make a mistake was unusual” (event-type reification), and “Who murdered Montague remains a mystery” (question reification); again intensionality is involved in many such examples; • metric/comparative attributes; e.g., “Thousands of men were employed for 20 years to build the Great Pyramid, which rose to a height of 485 feet”; “The frame of the bed is wider than the door is high”; • uncertainty and genericity; e.g., “If John receives a job offer from North Central Positronics, he will probably accept it”; “Elementary school children are usually bused to school”; “Dogs bark”; • metalinguistic capabilities; e.g., “What is the sum of binary numbers 111 and 1, expressed as a binary number?”; “Can you finish the opening sentence of Hamlet’s soliloquy, ‘To be or not to be, ...’?”; It is possible to contrive FOPC paraphrases by hand for some of the above examples, but doing so automatically is deeply problematic, and probably impossible in some cases (e.g., for some examples of modification, reification, genericity, and metalanguage). Our Episodic Logic (EL) representation (Schubert & Hwang 2000) covers many of the above constructs and implements them in the EPILOG system (Schaeffer et al. 1993; Schubert & Hwang 2000). Also (Fox & Lappin 2004; 2005) provide a two-level representation (PTCT –",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2006/AAAI06-314.pdf",
  "title": "Exploring GnuGo's Evaluation Function with a SVM",
  "year": 2006,
  "venue": "AAAI",
  "abstract": "This ongoing research project investigates articulatory feature (AF) classification using multiclass support vector machines (SVMs). SVMs are being constructed for each AF in the multi-valued feature set (Table 1), using speech data and annotation from the IFA Dutch “Open-Source” (van Son et al. 2001) and TIMIT English (Garofolo et al. 1993) corpora. The primary objective of this research is to assess the AF classification performance of different multiclass generalizations of the SVM, including one-versus-rest, one-versus-one, Decision Directed Acyclic Graph (DDAG), and direct methods for multiclass learning. Observing the successful application of SVMs to numerous classification problems (Bennett and Campbell 2000), it is hoped that multiclass SVMs will outperform existing state-of-the-art AF classifiers. One of the most basic challenges for speech recognition and other spoken language systems is to accurately map data from the acoustic domain into the linguistic domain. Much speech processing research has approached this task by taking advantage of the correlation between phones, the basic units of speech sound, and their acoustic manifestation (intuitively, there is a range of sounds that humans would consider to be an “e”). The mapping of acoustic data to phones has been largely successful, and is used in many speech systems today. Despite its success, there are drawbacks to using phones as the point of entry from the acoustic to linguistic domains. Notably, the granularity of the “phoneticsegmental” model, in which speech is represented as a series of phones, makes it difficult to account for various subphone phenomena that affect performance on spontaneous speech. Researchers have pursued an alternative approach to the acoustic-linguistic mapping through the use of articulatory modeling. This approach more directly exploits the intimate relation between articulation and acoustics: the state of one’s speech articulators (e.g. vocal folds, tongue) uniquely determines the parameters of the acoustic speech signal. Unfortunately, while the mapping from articulator to acoustics is straightforward, the problem of recovering the state of the articulators from an acoustic speech representation, acoustic-to-articulatory inversion, poses a formidable challenge (Toutios and Margaritis 2003). Nevertheless, re-",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2006/AAAI06-315.pdf",
  "title": "Further Investigations into Regular XORSAT",
  "year": 2006,
  "venue": "AAAI",
  "abstract": "Recent years have witnessed rapid progress both in the foundations of and in applying state-of-art solvers for the propositional satisfiability problem (SAT). The study of sources for hard SAT instances is motivated by the need for interesting benchmarks for solver development and on the other hand by theoretical analysis of different proof systems. In this respect satisfiable instance families are especially interesting. In contrast to unsatisfiable instance families, there are few theoretical results for satisfiable formulas (Alekhnovich, Hirsch, & Itsykson); for the successful DPLL method, restricted heuristics need to be considered. While real-world problems serve as best benchmark instances in many sense, such instances are typically very large and unavailable in abundance. More “artificial” empirically hard satisfiable CNF families include (see references therein for more) regular random k-SAT (Boufkhad et al.), encodings of quasi-group completion (Achlioptas et al. 2000), XORSAT models inspired by statistical physics (Ricci-Tersenghi, Weight, & Zecchina 2001; Jia, Moore, & Selman 2005), and the regular XORSAT model (Haanpää et al. 2006) motivated by expansion properties of random regular bipartite graphs. Experimental comparison with other available generators for notably hard satisfiable 3-CNF formulas shows that the regular XORSAT model gives extremely hard instances for state-of-the art clausal SAT solvers (Haanpää et al. 2006). In this paper we generalize the regular XORSAT model for k > 3, and investigate how this relates to the hardness of the instances. By increasing the degree of the underlying regular constraint graphs, we observe a sharp increase in problem difficulty with respect to the number of variables, motivating further analysis of regular XORSAT.",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2008/AAAI08-028.pdf",
  "title": "Semantical Considerations on Dialectical and Practical Commitments",
  "year": 2008,
  "venue": "AAAI",
  "abstract": "This paper studies commitments in multiagent systems. A dialectical commitment corresponds to an agent taking a position about a putative fact, including for the sake of argument. A practical commitment corresponds to an agent being obliged to another to bring about a condition. Although commitments have been used in many works, an adequate formal semantics and axiomatization for them does not yet exist. This paper presents a logic of commitments that illustrates the commonalities and differences of the two kinds of commitments. In this manner, it generalizes the developments of previous papers, precisely delineates the meanings of commitments, and identifies important postulates used informally or semiformally in previous work. This paper considers “social” commitments as introduced in (Singh 1991): by one agent to another, not of an agent to itself. Commitments help formalize a variety of interactive, loosely contractual, settings especially including argumentation and business protocols. Despite several formalizations that use commitments, there is surprisingly little work treating them as an abstraction in their own right. With few exceptions (reviewed in the last section), existing work has generally not emphasized the model-theoretic semantics of commitments as such, concentrating on ways of reasoning with or using them. It was a sensible research strategy to first establish that commitments were a useful concept. However, now that the case for commitments has been made well, further progress is hampered by the lack of a clear modeltheoretic semantics. For example, tools for designing correct protocols or verifying the interoperability or compliance of agents would rely upon a precise notion of what it means for an agent to be committed, which unfortunately is lacking. Analyses of commitments range in complexity from obligations to extensive conglomerates of social expectations and obligations. Following (Singh 1999), this paper takes a middle ground, erring perhaps toward simplicity. A commitment here is somewhat like a directed obligation, but one that arises in a context, and which can be manipulated in standardized ways. This paper doesn’t discuss context and manipulation, but its semantics provides a basis for specifying them precisely. Richer notions are readily accommodated in this approach. For example, Castelfranchi (1995) Copyright c © 2008, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. requires that a commitment be explicitly accepted by its creditor. This is reasonable in some applications but not others. We can easily define additional concepts that combine multiple instances of the more basic kinds of commitments studied here to achieve the various intuitive requirements of Castelfranchi and other researchers. Dialectical and Practical Commitments Commitments fall into two main varieties. Dialectical or dialogical commitments (Norman et al. 2004) reflect positions taken in dialogue or argumentation. By contrast, practical commitments reflect promises made during negotiation or trade. Thus dialectical commitments are about what holds and practical commitments about what is to be done. For example, a stock quote may be a dialectical commitment about the price; or a practical commitment to sell at the specified price. The two commitments may go together but not necessarily so. This paper lays the groundwork for formulating any such constraints as needed for different applications. Some key patterns of reasoning arise in dealing with commitments. Examples of natural reasoning patterns during modeling include (Ex 1) if a pharmacy commits to delivering medicines if the customer pays and shows a prescription, then once the customer shows the prescription, the pharmacy is committed to delivering medicines if the customer pays; (Ex 2) if a merchant commits to a customer to ship goods and commits to the same customer to send warranty paperwork, then the merchant commits to the customer to ship goods and send warranty paperwork; (Ex 3) a commitment that if the light is on, the light will be on would not be meaningful. We capture these patterns below as postulates B2, B5, and B8, respectively (along with several other postulates). How commitments relate to time is important. Dialectical commitments are about claims staked now (even if about future conditions) whereas practical commitments are about actions to be performed or conditions to be brought about in the future. Thus, postulates that hold for one kind of commitment can fail for the other kind. Examples Ex 1 and Ex 2 above would hold for dialectical but fail for practical commitments unless we impose additional constraints: because conjunction means that the conditions must hold simultaneously. Clearly, the above examples illustrate practical commitments. Therefore, we identify constraints under which various postulates hold for both kinds of commitments. Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence (2008)",
  "stance": 0.2
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2008/AAAI08-150.pdf",
  "title": "How Good is Almost Perfect?",
  "year": 2008,
  "venue": "AAAI",
  "abstract": "Heuristic search using algorithms such as A and IDA is the prevalent method for obtaining optimal sequential solutions for classical planning tasks. Theoretical analyses of these classical search algorithms, such as the well-known results of Pohl, Gaschnig and Pearl, suggest that such heuristic search algorithms can obtain better than exponential scaling behaviour, provided that the heuristics are accurate enough. Here, we show that for a number of common planning benchmark domains, including ones that admit optimal solution in polynomial time, general search algorithms such as A must necessarily explore an exponential number of search nodes even under the optimistic assumption of almost perfect heuristic estimators, whose heuristic error is bounded by a small additive constant. Our results shed some light on the comparatively bad performance of optimal heuristic search approaches in “simple” planning domains such as GRIPPER. They suggest that in many applications, further improvements in run-time require changes to other parts of the search algorithm than the heuristic estimator.",
  "stance": 0.3
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2008/AAAI08-216.pdf",
  "title": "CIGAR: Concurrent and Interleaving Goal and Activity Recognition",
  "year": 2008,
  "venue": "AAAI",
  "abstract": "In artificial intelligence and pervasive computing research, inferring users’ high-level goals from activity sequences is an important task. A major challenge in goal recognition is that users often pursue several high-level goals in a concurrent and interleaving manner, where the pursuit of goals may spread over different parts of an activity sequence and may be pursued in parallel. Existing approaches to recognizing multiple goals often formulate this problem either as a single-goal recognition problem or in a deterministic way, ignoring uncertainty. In this paper, we propose CIGAR (Concurrent and Interleaving Goal and Activity Recognition) a novel and simple two-level probabilistic framework for multiple-goal recognition where we can recognize both concurrent and interleaving goals. We use skip-chain conditional random fields (SCCRF) for modeling interleaving goals and we model concurrent goals by adjusting inferred probabilities through a correlation graph, which is a major advantage in that we are able to reason about goal interactions explicitly through the correlation graph. The two-level framework also avoids the high training complexity when modeling concurrency and interleaving together in a unified CRF model. Experimental results show that our method can effectively improve recognition accuracies on several real-world datasets collected from various wireless and sensor networks.",
  "stance": 1.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2008/AAAI08-273.pdf",
  "title": "Game Theory Pragmatics: A Challenge for AI",
  "year": 2008,
  "venue": "AAAI",
  "abstract": "Game theory has been playing an increasingly visible role in computer science in general and AI in particular, most notably in the area of multiagent systems. I briefly list the areas where most of the action has been in the past decade or so. I then suggest that going forward, the most dramatic interaction between computer science and game theory – with a special role for AI – could be around what might be called game theory pragmatics.",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/2008/AAAI08-322.pdf",
  "title": "Computational Influence for Training and Entertainment",
  "year": 2008,
  "venue": "AAAI",
  "abstract": "An interactive narrative is an education, training, or entertainment experience. There are two qualities that make an experience an interactive narrative. To understand those, let us clarify the meanings of interactive and narrative. Interactive: capable of acting on or influencing each other; and Narrative: presentation of events in a purposeful sequence. Thus, the qualities that define an interactive narrative are autonomy for players to act and intent of the system for the player to experience a narrative prescribed by an author. This conception of interactive narrative is broad and encompasses many types of entertainment and training experiences. One characteristic that sets it apart from other experiences and common games like Chess is that authorial intent often requires the player’s experience to be dramatic or adhere to some aesthetic. There is a tension between the systematic control required to ensure the intent of the narrative and the player autonomy required for interactivity— the player-driven exploration that results in an interactive quality is a potential threat to the narrative. Therefore, AI researchers in the field of interactive narrative are interested in balancing the conflicting requirements of autonomy and authorial intent. It is in this area that both I and many interactive narrative researchers have focused their efforts (Roberts and Isbell 2008). While many of the existing research projects have met with success in their own right, they have also uncovered limitations in the state of the art. I plan to complete my dissertation by addressing some of these limitations. The tools of artificial intelligence and machine learning have often been applied in domains where the limits of human ability are stretched. For example, sophisticated AI algorithms retrieve information from the internet, filter spam from email inboxes, and aid doctors in diagnosing illnesses. In each of these cases, a technically-minded AI expert has created a technique to solve a class of problems and has put the power of that technology in the hands of a practitioner. Similarly, I plan to develop AI technologies for computerbased gaming and simulation that when given to game designers will ease their authorial burden and increase their expressive power. The fundamental question I plan to an-",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/7845/7704",
  "title": "Preferred Explanations: Theory and Generation via Planning",
  "year": 2011,
  "venue": "AAAI",
  "abstract": "In this paper we examine the general problem of generating preferred explanations for observed behavior with respect to a model of the behavior of a dynamical system. This problem arises in a diversity of applications including diagnosis of dynamical systems and activity recognition. We provide a logical characterization of the notion of an explanation. To generate explanations we identify and exploit a correspondence between explanation generation and planning. The determination of good explanations requires additional domainspecific knowledge which we represent as preferences over explanations. The nature of explanations requires us to formulate preferences in a somewhat retrodictive fashion by utilizing Past Linear Temporal Logic. We propose methods for exploiting these somewhat unique preferences effectively within state-of-the-art planners and illustrate the feasibility of generating (preferred) explanations via planning.",
  "stance": 0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/7856/7715",
  "title": "A Closer Look at the Probabilistic Description Logic Prob-EL",
  "year": 2011,
  "venue": "AAAI",
  "abstract": "We study probabilistic variants of the description logic EL. For the case where probabilities apply only to concepts, we provide a careful analysis of the borderline between tractability and EXPTIME-completeness. One outcome is that any probability value except zero and one leads to intractability in the presence of general TBoxes, while this is not the case for classical TBoxes. For the case where probabilities can also be applied to roles, we show PSPACE-completeness. This result is (positively) surprising as the best previously known upper bound was 2-EXPTIME and there were reasons to believe in completeness for this class.",
  "stance": 0.9
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/7861/7720",
  "title": "Transportability of Causal and Statistical Relations: A Formal Approach",
  "year": 2011,
  "venue": "AAAI",
  "abstract": "We address the problem of transferring information learned from experiments to a different environment, in which only passive observations can be collected. We introduce a formal representation called “selection diagrams” for expressing knowledge about differences and commonalities between environments and, using this representation, we derive procedures for deciding whether effects in the target environment can be inferred from experiments conducted elsewhere. When the answer is affirmative, the procedures identify the set of experiments and observations that need be conducted to license the transport. We further discuss how transportability analysis can guide the transfer of knowledge in non-experimental learning to minimize re-measurement cost and improve prediction power. Introduction: Threats vs. Assumptions Science is about generalization; conclusions that are obtained in a laboratory setting are transported and applied elsewhere, in an environment that differs in many aspects from that of the laboratory. If the target environment is arbitrary, or drastically different from the study environment nothing can be learned from the latter. However, the fact that most experiments are conducted with the intention of applying the results elsewhere means that we usually deem the target environment sufficiently similar to the study environment to justify the transport of experimental results or their ramifications. Remarkably, the conditions that permit such transport have not received systematic formal treatment. The standard literature on this topic, falling under rubrics such as “quasi-experiments,” “meta analysis,” and “external validity,” consists primarily of “threats,” namely, verbal narratives of what can go wrong when we try to transport results from one study to another (e.g., [Shadish, Cook, and Campbell, 2002, chapter 3]). In contrast, we seek to establish “licensing assumptions,” namely, formal conditions under which the ∗This work was supported in parts by National Institutes of Health #1R01 LM009961-01, National Science Foundation #IIS0914211 and #IIS-1018922, and Office of Naval Research #N00014-09-1-0665 and #N00014-10-1-0933. Copyright c © 2011, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. transport of results across diverse environments is licensed from first principles. The machine learning literature, on the other hand, while seriously concerned about discrepancies between training and test environments [Daume III and Marcu, 2006; Storkey, 2009], has focused almost exclusively on predictive, or classification tasks as opposed to effect-learning tasks. Moreover, even in classification tasks, machine learning researchers have rarely allowed apriori causal knowledge to guide the learning process and, as a result, have not sought theoretical guarantees in the form of sufficient conditions under which discrepancies between the training and test environments can be circumvented, or necessary conditions without which bias will persist regardless of sample size. This paper establishes such conditions (see section on “Transportability across Observational domains”) and thus informs researchers on what can be gained by domain-specific knowledge when available, and what could explain why transfer-learning algorithms fail to converge or perform. Transportability analysis requires a formal language within which the notion of “environment” is given precise characterization, and differences among environments can be encoded and analyzed. The advent of causal diagrams [Pearl, 1995; Spirtes, Glymour, and Scheines, 2000; Pearl, 2009; Koller and Friedman, 2009] provides such a language and renders the formalization of transportability possible. Using this language, this paper offers a precise definition for the notion of transportability and establishes formal conditions that, if held true, would permit us to transport results across domains, environments, or populations.",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/7902/7761",
  "title": "Size Adaptive Selection of Most Informative Features",
  "year": 2011,
  "venue": "AAAI",
  "abstract": "In this paper, we propose a novel method to select the most informative subset of features, which has little redundancy and very strong discriminating power. Our proposed approach automatically determines the optimal number of features and selects the best subset accordingly by maximizing the average pairwise informativeness, thus has obvious advantage over traditional filter methods. By relaxing the essential combinatorial optimization problem into the standard quadratic programming problem, the most informative feature subset can be obtained efficiently, and a strategy to dynamically compute the redundancy between feature pairs further greatly accelerates our method through avoiding unnecessary computations of mutual information. As shown by the extensive experiments, the proposed method can successfully select the most informative subset of features, and the obtained classification results significantly outperform the state-of-the-art results on most test datasets.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/8111/7969",
  "title": "A Data-Driven Approach to Question Subjectivity Identification in Community Question Answering",
  "year": 2012,
  "venue": "AAAI",
  "abstract": "Automatic Subjective Question Answering (ASQA), which aims at answering users’ subjective questions using summaries of multiple opinions, becomes increasingly important. One challenge of ASQA is that expected answers for subjective questions may not readily exist in the Web. The rising and popularity of Community Question Answering (CQA) sites, which provide platforms for people to post and answer questions, provides an alternative to ASQA. One important task of ASQA is question subjectivity identification, which identifies whether a user is asking a subjective question. Unfortunately, there has been little labeled training data available for this task. In this paper, we propose an approach to collect training data automatically by utilizing social signals in CQA sites without involving any manual labeling. Experimental results show that our data-driven approach achieves 9.37% relative improvement over the supervised approach using manually labeled data, and achieves 5.15% relative gain over a stateof-the-art semi-supervised approach. In addition, we propose several heuristic features for question subjectivity identification. By adding these features, we achieve 11.23% relative improvement over word n-gram feature under the same experimental setting.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/8221/8079",
  "title": "Basing Decisions on Sentences in Decision Diagrams",
  "year": 2012,
  "venue": "AAAI",
  "abstract": "The Sentential Decision Diagram (SDD) is a recently proposed representation of Boolean functions, containing Ordered Binary Decision Diagrams (OBDDs) as a distinguished subclass. While OBDDs are characterized by total variable orders, SDDs are characterized by dissections of variable orders, known as vtrees. Despite this generality, SDDs retain a number of properties, such as canonicity and a polytime Apply operator, that have been critical to the practical success of OBDDs. Moreover, upper bounds on the size of SDDs were also given, which are tighter than comparable upper bounds on the size of OBDDs. In this paper, we analyze more closely some of the theoretical properties of SDDs and their size. In particular, we consider the impact of basing decisions on sentences (using dissections as in SDDs), in comparison to basing decisions on variables (using total variable orders as in OBDDs). Here, we identify a class of Boolean functions where basing decisions on sentences using dissections of a variable order can lead to exponentially more compact SDDs, compared to OBDDs based on the same variable order. Moreover, we identify a fundamental property of the decompositions that underlie SDDs and use it to show how certain changes to a vtree can also lead to exponential differences in the size of an SDD.",
  "stance": 0.2
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/8453/8312",
  "title": "Learning to Rank Effective Paraphrases from Query Logs for Community Question Answering",
  "year": 2013,
  "venue": "AAAI",
  "abstract": "We present a novel method for ranking query paraphrases for effective search in community question answering (cQA). The method uses query logs from Yahoo! Search and Yahoo! Answers for automatically extracting a corpus of paraphrases of queries and questions using the query-question click history. Elements of this corpus are automatically ranked according to recall and mean reciprocal rank, and then used for learning two independent learning to rank models (SVMRank), whereby a set of new query paraphrases can be scored according to recall and MRR. We perform several automatic evaluation procedures using cross-validation for analyzing the behavior of various aspects of our learned ranking functions, which show that our method is useful and effective for search in cQA.",
  "stance": 0.9
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/8464/8323",
  "title": "Heterogeneous Metric Learning with Joint Graph Regularization for Cross-Media Retrieval",
  "year": 2013,
  "venue": "AAAI",
  "abstract": "As the major component of big data, unstructured heterogeneous multimedia content such as text, image, audio, video and 3D increasing rapidly on the Internet. User demand a new type of cross-media retrieval where user can search results across various media by submitting query of any media. Since the query and the retrieved results can be of different media, how to learn a heterogeneous metric is the key challenge. Most existing metric learning algorithms only focus on a single media where all of the media objects share the same data representation. In this paper, we propose a joint graph regularized heterogeneous metric learning (JGRHML) algorithm, which integrates the structure of different media into a joint graph regularization. In JGRHML, different media are complementary to each other and optimizing them simultaneously can make the solution smoother for both media and further improve the accuracy of the final metric. Based on the heterogeneous metric, we further learn a high-level semantic metric through label propagation. JGRHML is effective to explore the semantic relationship hidden across different modalities. The experimental results on two datasets with up to five media types show the effectiveness of our proposed approach.",
  "stance": 0.7
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/8515/8374",
  "title": "Invited Talks",
  "year": 2013,
  "venue": "AAAI",
  "abstract": "Most approaches to semantics in computational linguistics represent meaning in terms of words or abstract symbols. Grounded-language research bases the meaning of natural language on perception and/or action in the (real or virtual) world. Machine learning has become the most effective approach to constructing natural-language systems; however, current methods require a great deal of laboriously annotated training data. Ideally, a computer would be able to acquire language like a child, by being exposed to language in the context of a relevant but ambiguous environment, thereby grounding its learning in perception and action. We will review recent research in grounded language learning and discuss future directions. Raymond J. Mooney is a professor in the Department of Computer Science at the University of Texas at Austin. He received his Ph.D. in 1988 from the University of Illinois at Urbana/Champaign. He is an author of over 150 published research papers, primarily in the areas of machine learning and natural language processing. He was the president of the International Machine Learning Society from 2008-2011, program cochair for AAAI 2006, and is a AAAI and ACM Fellow. His recent research has focused on learning for natural-language processing, statistical relational learning, active transfer learning, and connecting language, perception and action.",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/8564/8423",
  "title": "Reasoning about Saturated Conditional Independence Under Uncertainty: Axioms, Algorithms, and Levesque's Situations to the Rescue",
  "year": 2013,
  "venue": "AAAI",
  "abstract": "The implication problem of probabilistic conditional independencies is investigated in the presence of missing data. Here, graph separation axioms fail to hold for saturated conditional independencies, unlike the known idealized case with no missing data. Several axiomatic, algorithmic, and logical characterizations of the implication problem for saturated conditional independencies are established. In particular, equivalences are shown to the implication problem of a propositional fragment under Levesque’s situations, and that of Lien’s class of multivalued database dependencies under null values.",
  "stance": -0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/8566/8425",
  "title": "A Topic-Based Coherence Model for Statistical Machine Translation",
  "year": 2013,
  "venue": "AAAI",
  "abstract": "Coherence that ties sentences of a text into a meaningfully connected structure is of great importance to text generation and translation. In this paper, we propose a topic-based coherence model to produce coherence for document translation, in terms of the continuity of sentence topics in a text. We automatically extract a coherence chain for each source text to be translated. Based on the extracted source coherence chain, we adopt a maximum entropy classifier to predict the target coherence chain that defines a linear topic structure for the target document. The proposed topic-based coherence model then uses the predicted target coherence chain to help decoder select coherent word/phrase translations. Our experiments show that incorporating the topic-based coherence model into machine translation achieves substantial improvement over both the baseline and previous methods that integrate document topics rather than coherence chains into machine translation.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/8567/8426",
  "title": "An Extended GHKM Algorithm for Inducing Lambda-SCFG",
  "year": 2013,
  "venue": "AAAI",
  "abstract": "Semantic parsing, which aims at mapping a natural language (NL) sentence into its formal meaning representation (e.g., logical form), has received increasing attention in recent years. While synchronous context-free grammar (SCFG) augmented with lambda calculus (λSCFG) provides an effective mechanism for semantic parsing, how to learn such λ-SCFG rules still remains a challenge because of the difficulty in determining the correspondence between NL sentences and logical forms. To alleviate this structural divergence problem, we extend the GHKM algorithm, which is a state-ofthe-art algorithm for learning synchronous grammars in statistical machine translation, to induce λ-SCFG from pairs of NL sentences and logical forms. By treating logical forms as trees, we reformulate the theory behind GHKM that gives formal semantics to the alignment between NL words and logical form tokens. Experiments on the GEOQUERY dataset show that our semantic parser achieves an F-measure of 90.2%, the best result published to date.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/8702/8561",
  "title": "Fraudulent Support Telephone Number Identification Based on Co-Occurrence Information on the Web",
  "year": 2014,
  "venue": "AAAI",
  "abstract": "“Fraudulent support phones” refers to the misleading telephone numbers placed on Web pages or other media that claim to provide services with which they are not associated. Most fraudulent support phone information is found on search engine result pages (SERPs), and such information substantially degrades the search engine user experience. In this paper, we propose an approach to identify fraudulent support telephone numbers on the Web based on the co-occurrence relations between telephone numbers that appear on SERPs. We start from a small set of seed official support phone numbers and seed fraudulent numbers. Then, we construct a co-occurrence graph according to the co-occurrence relationships of the telephone numbers that appear on Web pages. Additionally, we take the page layout information into consideration on the assumption that telephone numbers that appear in nearby page blocks should be regarded as more closely related. Finally, we develop a propagation algorithm to diffuse the trust scores of seed official support phone numbers and the distrust scores of the seed fraudulent numbers on the co-occurrence graph to detect additional fraudulent numbers. Experimental results based on over 1.5 million SERPs produced by a popular Chinese commercial search engine indicate that our approach outperforms TrustRank, Anti-TrustRank and Good-Bad Rank algorithms by achieving an AUC value of over 0.90.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/8704/8563",
  "title": "A Joint Optimization Model for Image Summarization Based on Image Content and Tags",
  "year": 2014,
  "venue": "AAAI",
  "abstract": "As an effective technology for navigating a large number of images, image summarization is becoming a promising task with the rapid development of image sharing sites and social networks. Most existing summarization approaches use the visual-based features for image representation without considering tag information. In this paper, we propose a novel framework, named JOINT, which employs both image content and tag information to summarize images. Our model generates the summary images which can best reconstruct the original collection. Based on the assumption that an image with representative content should also have typical tags, we introduce a similarity-inducing regularizer to our model. Furthermore, we impose the lasso penalty on the objective function to yield a concise summary set. Extensive experiments demonstrate our model outperforms the state-of-the-art approaches.",
  "stance": 0.9
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/8745/8604",
  "title": "Synthesis of Geometry Proof Problems",
  "year": 2014,
  "venue": "AAAI",
  "abstract": "This paper presents a semi-automated methodology for generating geometric proof problems of the kind found in a highschool curriculum. We formalize the notion of a geometry proof problem and describe an algorithm for generating such problems over a user-provided figure. Our experimental results indicate that our problem generation algorithm can effectively generate proof problems in elementary geometry. On a corpus of 110 figures taken from popular geometry textbooks, our system generated an average of about 443 problems per figure in an average time of 4.7 seconds per figure.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/8751/8610",
  "title": "Learning Unknown Event Models",
  "year": 2014,
  "venue": "AAAI",
  "abstract": "Agents with incomplete environment models are likely to be surprised, and this represents an opportunity to learn. We investigate approaches for situated agents to detect surprises, discriminate among different forms of surprise, and hypothesize new models for the unknown events that surprised them. We instantiate these approaches in a new goal reasoning agent (named FOOLMETWICE), investigate its performance in simulation studies, and report that it produces plans with significantly reduced execution cost in comparison to not learning models for surprising events.",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/8799/8658",
  "title": "Lazy Defenders Are Almost Optimal against Diligent Attackers",
  "year": 2014,
  "venue": "AAAI",
  "abstract": "Most work building on the Stackelberg security games model assumes that the attacker can perfectly observe the defender’s randomized assignment of resources to targets. This assumption has been challenged by recent papers, which designed tailor-made algorithms that compute optimal defender strategies for security games with limited surveillance. We analytically demonstrate that in zero-sum security games, lazy defenders, who simply keep optimizing against perfectly informed attackers, are almost optimal against diligent attackers, who go to the effort of gathering a reasonable number of observations. This result implies that, in some realistic situations, limited surveillance may not need to be explicitly addressed.",
  "stance": -0.7
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/8819/8678",
  "title": "Equilibria in Epidemic Containment Games",
  "year": 2014,
  "venue": "AAAI",
  "abstract": "The spread of epidemics and malware is commonly modeled by diffusion processes on networks. Protective interventions such as vaccinations or installing anti-virus software are used to contain their spread. Typically, each node in the network has to decide its own strategy of securing itself, and its benefit depends on which other nodes are secure, making this a natural game-theoretic setting. There has been a lot of work on network security game models, but most of the focus has been either on simplified epidemic models or homogeneous",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/9162/9021",
  "title": "Clustering-Based Collaborative Filtering for Link Prediction",
  "year": 2015,
  "venue": "AAAI",
  "abstract": "In this paper, we propose a novel collaborative filtering approach for predicting the unobserved links in a network (or graph) with both topological and node features. Our approach improves the well-known compressed sensing based matrix completion method by introducing a new multipleindependent-Bernoulli-distribution model as the data sampling mask. It makes better link predictions since the model is more general and better matches the data distributions in many real-world networks, such as social networks like Facebook. As a result, a satisfying stability of the prediction can be guaranteed. To obtain an accurate multiple-independentBernoulli-distribution model of the topological feature space, our approach adjusts the sampling of the adjacency matrix of the network (or graph) using the clustering information in the node feature space. This yields a better performance than those methods which simply combine the two types of features. Experimental results on several benchmark datasets suggest that our approach outperforms the best existing link prediction methods.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/9166/9025",
  "title": "Mining Query Subtopics from Questions in Community Question Answering",
  "year": 2015,
  "venue": "AAAI",
  "abstract": "This paper proposes mining query subtopics from questions in community question answering (CQA). The subtopics are represented as a number of clusters of questions with keywords summarizing the clusters. The task is unique in that the subtopics from questions can not only facilitate user browsing in CQA search, but also describe aspects of queries from a question-answering perspective. The challenges of the task include how to group semantically similar questions and how to find keywords capable of summarizing the clusters. We formulate the subtopic mining task as a non-negative matrix factorization (NMF) problem and further extend the model of NMF to incorporate question similarity estimated from metadata of CQA into learning. Compared with existing methods, our method can jointly optimize question clustering and keyword extraction and encourage the former task to enhance the latter. Experimental results on large scale real world CQA datasets show that the proposed method significantly outperforms the existing methods in terms of keyword extraction, while achieving a comparable performance to the state-ofthe-art methods for question clustering.",
  "stance": 0.9
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/9176/9035",
  "title": "Extracting Bounded-Level Modules from Deductive RDF Triplestores",
  "year": 2015,
  "venue": "AAAI",
  "abstract": "We present a novel semantics for extracting bounded-level modules from RDF ontologies and databases augmented with safe inference rules, à la Datalog. Dealing with a recursive rule language poses challenging issues for defining the module semantics, and also makes module extraction algorithmically unsolvable in some cases. Our results include a set of module extraction algorithms compliant with the novel semantics. Experimental results show that the resulting framework is effective in extracting expressive modules from RDF datasets with formal guarantees, whilst controlling their suc-",
  "stance": 0.8
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/9293/9152",
  "title": "Assessing the Robustness of Cremer-McLean with Automated Mechanism Design",
  "year": 2015,
  "venue": "AAAI",
  "abstract": "In a classic result in the mechanism design literature, Cremer and McLean (1985) show that if buyers’ valuations are sufficiently correlated, a mechanism exists that allows the seller to extract the full surplus from efficient allocation as revenue. This result is commonly seen as “too good to be true” (in practice), casting doubt on its modeling assumptions. In this paper, we use an automated mechanism design approach to assess how sensitive the Cremer-McLean result is to relaxing its main technical assumption. That assumption implies that each valuation that a bidder can have results in a unique conditional distribution over the external signal(s). We relax this, allowing multiple valuations to be consistent with the same distribution over the external signal(s). Using similar insights to Cremer-McLean, we provide a highly efficient algorithm for computing the optimal revenue in this more general case. Using this algorithm, we observe that indeed, as the number of valuations consistent with a distribution grows, the optimal revenue quickly drops to that of a reserve-price mechanism. Thus, automated mechanism design allows us to gain insight into the precise sense in which Cremer-McLean is “too good to be true.”",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/9305/9164",
  "title": "Hedonic Coalition Formation in Networks",
  "year": 2015,
  "venue": "AAAI",
  "abstract": "Coalition formation is a fundamental problem in the organization of many multi-agent systems. In large populations, the formation of coalitions is often restricted by structural visibility and locality constraints under which agents can reorganize. We capture and study this aspect using a novel network-based model for dynamic locality within the popular framework of hedonic coalition formation games. We analyze the effects of network-based visibility and structure on the convergence of coalition formation processes to stable states. Our main result is a tight characterization of the structures based on which dynamic coalition formation can stabilize quickly. Maybe surprisingly, polynomial-time convergence can be achieved if and only if coalition formation is based on complete or star graphs.",
  "stance": 0.1
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/10033/9892",
  "title": "Refining Subgames in Large Imperfect Information Games",
  "year": 2016,
  "venue": "AAAI",
  "abstract": "The leading approach to solving large imperfect information games is to pre-calculate an approximate solution using a simplified abstraction of the full game; that solution is then used to play the original, full-scale game. The abstraction step is necessitated by the size of the game tree. However, as the original game progresses, the remaining portion of the tree (the subgame) becomes smaller. An appealing idea is to use the simplified abstraction to play the early parts of the game and then, once the subgame becomes tractable, to calculate a solution using a finer-grained abstraction in real time, creating a combined final strategy. While this approach is straightforward for perfect information games, it is a much more complex problem for imperfect information games. If the subgame is solved locally, the opponent can alter his play in prior to this subgame to exploit our combined strategy. To prevent this, we introduce the notion of subgame margin, a simple value with appealing properties. If any best response reaches the subgame, the improvement of exploitability of the combined strategy is (at least) proportional to the subgame margin. This motivates subgame refinements resulting in large positive margins. Unfortunately, current techniques either neglect subgame margin (potentially leading to a large negative subgame margin and drastically more exploitable strategies), or guarantee only non-negative subgame margin (possibly producing the original, unrefined strategy, even if much stronger strategies are possible). Our technique remedies this problem by maximizing the subgame margin and is guaranteed to find the optimal solution. We evaluate our technique using one of the top participants of the AAAI-14 Computer Poker Competition, the leading playground for agents in imperfect information settings.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/9992/9851",
  "title": "Commonsense in Parts: Mining Part-Whole Relations from the Web and Image Tags",
  "year": 2016,
  "venue": "AAAI",
  "abstract": "Commonsense knowledge about part-whole relations (e.g., screen partOf notebook) is important for interpreting user input in web search and question answering, or for object detection in images. Prior work on knowledge base construction has compiled part-whole assertions, but with substantial limitations: i) semantically different kinds of part-whole relations are conflated into a single generic relation, ii) the arguments of a part-whole assertion are merely words with ambiguous meaning, iii) the assertions lack additional attributes like visibility (e.g., a nose is visible but a kidney is not) and cardinality information (e.g., a bird has two legs while a spider eight), iv) limited coverage of only tens of thousands of assertions. This paper presents a new method for automatically acquiring part-whole commonsense from Web contents and image tags at an unprecedented scale, yielding many millions of assertions, while specifically addressing the four shortcomings of prior work. Our method combines pattern-based information extraction methods with logical reasoning. We carefully distinguish different relations: physicalPartOf, memberOf, substanceOf. We consistently map the arguments of all assertions onto WordNet senses, eliminating the ambiguity of wordlevel assertions. We identify whether the parts can be visually perceived, and infer cardinalities for the assertions. The resulting commonsense knowledge base has very high quality and high coverage, with an accuracy of 89% determined by extensive sampling, and is publicly available.",
  "stance": 0.8
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/9994/9853",
  "title": "Inferring a Personalized Next Point-of-Interest Recommendation Model with Latent Behavior Patterns",
  "year": 2016,
  "venue": "AAAI",
  "abstract": "In this paper, we address the problem of personalized next Point-of-interest (POI) recommendation which has become an important and very challenging task in location-based social networks (LBSNs), but not well studied yet. With the conjecture that, under different contextual scenario, human exhibits distinct mobility patterns, we attempt here to jointly model the next POI recommendation under the influence of user’s latent behavior pattern. We propose to adopt a third-rank tensor to model the successive check-in behaviors. By incorporating softmax function to fuse the personalized Markov chain with latent pattern, we furnish a Bayesian Personalized Ranking (BPR) approach and derive the optimization criterion accordingly. Expectation Maximization (EM) is then used to estimate the model parameters. Extensive experiments on two large-scale LBSNs datasets demonstrate the significant improvements of our model over several state-of-the-art methods.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/10479/10338",
  "title": "CLARE: A Joint Approach to Label Classification and Tag Recommendation",
  "year": 2017,
  "venue": "AAAI",
  "abstract": "Data classification and tag recommendation are both important and challenging tasks in social media. These two tasks are often considered independently and most efforts have been made to tackle them separately. However, labels in data classification and tags in tag recommendation are inherently related. For example, a Youtube video annotated with NCAA, stadium, pac12 is likely to be labeled as football, while a video/image with the class label of coast is likely to be tagged with beach, sea, water and sand. The existence of relations between labels and tags motivates us to jointly perform classification and tag recommendation for social media data in this paper. In particular, we provide a principled way to capture the relations between labels and tags, and propose a novel framework CLARE, which fuses data CLAssification and tag REcommendation into a coherent model. With experiments on three social media datasets, we demonstrate that the proposed framework CLARE achieves superior performance on both tasks compared to the state-of-the-art methods.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/10490/10349",
  "title": "Web-Based Semantic Fragment Discovery for On-Line Lingual-Visual Similarity",
  "year": 2017,
  "venue": "AAAI",
  "abstract": "In this paper, we present an automatic approach for on-line discovery of visual-lingual semantic fragments from weakly labeled Internet images. Instead of learning region-entity correspondences from well-labeled image-sentence pairs, our approach directly collects and enhances the weakly labeled visual contents from the Web and constructs an adaptive visual representation which automatically links generic lingual phrases to their related visual contents. To ensure reliable and efficient semantic discovery, we adopt non-parametric density estimation to re-rank the related visual instances and proposed a fast self-similarity-based quality assessment method to identify the high-quality semantic fragments. The discovered semantic fragments provide an adaptive joint representation for texts and images, based on which lingual-visual similarity can be defined for further co-analysis of heterogeneous multimedia data. Experimental results on semantic fragment quality assessment, sentence-based image retrieval, automatic multimedia insertion and ordering demonstrated the effectiveness of the proposed framework.The experiments show that the proposed methods can make effective use of the Web knowledge, and are able to generate competitive results compared to state-of-the-art approaches in various tasks.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/10499/10358",
  "title": "A Dependency-Based Neural Reordering Model for Statistical Machine Translation",
  "year": 2017,
  "venue": "AAAI",
  "abstract": "In machine translation (MT) that involves translating between two languages with significant differences in word order, determining the correct word order of translated words is a major challenge. The dependency parse tree of a source sentence can help to determine the correct word order of the translated words. In this paper, we present a novel reordering approach utilizing a neural network and dependency-based embeddings to predict whether the translations of two source words linked by a dependency relation should remain in the same order or should be swapped in the translated sentence. Experiments on Chinese-to-English translation show that our approach yields a statistically significant improvement of 0.57 BLEU point on benchmark NIST test sets, compared to our prior state-of-theart statistical MT system that uses sparse dependency-based reordering features.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/10508/10367",
  "title": "SnapNETS: Automatic Segmentation of Network Sequences with Node Labels",
  "year": 2017,
  "venue": "AAAI",
  "abstract": "Given a sequence of snapshots of flu propagating over a population network, can we find a segmentation when the patterns of the disease spread change, possibly due to interventions? In this paper, we study the problem of segmenting graph sequences with labeled nodes. Memes on the Twitter network, diseases over a contact network, movie-cascades over a social network, etc. are all graph sequences with labeled nodes. Most related work is on plain graphs (and hence ignore the label dynamics) or fix parameters or require much feature engineering. Instead, we propose SNAPNETS, to automatically find segmentations of such graph sequences, with different characteristics of nodes of each label in adjacent segments. It satisfies all the desired properties (being parameter-free, comprehensive and scalable) by leveraging a principled, multilevel, flexible framework which maps the problem to a path optimization problem over a weighted DAG. Extensive experiments on several diverse real datasets show that it finds cut points matching ground-truth or meaningful external signals outperforming non-trivial baselines. We also show that SNAPNETS scales near-linearly with the size of the input.",
  "stance": 0.8
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/10588/10447",
  "title": "Obvious Strategyproofness Needs Monitoring for Good Approximations",
  "year": 2017,
  "venue": "AAAI",
  "abstract": "Obvious strategyproofness (OSP) is an appealing concept as it allows to maintain incentive compatibility even in the presence of agents that are not fully rational, e.g., those who struggle with contingent reasoning (Li 2015). However, it has been shown to impose some limitations, e.g., no OSP mechanism can return a stable matching (Ashlagi and Gonczarowski",
  "stance": -1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/10607/10466",
  "title": "On Pareto Optimality in Social Distance Games",
  "year": 2017,
  "venue": "AAAI",
  "abstract": "We investigate Pareto stability in Social Distance Games, that are coalition forming games in which agents utilities are proportional to their harmonic centralities in the respective coalitions, i.e., to the average inverse distance from the other agents. Pareto optimal solutions have been already considered in the literature as outcomes arising from the strategic interaction of the agents. In particular, they are stable under the deviation of the grand coalition, as they do not permit a simultaneous deviation by all the agents making all of them weakly better off and some strictly better off. We first show that, while computing a Pareto stable solution maximizing the social welfare is NP-hard in bounded degree graphs, a 2min{Δ,√n}-approximating one can be determined in polynomial time, where n is the number of agents and Δ the maximum node degree. We then determine asymptotically tight bounds on the Price of Pareto Optimality for several classes of social graphs arising from the following combinations: unbounded and bounded node degree, undirected and directed edges, unweighted and weighted edges.",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/10618/10477",
  "title": "Resource Graph Games: A Compact Representation for Games with Structured Strategy Spaces",
  "year": 2017,
  "venue": "AAAI",
  "abstract": "In many real-world systems, strategic agents’ decisions can be understood as complex—i.e., consisting of multiple subdecisions—and hence can give rise to an exponential number of pure strategies. Examples include network congestion games, simultaneous auctions, and security games. However, agents’ sets of strategies are often structured, allowing them to be represented compactly. There currently exists no general modeling language that captures a wide range of commonly seen strategy structure and utility structure. We propose Resource Graph Games (RGGs), the first general compact representation for games with structured strategy spaces, which is able to represent a wide range of games studied in literature. We leverage recent results about multilinearity, a key property of games that allows us to represent the mixed strategies compactly, and, as a result, to compute various equilibrium concepts efficiently. While not all RGGs are multilinear, we provide a general method of converting RGGs to those that are multilinear, and identify subclasses of RGGs whose converted version allow efficient computation.",
  "stance": 0.8
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/10630/10489",
  "title": "The Benefit in Free Information Disclosure When Selling Information to People",
  "year": 2017,
  "venue": "AAAI",
  "abstract": "This paper studies the benefit for information providers in free public information disclosure in settings where the prospective information buyers are people. The underlying model, which applies to numerous real-life situations, considers a standard decision making setting where the decision maker is uncertain about the outcomes of her decision. The information provider can fully disambiguate this uncertainty and wish to maximize her profit from selling such information. We use a series of AMT-based experiments with people to test the benefit for the information provider from reducing some of the uncertainty associated with the decision maker’s problem, for free. Free information disclosure of this kind can be proved to be ineffective when the buyer is a fullyrational agent. Yet, when it comes to people we manage to demonstrate that a substantial improvement in the information provider’s profit can be achieved with such an approach. The analysis of the results reveals that the primary reason for this phenomena is people’s failure to consider the strategic nature of the interaction with the information provider. Peoples’ inability to properly calculate the value of information is found to be secondary in its influence.",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/10645/10504",
  "title": "A Generic Bet-and-Run Strategy for Speeding Up Stochastic Local Search",
  "year": 2017,
  "venue": "AAAI",
  "abstract": "A common strategy for improving optimization algorithms is to restart the algorithm when it is believed to be trapped in an inferior part of the search space. However, while specific restart strategies have been developed for specific problems (and specific algorithms), restarts are typically not regarded as a general tool to speed up an optimization algorithm. In fact, many optimization algorithms do not employ restarts at",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/11255/11114",
  "title": "Discovering and Distinguishing Multiple Visual Senses for Polysemous Words",
  "year": 2018,
  "venue": "AAAI",
  "abstract": "To reduce the dependence on labeled data, there have been increasing research efforts on learning visual classifiers by exploiting web images. One issue that limits their performance is the problem of polysemy. To solve this problem, in this work, we present a novel framework that solves the problem of polysemy by allowing sense-specific diversity in search results. Specifically, we first discover a list of possible semantic senses to retrieve sense-specific images. Then we merge visual similar semantic senses and prune noises by using the retrieved images. Finally, we train a visual classifier for each selected semantic sense and use the learned sense-specific classifiers to distinguish multiple visual senses. Extensive experiments on classifying images into sense-specific categories and re-ranking search results demonstrate the superiority of our proposed approach.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/11256/11115",
  "title": "Representation Learning for Scale-Free Networks",
  "year": 2018,
  "venue": "AAAI",
  "abstract": "Network embedding aims to learn the low-dimensional representations of vertexes in a network, while structure and inherent properties of the network is preserved. Existing network embedding works primarily focus on preserving the microscopic structure, such as the firstand second-order proximity of vertexes, while the macroscopic scale-free property is largely ignored. Scale-free property depicts the fact that vertex degrees follow a heavy-tailed distribution (i.e., only a few vertexes have high degrees) and is a critical property of realworld networks, such as social networks. In this paper, we study the problem of learning representations for scale-free networks. We first theoretically analyze the difficulty of embedding and reconstructing a scale-free network in the Euclidean space, by converting our problem to the sphere packing problem. Then, we propose the “degree penalty” principle for designing scale-free property preserving network embedding algorithm: punishing the proximity between high-degree vertexes. We introduce two implementations of our principle by utilizing the spectral techniques and a skip-gram model respectively. Extensive experiments on six datasets show that our algorithms are able to not only reconstruct heavy-tailed distributed degree distribution, but also outperform state-ofthe-art embedding models in various network mining tasks, such as vertex classification and link prediction.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/11277/11136",
  "title": "Deep Region Hashing for Generic Instance Search from Images",
  "year": 2018,
  "venue": "AAAI",
  "abstract": "Instance Search (INS) is a fundamental problem for many applications, while it is more challenging comparing to traditional image search since the relevancy is defined at the instance level. Existing works have demonstrated the success of many complex ensemble systems that are typically conducted by firstly generating object proposals, and then extracting handcrafted and/or CNN features of each proposal for matching. However, object bounding box proposals and feature extraction are often conducted in two separated steps, thus the effectiveness of these methods collapses. Also, due to the large amount of generated proposals, matching speed becomes the bottleneck that limits its application to largescale datasets. To tackle these issues, in this paper we propose an effective and efficient Deep Region Hashing (DRH) approach for large-scale INS using an image patch as the query. Specifically, DRH is an end-to-end deep neural network which consists of object proposal, feature extraction, and hash code generation. DRH shares full-image convolutional feature map with the region proposal network, thus enabling nearly cost-free region proposals. Also, each highdimensional, real-valued region features are mapped onto a low-dimensional, compact binary codes for the efficient object region level matching on large-scale dataset. Experimental results on four datasets show that our DRH can achieve even better performance than the state-of-the-arts in terms of mAP, while the efficiency is improved by nearly 100 times.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/11307/11166",
  "title": "Learning the Joint Representation of Heterogeneous Temporal Events for Clinical Endpoint Prediction",
  "year": 2018,
  "venue": "AAAI",
  "abstract": "The availability of a large amount of electronic health records (EHR) provides huge opportunities to improve health care service by mining these data. One important application is clinical endpoint prediction, which aims to predict whether a disease, a symptom or an abnormal lab test will happen in the future according to patients’ history records. This paper develops deep learning techniques for clinical endpoint prediction, which are effective in many practical applications. However, the problem is very challenging since patients’ history records contain multiple heterogeneous temporal events such as lab tests, diagnosis, and drug administrations. The visiting patterns of different types of events vary significantly, and there exist complex nonlinear relationships between different events. In this paper, we propose a novel model for learning the joint representation of heterogeneous temporal events. The model adds a new gate to control the visiting rates of different events which effectively models the irregular patterns of different events and their nonlinear correlations. Experiment results with real-world clinical data on the tasks of predicting death and abnormal lab tests prove the effectiveness of our proposed approach over competitive baselines.",
  "stance": 0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/11308/11167",
  "title": "Generating an Event Timeline About Daily Activities From a Semantic Concept Stream",
  "year": 2018,
  "venue": "AAAI",
  "abstract": "Recognizing activities of daily living (ADLs) in the real world is an important task for understanding everyday human life. However, even though our life events consist of chronological ADLs with the corresponding places and objects (e.g., drinking coffee in the living room after making coffee in the kitchen and walking to the living room), most existing works focus on predicting individual activity labels from sensor data. In this paper, we introduce a novel framework that produces an event timeline of ADLs in a home environment. The proposed method combines semantic concepts such as action, object, and place detected by sensors for generating stereotypical event sequences with the following three realworld properties. First, we use temporal interactions among concepts to remove objects and places unrelated to each action. Second, we use commonsense knowledge mined from a language resource to find a possible combination of concepts in the real world. Third, we use temporal variations of events to filter repetitive events, since our daily life changes over time. We use cross-place validation to evaluate our proposed method on a daily-activities dataset with manually labeled event descriptions. The empirical evaluation demonstrates that our method using real-world properties improves the performance of generating an event timeline over diverse",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/11320/11179",
  "title": "Learning Nonlinear Dynamics in Efficient, Balanced Spiking Networks Using Local Plasticity Rules",
  "year": 2018,
  "venue": "AAAI",
  "abstract": "The brain uses spikes in neural circuits to perform many dynamical computations. The computations are performed with properties such as spiking efficiency, i.e. minimal number of spikes, and robustness to noise. A major obstacle for learning computations in artificial spiking neural networks with such desired biological properties is due to lack of our understanding of how biological spiking neural networks learn",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/11508/11367",
  "title": "Understanding Over Participation in Simple Contests",
  "year": 2018,
  "venue": "AAAI",
  "abstract": "One key motivation for using contests in real-life is the substantial evidence reported in empirical contest-design literature for people’s tendency to act more competitively in contests than predicted by the Nash Equilibrium. This phenomenon has been traditionally explained by people’s eagerness to win and maximize their relative (rather than absolute) payoffs. In this paper we make use of “simple contests”, where contestants only need to strategize on whether to participate in the contest or not, as an infrastructure for studying whether indeed more effort is exerted in contests due to competitiveness, or perhaps this can be attributed to other factors that hold also in non-competitive settings. The experimental methodology we use compares contestants’ participation decisions in eight contest settings differing in the nature of the contest used, the number of contestants used and the theoretical participation predictions to those obtained (whenever applicable) by subjects facing equivalent non-competitive decision situations in the form of a lottery. We show that indeed people tend to over-participate in contests compared to the theoretical predictions, yet the same phenomenon holds (to a similar extent) also in the equivalent non-competitive settings. Meaning that many of the contests used nowadays as a means for inducing extra human effort, that are often complex to organize and manage, can be replaced by a simpler non-competitive mechanism that uses probabilistic prizes.",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/11512/11371",
  "title": "A Voting-Based System for Ethical Decision Making",
  "year": 2018,
  "venue": "AAAI",
  "abstract": "We present a general approach to automating ethical decisions, drawing on machine learning and computational social choice. In a nutshell, we propose to learn a model of societal preferences, and, when faced with a specific ethical dilemma at runtime, efficiently aggregate those preferences to identify a desirable choice. We provide a concrete algorithm that instantiates our approach; some of its crucial steps are informed by a new theory of swap-dominance efficient voting rules. Finally, we implement and evaluate a system for ethical decision making in the autonomous vehicle domain, using preference data collected from 1.3 million people through the Moral",
  "stance": 0.8
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/11537/11396",
  "title": "Stream Reasoning in Temporal Datalog",
  "year": 2018,
  "venue": "AAAI",
  "abstract": "In recent years, there has been an increasing interest in extending traditional stream processing engines with logical, rule-based, reasoning capabilities. This poses significant theoretical and practical challenges since rules can derive new information and propagate it both towards past and future time points; as a result, streamed query answers can depend on data that has not yet been received, as well as on data that arrived far in the past. Stream reasoning algorithms, however, must be able to stream out query answers as soon as possible, and can only keep a limited number of previous input facts in memory. In this paper, we propose novel reasoning problems to deal with these challenges, and study their computational properties on Datalog extended with a temporal sort and the successor function—a core rule-based language for stream reasoning applications.",
  "stance": 0.3
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/11553/11412",
  "title": "Fair Inference on Outcomes",
  "year": 2018,
  "venue": "AAAI",
  "abstract": "In this paper, we consider the problem of fair statistical inference involving outcome variables. Examples include classification and regression problems, and estimating treatment effects in randomized trials or observational data. The issue of fairness arises in such problems where some covariates or treatments are “sensitive,” in the sense of having potential of creating discrimination. In this paper, we argue that the presence of discrimination can be formalized in a sensible way as the presence of an effect of a sensitive covariate on the outcome along certain causal pathways, a view which generalizes (Pearl 2009). A fair outcome model can then be learned by solving a constrained optimization problem. We discuss a number of complications that arise in classical statistical inference due to this view and provide workarounds based on recent work in causal and semi-parametric inference.",
  "stance": -0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/3765/3643",
  "title": "ColNet: Embedding the Semantics of Web Tables for Column Type Prediction",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "Automatically annotating column types with knowledge base (KB) concepts is a critical task to gain a basic understanding of web tables. Current methods rely on either table metadata like column name or entity correspondences of cells in the KB, and may fail to deal with growing web tables with incomplete meta information. In this paper we propose a neural network based column type annotation framework named ColNet which is able to integrate KB reasoning and lookup with machine learning and can automatically train Convolutional Neural Networks for prediction. The prediction model not only considers the contextual semantics within a cell using word representation, but also embeds the semantics of a column by learning locality features from multiple cells. The method is evaluated with DBPedia and two different web table datasets, T2Dv2 from the general Web and Limaye from Wikipedia pages, and achieves higher performance than the state-of-the-art approaches.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/3792/3670",
  "title": "Mining Entity Synonyms with Efficient Neural Set Generation",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "Mining entity synonym sets (i.e., sets of terms referring to the same entity) is an important task for many entity-leveraging applications. Previous work either rank terms based on their similarity to a given query term, or treats the problem as a two-phase task (i.e., detecting synonymy pairs, followed by organizing these pairs into synonym sets). However, these approaches fail to model the holistic semantics of a set and suffer from the error propagation issue. Here we propose a new framework, named SynSetMine, that efficiently generates entity synonym sets from a given vocabulary, using example sets from external knowledge bases as distant supervision. SynSetMine consists of two novel modules: (1) a set-instance classifier that jointly learns how to represent a permutation invariant synonym set and whether to include a new instance (i.e., a term) into the set, and (2) a set generation algorithm that enumerates the vocabulary only once and applies the learned set-instance classifier to detect all entity synonym sets in it. Experiments on three real datasets from different domains demonstrate both effectiveness and efficiency of SynSetMine for mining entity synonym sets.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/3795/3673",
  "title": "Learning from Web Data Using Adversarial Discriminative Neural Networks for Fine-Grained Classification",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "Fine-grained classification is absorbed in recognizing the subordinate categories of one field, which need a large number of labeled images, while it is expensive to label these images. Utilizing web data has been an attractive option to meet the demands of training data for convolutional neural networks (CNNs), especially when the well-labeled data is not enough. However, directly training on such easily obtained images often leads to unsatisfactory performance due to factors such as noisy labels. This has been conventionally addressed by reducing the noise level of web data. In this paper, we take a fundamentally different view and propose an adversarial discriminative loss to advocate representation coherence between standard and web data. This is further encapsulated in a simple, scalable and end-to-end trainable multi-task learning framework. We experiment on three public datasets using large-scale web data to evaluate the effectiveness and generalizability of the proposed approach. Extensive experiments demonstrate that our approach performs favorably against the state-of-the-art methods.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/3845/3723",
  "title": "Image Aesthetic Assessment Assisted by Attributes through Adversarial Learning",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "The inherent connections among aesthetic attributes and aesthetics are crucial for image aesthetic assessment, but have not been thoroughly explored yet. In this paper, we propose a novel image aesthetic assessment assisted by attributes through both representation-level and label-level. The attributes are used as privileged information, which is only required during training. Specifically, we first propose a multitask deep convolutional rating network to learn the aesthetic score and attributes simultaneously. The attributes are explored to construct better feature representations for aesthetic assessment through multi-task learning. After that, we introduce a discriminator to distinguish the predicted attributes and aesthetics of the multi-task deep network from the ground truth label distribution embedded in the training data. The multi-task deep network wants to output aesthetic score and attributes as close to the ground truth labels as possible. Thus the deep network and the discriminator compete with each other. Through adversarial learning, the attributes are explored to enforce the distribution of the predicted attributes and aesthetics to converge to the ground truth label distribution. Experimental results on two benchmark databases demonstrate the superiority of the proposed method to state of the art work.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/3848/3726",
  "title": "Multi3Net: Segmenting Flooded Buildings via Fusion of Multiresolution, Multisensor, and Multitemporal Satellite Imagery",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "We propose a novel approach for rapid segmentation of flooded buildings by fusing multiresolution, multisensor, and multitemporal satellite imagery in a convolutional neural network. Our model significantly expedites the generation of satellite imagery-based flood maps, crucial for first responders and local authorities in the early stages of flood events. By incorporating multitemporal satellite imagery, our model allows for rapid and accurate post-disaster damage assessment and can be used by governments to better coordinate mediumand long-term financial assistance programs for affected areas. The network consists of multiple streams of encoder-decoder architectures that extract spatiotemporal information from medium-resolution images and spatial information from high-resolution images before fusing the resulting representations into a single medium-resolution segmentation map of flooded buildings. We compare our model to state-of-the-art methods for building footprint segmentation as well as to alternative fusion approaches for the segmentation of flooded buildings and find that our model performs best on both tasks. We also demonstrate that our model produces highly accurate segmentation maps of flooded buildings using only publicly available medium-resolution data instead of significantly more detailed but sparsely available very high-resolution data. We release the first open-source dataset of fully preprocessed and labeled multiresolution, multispectral, and multitemporal satellite images of disaster sites along with our source code.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/3892/3770",
  "title": "DeepSTN+: Context-Aware Spatial-Temporal Neural Network for Crowd Flow Prediction in Metropolis",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "Crowd flow prediction is of great importance in a wide range of applications from urban planning, traffic control to public safety. It aims to predict the inflow (the traffic of crowds entering a region in a given time interval) and outflow (the traffic of crowds leaving a region for other places) of each region in the city with knowing the historical flow data. In this paper, we propose DeepSTN+, a deep learning-based convolutional model, to predict crowd flows in the metropolis. First, DeepSTN+ employs the ConvPlus structure to model the longrange spatial dependence among crowd flows in different regions. Further, PoI distributions and time factor are combined to express the effect of location attributes to introduce prior knowledge of the crowd movements. Finally, we propose an effective fusion mechanism to stabilize the training process, which further improves the performance. Extensive experimental results based on two real-life datasets demonstrate the superiority of our model, i.e., DeepSTN+ reduces the error of the crowd flow prediction by approximately 8%∼13% compared with the state-of-the-art baselines.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/3928/3806",
  "title": "Cognitive Deficit of Deep Learning in Numerosity",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "Subitizing, or the sense of small natural numbers, is an innate cognitive function of humans and primates; it responds to visual stimuli prior to the development of any symbolic skills, language or arithmetic. Given successes of deep learning (DL) in tasks of visual intelligence and given the primitivity of number sense, a tantalizing question is whether DL can comprehend numbers and perform subitizing. But somewhat disappointingly, extensive experiments of the type of cognitive psychology demonstrate that the examples-driven black box DL cannot see through superficial variations in visual representations and distill the abstract notion of natural number, a task that children perform with high accuracy and confidence. The failure is apparently due to the learning method not the CNN computational machinery itself. A recurrent neural network capable of subitizing does exist, which we construct by encoding a mechanism of mathematical morphology into the CNN convolutional kernels. Also, we investigate, using subitizing as a test bed, the ways to aid the black box DL by cognitive priors derived from human insight. Our findings are mixed and interesting, pointing to both cognitive deficit of pure DL, and some measured successes of boosting DL by predetermined cognitive implements. This case study of DL in cognitive computing is meaningful for visual numerosity represents a minimum level of human intelligence.",
  "stance": -0.3
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/3975/3853",
  "title": "Algorithms for Average Regret Minimization",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "In this paper, we study a problem from the realm of multicriteria decision making in which the goal is to select from a given set S of d-dimensional objects a minimum sized subset S′ with bounded regret. Thereby, regret measures the unhappiness of users which would like to select their favorite object from set S but now can only select their favorite object from the subset S′. Previous work focused on bounding the maximum regret which is determined by the most unhappy user. We propose to consider the average regret instead which is determined by the sum of (un)happiness of all possible users. We show that this regret measure comes with desirable properties as supermodularity which allows to construct approximation algorithms. Furthermore, we introduce the regret minimizing permutation problem and discuss extensions of our algorithms to the recently proposed k-regret measure. Our theoretical results are accompanied with experiments on a variety of inputs with d up to 7.",
  "stance": 0.4
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/3977/3855",
  "title": "Bayesian Functional Optimisation with Shape Prior",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "Real world experiments are expensive, and thus it is important to reach a target in a minimum number of experiments. Experimental processes often involve control variables that change over time. Such problems can be formulated as functional optimisation problem. We develop a novel Bayesian optimisation framework for such functional optimisation of expensive black-box processes. We represent the control function using Bernstein polynomial basis and optimise in the coefficient space. We derive the theory and practice required to dynamically adjust the order of the polynomial degree, and show how prior information about shape can be integrated. We demonstrate the effectiveness of our approach for short polymer fibre design and optimising learning rate schedules for deep networks.",
  "stance": 0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/4021/3899",
  "title": "Very Hard Electoral Control Problems",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "It is important to understand how the outcome of an election can be modified by an agent with control over the structure of the election. Electoral control has been studied for many election systems, but for all these systems the winner problem is in P, and so control is in NP. There are election systems, such as Kemeny, that have many desirable properties, but whose winner problems are not in NP. Thus for such systems control is not in NP, and in fact we show that it is typically complete for Σp2 (i.e., NP , the second level of the polynomial hierarchy). This is a very high level of complexity. Approaches that perform quite well for solving NP problems do not necessarily work for Σp2-complete problems. However, answer set programming is suited to express problems in Σp2 , and we present an encoding for Kemeny control.",
  "stance": -0.1
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/4042/3920",
  "title": "When Do Envy-Free Allocations Exist?",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "We consider a fair division setting in which m indivisible items are to be allocated among n agents, where the agents have additive utilities and the agents’ utilities for individual items are independently sampled from a distribution. Previous work has shown that an envy-free allocation is likely to exist when m = Ω(n logn) but not when m = n+o(n), and left open the question of determining where the phase transition from non-existence to existence occurs. We show that, surprisingly, there is in fact no universal point of transition— instead, the transition is governed by the divisibility relation between m and n. On the one hand, if m is divisible by n, an envy-free allocation exists with high probability as long as m ≥ 2n. On the other hand, if m is not “almost” divisible by n, an envy-free allocation is unlikely to exist even when m = Θ(n logn/ log log n).",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/4075/3953",
  "title": "Pareto Optimization for Subset Selection with Dynamic Cost Constraints",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "In this paper, we consider the subset selection problem for function f with constraint boundB which changes over time. We point out that adaptive variants of greedy approaches commonly used in the area of submodular optimization are not able to maintain their approximation quality. Investigating the recently introduced POMC Pareto optimization approach, we show that this algorithm efficiently computes a φ = (αf/2)(1 − 1 ef )-approximation, where αf is the submodularity ratio of f , for each possible constraint bound b ≤ B. Furthermore, we show that POMC is able to adapt its set of solutions quickly in the case that B increases. Our experimental investigations for the influence maximization in social networks show the advantage of POMC over generalized greedy algorithms.",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/4096/3974",
  "title": "Geometry-Aware Face Completion and Editing",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "Face completion is a challenging generation task because it requires generating visually pleasing new pixels that are semantically consistent with the unmasked face region. This paper proposes a geometry-aware Face Completion and Editing NETwork (FCENet) by systematically studying facial geometry from the unmasked region. Firstly, a facial geometry estimator is learned to estimate facial landmark heatmaps and parsing maps from the unmasked face image. Then, an encoder-decoder structure generator serves to complete a face image and disentangle its mask areas conditioned on both the masked face image and the estimated facial geometry images. Besides, since low-rank property exists in manually labeled masks, a low-rank regularization term is imposed on the disentangled masks, enforcing our completion network to manage occlusion area with various shape and size. Furthermore, our network can generate diverse results from the same masked input by modifying estimated facial geometry, which provides a flexible mean to edit the completed face appearance. Extensive experimental results qualitatively and quantitatively demonstrate that our network is able to generate visually pleasing face completion results and edit face attributes",
  "stance": 0.7
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/4108/3986",
  "title": "Be Inaccurate but Don’t Be Indecisive: How Error Distribution Can Affect User Experience",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "System accuracy is a crucial factor influencing user experience in intelligent interactive systems. Although accuracy is known to be important, little is known about the role of the system’s error distribution in user experience. In this paper we study, in the context of background music selection for tabletop games, how the error distribution of an intelligent system affects the user’s perceived experience. In particular, we show that supervised learning algorithms that solely optimize for prediction accuracy can make the system “indecisive”. That is, it can make the system’s errors sparsely distributed throughout the game session. We hypothesize that sparsely distributed errors can harm the users’ perceived experience and it is preferable to use a model that is somewhat inaccurate but decisive, than a model that is accurate but often indecisive. In order to test our hypothesis we introduce an ensemble approach with a restrictive voting rule that instead of erring sparsely through time, it errs consistently for a period of time. A user study in which people watched videos of Dungeons and Dragons sessions supports our hypothesis.",
  "stance": 0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/4114/3992",
  "title": "Relaxing and Restraining Queries for OBDA",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "We advocate the use of ontologies for relaxing and restraining queries, so that they retrieve either more or less answers, enabling the exploration of a given dataset. We propose a set of rewriting rules to relax and restrain conjunctive queries (CQs) over datasets mediated by an ontology written in a dialect of DL-Lite with complex role inclusions (CRIs). The addition of CRI enables the representation of knowledge about data involving ordered hierarchies of categories, in the style of multi-dimensional data models. Although CRIs in general destroy the first-order rewritability of CQs, we identify settings in which CQs remain rewritable.",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/4129/4007",
  "title": "Qualitative Spatial Logic over 2D Euclidean Spaces Is Not Finitely Axiomatisable",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "Several qualitative spatial logics used in reasoning about geospatial data have a sound and complete axiomatisation over metric spaces. It has been open whether the same axiomatisation is also sound and complete for 2D Euclidean spaces. We answer this question negatively by showing that the axiomatisations presented in (Du et al. 2013; Du and Alechina 2016) are not complete for 2D Euclidean spaces and, moreover, the logics are not finitely axiomatisable.",
  "stance": -0.6
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/4186/4064",
  "title": "Adversarial Label Learning",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "We consider the task of training classifiers without labels. We propose a weakly supervised method—adversarial label learning—that trains classifiers to perform well against an adversary that chooses labels for training data. The weak supervision constrains what labels the adversary can choose. The method therefore minimizes an upper bound of the classifier’s error rate using projected primal-dual subgradient descent. Minimizing this bound protects against bias and dependencies in the weak supervision. Experiments on real datasets show that our method can train without labels and outperforms other approaches for weakly supervised learning.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/4191/4069",
  "title": "Enhanced Random Forest Algorithms for Partially Monotone Ordinal Classification",
  "year": 2019,
  "venue": "AAAI",
  "abstract": "One of the factors hindering the use of classification models in decision making is that their predictions may contradict expectations. In domains such as finance and medicine, the ability to include knowledge of monotone (nondecreasing) relationships is sought after to increase accuracy and user satisfaction. As one of the most successful classifiers, attempts have been made to do so for Random Forest. Ideally a solution would (a) maximise accuracy; (b) have low complexity and scale well; (c) guarantee global monotonicity; and (d) cater for multi-class. This paper first reviews the state-of-theart from both the literature and statistical libraries, and identifies opportunities for improvement. A new rule-based method is then proposed, with a maximal accuracy variant and a faster approximate variant. Simulated and real datasets are then used to perform the most comprehensive ordinal classification benchmarking in the monotone forest literature. The proposed approaches are shown to reduce the bias induced by monotonisation and thereby improve accuracy.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/5359/5215",
  "title": "Who Likes What? — SplitLBI in Exploring Preferential Diversity of Ratings",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "In recent years, learning user preferences has received significant attention. A shortcoming of existing learning to rank work lies in that they do not take into account the multilevel hierarchies from social choice to individuals. In this paper, we propose a multi-level model which learns both the common preference or utility function over the population based on features of alternatives to-be-compared, and preferential diversity functions conditioning on user categories. Such a multi-level model, enables us to simultaneously learn a coarse-grained social preference function together with a fine-grained personalized diversity. It provides us prediction power for the choices of new users on new alternatives. The key algorithm in this paper is based on Split Linearized Bregman Iteration (SplitLBI) algorithm which generates a dynamic path from the common utility to personalized preferential diversity, at different levels of sparsity on personalization. A synchronized parallel version of SplitLBI is proposed to meet the needs of fast analysis of large-scale data. The validity of the methodology are supported by experiments with both simulated and real-world datasets such as movie and dining restaurant ratings which provides us a coarse-to-fine grained preference learning.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/5414/5270",
  "title": "A Graph Auto-Encoder for Haplotype Assembly and Viral Quasispecies Reconstruction",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "Reconstructing components of a genomic mixture from data obtained by means of DNA sequencing is a challenging problem encountered in a variety of applications including single individual haplotyping and studies of viral communities. Highthroughput DNA sequencing platforms oversample mixture components to provide massive amounts of reads whose relative positions can be determined by mapping the reads to a known reference genome; assembly of the components, however, requires discovery of the reads’ origin – an NP-hard problem that the existing methods struggle to solve with the required level of accuracy. In this paper, we present a learning framework based on a graph auto-encoder designed to exploit structural properties of sequencing data. The algorithm is a neural network which essentially trains to ignore sequencing errors and infers the posterior probabilities of the origin of sequencing reads. Mixture components are then reconstructed by finding consensus of the reads determined to originate from the same genomic component. Results on realistic synthetic as well as experimental data demonstrate that the proposed framework reliably assembles haplotypes and reconstructs viral communities, often significantly outperforming state-ofthe-art techniques. Source codes, datasets and supplementary document are available at https://github.com/WuLoli/GAEseq.",
  "stance": 0.8
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/5492/5348",
  "title": "M3ER: Multiplicative Multimodal Emotion Recognition using Facial, Textual, and Speech Cues",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "We present M3ER, a learning-based method for emotion recognition from multiple input modalities. Our approach combines cues from multiple co-occurring modalities (such as face, text, and speech) and also is more robust than other methods to sensor noise in any of the individual modalities. M3ER models a novel, data-driven multiplicative fusion method to combine the modalities, which learn to emphasize the more reliable cues and suppress others on a persample basis. By introducing a check step which uses Canonical Correlational Analysis to differentiate between ineffective and effective modalities, M3ER is robust to sensor noise. M3ER also generates proxy features in place of the ineffectual modalities. We demonstrate the efficiency of our network through experimentation on two benchmark datasets, IEMOCAP and CMU-MOSEI. We report a mean accuracy of 82.7% on IEMOCAP and 89.0% on CMU-MOSEI, which, collectively, is an improvement of about 5% over prior work.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/5527/5383",
  "title": "Hard Examples for Common Variable Decision Heuristics",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "The CDCL algorithm for SAT is equivalent to the resolution proof system under a few assumptions, one of them being an optimal non-deterministic procedure for choosing the next variable to branch on. In practice this task is left to a variable decision heuristic, and since the so-called VSIDS decision heuristic is considered an integral part of CDCL, whether CDCL with a VSIDS-like heuristic is also equivalent to resolution remained a significant open question. We give a negative answer by building a family of formulas that have resolution proofs of polynomial size but require exponential time to decide in CDCL with common heuristics such as VMTF, CHB, and certain implementations of VSIDS and LRB.",
  "stance": -0.1
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/5626/5482",
  "title": "A Human-AI Loop Approach for Joint Keyword Discovery and Expectation Estimation in Micropost Event Detection",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "Microblogging platforms such as Twitter are increasingly being used in event detection. Existing approaches mainly use machine learning models and rely on event-related keywords to collect the data for model training. These approaches make strong assumptions on the distribution of the relevant microposts containing the keyword – referred to as the expectation of the distribution – and use it as a posterior regularization parameter during model training. Such approaches are, however, limited as they fail to reliably estimate the informativeness of a keyword and its expectation for model training. This paper introduces a Human-AI loop approach to jointly discover informative keywords for model training while estimating their expectation. Our approach iteratively leverages the crowd to estimate both keyword-specific expectation and the disagreement between the crowd and the model in order to discover new keywords that are most beneficial for model training. These keywords and their expectation not only improve the resulting performance but also make the model training process more transparent. We empirically demonstrate the merits of our approach, both in terms of accuracy and interpretability, on multiple real-world datasets and show that our approach improves the state of the art by 24.3%.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/5628/5484",
  "title": "Asymptotically Unambitious Artificial General Intelligence",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "General intelligence, the ability to solve arbitrary solvable problems, is supposed by many to be artificially constructible. Narrow intelligence, the ability to solve a given particularly difficult problem, has seen impressive recent development. Notable examples include self-driving cars, Go engines, image classifiers, and translators. Artificial General Intelligence (AGI) presents dangers that narrow intelligence does not: if something smarter than us across every domain were indifferent to our concerns, it would be an existential threat to humanity, just as we threaten many species despite no ill will. Even the theory of how to maintain the alignment of an AGI’s goals with our own has proven highly elusive. We present the first algorithm we are aware of for asymptotically unambitious AGI, where “unambitiousness” includes not seeking arbitrary power. Thus, we identify an exception to the Instrumental Convergence Thesis, which is roughly that by default, an AGI would seek power, including over us.",
  "stance": 0.3
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/5631/5487",
  "title": "Explainable Reinforcement Learning through a Causal Lens",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "Prominent theories in cognitive science propose that humans understand and represent the knowledge of the world through causal relationships. In making sense of the world, we build causal models in our mind to encode cause-effect relations of events and use these to explain why new events happen by referring to counterfactuals — things that did not happen. In this paper, we use causal models to derive causal explanations of the behaviour of model-free reinforcement learning agents. We present an approach that learns a structural causal model during reinforcement learning and encodes causal relationships between variables of interest. This model is then used to generate explanations of behaviour based on counterfactual analysis of the causal model. We computationally evaluate the model in 6 domains and measure performance and task prediction accuracy. We report on a study with 120 participants who observe agents playing a real-time strategy game (Starcraft II) and then receive explanations of the agents’ behaviour. We investigate: 1) participants’ understanding gained by explanations through task prediction; 2) explanation satisfaction and 3) trust. Our results show that causal model explanations perform better on these measures compared to two other baseline explanation models. Driven by lack of trust from users and proposed regulations, there are many calls for Artificial Intelligence (AI) systems to become more transparent, interpretable and explainable. This has renewed the interest in Explainable AI (XAI), which has been explored since the expert systems era (Chandrasekaran, Tanner, and Josephson 1989). A key pillar of XAI is explanation, a justification given for decisions and actions of the system. However, much research and practice in XAI pays little attention to people as intended users of these systems (Miller 2018b). If we are to build systems that are capable of providing ‘good’ explanations, it is plausible that explanation models should mimic models of human explanation (De Graaf and Malle 2017). Thus, to build XAI models it is essential to begin with a strong understanding of how people define, generate, select and evaluate explanations. There is a wealth of pertinent literature in cognitive psychology that explore the nature of explanations and how Copyright c © 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. people understand them. As humans, we view the world through a causal lens (Sloman 2005), building mental models with causal relationships to act in the world, to understand new events and also to explain events. Importantly, causal models give people the ability to consider counterfactuals — events that did not happen, but could have under different situations. Although this notion of causal explanation is also backed by literature in philosophy and social psychology (Hilton 2007), causality and counterfactuals are only just becoming more prevalent in XAI. Further, compared to the burst of XAI research in supervised learning, explainability in model-free reinforcement learning is hardly explored. We introduce an action influence model for model-free reinforcement learning (RL) agents and provide a formalisation of the model using structural causal models (Halpern and Pearl 2005). Action influence models approximate the causal model of the environment relative to actions taken by an agent. Our approach differs from previous work in explainable RL in that we use causal models to generate contrastive explanations for why and why not questions, which previous models lack. Given assumptions about the direction of causal relationships between variables, during the policy learning process, we also learn the quantitative influences that actions have on variables. Which enable our model to reason approximately about counterfactual states and actions. We define how to generate explanations for ‘why?’ and ‘why not?’ questions from the action influence model. We define minimally complete explanations taking inspiration from social psychology literature (McClure and Hilton 1997). We computationally evaluated our approach on 6 RL benchmarks domains using 6 different RL algorithms. Results indicate that these models are robust and accurate enough to perform task prediction (Hoffman et al. 2018, p.12) with a negligible performance impact. We conducted a human study using the implemented model for RL agents trained to play the real-time strategy game Starcraft II. Experiments were run for 120 participants, in which we evaluated the participants’ performance in task prediction, explanation satisfaction, and trust. Results show that our model performs better than the tested baseline, but its impact on",
  "stance": 0.4
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/5696/5552",
  "title": "COTSAE: CO-Training of Structure and Attribute Embeddings for Entity Alignment",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "Entity alignment is a fundamental and vital task in Knowledge Graph (KG) construction and fusion. Previous works mainly focus on capturing the structural semantics of entities by learning the entity embeddings on the relational triples and pre-aligned ”seed entities”. Some works also seek to incorporate the attribute information to assist refining the entity embeddings. However, there are still many problems not considered, which dramatically limits the utilization of attribute information in the entity alignment. Different KGs may have lots of different attribute types, and even the same attribute may have diverse data structures and value granularities. Most importantly, attributes may have various ”contributions” to the entity alignment. To solve these problems, we propose COTSAE that combines the structure and attribute information of entities by co-training two embedding learning components, respectively. We also propose a joint attention method in our model to learn the attentions of attribute types and values cooperatively. We verified our COTSAE on several datasets from real-world KGs, and the results showed that it is significantly better than the latest entity alignment methods. The structure and attribute information can complement each other and both contribute to performance improvement.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/5712/5568",
  "title": "Detecting Semantic Anomalies",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "We critically appraise the recent interest in out-of-distribution (OOD) detection and question the practical relevance of existing benchmarks. While the currently prevalent trend is to consider different datasets as OOD, we argue that outdistributions of practical interest are ones where the distinction is semantic in nature for a specified context, and that evaluative tasks should reflect this more closely. Assuming a context of object recognition, we recommend a set of benchmarks, motivated by practical applications. We make progress on these benchmarks by exploring a multi-task learning based approach, showing that auxiliary objectives for improved semantic awareness result in improved semantic anomaly detection, with accompanying generalization benefits.",
  "stance": 0.3
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/5793/5649",
  "title": "On the Discrepancy between the Theoretical Analysis and Practical Implementations of Compressed Communication for Distributed Deep Learning",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "Compressed communication, in the form of sparsification or quantization of stochastic gradients, is employed to reduce communication costs in distributed data-parallel training of deep neural networks. However, there exists a discrepancy between theory and practice: while theoretical analysis of most existing compression methods assumes compression is applied to the gradients of the entire model, many practical implementations operate individually on the gradients of each layer of the model. In this paper, we prove that layer-wise compression is, in theory, better, because the convergence rate is upper bounded by that of entire-model compression for a wide range of biased and unbiased compression methods. However, despite the theoretical bound, our experimental study of six well-known methods shows that convergence, in practice, may or may not be better, depending on the actual trained model and compression ratio. Our findings suggest that it would be advantageous for deep learning frameworks to include support for both layerwise and entire-model compression.",
  "stance": 0.4
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/6218/6074",
  "title": "Fair Procedures for Fair Stable Marriage Outcomes",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "Given a two-sided market where each agent ranks those on the other side by preference, the stable marriage problem calls for finding a perfect matching such that no pair of agents prefer each other to their matches. Recent studies show that the number of stable solutions can be large in practice. Yet the classical solution to the problem, the Gale-Shapley (GS) algorithm, assigns an optimal match to each agent on one side, and a pessimal one to each on the other side; such a solution may fare well in terms of equity only in highly asymmetric markets. Finding a stable matching that minimizes the sex equality cost, an equity measure expressing the discrepancy of mean happiness among the two sides, is strongly NP-hard. Extant heuristics either (a) oblige some agents to involuntarily abandon their matches, or (b) bias the outcome in favor of some agents, or (c) need high-polynomial or unbounded time. We provide the first procedurally fair algorithms that output equitable stable marriages and are guaranteed to terminate in at most cubic time; the key to this breakthrough is the monitoring of a monotonic state function and the use of a selective criterion for accepting proposals. Our experiments with diverse simulated markets show that: (a) extant heuristics fail to yield high equity; (b) the best solution found by the GS algorithm can be very far from optimal equity; and (c) our procedures stand out in both efficiency and equity, even when compared to a non-procedurally fair approximation scheme.",
  "stance": 0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/6253/6109",
  "title": "Learning to Map Frequent Phrases to Sub-Structures of Meaning Representation for Neural Semantic Parsing",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "Neural semantic parsers usually generate meaning representation tokens from natural language tokens via an encoderdecoder model. However, there is often a vocabularymismatch problem between natural language utterances and logical forms. That is, one word maps to several atomic logical tokens, which need to be handled as a whole, rather than individual logical tokens at multiple steps. In this paper, we propose that the vocabulary-mismatch problem can be effectively resolved by leveraging appropriate logical tokens. Specifically, we exploit macro actions, which are of the same granularity of words/phrases, and allow the model to learn mappings from frequent phrases to corresponding substructures of meaning representation. Furthermore, macro actions are compact, and therefore utilizing them can significantly reduce the search space, which brings a great benefit to weakly supervised semantic parsing. Experiments show that our method leads to substantial performance improvement on three benchmarks, in both supervised and weakly supervised settings.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/6254/6110",
  "title": "Attending to Entities for Better Text Understanding",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "Recent progress in NLP witnessed the development of largescale pre-trained language models (GPT, BERT, XLNet, etc.) based on Transformer (Vaswani et al. 2017), and in a range of end tasks, such models have achieved state-of-the-art results, approaching human performance. This clearly demonstrates the power of the stacked self-attention architecture when paired with a sufficient number of layers and a large amount of pre-training data. However, on tasks that require complex and long-distance reasoning where surface-level cues are not enough, there is still a large gap between the pre-trained models and human performance. Strubell et al. (2018) recently showed that it is possible to inject knowledge of syntactic structure into a model through supervised self-attention. We conjecture that a similar injection of semantic knowledge, in particular, coreference information, into an existing model would improve performance on such complex problems. On the LAMBADA (Paperno et al. 2016) task, we show that a model trained from scratch with coreference as auxiliary supervision for self-attention outperforms the largest GPT-2 model, setting the new state-of-the-art, while only containing a tiny fraction of parameters compared to GPT-2. We also conduct a thorough analysis of different variants of model architectures and supervision configurations, suggesting future directions on applying similar techniques to other problems.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/6541/6397",
  "title": "Novel Is Not Always Better: On the Relation between Novelty and Dominance Pruning",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "Novelty pruning is a planning technique that focuses on exploring states that are novel, i.e., those containing facts that have not been seen before. This seemingly simple idea has had a huge impact on the state of the art in planning though its effectiveness is not entirely understood yet. We relate novelty to dominance pruning, which compares states to previously seen states to eliminate those that are provably worse in terms of goal distance. Novelty can be interpreted as an unsafe approximation of dominance, where states containing novel facts are relevant because they enable new paths to the goal and, therefore, they are less likely to be dominated by others. This provides a framework to understand the success of novelty, resulting in new variants that combine both techniques.",
  "stance": 0.3
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/6559/6415",
  "title": "Deep Bayesian Nonparametric Learning of Rules and Plans from Demonstrations with a Learned Automaton Prior",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "We introduce a method to learn imitative policies from expert demonstrations that are interpretable and manipulable. We achieve interpretability by modeling the interactions between high-level actions as an automaton with connections to formal logic. We achieve manipulability by integrating this automaton into planning, so that changes to the automaton have predictable effects on the learned behavior. These qualities allow a human user to first understand what the model has learned, and then either correct the learned behavior or zeroshot generalize to new, similar tasks. We build upon previous work by no longer requiring additional supervised information which is hard to collect in practice. We achieve this by using a deep Bayesian nonparametric hierarchical model. We test our model on several domains and also show results for a real-world implementation on a mobile robotic arm platform.",
  "stance": 0.6
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/6596/6452",
  "title": "Modeling Probabilistic Commitments for Maintenance Is Inherently Harder than for Achievement",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "Most research on probabilistic commitments focuses on commitments to achieve enabling preconditions for other agents. Our work reveals that probabilistic commitments to instead maintain preconditions for others are surprisingly harder to use well than their achievement counterparts, despite strong semantic similarities. We isolate the key difference as being not in how the commitment provider is constrained, but rather in how the commitment recipient can locally use the commitment specification to approximately model the provider’s effects on the preconditions of interest. Our theoretic analyses show that we can more tightly bound the potential suboptimality due to approximate modeling for achievement than for maintenance commitments. We empirically evaluate alternative approximate modeling strategies, confirming that probabilistic maintenance commitments are qualitatively more challenging for the recipient to model well, and indicating the need for more detailed specifications that can sacrifice some of the agents’ autonomy.",
  "stance": -0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/6616/6470",
  "title": "Detecting Human-Object Interactions via Functional Generalization",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "We present an approach for detecting human-object interactions (HOIs) in images, based on the idea that humans interact with functionally similar objects in a similar manner. The proposed model is simple and efficiently uses the data, visual features of the human, relative spatial orientation of the human and the object, and the knowledge that functionally similar objects take part in similar interactions with humans. We provide extensive experimental validation for our approach and demonstrate state-of-the-art results for HOI detection. On the HICO-Det dataset our method achieves a gain of over 2.5% absolute points in mean average precision (mAP) over stateof-the-art. We also show that our approach leads to significant performance gains for zero-shot HOI detection in the seen object setting. We further demonstrate that using a generic object detector, our model can generalize to interactions involving previously unseen objects.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/6622/6476",
  "title": "Learning Deep Relations to Promote Saliency Detection",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "Though saliency detectors has made stunning progress recently. The performances of the state-of-the-art saliency detectors are not acceptable in some confusing areas, e.g., object boundary. We argue that the feature spatial independence should be one of the root cause. This paper explores the ubiquitous relations on the deep features to promote the existing saliency detectors efficiently. We establish the relation by maximizing the mutual information of the deep features of the same category via deep neural networks to break this independence. We introduce a threshold-constrained training pair construction strategy to ensure that we can accurately estimate the relations between different image parts in a selfsupervised way. The relation can be utilized to further excavate the salient areas and inhibit confusing backgrounds. The experiments demonstrate that our method can significantly boost the performance of the state-of-the-art saliency detectors on various benchmark datasets. Besides, our model is label-free and extremely efficient. The inference speed is 140 FPS on a single GTX1080 GPU.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/6636/6490",
  "title": "A Coarse-to-Fine Adaptive Network for Appearance-Based Gaze Estimation",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "Human gaze is essential for various appealing applications. Aiming at more accurate gaze estimation, a series of recent works propose to utilize face and eye images simultaneously. Nevertheless, face and eye images only serve as independent or parallel feature sources in those works, the intrinsic correlation between their features is overlooked. In this paper we make the following contributions: 1) We propose a coarseto-fine strategy which estimates a basic gaze direction from face image and refines it with corresponding residual predicted from eye images. 2) Guided by the proposed strategy, we design a framework which introduces a bi-gram model to bridge gaze residual and basic gaze direction, and an attention component to adaptively acquire suitable fine-grained feature. 3) Integrating the above innovations, we construct a coarse-to-fine adaptive network named CA-Net and achieve state-of-the-art performances on MPIIGaze and EyeDiap.",
  "stance": 0.9
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/6705/6559",
  "title": "CIAN: Cross-Image Affinity Net for Weakly Supervised Semantic Segmentation",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "Weakly supervised semantic segmentation with only imagelevel labels saves large human effort to annotate pixel-level labels. Cutting-edge approaches rely on various innovative constraints and heuristic rules to generate the masks for every single image. Although great progress has been achieved by these methods, they treat each image independently and do not take account of the relationships across different images. In this paper, however, we argue that the cross-image relationship is vital for weakly supervised segmentation. Because it connects related regions across images, where supplementary representations can be propagated to obtain more consistent and integral regions. To leverage this information, we propose an end-to-end cross-image affinity module, which exploits pixel-level cross-image relationships with only imagelevel labels. By means of this, our approach achieves 64.3% and 65.3% mIoU on Pascal VOC 2012 validation and test set respectively, which is a new state-of-the-art result by only using image-level labels for weakly supervised semantic segmentation, demonstrating the superiority of our approach.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/6775/6629",
  "title": "Semantics-Aligned Representation Learning for Person Re-Identification",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "Person re-identification (reID) aims to match person images to retrieve the ones with the same identity. This is a challenging task, as the images to be matched are generally semantically misaligned due to the diversity of human poses and capture viewpoints, incompleteness of the visible bodies (due to occlusion), etc. In this paper, we propose a framework that drives the reID network to learn semantics-aligned feature representation through delicate supervision designs. Specifically, we build a Semantics Aligning Network (SAN) which consists of a base network as encoder (SA-Enc) for reID, and a decoder (SA-Dec) for reconstructing/regressing the densely semantics aligned full texture image. We jointly train the SAN under the supervisions of person re-identification and aligned texture generation. Moreover, at the decoder, besides the reconstruction loss, we add Triplet ReID constraints over the feature maps as the perceptual losses. The decoder is discarded in the inference and thus our scheme is computationally efficient. Ablation studies demonstrate the effectiveness of our design. We achieve the state-of-the-art performances on the benchmark datasets CUHK03, Market1501, MSMT17, and the partial person reID dataset Partial REID.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/6779/6633",
  "title": "Real-Time Object Tracking via Meta-Learning: Efficient Model Adaptation and One-Shot Channel Pruning",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "We propose a novel meta-learning framework for real-time object tracking with efficient model adaptation and channel pruning. Given an object tracker, our framework learns to fine-tune its model parameters in only a few gradient-descent iterations during tracking while pruning its network channels using the target ground-truth at the first frame. Such a learning problem is formulated as a meta-learning task, where a meta-tracker is trained by updating its meta-parameters for initial weights, learning rates, and pruning masks through carefully designed tracking simulations. The integrated metatracker greatly improves tracking performance by accelerating the convergence of online learning and reducing the cost of feature computation. Experimental evaluation on the standard datasets demonstrates its outstanding accuracy and speed compared to the state-of-the-art methods.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/7042/6896",
  "title": "A System for Medical Information Extraction and Verification from Unstructured Text",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "A wealth of medical knowledge has been encoded in terminologies like SNOMED CT, NCI, FMA, and more. However, these resources are usually lacking information like relations between diseases, symptoms, and risk factors preventing their use in diagnostic or other decision making applications. In this paper we present a pipeline for extracting such information from unstructured text and enriching medical knowledge bases. Our approach uses Semantic Role Labelling and is unsupervised. We show how we dealt with several deficiencies of SRL-based extraction, like copula verbs, relations expressed through nouns, and assigning scores to extracted triples. The system have so far extracted about 120K relations and in-house doctors verified about 5k relationships. We compared the output of the system with a manually constructed network of diseases, symptoms and risk factors build by doctors in the course of a year. Our results show that our pipeline extracts good quality and precise relations and speeds up the knowledge acquisition process considerably.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/7044/6898",
  "title": "Machine-Learning-Based Functional Microcirculation Analysis",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "Analysis of microcirculation is an important clinical and research task. Functional analysis of the microcirculation allows researchers to understand how blood flowing in a tissues’ smallest vessels affects disease progression, organ function, and overall health. Current methods of manual analysis of microcirculation are tedious and timeconsuming, limiting the quick turnover of results. There has been limited research on automating functional analysis of microcirculation. As such, in this paper, we propose a twostep machine-learning-based algorithm to functionally assess microcirculation videos. The first step uses a modified vessel segmentation algorithm to extract the location of vessel-like structures. While the second step uses a 3D-CNN to assess whether the vessel-like structures contained flowing blood. To our knowledge, this is the first application of machine learning for functional analysis of microcirculation. We use real-world labelled microcirculation videos to train and test our algorithm and assess its performance. More precisely, we demonstrate that our two-step algorithm can efficiently analyze real data with high accuracy (90%).",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/7056/6910",
  "title": "AI Trust in Business Processes: The Need for Process-Aware Explanations",
  "year": 2020,
  "venue": "AAAI",
  "abstract": "Business processes underpin a large number of enterprise operations including processing loan applications, managing invoices, and insurance claims. There is a large opportunity for infusing AI to reduce cost or provide better customer experience and the BPM literature is rich in machine learning solutions. More recently, deep learning models have been applied to process predictions. Unfortunately, companies have applied or adopted very few of these innovations. We assert that a reason for this lack of adoption is that business users are risk-averse and do not implicitly trust AI models. We challenge the BPM community to build on the AI interpretability literature, and the AI Trust community to understand what it means to take advantage of business process artifacts in order to provide business level explanations.",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16109/15916",
  "title": "In-game Residential Home Planning via Visual Context-aware Global Relation Learning",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "In this paper, we propose an effective global relation learning algorithm to recommend an appropriate location of a building unit for in-game customization of residential home complex. Given a construction layout, we propose a visual contextaware graph generation network that learns the implicit global relations among the scene components and infers the location of a new building unit. The proposed network takes as input the scene graph and the corresponding top-view depth image. It provides the location recommendations for a newlyadded building units by learning an auto-regressive edge distribution conditioned on existing scenes. We also introduce a global graph-image matching loss to enhance the awareness of essential geometry semantics of the site. Qualitative and quantitative experiments demonstrate that the recommended location well reflects the implicit spatial rules of components in the residential estates, and it is instructive and practical to locate the building units in the 3D scene of the complex con-",
  "stance": 0.6
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16131/15938",
  "title": "GTA: Graph Truncated Attention for Retrosynthesis",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Retrosynthesis is the task of predicting reactant molecules from a given product molecule and is, important in organic chemistry because the identification of a synthetic path is as demanding as the discovery of new chemical compounds. Recently, the retrosynthesis task has been solved automatically without human expertise using powerful deep learning models. Recent deep models are primarily based on seq2seq or graph neural networks depending on the function of molecular representation, sequence, or graph. Current state-of-theart models represent a molecule as a graph, but they require joint training with auxiliary prediction tasks, such as the most probable reaction template or reaction center prediction. Furthermore, they require additional labels by experienced chemists, thereby incurring additional cost. Herein, we propose a novel template-free model, i.e., Graph Truncated Attention (GTA), which leverages both sequence and graph representations by inserting graphical information into a seq2seq model. The proposed GTA model masks the self-attention layer using the adjacency matrix of product molecule in the encoder and applies a new loss using atom mapping acquired from an automated algorithm to the cross-attention layer in the decoder. Our model achieves new state-of-the-art records, i.e., exact match top-1 and top-10 accuracies of 51.1% and 81.6% on the USPTO-50k benchmark dataset, respectively, and 46.0% and 70.0% on the USPTO-full dataset, respectively, both without any reaction class information. The GTA model surpasses prior graph-based template-free models by 2% and 7% in terms of the top-1 and top-10 accuracies on the USPTO-50k dataset, respectively, and by over 6% for both the top-1 and top-10 accuracies on the USPTO-full dataset.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16140/15947",
  "title": "Sketch Generation with Drawing Process Guided by Vector Flow and Grayscale",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "We propose a novel image-to-pencil translation method that could not only generate high-quality pencil sketches but also offer the drawing process. Existing pencil sketch algorithms are based on texture rendering rather than the direct imitation of strokes, making them unable to show the drawing process but only a final result. To address this challenge, we first establish a pencil stroke imitation mechanism. Next, we develop a framework with three branches to guide stroke drawing: the first branch guides the direction of the strokes, the second branch determines the shade of the strokes, and the third branch enhances the details further. Under this framework’s guidance, we can produce a pencil sketch by drawing one stroke every time. Our method is fully interpretable. Comparison with existing pencil drawing algorithms shows that our method is superior to others in terms of texture quality, style, and user evaluation. Our code and supplementary material are now available at: https://github.com/TZYSJTU/Sketch-Generation-withDrawing-Process-Guided-by-Vector-Flow-and-Grayscale",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16194/16001",
  "title": "Deductive Learning for Weakly-Supervised 3D Human Pose Estimation via Uncalibrated Cameras",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Without prohibitive and laborious 3D annotations, weaklysupervised 3D human pose methods mainly employ the model regularization with geometric projection consistency or geometry estimation from multi-view images. Nevertheless, those approaches explicitly need known parameters of calibrated cameras, exhibiting a limited model generalization in various realistic scenarios. To mitigate this issue, in this paper, we propose a Deductive Weakly-Supervised Learning (DWSL) for 3D human pose machine. Our DWSL firstly learns latent representations on depth and camera pose for 3D pose reconstruction. Since weak supervision usually causes ill-conditioned learning or inferior estimation, our DWSL introduces deductive reasoning to make an inference for human pose from a view to another and develops a reconstruction loss to demonstrate what the model learns and infers is reliable. This learning by deduction strategy employs the view-transform demonstration and structural rules derived from depth, geometry and angle constraints, which improves the reliability of the model training with weak supervision. On three 3D human pose benchmarks, we conduct extensive experiments to evaluate our proposed method, which achieves superior performance in comparison with state-of-the-art weak-supervised methods. Particularly, our model shows an appealing potential for learning from 2D data captured in dynamic outdoor scenes, which demonstrates promising robustness and generalization in realistic scenarios. Our code is publicly available at https://github.com/XipengChen/DWSL-3D-pose. Introduction 3D human pose estimation is a fundamental problem in computer vision for many applications, such as human-robot interaction, virtual reality, and action recognition, etc. However, it is greatly bottlenecked by the availability of abundant 3D annotated data, since 3D images are usually subject to specific conditions with constrained laboratory environments and thus have limited pose variations and simple backgrounds, and particularly, accurate 3D annotation demands prohibitively expensive cost. Accordingly, they cause the poor generalization of 3D pose models to the cases in the wild. Without any 3D pose annotation, many researchers resort to Weakly-Supervised Learning (WSL) methods (Kocabas, *Pengxu Wei is the corresponding author. Copyright © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Karagoz, and Akbas 2019; Rhodin et al. 2018; Rhodin, Salzmann, and Fua 2018; Chen et al. 2019a), which inherit the benefits of rich annotation and diversity of 2D pose datasets. They usually utilize annotated 2D pose images by lifting 2D poses to the 3D space together with geometric consistency constraints and train models without 3D pose labels for 3D human pose estimation. (Chen et al. 2019a) proposes a method to learn from single-view self-supervision, but requires a very large amount of diverse 2D human poses. (Kocabas, Karagoz, and Akbas 2019; Rhodin et al. 2018; Rhodin, Salzmann, and Fua 2018) propose a multi-view consistency from images which are taken for the same person from different viewpoints. Nevertheless, these methods have to obtain well-defined rigid transformation from annotations (Rhodin, Salzmann, and Fua 2018) or predictions from off-the-shelf methods (Kocabas, Karagoz, and Akbas 2019; Rhodin et al. 2018). Meanwhile, they employ the view synthesis strategy to produce 3D poses which supervise the training of 3D pose detectors. This casts the weakly-supervised learning problem of 3D pose estimation with only 2D annotation into a conventional fully-supervised learning task with synchronized information from multi-view images (Chen et al. 2019a). Essentially, fully supervised models are trained inductively in a data-driven manner, which greatly depends on abundant observations or samples with labels. Nevertheless, following the same spirit, with weak supervision or without annotation, the training of models suffers from a large knowledge of uncertainty or controversial ambiguity, which would cause ill-conditioned learning or inferior estimation. To mitigate this problem, we propose Deductive WeaklySupervised Learning (DWSL) for 3D human pose estimation. Rather than following the spirit of data-driven inductive learning in most existing methods, the proposed paradigm of learning by deduction utilizes deduction with view-transform demonstration and structural rules to infer the plausible 2D pose from another view and develop a reconstruction loss for training. This is regarded as a self-demonstration with deductive reasoning from one view to another view, namely, deduction with view-transform demonstration, and the derived reconstruction loss provides a checkpoint for the current weakly-supervised learning. At the same time, we also introduce structural rules to further promote the learning by deduction, which would ease the model training and reduce the searching space of parameters. We conduct experiments The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21)",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16226/16033",
  "title": "Deep Metric Learning with Self-Supervised Ranking",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Deep metric learning aims to learn a deep embedding space, where similar objects are pushed towards together and different objects are repelled against. Existing approaches typically use inter-class characteristics, e.g., class-level information or instance-level similarity, to obtain semantic relevance of data points and get a large margin between different classes in the embedding space. However, the intra-class characteristics, e.g., local manifold structure or relative relationship within the same class, are usually overlooked in the learning process. Hence the data structure cannot be fully exploited and the output embeddings have limitation in retrieval. More importantly, retrieval results lack in a good ranking. This paper presents a novel self-supervised ranking auxiliary framework, which captures intra-class characteristics as well as inter-class characteristics for better metric learning. Our method defines specific transform functions to simulates the local structure change of intra-class in the initial image domain, and formulates a self-supervised learning procedure to fully exploit this property and preserve it in the embedding space. Extensive experiments on three standard benchmarks show that our method significantly improves and outperforms the state-of-the-art methods on the performances of both retrieval and ranking by 2%-4%.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16229/16036",
  "title": "Learning Local Neighboring Structure for Robust 3D Shape Representation",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Mesh is a powerful data structure for 3D shapes. Representation learning for 3D meshes is important in many computer vision and graphics applications. The recent success of convolutional neural networks (CNNs) for structured data (e.g., images) suggests the value of adapting insight from CNN for 3D shapes. However, 3D shape data are irregular since each node’s neighbors are unordered. Various graph neural networks for 3D shapes have been developed with isotropic filters or predefined local coordinate systems to overcome the node inconsistency on graphs. However, isotropic filters or predefined local coordinate systems limit the representation power. In this paper, we propose a local structure-aware anisotropic convolutional operation (LSA-Conv) that learns adaptive weighting matrices for each node according to the local neighboring structure and performs shared anisotropic filters. In fact, the learnable weighting matrix is similar to the attention matrix in the random synthesizer – a new Transformer model for natural language processing (NLP). Comprehensive experiments demonstrate that our model produces significant improvement in 3D shape reconstruction compared to state-of-the-art methods.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16253/16060",
  "title": "Text-Guided Graph Neural Networks for Referring 3D Instance Segmentation",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "This paper addresses a new task called referring 3D instance segmentation, which aims to segment out the target instance in a 3D scene given a query sentence. Previous work on scene understanding has explored visual grounding with natural language guidance, yet the emphasis is mostly constrained on images and videos. We propose a Text-guided Graph Neural Network (TGNN) for referring 3D instance segmentation on point clouds. Given a query sentence and the point cloud of a 3D scene, our method learns to extract per-point features and predicts an offset to shift each point toward its object center. Based on the point features and the offsets, we cluster the points to produce fused features and coordinates for the candidate objects. The resulting clusters are modeled as nodes in a Graph Neural Network to learn the representations that encompass the relation structure for each candidate object. The GNN layers leverage each object’s features and its relations with neighbors to generate an attention heatmap for the input sentence expression. Finally, the attention heatmap is used to “guide” the aggregation of information from neighborhood nodes. Our method achieves state-of-the-art performance on referring 3D instance segmentation and 3D localization on ScanRefer, Nr3D, and Sr3D benchmarks, respectively.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16254/16061",
  "title": "Initiative Defense against Facial Manipulation",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Benefiting from the development of generative adversarial networks (GAN), facial manipulation has achieved significant progress in both academia and industry recently. It inspires an increasing number of entertainment applications but also incurs severe threats to individual privacy and even political security meanwhile. To mitigate such risks, many countermeasures have been proposed. However, the great majority methods are designed in a passive manner, which is to detect whether the facial images or videos are tampered after their wide propagation. These detection-based methods have a fatal limitation, that is, they only work for ex-post forensics but can not prevent the engendering of malicious behavior. To address the limitation, in this paper, we propose a novel framework of initiative defense to degrade the performance of facial manipulation models controlled by malicious users. The basic idea is to actively inject imperceptible venom into target facial data before manipulation. To this end, we first imitate the target manipulation model with a surrogate model, and then devise a poison perturbation generator to obtain the desired venom. An alternating training strategy are further leveraged to train both the surrogate model and the perturbation generator. Two typical facial manipulation tasks: face attribute editing and face reenactment, are considered in our initiative defense framework. Extensive experiments demonstrate the effectiveness and robustness of our framework in different settings. Finally, we hope this work can shed some light on initiative countermeasures against more adversarial",
  "stance": 0.6
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16269/16076",
  "title": "Discriminative Region Suppression for Weakly-Supervised Semantic Segmentation",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Weakly-supervised semantic segmentation (WSSS) using image-level labels has recently attracted much attention for reducing annotation costs. Existing WSSS methods utilize localization maps from the classification network to generate pseudo segmentation labels. However, since localization maps obtained from the classifier focus only on sparse discriminative object regions, it is difficult to generate highquality segmentation labels. To address this issue, we introduce discriminative region suppression (DRS) module that is a simple yet effective method to expand object activation regions. DRS suppresses the attention on discriminative regions and spreads it to adjacent non-discriminative regions, generating dense localization maps. DRS requires few or no additional parameters and can be plugged into any network. Furthermore, we introduce an additional learning strategy to give a self-enhancement of localization maps, named localization map refinement learning. Benefiting from this refinement learning, localization maps are refined and enhanced by recovering some missing parts or removing noise itself. Due to its simplicity and effectiveness, our approach achieves mIoU 71.4% on the PASCAL VOC 2012 segmentation benchmark using only image-level labels. Extensive experiments demonstrate the effectiveness of our approach.",
  "stance": 0.8
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16281/16088",
  "title": "Learning Monocular Depth in Dynamic Scenes via Instance-Aware Projection Consistency",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "We present an end-to-end joint training framework that explicitly models 6-DoF motion of multiple dynamic objects, ego-motion, and depth in a monocular camera setup without supervision. Our technical contributions are three-fold. First, we highlight the fundamental difference between inverse and forward projection while modeling the individual motion of each rigid object, and propose a geometrically correct projection pipeline using a neural forward projection module. Second, we design a unified instance-aware photometric and geometric consistency loss that holistically imposes self-supervisory signals for every background and object region. Lastly, we introduce a general-purpose auto-annotation scheme using any off-the-shelf instance segmentation and optical flow models to produce video instance segmentation maps that will be utilized as input to our training pipeline. These proposed elements are validated in a detailed ablation study. Through extensive experiments conducted on the KITTI and Cityscapes dataset, our framework is shown to outperform the state-of-the-art depth and motion estimation methods. Our code, dataset, and models are publicly available.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16291/16098",
  "title": "Joint Semantic-geometric Learning for Polygonal Building Segmentation",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Building extraction from aerial or satellite images has been an important research problem in remote sensing and computer vision domains for decades. Compared with pixel-wise semantic segmentation models that output raster building segmentation map, polygonal building segmentation approaches produce more realistic building polygons that are in the desirable vector format for practical applications. Despite the substantial efforts over recent years, state-of-the-art polygonal building segmentation methods still suffer from several limitations, e.g., (1) relying on a perfect segmentation map to guarantee the vectorization quality; (2) requiring a complex post-processing procedure; (3) generating inaccurate vertices with a fixed quantity, a wrong sequential order, self-intersections, etc. To tackle the above issues, in this paper, we propose a polygonal building segmentation approach and make the following contributions: (1) We design a multitask segmentation network for joint semantic and geometric learning via three tasks, i.e., pixel-wise building segmentation, multi-class corner prediction, and edge orientation prediction. (2) We propose a simple but effective vertex generation module for transforming the segmentation contour into high-quality polygon vertices. (3) We further propose a polygon refinement network that automatically moves the polygon vertices into more accurate locations. Results on two popular building segmentation datasets demonstrate that our approach achieves significant improvements for both building instance segmentation (with 2% F1-score gain) and polygon vertex prediction (with 6% F1-score gain) compared with current state-of-the-art methods.",
  "stance": 0.9
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16295/16102",
  "title": "Inference Fusion with Associative Semantics for Unseen Object Detection",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "We study the problem of object detection when training and test objects are disjoint, i.e. no training examples of the target classes are available. Existing unseen object detection approaches usually combine generic detection frameworks with a single-path unseen classifier, by aligning object regions with semantic class embeddings. In this paper, inspired from human cognitive experience, we propose a simple but effective dual-path detection model that further explores associative semantics to supplement the basic visual-semantic knowledge transfer. We use a novel target-centric multipleassociation strategy to establish concept associations, to ensure that the predictor generalized to unseen domain can be learned during training. In this way, through a reasonable inference fusion mechanism, those two parallel reasoning paths can strengthen the correlation between seen and unseen objects, thus improving detection performance. Experiments show that our inductive method can significantly boost the performance by 7.42% over inductive models, and even 5.25% over transductive models on MSCOCO dataset.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16298/16105",
  "title": "SD-Pose: Semantic Decomposition for Cross-Domain 6D Object Pose Estimation",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "The current leading 6D object pose estimation methods rely heavily on annotated real data, which is highly costly to acquire. To overcome this, many works have proposed to introduce computer-generated synthetic data. However, bridging the gap between the synthetic and real data remains a severe problem. Images depicting different levels of realism/semantics usually have different transferability between the synthetic and real domains. Inspired by this observation, we introduce an approach, SD-Pose, that explicitly decomposes the input image into multi-level semantic representations and then combines the merits of each representation to bridge the domain gap. Our comprehensive analyses and experiments show that our semantic decomposition strategy can fully utilize the different domain similarities of different representations, thus allowing us to outperform the state of the art on modern 6D object pose datasets without accessing any real data during training.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16307/16114",
  "title": "Spatiotemporal Graph Neural Network based Mask Reconstruction for Video Object Segmentation",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "This paper addresses the task of segmenting class-agnostic objects in semi-supervised setting. Although previous detection based methods achieve relatively good performance, these approaches extract the best proposal by a greedy strategy, which may lose the local patch details outside the chosen candidate. In this paper, we propose a novel spatiotemporal graph neural network (STG-Net) to reconstruct more accurate masks for video object segmentation, which captures the local contexts by utilizing all proposals. In the spatial graph, we treat object proposals of a frame as nodes and represent their correlations with an edge weight strategy for mask context aggregation. To capture temporal information from previous frames, we use a memory network to refine the mask of current frame by retrieving historic masks in a temporal graph. The joint use of both local patch details and temporal relationships allow us to better address the challenges such as object occlusion and missing. Without online learning and finetuning, our STG-Net achieves state-of-the-art performance on four large benchmarks (DAVIS, YouTube-VOS, SegTrackv2, and YouTube-Objects), demonstrating the effectiveness of the proposed approach.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16345/16152",
  "title": "Dual Adversarial Graph Neural Networks for Multi-label Cross-modal Retrieval",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Cross-modal retrieval has become an active study field with the expanding scale of multimodal data. To date, most existing methods transform multimodal data into a common representation space where semantic similarities between items can be directly measured across different modalities. However, these methods typically suffer from following limitations: 1) They usually attempt to bridge the modality gap by designing losses in the common representation space which may not be sufficient to eliminate potential heterogeneity of different modalities in the common space. 2) They typically treat labels as independent individuals and ignore label relationships which are important for constructing semantic links between multimodal data. In this work, we propose a novel Dual Adversarial Graph Neural Networks (DAGNN) composed of the dual generative adversarial networks and the multi-hop graph neural networks, which learn modality-invariant and discriminative common representations for cross-modal retrieval. Firstly, we construct the dual generative adversarial networks to project multimodal data into a common representation space. Secondly, we leverage the multi-hop graph neural networks, in which a layer aggregation mechanism is proposed to exploit multi-hop propagation information, to capture the label correlation dependency and learn inter-dependent classifiers. Comprehensive experiments conducted on two cross-modal retrieval benchmark datasets, NUS-WIDE and MIRFlickr, indicate the superiority of DAGNN.",
  "stance": 0.8
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16357/16164",
  "title": "Social-DPF: Socially Acceptable Distribution Prediction of Futures",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "We consider long-term path forecasting problems in crowds, where future sequence trajectories are generated given a short observation. Recent methods for this problem have focused on modeling social interactions and predicting multi-modal futures. However, it is not easy for machines to successfully consider social interactions, such as avoiding collisions while considering the uncertainty of futures under a highly interactive and dynamic scenario. In this paper, we propose a model that incorporates multiple interacting motion sequences jointly and predicts multi-modal socially acceptable distributions of futures. Specifically, we introduce a new aggregation mechanism for social interactions, which selectively models long-term inter-related dynamics between movements in a shared environment through a message passing mechanism. Moreover, we propose a loss function that not only accesses how accurate the estimated distributions of the futures are but also considers collision avoidance. We further utilize mixture density functions to describe the trajectories and learn multi-modality of future paths. Extensive experiments over several trajectory prediction benchmarks demonstrate that our method is able to forecast socially acceptable distributions in complex scenarios.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16431/16238",
  "title": "ERNIE-ViL: Knowledge Enhanced Vision-Language Representations through Scene Graphs",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "We propose a knowledge-enhanced approach, ERNIE-ViL, which incorporates structured knowledge obtained from scene graphs to learn joint representations of vision-language. ERNIE-ViL tries to build the detailed semantic connections (objects, attributes of objects and relationships between objects) across vision and language, which are essential to vision-language cross-modal tasks. Utilizing scene graphs of visual scenes, ERNIE-ViL constructs Scene Graph Prediction tasks, i.e., Object Prediction, Attribute Prediction and Relationship Prediction tasks in the pre-training phase. Specifically, these prediction tasks are implemented by predicting nodes of different types in the scene graph parsed from the sentence. Thus, ERNIE-ViL can learn the joint representations characterizing the alignments of the detailed semantics across vision and language. After pre-training on large scale image-text aligned datasets, we validate the effectiveness of ERNIE-ViL on 5 cross-modal downstream tasks. ERNIE-ViL achieves state-of-the-art performances on all these tasks and ranks the first place on the VCR leaderboard with an absolute improvement of 3.7%.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16469/16276",
  "title": "RESA: Recurrent Feature-Shift Aggregator for Lane Detection",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Lane detection is one of the most important tasks in selfdriving. Due to various complex scenarios (e.g., severe occlusion, ambiguous lanes, etc.) and the sparse supervisory signals inherent in lane annotations, lane detection task is still challenging. Thus, it is difficult for the ordinary convolutional neural network (CNN) to train in general scenes to catch subtle lane feature from the raw image. In this paper, we present a novel module named REcurrent Feature-Shift Aggregator (RESA) to enrich lane feature after preliminary feature extraction with an ordinary CNN. RESA takes advantage of strong shape priors of lanes and captures spatial relationships of pixels across rows and columns. It shifts sliced feature map recurrently in vertical and horizontal directions and enables each pixel to gather global information. RESA can conjecture lanes accurately in challenging scenarios with weak appearance clues by aggregating sliced feature map. Moreover, we propose a Bilateral Up-Sampling Decoder that combines coarse-grained and fine-detailed features in the upsampling stage. It can recover the low-resolution feature map into pixel-wise prediction meticulously. Our method achieves state-of-the-art results on two popular lane detection benchmarks (CULane and Tusimple). Code has been made available at: https://github.com/ZJULearning/resa.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16483/16290",
  "title": "A SAT-based Resolution of Lam's Problem",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "In 1989, computer searches by Lam, Thiel, and Swiercz experimentally resolved Lam’s problem from projective geometry—the long-standing problem of determining if a projective plane of order ten exists. Both the original search and an independent verification in 2011 discovered no such projective plane. However, these searches were each performed using highly specialized custom-written code and did not produce nonexistence certificates. In this paper, we resolve Lam’s problem by translating the problem into Boolean logic and use satisfiability (SAT) solvers to produce nonexistence certificates that can be verified by a third party. Our work uncovered consistency issues in both previous searches—highlighting the difficulty of relying on specialpurpose search code for nonexistence results.",
  "stance": -0.6
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16508/16315",
  "title": "Turbocharging Treewidth-Bounded Bayesian Network Structure Learning",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "We present a new approach for learning the structure of a treewidth-bounded Bayesian Network (BN). The key to our approach is applying an exact method (based on MaxSAT) locally, to improve the score of a heuristically computed BN. This approach allows us to scale the power of exact methods— so far only applicable to BNs with several dozens of random variables—to large BNs with several thousands of random variables. Our experiments show that our method improves the score of BNs provided by state-of-the-art heuristic methods, often significantly.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16555/16362",
  "title": "Learning Accurate and Interpretable Decision Rule Sets from Neural Networks",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "This paper proposes a new paradigm for learning a set of independent logical rules in disjunctive normal form as an interpretable model for classification. We consider the problem of learning an interpretable decision rule set as training a neural network in a specific, yet very simple two-layer architecture. Each neuron in the first layer directly maps to an interpretable if-then rule after training, and the output neuron in the second layer directly maps to a disjunction of the firstlayer rules to form the decision rule set. Our representation of neurons in this first rules layer enables us to encode both the positive and the negative association of features in a decision rule. State-of-the-art neural net training approaches can be leveraged for learning highly accurate classification models. Moreover, we propose a sparsity-based regularization approach to balance between classification accuracy and the simplicity of the derived rules. Our experimental results show that our method can generate more accurate decision rule sets than other state-of-the-art rule-learning algorithms with better accuracy-simplicity trade-offs. Further, when compared with uninterpretable black-box machine learning approaches such as random forests and full-precision deep neural networks, our approach can easily find interpretable decision rule sets that have comparable predictive performance.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16562/16369",
  "title": "A Hybrid Probabilistic Approach for Table Understanding",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Tables of data are used to record vast amounts of socioeconomic, scientific, and governmental information. Although humans create tables using underlying organizational principles, unfortunately AI systems struggle to understand the contents of these tables. This paper introduces an end-to-end system for table understanding, the process of capturing the relational structure of data in tables. We introduce models that identify cell types, group these cells into blocks of data that serve a similar functional role, and predict the relationships between these blocks. We introduce a hybrid, neuro-symbolic approach, combining embedded representations learned from thousands of tables with probabilistic constraints that capture regularities in how humans organize tables. Our neurosymbolic model is better able to capture positional invariants of headers and enforce homogeneity of data types. One limitation in this research area is the lack of rich datasets for evaluating end-to-end table understanding, so we introduce a new benchmark dataset comprised of 431 diverse tables from data.gov. The evaluation results show that our system achieves the state-of-the-art performance on cell type classification, block identification, and relationship prediction, improving over prior efforts by up to 7% of macro F1 score.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16625/16432",
  "title": "Dynamic Neuro-Symbolic Knowledge Graph Construction for Zero-shot Commonsense Question Answering",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Understanding narratives requires reasoning about implicit world knowledge related to the causes, effects, and states of situations described in text. At the core of this challenge is how to access contextually relevant knowledge on demand and reason over it. In this paper, we present initial studies toward zero-shot commonsense question answering by formulating the task as inference over dynamically generated commonsense knowledge graphs. In contrast to previous studies for knowledge integration that rely on retrieval of existing knowledge from static knowledge graphs, our study requires commonsense knowledge integration where contextually relevant knowledge is often not present in existing knowledge bases. Therefore, we present a novel approach that generates contextually-relevant symbolic knowledge structures on demand using generative neural commonsense knowledge models. Empirical results on two datasets demonstrate the efficacy of our neuro-symbolic approach for dynamically constructing knowledge graphs for reasoning. Our approach achieves significant performance boosts over pretrained language models and vanilla knowledge models, all while providing interpretable reasoning paths for its predictions.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16630/16437",
  "title": "Answering Complex Queries in Knowledge Graphs with Bidirectional Sequence Encoders",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Representation learning for knowledge graphs (KGs) has focused on the problem of answering simple link prediction queries. In this work we address the more ambitious challenge of predicting the answers of conjunctive queries with multiple missing entities. We propose Bidirectional Query Embedding (BIQE), a method that embeds conjunctive queries with models based on bi-directional attention mechanisms. Contrary to prior work, bidirectional selfattention can capture interactions among all the elements of a query graph. We introduce two new challenging datasets for studying conjunctive query inference and conduct experiments on several benchmark datasets that demonstrate BIQE significantly outperforms state of the art baselines.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16633/16440",
  "title": "A Scalable Reasoning and Learning Approach for Neural-Symbolic Stream Fusion",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Driven by deep neural networks (DNN), the recent development of computer vision makes vision sensors such as stereo cameras and Lidars ubiquitous in autonomous cars, robotics and traffic monitoring. However, a traditional DNN-based data fusion pipeline like object tracking has to hard-wire an engineered set of DNN models to a fixed processing logic, which makes it difficult to infuse new models to that pipeline. To overcome this, we propose a novel neural-symbolic stream reasoning approach realised by semantic stream reasoning programs which specify DNN-based data fusion pipelines via logic rules with learnable probabilistic degrees as weights. The reasoning task over this program is governed by a novel incremental reasoning algorithm, which lends itself also as a core building block for a scalable and parallel algorithm to learn the weights for such program. Extensive experiments with our first prototype on multi-object tracking benchmarks for autonomous driving and traffic monitoring show that our flexible approach can considerably improve both accuracy and processing throughput compared to the DNN-based counterparts.",
  "stance": 0.9
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16642/16449",
  "title": "A Few Queries Go a Long Way: Information-Distortion Tradeoffs in Matching",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Georgios Amanatidis,1,2 Georgios Birmpas,3 Aris Filos-Ratsikas,4 Alexandros A. Voudouris5 1Department of Mathematical Sciences, University of Essex 2ILLC, University of Amsterdam 3Department of Computer, Control and Management Engineering, Sapienza University of Rome 4Department of Computer Science, University of Liverpool 5School of Computer Science and Electronic Engineering, University of Essex georgios.amanatidis@essex.ac.uk, george.birbas@diag.uniroma1.it, aris.filos-ratsikas@liverpool.ac.uk, alexandros.voudouris@essex.ac.uk Abstract",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16671/16478",
  "title": "PoA of Simple Auctions with Interdependent Values",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "We expand the literature on the price of anarchy (PoA) of simultaneous item auctions by considering settings with correlated values; we do this via the fundamental economic model of interdependent values (IDV). It is well-known that in multi-item settings with private values, correlated values can lead to bad PoA, which can be polynomially large in the number of agents n. In the more general model of IDV, we show that the PoA can be polynomially large even in singleitem settings. On the positive side, we identify a natural condition on information dispersion in the market, which enables good PoA guarantees. Under this condition, we show that for single-item settings, the PoA of standard mechanisms degrades gracefully. For settings with multiple items we show a separation between two domains: If there are more buyers, we devise a new simultaneous item auction with good PoA, under limited information asymmetry. To the best of our knowledge, this is the first positive PoA result for correlated values in multi-item settings. The main technical difficulty in establishing this result is that the standard tool for establishing PoA results — the smoothness framework — is unsuitable for IDV settings, and so we must introduce new techniques to address the unique challenges imposed by such settings. In the domain of more items, we establish impossibility results even for surprisingly simple scenarios.",
  "stance": 0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16686/16493",
  "title": "An Analysis of Approval-Based Committee Rules for 2D-Euclidean Elections",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "We study approval-based committee elections for the case where the voters’ preferences come from a 2D-Euclidean model. We consider two main issues: First, we ask for the complexity of computing election results. Second, we evaluate election outcomes experimentally, following the visualization technique of Elkind et al. (2017). Regarding the first issue, we find that many NP-hard rules remain intractable for 2D-Euclidean elections. For the second one, we observe that the behavior and nature of many rules strongly depend on the exact protocol for choosing the approved candidates.",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16705/16512",
  "title": "Majority Opinion Diffusion in Social Networks: An Adversarial Approach",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "We introduce and study a novel majority based opinion diffusion model. Consider a graph G, which represents a social network. Assume that initially a subset of nodes, called seed nodes or early adopters, are colored either black or white, which correspond to positive or negative opinion regarding a consumer product or a technological innovation. Then, in each round an uncolored node, which is adjacent to at least one colored node, chooses the most frequent color among its",
  "stance": 0.7
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16734/16541",
  "title": "MARTA: Leveraging Human Rationales for Explainable Text Classification",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Explainability is a key requirement for text classification in many application domains ranging from sentiment analysis to medical diagnosis or legal reviews. Existing methods often rely on “attention” mechanisms for explaining classification results by estimating the relative importance of input units. However, recent studies have shown that such mechanisms tend to mis-identify irrelevant input units in their explanation. In this work, we propose a hybrid human-AI approach that incorporates human rationales into attention-based text classification models to improve the explainability of classification results. Specifically, we ask workers to provide rationales for their annotation by selecting relevant pieces of text. We introduce MARTA, a Bayesian framework that jointly learns an attention-based model and the reliability of workers while injecting human rationales into model training. We derive a principled optimization algorithm based on variational inference with efficient updating rules for learning MARTA parameters. Extensive validation on real-world datasets shows that our framework significantly improves the state of the art both in terms of classification explainability and accuracy.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16758/16565",
  "title": "Consistent Right-Invariant Fixed-Lag Smoother with Application to Visual Inertial SLAM",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "State estimation problems without absolute position measurements routinely arise in navigation of unmanned aerial vehicles, autonomous ground vehicles, etc. whose proper operation relies on accurate state estimates and reliable covariances. Unaware of absolute positions, these problems have immanent unobservable directions. Traditional causal estimators, however, usually gain spurious information on the unobservable directions, leading to over-confident covariance inconsistent with actual estimator errors. The consistency problem of fixed-lag smoothers (FLSs) has only been attacked by the first estimate Jacobian (FEJ) technique because of the complexity to analyze their observability property. But the FEJ has several drawbacks hampering its wide adoption. To ensure the consistency of a FLS, this paper introduces the right invariant error formulation into the FLS framework. To our knowledge, we are the first to analyze the observability of a FLS with the right invariant error. Our main contributions are twofold. As the first novelty, to bypass the complexity of analysis with the classic observability matrix, we show that observability analysis of FLSs can be done equivalently on the linearized system. Second, we prove that the inconsistency issue in the traditional FLS can be elegantly solved by the right invariant error formulation without artificially correcting Jacobians. By applying the proposed FLS to the monocular visual inertial simultaneous localization and mapping (SLAM) problem, we confirm that the method consistently estimates covariance similarly to a batch smoother in simulation and that our method achieved comparable accuracy as traditional FLSs on real data. Introduction Positioning and navigation of a variety of vehicles, e.g., unmanned aerial vehicles (UAVs), autonomous ground vehicles (AGVs), depends on real-time state estimation. Accurate system state and reasonable covariance output by state estimators in real time are necessary for the proper operation of these systems. For state estimation, these systems usually fuse measurements captured by sensors that do not provide absolute positions, like cameras, lidars, inertial measurement units (IMUs), etc. It is well known that estimators which fuse such measurements have unobservable directions (Jones and Soatto 2011). ∗Corresponding author, yuan.zhuang@whu.edu.cn Copyright c © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. As reported in the literature, traditional real-time estimators, e.g., filters, fixed-lag smoothers (FLSs), tend to gain fictitious information on unobservable directions (Huang, Mourikis, and Roumeliotis 2010; Dong-Si and Mourikis 2011), and to output falsely optimistic covariance inconsistent to the actual state error. This inconsistency is caused by the marginalization step of real-time estimators which removes old state variables and measurements (i.e., factors) from an estimator and approximates those measurements by a linear prior factor. A deeper cause is that for a variable in the prior factor, its linearization point used by the prior factor differs from that used by the remaining factors. Obviously, the batch estimator and its incremental variants, e.g., iSAM2 (Kaess et al. 2012), do not have this issue as they do not marginalize variables. To fix the estimator inconsistency, techniques that modify the measurement Jacobians to fit certain criteria have been proposed. For instance, the “first estimate Jacobian (FEJ)” technique (Huang, Mourikis, and Roumeliotis 2010) evaluates Jacobians relative to variables in the linear prior factor at their estimates upon marginalization. Because the Jacobian computation depends on specifics, such as an earlier estimate of a variable, it is usually difficult to apply such techniques to an existing estimator framework. A new trend is to use right invariant error formulation (Barrau and Bonnabel 2016a) where a navigation state variable (consisting of orientation, position, and velocity) is associated to a Lie group SE2(3) and the error vector is invariant to transforming the trajectory by a right multiplication. Besides mathematically elegant, it is easy to implement as it fits the conventional filtering framework. However, this formulation has not been used in FLSs, mainly because of the challenge to analyze their consistency property. Previous work has shown that the estimator inconsistency comes along with the observability issue where the unobservable directions become spuriously observable (Hesch et al. 2014a). Thus, consistency has been predominantly studied by examining rank deficiency of the linearized observability matrix, e.g., (Huang, Mourikis, and Roumeliotis 2010; Dong-Si and Mourikis 2012; Brossard, Barrau, and Bonnabel 2018). The local observability matrix is acceptable in complexity for analyzing filters, but becomes very involved for dealing with FLSs, e.g., (Dong-Si and Mourikis 2012). Because the observability matrix is a derivative of the The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21)",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16760/16567",
  "title": "DenserNet: Weakly Supervised Visual Localization Using Multi-Scale Feature Aggregation",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "In this work, we introduce a Denser Feature Network (DenserNet) for visual localization. Our work provides three principal contributions. First, we develop a convolutional neural network (CNN) architecture which aggregates feature maps at different semantic levels for image representations. Using denser feature maps, our method can produce more keypoint features and increase image retrieval accuracy. Second, our model is trained end-to-end without pixel-level annotation other than positive and negative GPS-tagged image pairs. We use a weakly supervised triplet ranking loss to learn discriminative features and encourage keypoint feature repeatability for image representation. Finally, our method is computationally efficient as our architecture has shared features and parameters during forwarding propagation. Our method is flexible and can be crafted on a light-weighted backbone architecture to achieve appealing efficiency with a small penalty on accuracy. Extensive experiment results indicate that our method sets a new state-of-the-art on four challenging large-scale localization benchmarks and three image retrieval benchmarks with the same level of supervision. The code is available at https://github.com/goodproj13/",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16769/16576",
  "title": "A General Setting for Gradual Semantics Dealing with Similarity",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "The paper discusses theoretical foundations that describe principles and processes involved in defining semantics that deal with similarity between arguments. Such semantics compute the strength of an argument on the basis of the strengths of its attackers, similarities between those attackers, and an initial weight ascribed to the argument. We define a semantics by three functions: an adjustment function that updates the strengths of attackers on the basis of their similarities, an aggregation function that computes the strength of the group of attackers, and an influence function that evaluates the impact of the group on the argument’s initial weight. We propose intuitive constraints for the three functions and key rationality principles for semantics, and show how the former lead to the satisfaction of the latter. Then, we propose a broad family of semantics whose instances satisfy the principles. Finally, we analyse the existing adjustment functions and show that they violate some properties, then we propose novel ones and use them for generalizing h-Categorizer.",
  "stance": 0.2
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16808/16615",
  "title": "Focused Inference and System P",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "We bring in the concept of focused inference into the field of qualitative nonmonotonic reasoning by applying focused inference to System P. The idea behind drawing focused inferences is to concentrate on knowledge which seems to be relevant for answering a query while completely disregarding the remaining knowledge even at the risk of missing some meaningful information. Focused inference is motivated by mimicking snap decisions of human reasoners and aims on rapidly drawing still reasonable inferences from large sets of knowledge. In this paper, we define a series of query-dependent, syntactically-driven focused inference relations, elaborate on their formal properties, and show that the series converges against System P. We take advantage of this result in form of an anytime algorithm for drawing inferences which is accompanied by a thorough complexity analysis.",
  "stance": 0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16819/16626",
  "title": "Does Explainable Artificial Intelligence Improve Human Decision-Making?",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Explainable AI provides insights to users into the why for model predictions, offering potential for users to better understand and trust a model, and to recognize and correct AI predictions that are incorrect. Prior research on human and explainable AI interactions has typically focused on measures such as interpretability, trust, and usability of the explanation. There are mixed findings whether explainable AI can improve actual human decisionmaking and the ability to identify the problems with the underlying model. Using real datasets, we compare objective human decision accuracy without AI (control), with an AI prediction (no explanation), and AI prediction with explanation. We find providing any kind of AI prediction tends to improve user decision accuracy, but no conclusive evidence that explainable AI has a meaningful impact. Moreover, we observed the strongest predictor for human decision accuracy was AI accuracy and that users were somewhat able to detect when the AI was correct vs. incorrect, but this was not significantly affected by including an explanation. Our results indicate that, at least in some situations, the why information provided in explainable AI may not enhance user decisionmaking, and further research may be needed to understand how to integrate explainable AI into real systems.",
  "stance": -0.8
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16837/16644",
  "title": "Understanding Decoupled and Early Weight Decay",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Weight decay (WD) is a traditional regularization technique in deep learning, but despite its ubiquity, its behavior is still an area of active research. Golatkar et al. have recently shown that WD only matters at the start of the training in computer vision, upending traditional wisdom. Loshchilov et al. show that for adaptive optimizers, manually decaying weights can outperform adding an l2 penalty to the loss. This technique has become increasingly popular and is referred to as decoupled WD. The goal of this paper is to investigate these two recent empirical observations. We demonstrate that by applying WD only at the start, the network norm stays small throughout training. This has a regularizing effect as the effective gradient updates become larger. However, traditional generalizations metrics fail to capture this effect of WD, and we show how a simple scale-invariant metric can. We also show how the growth of network weights is heavily influenced by the dataset and its generalization properties. For decoupled WD, we perform experiments in NLP and RL where adaptive optimizers are the norm. We demonstrate that the primary issue that decoupled WD alleviates is the mixing of gradients from the objective function and the l2 penalty in the buffers of Adam (which stores the estimates of the first-order moment). Adaptivity itself is not problematic and decoupled WD ensures that the gradients from the l2 term cannot ”drown out” the true objective, facilitating easier hyperparameter tuning.",
  "stance": -0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16853/16660",
  "title": "Frivolous Units: Wider Networks Are Not Really That Wide",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "A remarkable characteristic of overparameterized deep neural networks (DNNs) is that their accuracy does not degrade when the network width is increased. Recent evidence suggests that developing compressible representations allows the complexity of large networks to be adjusted for the learning task at hand. However, these representations are poorly understood. A promising strand of research inspired from biology involves studying representations at the unit level as it offers a more granular interpretation of the neural mechanisms. In order to better understand what facilitates increases in width without decreases in accuracy, we ask: Are there mechanisms at the unit level by which networks control their effective complexity? If so, how do these depend on the architecture, dataset, and hyperparameters? We identify two distinct types of “frivolous” units that proliferate when the network’s width increases: prunable units which can be dropped out of the network without significant change to the output and redundant units whose activities can be expressed as a linear combination of others. These units imply complexity constraints as the function the network computes could be expressed without them. We also identify how the development of these units can be influenced by architecture and a number of training factors. Together, these results help to explain why the accuracy of DNNs does not degrade when width is increased and highlight the importance of frivolous units toward understanding implicit regularization in DNNs.",
  "stance": 0.2
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16873/16680",
  "title": "Neighborhood Consensus Networks for Unsupervised Multi-view Outlier Detection",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Multi-view outlier detection recently attracted rapidly growing attention with the development of multi-view learning. Although promising performance demonstrated, we observe that identifying outliers in multi-view data is still a challenging task due to the complicated characteristics of multi-view data. Specifically, an effective multi-view outlier detection method should be able to handle (1) different types of outliers; (2) two or more views; (3) samples without clusters; (4) high dimensional data. Unfortunately, little is known about how these four issues can be handled simultaneously. In this paper, we propose an unsupervised multi-view outlier detection method to address these issues. Our method is based on the proposed novel neighborhood consensus networks termed NC-Nets, which automatically encodes intrinsic information into a comprehensive latent space for each view (for issue (4)) and uniforms the neighborhood structures among different views (for issue (2)). Accordingly, we propose an outlier score measurement which consists of two parts: the withinview reconstruction score and the cross-view neighborhood consensus score. The measurement is designed based on the characteristics of the different outlier types (for issue (1)) and no cluster assumption is needed (for issue (3)). Experimental results show that our method significantly outperforms state-of-the-art methods. On average, our method achieves 11.2% ∼ 96.2% improvement in term of AUC and 33.5% ∼ 352.7% improvement in term of F1-Score.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16883/16690",
  "title": "Generalized Adversarially Learned Inference",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Allowing effective inference of latent vectors while training GANs can greatly increase their applicability in various downstream tasks. Recent approaches, such as ALI and BiGAN frameworks, develop methods of inference of latent variables in GANs by adversarially training an image generator along with an encoder to match two joint distributions of image and latent vector pairs. We generalize these approaches to incorporate multiple layers of feedback on reconstructions, self-supervision, and other forms of supervision based on prior or learned knowledge about the desired solutions. We achieve this by modifying the discriminator’s objective to correctly identify more than two joint distributions of tuples of an arbitrary number of random variables consisting of images, latent vectors, and other variables generated through auxiliary tasks, such as reconstruction and inpainting or as outputs of suitable pre-trained models. We design a non-saturating maximization objective for the generator-encoder pair and prove that the resulting adversarial game corresponds to a global optimum that simultaneously matches all the distributions. Within our proposed framework, we introduce a novel set of techniques for providing self-supervised feedback to the model based on properties, such as patch-level correspondence and cycle consistency of reconstructions. Through comprehensive experiments, we demonstrate the efficacy, scalability, and flexibility of the proposed approach for a variety of tasks. The appendix of the paper can be found at the following link: https://drive.google.com/file/ d/1i99e682CqYWMEDXlnqkqrctGLVA9viiz/view?usp= sharing",
  "stance": 0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16887/16694",
  "title": "Differentially Private and Communication Efficient Collaborative Learning",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Collaborative learning has received huge interests due to its capability of exploiting the collective computing power of the wireless edge devices. However, during the learning process, model updates using local private samples and large-scale parameter exchanges among agents impose severe privacy concerns and communication bottleneck. In this paper, to address these problems, we propose two differentially private (DP) and communication efficient algorithms, called Q-DPSGD-1 and Q-DPSGD-2. In Q-DPSGD-1, each agent first performs local model updates by a DP gradient descent method to provide the DP guarantee and then quantizes the local model before transmitting it to neighbors to improve communication efficiency. In Q-DPSGD-2, each agent injects discrete Gaussian noise to enforce DP guarantee after first quantizing the local model. Moreover, we track the privacy loss of both approaches under the Rényi DP and provide convergence analysis for both convex and non-convex loss functions. The proposed methods are evaluated in extensive experiments on real-world datasets and the empirical results validate our theoretical findings.",
  "stance": 0.7
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16906/16713",
  "title": "Learning to Reweight with Deep Interactions",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Recently, the concept of teaching has been introduced into machine learning, in which a teacher model is used to guide the training of a student model (which will be used in real tasks) through data selection, loss function design, etc. Learning to reweight, which is a specific kind of teaching that reweights training data using a teacher model, receives much attention due to its simplicity and effectiveness. In existing learning to reweight works, the teacher model only utilizes shallow/surface information such as training iteration number and loss/accuracy of the student model from training/validation sets, but ignores the internal states of the student model, which limits the potential of learning to reweight. In this work, we propose an improved data reweighting algorithm, in which the student model provides its internal states to the teacher model, and the teacher model returns adaptive weights of training samples to enhance the training of the student model. The teacher model is jointly trained with the student model using meta gradients propagated from a validation set. Experiments on image classification with clean/noisy labels and neural machine translation empirically demonstrate that our algorithm makes significant improvement over previous methods.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16935/16742",
  "title": "DeepSynth: Automata Synthesis for Automatic Task Segmentation in Deep Reinforcement Learning",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "This paper proposes DeepSynth, a method for effective training of deep Reinforcement Learning (RL) agents when the reward is sparse and non-Markovian, but at the same time progress towards the reward requires achieving an unknown sequence of high-level objectives. Our method employs a novel algorithm for synthesis of compact automata to uncover this sequential structure automatically. We synthesise a humaninterpretable automaton from trace data collected by exploring the environment. The state space of the environment is then enriched with the synthesised automaton so that the generation of a control policy by deep RL is guided by the discovered structure encoded in the automaton. The proposed approach is able to cope with both high-dimensional, low-level features and unknown sparse non-Markovian rewards. We have evaluated DeepSynth’s performance in a set of experiments that includes the Atari game Montezuma’s Revenge. Compared to existing approaches, we obtain a reduction of two orders of magnitude in the number of iterations required for policy synthesis, and also a significant improvement in scalability.",
  "stance": 0.8
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16987/16794",
  "title": "A Flexible Framework for Communication-Efficient Machine Learning",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "With the increasing scale of machine learning tasks, it has become essential to reduce the communication between computing nodes. Early work on gradient compression focused on the bottleneck between CPUs and GPUs, but communicationefficiency is now needed in a variety of different system architectures, from high-performance clusters to energyconstrained IoT devices. In the current practice, compression levels are typically chosen before training and settings that work well for one task may be vastly sub-optimal for another dataset on another architecture. In this paper, we propose a flexible framework which adapts the compression level to the true gradient at each iteration, maximizing the improvement in the objective function that is achieved per communicated bit. Our framework is easy to adapt from one technology to the next by modeling how the communication cost depends on the compression level for the specific technology. Theoretical results and practical experiments indicate that the automatic tuning strategies significantly increase communication efficiency on several state-of-the-art compression schemes.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17037/16844",
  "title": "Contrastive Clustering",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "In this paper, we propose an online clustering method called Contrastive Clustering (CC) which explicitly performs the instanceand cluster-level contrastive learning. To be specific, for a given dataset, the positive and negative instance pairs are constructed through data augmentations and then projected into a feature space. Therein, the instanceand cluster-level contrastive learning are respectively conducted in the row and column space by maximizing the similarities of positive pairs while minimizing those of negative ones. Our key observation is that the rows of the feature matrix could be regarded as soft labels of instances, and accordingly the columns could be further regarded as cluster representations. By simultaneously optimizing the instanceand cluster-level contrastive loss, the model jointly learns representations and cluster assignments in an end-to-end manner. Besides, the proposed method could timely compute the cluster assignment for each individual, even when the data is presented in streams. Extensive experimental results show that CC remarkably outperforms 17 competitive clustering methods on six challenging image benchmarks. In particular, CC achieves an NMI of 0.705 (0.431) on the CIFAR-10 (CIFAR-100) dataset, which is an up to 19% (39%) performance improvement compared with the best baseline. The code is available at https://github.com/XLearning-SCU/2021-AAAI-CC.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17098/16905",
  "title": "Modular Graph Transformer Networks for Multi-Label Image Classification",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "With the recent advances in graph neural networks, there is a rising number of studies on graph-based multi-label classification with the consideration of object dependencies within visual data. Nevertheless, graph representations can become indistinguishable due to the complex nature of label relationships. We propose a multi-label image classification framework based on graph transformer networks to fully exploit inter-label interactions. The paper presents a modular learning scheme to enhance the classification performance by segregating the computational graph into multiple sub-graphs based on modularity. Our approach, named Modular Graph Transformer Networks (MGTN), is capable of employing multiple backbones for better information propagation over different sub-graphs guided by graph transformers and convolutions. We validate our framework on MS-COCO and Fashion550K datasets to demonstrate improvements for multilabel image classification. The source code is available at https://github.com/ReML-AI/MGTN.",
  "stance": 0.7
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17112/16919",
  "title": "Learning Deep Generative Models for Queuing Systems",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Modern society is heavily dependent on large scale client-server systems with applications ranging from Internet and Communication Services to sophisticated logistics and deployment of goods. To maintain and improve such a system, a careful study of client and server dynamics is needed – e.g. response/service times, average number of clients at given times, etc. To this end, one traditionally relies, within the queuing theory formalism, on parametric analysis and explicit distribution forms. However, parametric forms limit the model’s expressiveness and could struggle on extensively large",
  "stance": -0.7
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17175/16982",
  "title": "PAC Learning of Causal Trees with Latent Variables",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Learning causal probabilistic models with latent variables from observational and experimental data is an important problem. In this paper we present a polynomial-time algorithm that PAC-learns the structure and parameters of a rooted, tree-structured causal network of bounded degree where the internal nodes of the tree cannot be observed or manipulated. Our algorithm is the first of its kind to provably learn the structure and parameters of tree-structured causal models with latent internal variables from random examples and active experiments.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17234/17041",
  "title": "Peer Collaborative Learning for Online Knowledge Distillation",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Traditional knowledge distillation uses a two-stage training strategy to transfer knowledge from a high-capacity teacher model to a compact student model, which relies heavily on the pre-trained teacher. Recent online knowledge distillation alleviates this limitation by collaborative learning, mutual learning and online ensembling, following a one-stage end-to-end training fashion. However, collaborative learning and mutual learning fail to construct an online high-capacity teacher, whilst online ensembling ignores the collaboration among branches and its logit summation impedes the further optimisation of the ensemble teacher. In this work, we propose a novel Peer Collaborative Learning method for online knowledge distillation, which integrates online ensembling and network collaboration into a unified framework. Specifically, given a target network, we construct a multi-branch network for training, in which each branch is called a peer. We perform random augmentation multiple times on the inputs to peers and assemble feature representations outputted from peers with an additional classifier as the peer ensemble teacher. This helps to transfer knowledge from a highcapacity teacher to peers, and in turn further optimises the ensemble teacher. Meanwhile, we employ the temporal mean model of each peer as the peer mean teacher to collaboratively transfer knowledge among peers, which helps each peer to learn richer knowledge and facilitates to optimise a more stable model with better generalisation. Extensive experiments on CIFAR-10, CIFAR-100 and ImageNet show that the proposed method significantly improves the generalisation of various backbone networks and outperforms the state-of-theart methods.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17291/17098",
  "title": "Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Data heterogeneity has been identified as one of the key features in federated learning but often overlooked in the lens of robustness to adversarial attacks. This paper focuses on characterizing and understanding its impact on backdooring attacks in federated learning through comprehensive experiments using synthetic and the LEAF benchmarks. The initial impression driven by our experimental results suggests that data heterogeneity is the dominant factor in the effectiveness of attacks and it may be a redemption for defending against backdooring as it makes the attack less efficient, more challenging to design effective attack strategies, and the attack result also becomes less predictable. However, with further investigations, we found data heterogeneity is more of a curse than a redemption as the attack effectiveness can be significantly boosted by simply adjusting the client-side backdooring timing. More importantly, data heterogeneity may result in overfitting at the local training of benign clients, which can be utilized by attackers to disguise themselves and fool skewed-feature based defenses. In addition, effective attack strategies can be made by adjusting attack data distribution. Finally, we discuss the potential directions of defending the curses brought by data heterogeneity. The results and lessons learned from our extensive experiments and analysis offer new insights for designing robust federated learning methods",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17296/17103",
  "title": "CloudLSTM: A Recurrent Neural Model for Spatiotemporal Point-cloud Stream Forecasting",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "This paper introduces CloudLSTM, a new branch of recurrent neural models tailored to forecasting over data streams generated by geospatial point-cloud sources. We design a Dynamic Point-cloud Convolution (DConv) operator as the core component of CloudLSTMs, which performs convolution directly over point-clouds and extracts local spatial features from sets of neighboring points that surround different elements of the input. This operator maintains the permutation invariance of sequence-to-sequence learning frameworks, while representing neighboring correlations at each time step – an important aspect in spatiotemporal predictive learning. The DConv operator resolves the grid-structural data requirements of existing spatiotemporal forecasting models and can be easily plugged into traditional LSTM architectures with sequenceto-sequence learning and attention mechanisms. We apply our proposed architecture to two representative, practical use cases that involve point-cloud streams, i.e., mobile service traffic forecasting and air quality indicator forecasting. Our results, obtained with real-world datasets collected in diverse scenarios for each use case, show that CloudLSTM delivers accurate long-term predictions, outperforming a variety of competitor neural network models.",
  "stance": 0.6
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17313/17120",
  "title": "Exploratory Machine Learning with Unknown Unknowns",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "In conventional supervised learning, a training dataset is given with ground-truth labels from a known label set, and the learned model will classify unseen instances to known labels. In real situations, when the learned models do not work well, learners generally attribute the model failure to the inadequate selection of learning algorithms or the lack of enough labeled training samples. In this paper, we point out that there is an important category of failure, which owes to the fact that there are unknown classes in the training data misperceived as other labels, and thus their existence is unknown from the given supervision. Such problems of unknown unknown classes can hardly be addressed by common re-selection of algorithms or accumulation of training samples. For this purpose, we propose the exploratory machine learning, where in this paradigm once learner encounters unsatisfactory learning performance, she can examine the possibility and, if unknown unknowns really exist, deploy the optimal strategy of feature space augmentation to make unknown classes observable and be enabled for learning. Theoretical analysis and empirical study on both synthetic and real datasets validate the efficacy of our proposal.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17341/17148",
  "title": "Learning to Resolve Conflicts for Multi-Agent Path Finding with Conflict-Based Search",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Conflict-Based Search (CBS) is a state-of-the-art algorithm for multi-agent path finding. On the high level, CBS repeatedly detects conflicts and resolves one of them by splitting the current problem into two subproblems. Previous work chooses the conflict to resolve by categorizing conflicts into three classes and always picking one from the highest-priority class. In this work, we propose an oracle for conflict selection that results in smaller search tree sizes than the one used in previous work. However, the computation of the oracle is slow. Thus, we propose a machine-learning (ML) framework for conflict selection that observes the decisions made by the oracle and learns a conflict-selection strategy represented by a linear ranking function that imitates the oracle’s decisions accurately and quickly. Experiments on benchmark maps indicate that our approach, ML-guided CBS, significantly improves the success rates, search tree sizes and runtimes of the current state-of-the-art CBS solver.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17359/17166",
  "title": "Is the Most Accurate AI the Best Teammate? Optimizing AI for Teamwork",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "AI practitioners typically strive to develop the most accurate systems, making an implicit assumption that the AI system will function autonomously. However, in practice, AI systems often are used to provide advice to people in domains ranging from criminal justice and finance to healthcare. In such AI-advised decision making, humans and machines form a team, where the human is responsible for making final decisions. But is the most accurate AI the best teammate? We argue “not necessarily” — predictable performance may be worth a slight sacrifice in AI accuracy. Instead, we argue that AI systems should be trained in a human-centered manner, directly optimized for team performance. We study this proposal for a specific type of human-AI teaming, where the human overseer chooses to either accept the AI recommendation or solve the task themselves. To optimize the team performance for this setting we maximize the team’s expected utility, expressed in terms of the quality of the final decision, cost of verifying, and individual accuracies of people and machines. Our experiments with linear and non-linear models on real-world, high-stakes datasets show that the most accuracy AI may not lead to highest team performance and show the benefit of modeling teamwork during training through improvements in expected team utility across datasets, considering parameters such as human skill and the cost of mistakes. We discuss the shortcoming of current optimization approaches beyond well-studied loss functions such as log-loss, and encourage future work on AI optimization problems motivated by human-AI collaboration.",
  "stance": -0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17417/17224",
  "title": "Minimax Regret Optimisation for Robust Planning in Uncertain Markov Decision Processes",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "The parameters for a Markov Decision Process (MDP) often cannot be specified exactly. Uncertain MDPs (UMDPs) capture this model ambiguity by defining sets which the parameters belong to. Minimax regret has been proposed as an objective for planning in UMDPs to find robust policies which are not overly conservative. In this work, we focus on planning for Stochastic Shortest Path (SSP) UMDPs with uncertain cost and transition functions. We introduce a Bellman equation to compute the regret for a policy. We propose a dynamic programming algorithm that utilises the regret Bellman equation, and show that it optimises minimax regret exactly for UMDPs with independent uncertainties. For coupled uncertainties, we extend our approach to use options to enable a trade off between computation and solution quality. We evaluate our approach on both synthetic and real-world domains, showing that it significantly outperforms existing baselines.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17451/17258",
  "title": "Generalization in Portfolio-Based Algorithm Selection",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Portfolio-based algorithm selection has seen tremendous practical success over the past two decades. This algorithm configuration procedure works by first selecting a portfolio of diverse algorithm parameter settings, and then, on a given problem instance, using an algorithm selector to choose a parameter setting from the portfolio with strong predicted performance. Oftentimes, both the portfolio and the algorithm selector are chosen using a training set of typical problem instances from the application domain at hand. In this paper, we provide the first provable guarantees for portfolio-based algorithm selection. We analyze how large the training set should be to ensure that the resulting algorithm selector’s average performance over the training set is close to its future (expected) performance. This involves analyzing three key reasons why these two quantities may diverge: 1) the learningtheoretic complexity of the algorithm selector, 2) the size of the portfolio, and 3) the learning-theoretic complexity of the algorithm’s performance as a function of its parameters. We introduce an end-to-end learning-theoretic analysis of the portfolio construction and algorithm selection together. We prove that if the portfolio is large, overfitting is inevitable, even with an extremely simple algorithm selector. With experiments, we illustrate a tradeoff exposed by our theoretical analysis: as we increase the portfolio size, we can hope to include a well-suited parameter setting for every possible problem instance, but it becomes impossible to avoid overfitting.",
  "stance": -0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17457/17264",
  "title": "Escaping Local Optima with Non-Elitist Evolutionary Algorithms",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Most discrete evolutionary algorithms (EAs) implement elitism, meaning that they make the biologically implausible assumption that the fittest individuals never die. While elitism favours exploitation and ensures that the best seen solutions are not lost, it has been widely conjectured that non-elitism is necessary to explore promising fitness valleys without getting stuck in local optima. Determining when non-elitist EAs outperform elitist EAs has been one of the most fundamental open problems in evolutionary computation. A recent analysis of a non-elitist EA shows that this algorithm does not outperform its elitist counterparts on the benchmark problem JUMP. We solve this open problem through rigorous runtime analysis of elitist and non-elitist population-based EAs on a class of multi-modal problems. We show that with 3-tournament selection and appropriate mutation rates, the non-elitist EA optimises the multi-modal problem in expected polynomial time, while an elitist EA requires exponential time with overwhelmingly high probability. A key insight in our analysis is the non-linear selection profile of the tournament selection mechanism which, with appropriate mutation rates, allows a small sub-population to reside on the local optimum while the rest of the population explores the fitness valley. In contrast, we show that the comma-selection mechanism which does not have this non-linear profile, fails to optimise this problem in polynomial time. The theoretical analysis is complemented with an empirical investigation on instances of the set cover problem, showing that non-elitist EAs can perform better than the elitist ones. We also provide examples where usage of mutation rates close to the error thresholds is beneficial when employing non-elitist population-based EAs.",
  "stance": -1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17483/17290",
  "title": "Multi-Dimensional Explanation of Target Variables from Documents",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Automated predictions require explanations to be interpretable by humans. Past work used attention and rationale mechanisms to find words that predict the target variable of a document. Often though, they result in a tradeoff between noisy explanations or a drop in accuracy. Furthermore, rationale methods cannot capture the multi-faceted nature of justifications for multiple targets, because of the non-probabilistic nature of the mask. In this paper, we propose the Multi-Target Masker (MTM) to address these shortcomings. The novelty lies in the soft multi-dimensional mask that models a relevance probability distribution over the set of target variables to handle ambiguities. Additionally, two regularizers guide MTM to induce long, meaningful explanations. We evaluate MTM on two datasets and show, using standard metrics and human annotations, that the resulting masks are more accurate and coherent than those generated by the state-of-the-art methods. Moreover, MTM is the first to also achieve the highest F1 scores for all the target variables simultaneously.",
  "stance": 0.8
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17499/17306",
  "title": "A Lightweight Neural Model for Biomedical Entity Linking",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Biomedical entity linking aims to map biomedical mentions, such as diseases and drugs, to standard entities in a given knowledge base. The specific challenge in this context is that the same biomedical entity can have a wide range of names, including synonyms, morphological variations, and names with different word orderings. Recently, BERT-based methods have advanced the state-of-the-art by allowing for rich representations of word sequences. However, they often have hundreds of millions of parameters and require heavy computing resources, which limits their applications in resource-limited scenarios. Here, we propose a lightweight neural method for biomedical entity linking, which needs just a fraction of the parameters of a BERT model and much less computing resources. Our method uses a simple alignment layer with attention mechanisms to capture the variations between mention and entity names. Yet, we show that our model is competitive with previous work on standard evaluation benchmarks.",
  "stance": 0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17505/17312",
  "title": "How Linguistically Fair Are Multilingual Pre-Trained Language Models?",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Massively multilingual pre-trained language models, such as mBERT and XLM-RoBERTa, have received significant attention in the recent NLP literature for their excellent capability towards crosslingual zero-shot transfer of NLP tasks. This is especially promising because a large number of languages have no or very little labeled data for supervised learning. Moreover, a substantially improved performance on low resource languages without any significant degradation of accuracy for high resource languages lead us to believe that these models will help attain a fairer distribution of language technologies despite the prevalent unfair and extremely skewed distribution of resources across the world’s languages. Nevertheless, these models, and the experimental approaches adopted by the researchers to arrive at those, have been criticised by some for lacking a nuanced and thorough comparison of benefits across languages and tasks. A related and important question that has received little attention is how to choose from a set of models, when no single model significantly outperforms the others on all tasks and languages. As we discuss in this paper, this is often the case, and the choices are usually made without a clear articulation of reasons or underlying fairness assumptions. In this work, we scrutinize the choices made in previous work, and propose a few different strategies for fair and efficient model selection based on the principles of fairness in economics and social choice theory. In particular, we emphasize Rawlsian fairness, which provides an appropriate framework for making fair (with respect to languages, or tasks, or both) choices while selecting multilingual pre-trained language models for a practical or",
  "stance": 0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17512/17319",
  "title": "FILTER: An Enhanced Fusion Method for Cross-lingual Language Understanding",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Large-scale cross-lingual language models (LM), such as mBERT, Unicoder and XLM, have achieved great success in cross-lingual representation learning. However, when applied to zero-shot cross-lingual transfer tasks, most existing methods use only single-language input for LM finetuning, without leveraging the intrinsic cross-lingual alignment between different languages that proves essential for multilingual tasks. In this paper, we propose FILTER, an enhanced fusion method that takes cross-lingual data as input for XLM finetuning. Specifically, FILTER first encodes text input in the source language and its translation in the target language independently in the shallow layers, then performs crosslanguage fusion to extract multilingual knowledge in the intermediate layers, and finally performs further languagespecific encoding. During inference, the model makes predictions based on the text input in the target language and its translation in the source language. For simple tasks such as classification, translated text in the target language shares the same label as the source language. However, this shared label becomes less accurate or even unavailable for more complex tasks such as question answering, NER and POS tagging. To tackle this issue, we further propose an additional KL-divergence self-teaching loss for model training, based on auto-generated soft pseudo-labels for translated text in the target language. Extensive experiments demonstrate that FILTER achieves new state of the art on two challenging multilingual multi-task benchmarks, XTREME and XGLUE.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17531/17338",
  "title": "BERT & Family Eat Word Salad: Experiments with Text Understanding",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "In this paper, we study the response of large models from the BERT family to incoherent inputs that should confuse any model that claims to understand natural language. We define simple heuristics to construct such examples. Our experiments show that state-of-the-art models consistently fail to recognize them as ill-formed, and instead produce high confidence predictions on them. As a consequence of this phenomenon, models trained on sentences with randomly permuted word order perform close to state-of-the-art models. To alleviate these issues, we show that if models are explicitly trained to recognize invalid inputs, they can be robust to such attacks without a drop in performance.",
  "stance": -0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17534/17341",
  "title": "Humor Knowledge Enriched Transformer for Understanding Multimodal Humor",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Recognizing humor from a video utterance requires understanding the verbal and non-verbal components as well as incorporating the appropriate context and external knowledge. In this paper, we propose Humor Knowledge enriched Transformer (HKT) that can capture the gist of a multimodal humorous expression by integrating the preceding context and external knowledge. We incorporate humor centric external knowledge into the model by capturing the ambiguity and sentiment present in the language. We encode all the language, acoustic, vision, and humor centric features separately using Transformer based encoders, followed by a cross attention layer to exchange information among them. Our model achieves 77.36% and 79.41% accuracy in humorous punchline detection on UR-FUNNY and MUStaRD datasets – achieving a new state-of-the-art on both datasets with the margin of 4.93% and 2.94% respectively. Furthermore, we demonstrate that our model can capture interpretable, humorinducing patterns from all modalities.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17564/17371",
  "title": "Towards Topic-Aware Slide Generation For Academic Papers With Unsupervised Mutual Learning",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Slides are commonly used to present information and tell stories. In academic and research communities, slides are typically used to summarize findings in accepted papers for presentation in meetings and conferences. These slides for academic papers usually contain common and essential topics such as major contributions, model design, experiment details and future work. In this paper, we aim to automatically generate slides for academic papers. We first conducted an in-depth analysis of how humans create slides. We then mined frequently used slide topics. Given a topic, our approach extracts relevant sentences in the paper to provide the draft slides. Due to the lack of labeling data, we integrate prior knowledge of ground truth sentences into a log-linear model to create an initial pseudo-target distribution. Two sentence extractors are learned collaboratively and bootstrap the performance of each other. Evaluation results on a labeled test set show that our model can extract more relevant sentences than baseline methods. Human evaluation also shows slides generated by our model can serve as a good basis for preparing the final",
  "stance": 0.8
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17566/17373",
  "title": "ACT: an Attentive Convolutional Transformer for Efficient Text Classification",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Recently, Transformer has been demonstrating promising performance in many NLP tasks and showing a trend of replacing Recurrent Neural Network (RNN). Meanwhile, less attention is drawn to Convolutional Neural Network (CNN) due to its weak ability in capturing sequential and longdistance dependencies, although it has excellent local feature extraction capability. In this paper, we introduce an Attentive Convolutional Transformer (ACT) that takes the advantages of both Transformer and CNN for efficient text classification. Specifically, we propose a novel attentive convolution mechanism that utilizes the semantic meaning of convolutional filters attentively to transform text from complex word space to a more informative convolutional filter space where important n-grams are captured. ACT is able to capture both local and global dependencies effectively while preserving sequential information. Experiments on various text classification tasks and detailed analyses show that ACT is a lightweight, fast, and effective universal text classifier, outperforming CNNs, RNNs, and attentive models including Transformer.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17575/17382",
  "title": "Infusing Multi-Source Knowledge with Heterogeneous Graph Neural Network for Emotional Conversation Generation",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "The success of emotional conversation systems depends on sufficient perception and appropriate expression of emotions. In a real-world conversation, we firstly instinctively perceive emotions from multi-source information, including the emotion flow of dialogue history, facial expressions, and personalities of speakers, and then express suitable emotions according to our personalities, but these multiple types of information are insufficiently exploited in emotional conversation fields. To address this issue, we propose a heterogeneous graph-based model for emotional conversation generation. Specifically, we design a Heterogeneous Graph-Based Encoder to represent the conversation content (i.e., the dialogue history, its emotion flow, facial expressions, and speakers’ personalities) with a heterogeneous graph neural network, and then predict suitable emotions for feedback. After that, we employ an Emotion-Personality-Aware Decoder to generate a response not only relevant to the conversation context but also with appropriate emotions, by taking the encoded graph representations, the predicted emotions from the encoder and the personality of the current speaker as inputs. Experimental results show that our model can effectively perceive emotions from multi-source knowledge and generate a satisfactory response, which significantly outperforms previous state-of-the-art models.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17608/17415",
  "title": "On the Softmax Bottleneck of Recurrent Language Models",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Recent research has pointed to a limitation of word-level neural language models with softmax outputs. This limitation, known as the “softmax bottleneck” refers to the inability of these models to produce high-rank log probability (logP ) matrices. Various solutions have been proposed to break this bottleneck, including Mixture of Softmaxes, SigSoftmax, and Linear Monotonic Softmax with Piecewise Linear Increasing Functions. They were reported to offer better performance in terms of perplexity on test data. A natural perception from these results is a strong positive correlation between the rank of the logP matrix and the model’s performance. In this work, we show via an extensive empirical study that such a correlation is fairly weak and that the high-rank of the logP matrix is neither necessary nor sufficient for better test perplexity. Although our results are empirical, they are established in part via the construction of a rich family of models, which we call Generalized SigSoftmax. They are able to create diverse ranks for the logP matrices. We also present an investigation as to why the proposed solutions achieve better performance.",
  "stance": 0.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17623/17430",
  "title": "Learning from the Best: Rationalizing Predictions by Adversarial Information Calibration",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Explaining the predictions of AI models is paramount in safety-critical applications, such as in legal or medical domains. One form of explanation for a prediction is an extractive rationale, i.e., a subset of features of an instance that lead the model to give its prediction on the instance. Previous works on generating extractive rationales usually employ a two-phase model: a selector that selects the most important features (i.e., the rationale) followed by a predictor that makes the prediction based exclusively on the selected features. One disadvantage of these works is that the main signal for learning to select features comes from the comparison of the answers given by the predictor and the ground-truth answers. In this work, we propose to squeeze more information from the predictor via an information calibration method. More precisely, we train two models jointly: one is a typical neural model that solves the task at hand in an accurate but black-box manner, and the other is a selector-predictor model that additionally produces a rationale for its prediction. The first model is used as a guide to the second model. We use an adversarial-based technique to calibrate the information extracted by the two models such that the difference between them is an indicator of the missed or over-selected features. In addition, for natural language tasks, we propose to use a language-model-based regularizer to encourage the extraction of fluent rationales. Experimental results on a sentiment analysis task as well as on three tasks from the legal domain show the effectiveness of our approach to rationale extraction.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17624/17431",
  "title": "Nutri-bullets: Summarizing Health Studies by Composing Segments",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "We introduce Nutri-bullets, a multi-document summarization task for health and nutrition. First, we present two datasets of food and health summaries from multiple scientific studies. Furthermore, we propose a novel extract-compose model to solve the problem in the regime of limited parallel data. We explicitly select key spans from several abstracts using a policy network, followed by composing the selected spans to present a summary via a task specific language model. Compared to state-of-the-art methods, our approach leads to more faithful, relevant and diverse summarization – properties imperative to this application. For instance, on the BreastCancer dataset our approach gets a more than 50% improvement on relevance and faithfulness.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17679/17486",
  "title": "Simpson's Bias in NLP Training",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "In most machine learning tasks, we evaluate a modelM on a given data population S by measuring a population-level metric F(S;M). Examples of such evaluation metric F include precision/recall for (binary) recognition, the F1 score for multi-class classification, and the BLEU metric for language generation. On the other hand, the modelM is trained by optimizing a sample-level loss G(St;M) at each learning step t, where St is a subset of S (a.k.a. the mini-batch). Popular choices of G include cross-entropy loss, the Dice loss, and sentence-level BLEU scores. A fundamental assumption behind this paradigm is that the mean value of the samplelevel loss G, if averaged over all possible samples, should effectively represent the population-level metric F of the task, such as, that E[G(St;M)] ≈ F(S;M). In this paper, we systematically investigate the above assumption in several NLP tasks. We show, both theoretically and experimentally, that some popular designs of the sample-level loss G may be inconsistent with the true population-level metric F of the task, so that models trained to optimize the former can be substantially sub-optimal to the latter, a phenomenon we call it, Simpson’s bias, due to its deep connections with the classic paradox known as Simpson’s reversal paradox in statistics and social sciences.",
  "stance": -1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17681/17488",
  "title": "What's the Best Place for an AI Conference, Vancouver or _______: Why Completing Comparative Questions is Difficult",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "Although large neural language models (LMs) like BERT can be finetuned to yield state-of-the-art results on many NLP tasks, it is often unclear what these models actually learn. Here we study using such LMs to fill in entities in humanauthored comparative questions, like “Which country is older, India or ?”—i.e., we study the ability of neural LMs to ask (not answer) reasonable questions. We show that accuracy in this fill-in-the-blank task is well-correlated with human judgements of whether a question is reasonable, and that these models can be trained to achieve nearly human-level performance in completing comparative questions in three different subdomains. However, analysis shows that what they learn fails to model any sort of broad notion of which entities are semantically comparable or similar—instead the trained models are very domain-specific, and performance is highly correlated with co-occurrences between specific entities observed in the training set. This is true both for models that are pretrained on general text corpora, as well as models trained on a large corpus of comparison questions. Our study thus reinforces recent results on the difficulty of making claims about a deep model’s world knowledge or linguistic competence based on performance on specific benchmark problems. We make our evaluation datasets publicly available to foster future research on complex understanding and reasoning in such models at standards of human interaction.",
  "stance": -0.5
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17693/17500",
  "title": "TaLNet: Voice Reconstruction from Tongue and Lip Articulation with Transfer Learning from Text-to-Speech Synthesis",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "This paper presents TaLNet, a model for voice reconstruction with ultrasound tongue and optical lip videos as inputs. TaLNet is based on an encoder-decoder architecture. Separate encoders are dedicated to processing the tongue and lip data streams respectively. The decoder predicts acoustic features conditioned on encoder outputs and speaker codes. To mitigate for having only relatively small amounts of dual articulatory-acoustic data available for training, and since our task here shares with text-to-speech (TTS) the common goal of speech generation, we propose a novel transfer learning strategy to exploit the much larger amounts of acoustic-only data available to train TTS models. For this, a Tacotron 2 TTS model is first trained, and then the parameters of its decoder are transferred to the TaLNet decoder. We have evaluated our approach on an unconstrained multi-speaker voice recovery task. Our results show the effectiveness of both the proposed model and the transfer learning strategy. Speech reconstructed using our proposed method significantly outperformed all baselines (DNN, BLSTM and without transfer learning) in terms of both naturalness and intelligibility. When using an ASR model decoding the recovery speech, the WER of our proposed method shows a relative reduction of over 30% compared to baselines.",
  "stance": 1.0
 },
 {
  "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17697/17504",
  "title": "Semantics-Aware Inferential Network for Natural Language Understanding",
  "year": 2021,
  "venue": "AAAI",
  "abstract": "For natural language understanding tasks, either machine reading comprehension or natural language inference, both semantics-aware and inference are favorable features of the concerned modeling for better understanding performance. Thus we propose a Semantics-Aware Inferential Network (SAIN) to meet such a motivation. Taking explicit contextualized semantics as a complementary input, the inferential module of SAIN enables a series of reasoning steps over semantic clues through an attention mechanism. By stringing these steps, the inferential network effectively learns to perform iterative reasoning which incorporates both explicit semantics and contextualized representations. In terms of well pre-trained language models as front-end encoder, our model achieves significant improvement on 11 tasks including machine reading comprehension and natural language inference.",
  "stance": 1.0
 },
 {
  "url": "https://arxiv.org/abs/1301.4083",
  "title": "Knowledge Matters: Importance of Prior Information for Optimization",
  "year": 2013,
  "venue": "ICLR",
  "abstract": "We explore the effect of introducing prior information into the intermediate level of deep supervised neural networks for a learning task on which all the black-box state-of-the-art machine learning algorithms tested have failed to learn. We motivate our work from the hypothesis that there is an optimization obstacle involved in the nature of such tasks, and that humans learn useful intermediate concepts from other individuals via a form of supervision or guidance using a curriculum. The experiments we have conducted provide positive evidence in favor of this hypothesis. In our experiments, a two-tiered MLP architecture is trained on a dataset for which each image input contains three sprites, and the binary target class is 1 if all three have the same shape. Black-box machine learning algorithms only got chance on this task. Standard deep supervised neural networks also failed. However, using a particular structure and guiding the learner by providing intermediate targets in the form of intermediate concepts (the presence of each object) allows to nail the task. Much better than chance but imperfect results are also obtained by exploring architecture and optimization variants, pointing towards a difficult optimization task. We hypothesize that the learning difficulty is due to the composition of two highly non-linear tasks. Our findings are also consistent with hypotheses on cultural learning inspired by the observations of effective local minima (possibly due to ill-conditioning and the training procedure not being able to escape what appears like a local minimum).",
  "stance": 0.7
 },
 {
  "url": "https://arxiv.org/abs/1312.5921",
  "title": "Group-sparse Embeddings in Collective Matrix Factorization",
  "year": 2014,
  "venue": "ICLR",
  "abstract": "CMF is a technique for simultaneously learning low-rank representations based on a collection of matrices with shared entities. A typical example is the joint modeling of useritem, item-property, and user-feature matrices in a recommender system. The key idea in CMF is that the embeddings are shared across the matrices, which enables transferring information between them. The existing solutions, however, break down when the individual matrices have low-rank structure not shared with others. In this work we present a novel CMF solution that allows each of the matrices to have a separate low-rank structure that is independent of the other matrices, as well as structures that are shared only by a subset of them. We compare MAP and variational Bayesian solutions based on alternating optimization algorithms and show that the model automatically infers the nature of each factor using group-wise sparsity. Our approach supports in a principled way continuous, binary and count observations and is efficient for sparse matrices involving missing data. We illustrate the solution on a number of examples, focusing in particular on an interesting use-case of augmented multi-view learning.",
  "stance": 0.8
 },
 {
  "url": "https://arxiv.org/abs/1312.6095",
  "title": "Multi-View Priors for Learning Detectors from Sparse Viewpoint Data",
  "year": 2014,
  "venue": "ICLR",
  "abstract": "While the majority of today’s object class models provide only 2D bounding boxes, far richer output hypotheses are desirable including viewpoint, fine-grained category, and 3D geometry estimate. However, models trained to provide richer output require larger amounts of training data, preferably well covering the relevant aspects such as viewpoint and fine-grained categories. In this paper, we address this issue from the perspective of transfer learning, and design an object class model that explicitly leverages correlations between visual features. Specifically, our model represents prior distributions over permissible multi-view detectors in a parametric way – the priors are learned once from training data of a source object class, and can later be used to facilitate the learning of a detector for a target class. As we show in our experiments, this transfer is not only beneficial for detectors based on basic-level category representations, but also enables the robust learning of detectors that represent classes at finer levels of granularity, where training data is typically even scarcer and more unbalanced. As a result, we report largely improved performance in simultaneous 2D object localization and viewpoint estimation on a recent dataset of challenging street scenes.",
  "stance": 0.9
 },
 {
  "url": "https://arxiv.org/abs/1312.6199",
  "title": "Intriguing properties of neural networks",
  "year": 2014,
  "venue": "ICLR",
  "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network’s prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
  "stance": -1.0
 },
 {
  "url": "https://arxiv.org/abs/1312.6211",
  "title": "An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks",
  "year": 2014,
  "venue": "ICLR",
  "abstract": "Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models “forget” how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting. We find that it is always best to train using the dropout algorithm– the dropout algorithm is consistently best at adapting to the new task, remembering the old task, and has the best tradeoff curve between these two extremes. We find that different tasks and relationships between tasks result in very different rankings of activation function performance. This suggests that the choice of activation function should always be cross-validated.",
  "stance": 0.2
 },
 {
  "url": "https://arxiv.org/abs/1312.6229",
  "title": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",
  "year": 2014,
  "venue": "ICLR",
  "abstract": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.",
  "stance": 1.0
 },
 {
  "url": "https://arxiv.org/abs/1412.6856",
  "title": "Object Detectors Emerge in Deep Scene CNNs",
  "year": 2015,
  "venue": "ICLR",
  "abstract": "With the success of new computational architectures for visual processing, such as convolutional neural networks (CNN) and access to image databases with millions of labeled examples (e.g., ImageNet, Places), the state of the art in computer vision is advancing rapidly. One important factor for continued progress is to understand the representations that are learned by the inner layers of these deep architectures. Here we show that object detectors emerge from training CNNs to perform scene classification. As scenes are composed of objects, the CNN for scene classification automatically discovers meaningful objects detectors, representative of the learned scene categories. With object detectors emerging as a result of learning to recognize scenes, our work demonstrates that the same network can perform both scene recognition and object localization in a single forward-pass, without ever having been explicitly taught the notion of objects.",
  "stance": 0.2
 },
 {
  "url": "https://arxiv.org/abs/1411.7676",
  "title": "Visual Representations: Defining Properties and Deep Approximations",
  "year": 2016,
  "venue": "ICLR",
  "abstract": "Visual representations are defined in terms of minimal sufficient statistics of visual data, for a class of tasks, that are also invariant to nuisance variability. Minimal sufficiency guarantees that we can store a representation in lieu of raw data with smallest complexity and no performance loss on the task at hand. Invariance guarantees that the statistic is constant with respect to uninformative transformations of the data. We derive analytical expressions for such representations and show they are related to feature descriptors commonly used in computer vision, as well as to convolutional neural networks. This link highlights the assumptions and approximations tacitly assumed by these methods and explains empirical practices such as clamping, pooling and joint normalization.",
  "stance": 0.3
 },
 {
  "url": "https://arxiv.org/abs/1511.01844",
  "title": "A note on the evaluation of generative models",
  "year": 2016,
  "venue": "ICLR",
  "abstract": "Probabilistic generative models can be used for compression, denoising, inpainting, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often difficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most commonly used criteria—average log-likelihood, Parzen window estimates, and visual fidelity of samples—are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided.",
  "stance": 0.0
 },
 {
  "url": "https://arxiv.org/abs/1511.02301",
  "title": "The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations",
  "year": 2016,
  "venue": "ICLR",
  "abstract": "We introduce a new test of how well language models capture meaning in children’s books. Unlike standard language modelling benchmarks, it distinguishes the task of predicting syntactic function words from that of predicting lowerfrequency words, which carry greater semantic content. We compare a range of state-of-the-art models, each with a different way of encoding what has been previously read. We show that models which store explicit representations of long-term contexts outperform state-of-the-art neural language models at predicting semantic content words, although this advantage is not observed for syntactic function words. Interestingly, we find that the amount of text encoded in a single memory representation is highly influential to the performance: there is a sweet-spot, not too big and not too small, between single words and full sentences that allows the most meaningful information in a text to be effectively retained and recalled. Further, the attention over such window-based memories can be trained effectively through self-supervision. We then assess the generality of this principle by applying it to the CNN QA benchmark, which involves identifying named entities in paraphrased summaries of news articles, and achieve state-of-the-art performance.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=BJ6oOfqge",
  "title": "Temporal Ensembling for Semi-Supervised Learning",
  "year": 2017,
  "venue": "ICLR",
  "abstract": "In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce self-ensembling, where we form a consensus prediction of the unknown labels using the outputs of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the (non-augmented) classification error rate from 18.44% to 7.05% in SVHN with 500 labels and from 18.63% to 16.55% in CIFAR-10 with 4000 labels, and further to 5.12% and 12.16% by enabling the standard augmentations. We additionally obtain a clear improvement in CIFAR-100 classification accuracy by using random images from the Tiny Images dataset as unlabeled extra inputs during training. Finally, we demonstrate good tolerance to incorrect labels.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=BJC_jUqxe",
  "title": "A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING",
  "year": 2017,
  "venue": "ICLR",
  "abstract": "This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=BJKYvt5lg",
  "title": "PixelVAE: A Latent Variable Model for Natural Images",
  "year": 2017,
  "venue": "ICLR",
  "abstract": "Natural image modeling is a landmark challenge of unsupervised learning. Variational Autoencoders (VAEs) learn a useful latent representation and model global structure well but have difficulty capturing small details. PixelCNN models details very well, but lacks a latent code and is difficult to scale for capturing large structures. We present PixelVAE, a VAE model with an autoregressive decoder based on PixelCNN. Our model requires very few expensive autoregressive layers compared to PixelCNN and learns latent codes that are more compressed than a standard VAE while still capturing most non-trivial structure. Finally, we extend our model to a hierarchy of latent variables at different scales. Our model achieves state-of-the-art performance on binarized MNIST, competitive performance on 64 × 64 ImageNet, and high-quality samples on the LSUN bedrooms dataset.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=H1oyRlYgg",
  "title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima",
  "year": 2017,
  "venue": "ICLR",
  "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say 32–512 data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions—and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.",
  "stance": -0.5
 },
 {
  "url": "https://openreview.net/forum?id=HJ1kmv9xx",
  "title": "LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation",
  "year": 2017,
  "venue": "ICLR",
  "abstract": "We present LR-GAN: an adversarial image generation model which takes scene structure and context into account. Unlike previous generative adversarial networks (GANs), the proposed GAN learns to generate image background and foregrounds separately and recursively, and stitch the foregrounds on the background in a contextually relevant manner to produce a complete natural image. For each foreground, the model learns to generate its appearance, shape and pose. The whole model is unsupervised, and is trained in an end-to-end manner with gradient descent methods. The experiments demonstrate that LR-GAN can generate more natural images with objects that are more human recognizable than DCGAN.",
  "stance": 0.5
 },
 {
  "url": "https://openreview.net/forum?id=HJKkY35le",
  "title": "Mode Regularized Generative Adversarial Networks",
  "year": 2017,
  "venue": "ICLR",
  "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution. We introduce several ways of regularizing the objective, which can dramatically stabilize the training of GAN models. We also show that our regularizers can help the fair distribution of probability mass across the modes of the data generating distribution, during the early phases of training and thus providing a unified solution to the missing modes problem.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=Hk3mPK5gg",
  "title": "Training Agent for First-Person Shooter Game with Actor-Critic Curriculum Learning",
  "year": 2017,
  "venue": "ICLR",
  "abstract": "In this paper, we propose a new framework for training vision-based agent for First-Person Shooter (FPS) Game, in particular Doom. Our framework combines the state-of-the-art reinforcement learning approach (Asynchronous Advantage Actor-Critic (A3C) model [Mnih et al. (2016)]) with curriculum learning. Our model is simple in design and only uses game states from the AI side, rather than using opponents’ information [Lample & Chaplot (2016)]. On a known map, our agent won 10 out of the 11 attended games and the champion of Track1 in ViZDoom AI Competition 2016 by a large margin, 35% higher score than the second place.",
  "stance": 0.7
 },
 {
  "url": "https://openreview.net/forum?id=HkYhZDqxg",
  "title": "Tree-structured decoding with doubly-recurrent neural networks",
  "year": 2017,
  "venue": "ICLR",
  "abstract": "We propose a neural network architecture for generating tree-structured objects from encoded representations. The core of the method is a doubly recurrent neural network model comprised of separate width and depth recurrences that are combined inside each cell (node) to generate an output. The topology of the tree is modeled explicitly together with the content. That is, in response to an encoded vector representation, co-evolving recurrences are used to realize the associated tree and the labels for the nodes in the tree. We test this architecture in an encoderdecoder framework, where we train a network to encode a sentence as a vector, and then generate a tree structure from it. The experimental results show the effectiveness of this architecture at recovering latent tree structure in sequences and at mapping sentences to simple functional programs.",
  "stance": 0.4
 },
 {
  "url": "https://openreview.net/forum?id=HyTqHL5xg",
  "title": "Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data",
  "year": 2017,
  "venue": "ICLR",
  "abstract": "We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning and identification of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions via variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=SJMGPrcle",
  "title": "Learning to Navigate in Complex Environments",
  "year": 2017,
  "venue": "ICLR",
  "abstract": "Learning to navigate in complex environments with dynamic elements is an important milestone in developing AI agents. In this work we formulate the navigation question as a reinforcement learning problem and show that data efficiency and task performance can be dramatically improved by relying on additional auxiliary tasks leveraging multimodal sensory inputs. In particular we consider jointly learning the goal-driven reinforcement learning problem with auxiliary depth prediction and loop closure classification tasks. This approach can learn to navigate from raw sensory input in complicated 3D mazes, approaching human-level performance even under conditions where the goal location changes frequently. We provide detailed analysis of the agent behaviour1, its ability to localise, and its network activity dynamics, showing that the agent implicitly learns key navigation abilities.",
  "stance": 0.3
 },
 {
  "url": "https://openreview.net/forum?id=SJU4ayYgl",
  "title": "Semi-Supervised Classification with Graph Convolutional Networks",
  "year": 2017,
  "venue": "ICLR",
  "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=Sy8gdB9xx",
  "title": "Understanding deep learning requires rethinking generalization",
  "year": 2017,
  "venue": "ICLR",
  "abstract": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.",
  "stance": 0.0
 },
 {
  "url": "https://openreview.net/forum?id=r10FA8Kxg",
  "title": "Do Deep Convolutional Nets Really Need to be Deep and Convolutional?",
  "year": 2017,
  "venue": "ICLR",
  "abstract": "Yes, they do. This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with methods such as distillation that allow small or shallow models of high accuracy to be trained. Although previous research showed that shallow feed-forward nets sometimes can learn the complex functions previously learned by deep nets while using the same number of parameters as the deep models they mimic, in this paper we demonstrate that the same methods cannot be used to train accurate models on CIFAR-10 unless the student models contain multiple layers of convolution. Although the student models do not have to be as deep as the teacher model they mimic, the students need multiple convolutional layers to learn functions of comparable accuracy as the deep convolutional teacher.",
  "stance": 0.0
 },
 {
  "url": "https://openreview.net/forum?id=r1fYuytex",
  "title": "Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks",
  "year": 2017,
  "venue": "ICLR",
  "abstract": "Recently deep neural networks have received considerable attention due to their ability to extract and represent high-level abstractions in data sets. Deep neural networks such as fully-connected and convolutional neural networks have shown excellent performance on a wide range of recognition and classification tasks. However, their hardware implementations currently suffer from large silicon area and high power consumption due to the their high degree of complexity. The power/energy consumption of neural networks is dominated by memory accesses, the majority of which occur in fully-connected networks. In fact, they contain most of the deep neural network parameters. In this paper, we propose sparsely-connected networks, by showing that the number of connections in fullyconnected networks can be reduced by up to 90% while improving the accuracy performance on three popular datasets (MNIST, CIFAR10 and SVHN). We then propose an efficient hardware architecture based on linear-feedback shift registers to reduce the memory requirements of the proposed sparsely-connected networks. The proposed architecture can save up to 90% of memory compared to the conventional implementations of fully-connected neural networks. Moreover, implementation results show up to 84% reduction in the energy consumption of a single neuron of the proposed sparsely-connected networks compared to a single neuron of fully-connected neural networks.",
  "stance": 0.9
 },
 {
  "url": "https://openreview.net/forum?id=B18WgG-CZ",
  "title": "Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general purpose features for words across a range of NLP problems. However, extending this success to learning representations of sequences of words, such as sentences, remains an open problem. Recent work has explored unsupervised as well as supervised learning techniques with different training objectives to learn general purpose fixed-length sentence representations. In this work, we present a simple, effective multi-task learning framework for sentence representations that combines the inductive biases of diverse training objectives in a single model. We train this model on several data sources with multiple training objectives on over 100 million sentences. Extensive experiments demonstrate that sharing a single recurrent sentence encoder across weakly related tasks leads to consistent improvements over previous methods. We present substantial improvements in the context of transfer learning and low-resource settings using our learned general-purpose representations.",
  "stance": 0.6
 },
 {
  "url": "https://openreview.net/forum?id=BJ8c3f-0b",
  "title": "Auto-Encoding Sequential Monte Carlo",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "We build on auto-encoding sequential Monte Carlo (AESMC):1 a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models. Our approach relies on the efficiency of sequential Monte Carlo (SMC) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions. We develop additional theoretical insights and experiment with a new training procedure which can improve both model and proposal learning. We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.",
  "stance": 0.9
 },
 {
  "url": "https://openreview.net/forum?id=BJehNfW0-",
  "title": "Do GANs learn the distribution? Some Theory and Empirics",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "Do GANS (Generative Adversarial Nets) actually learn the target distribution? The foundational paper of Goodfellow et al. (2014) suggested they do, if they were given “sufficiently large” deep nets, sample size, and computation time. A recent theoretical analysis in Arora et al. (2017) raised doubts whether the same holds when discriminator has bounded size. It showed that the training objective can approach its optimum value even if the generated distribution has very low support —in other words, the training objective is unable to prevent mode collapse. The current paper makes two contributions. (1) It proposes a novel test for estimating support size using the birthday paradox of discrete probability. Using this evidence is presented that well-known GANs approaches do learn distributions of fairly low support. (2) It theoretically studies encoder-decoder GANs architectures (e.g., BiGAN/ALI), which were proposed to learn more meaningful features via GANs and (consequently) to also solve the mode-collapse issue. Our result shows that such encoder-decoder training objectives also cannot guarantee learning of the full distribution because they cannot prevent serious mode collapse. More seriously, they cannot prevent learning meaningless codes for data, contrary to usual intuition.",
  "stance": -0.3
 },
 {
  "url": "https://openreview.net/forum?id=ByJHuTgA-",
  "title": "On the State of the Art of Evaluation in Neural Language Models",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=ByQpn1ZA-",
  "title": "Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players’ parameters. One useful approach for the theory of GANs is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium. Several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence. We show that this view is overly restrictive. During GAN training, the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful. We provide empirical counterexamples to the view of GAN training as divergence minimization. Specifically, we demonstrate that GANs are able to learn distributions in situations where the divergence minimization point of view predicts they would fail. We also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful. This contributes to a growing body of evidence that GAN training may be more usefully viewed as approaching Nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step.",
  "stance": -1.0
 },
 {
  "url": "https://openreview.net/forum?id=BydLzGb0Z",
  "title": "Twin Networks: Matching the Future for Sequence Generation",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "We propose a simple technique for encouraging generative RNNs to plan ahead. We train a “backward” recurrent network to generate a given sequence in reverse order, and we encourage states of the forward model to predict cotemporal states of the backward model. The backward network is used only during training, and plays no role during sampling or inference. We hypothesize that our approach eases modeling of long-term dependencies by implicitly forcing the forward states to hold information about the longer-term future (as contained in the backward states). We show empirically that our approach achieves 9% relative improvement for a speech recognition task, and achieves significant improvement on a COCO caption generation task.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=BydjJte0-",
  "title": "Towards Reverse-Engineering Black-Box Neural Networks",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "Many deployed learned models are black boxes: given input, returns output. Internal information about the model, such as the architecture, optimisation procedure, or training data, is not disclosed explicitly as it might contain proprietary information or make the system more vulnerable. This work shows that such attributes of neural networks can be exposed from a sequence of queries. This has multiple implications. On the one hand, our work exposes the vulnerability of black-box neural networks to different types of attacks – we show that the revealed internal information helps generate more effective adversarial examples against the black box model. On the other hand, this technique can be used for better protection of private content from automatic recognition models using adversarial examples. Our paper suggests that it is actually hard to draw a line between white box and black box models. The code is available at goo.gl/MbYfsv.",
  "stance": -0.2
 },
 {
  "url": "https://openreview.net/forum?id=H1Dy---0Z",
  "title": "Distributed Prioritized Experience Replay",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "We propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible. The algorithm decouples acting from learning: the actors interact with their own instances of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared experience replay memory; the learner replays samples of experience and updates the neural network. The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors. Our architecture substantially improves the state of the art on the Arcade Learning Environment, achieving better final performance in a fraction of the wall-clock training time.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=H1MczcgR-",
  "title": "Understanding Short-Horizon Bias in Stochastic Meta-Optimization",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "Careful tuning of the learning rate, or even schedules thereof, can be crucial to effective neural net training. There has been much recent interest in gradient-based meta-optimization, where one tunes hyperparameters, or even learns an optimizer, in order to minimize the expected loss when the training procedure is unrolled. But because the training procedure must be unrolled thousands of times, the metaobjective must be defined with an orders-of-magnitude shorter time horizon than is typical for neural net training. We show that such short-horizon meta-objectives cause a serious bias towards small step sizes, an effect we term short-horizon bias. We introduce a toy problem, a noisy quadratic cost function, on which we analyze short-horizon bias by deriving and comparing the optimal schedules for short and long time horizons. We then run meta-optimization experiments (both offline and online) on standard benchmark datasets, showing that meta-optimization chooses too small a learning rate by multiple orders of magnitude, even when run with a moderately long time horizon (100 steps) typical of work in the area. We believe short-horizon bias is a fundamental problem that needs to be addressed if metaoptimization is to scale to practical neural net training regimes.",
  "stance": -1.0
 },
 {
  "url": "https://openreview.net/forum?id=HJGv1Z-AW",
  "title": "Emergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "The ability of algorithms to evolve or learn (compositional) communication protocols has traditionally been studied in the language evolution literature through the use of emergent communication tasks. Here we scale up this research by using contemporary deep learning methods and by training reinforcement-learning neural network agents on referential communication games. We extend previous work, in which agents were trained in symbolic environments, by developing agents which are able to learn from raw pixel data, a more challenging and realistic input representation. We find that the degree of structure found in the input data affects the nature of the emerged protocols, and thereby corroborate the hypothesis that structured compositional language is most likely to emerge when agents perceive the world as being structured.",
  "stance": 0.4
 },
 {
  "url": "https://openreview.net/forum?id=HJIoJWZCZ",
  "title": "Adversarial Dropout Regularization",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "We present a domain adaptation method for transferring neural representations from label-rich source domains to unlabeled target domains. Recent adversarial methods proposed for this task learn to align features across domains by “fooling” a special domain classifier network. However, a drawback of this approach is that the domain classifier simply labels the generated features as in-domain or not, without considering the boundaries between classes. This means that ambiguous target features can be generated near class boundaries, reducing target classification accuracy. We propose a novel approach, Adversarial Dropout Regularization (ADR), which encourages the generator to output more discriminative features for the target domain. Our key idea is to replace the traditional domain critic with a critic that detects non-discriminative features by using dropout on the classifier network. The generator then learns to avoid these areas of the feature space and thus creates better features. We apply our ADR approach to the problem of unsupervised domain adaptation for image classification and semantic segmentation tasks, and demonstrate significant improvements over the state of the art.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=HkAClQgA-",
  "title": "A Deep Reinforced Model for Abstractive Summarization",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intraattention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). Models trained only with supervised learning often exhibit “exposure bias” – they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=HkTEFfZRb",
  "title": "Attacking Binarized Neural Networks",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "Neural networks with low-precision weights and activations offer compelling efficiency advantages over their full-precision equivalents. The two most frequently discussed benefits of quantization are reduced memory consumption, and a faster forward pass when implemented with efficient bitwise operations. We propose a third benefit of very low-precision neural networks: improved robustness against some adversarial attacks, and in the worst case, performance that is on par with full-precision models. We focus on the very low-precision case where weights and activations are both quantized to ±1, and note that stochastically quantizing weights in just one layer can sharply reduce the impact of iterative attacks. We observe that non-scaled binary neural networks exhibit a similar effect to the original defensive distillation procedure that led to gradient masking, and a false notion of security. We address this by conducting both black-box and white-box experiments with binary models that do not artificially mask gradients.1",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=Hkc-TeZ0W",
  "title": "A Hierarchical Model for Device Placement",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "We introduce a hierarchical model for efficient placement of computational graphs onto hardware devices, especially in heterogeneous environments with a mixture of CPUs, GPUs, and other computational devices. Our method learns to assign graph operations to groups and to allocate those groups to available devices. The grouping and device allocations are learned jointly. The proposed method is trained with policy gradient and requires no human intervention. Experiments with widely-used computer vision and natural language models show that our algorithm can find optimized, non-trivial placements for TensorFlow computational graphs with over 80,000 operations. In addition, our approach outperforms placements by human experts as well as a previous state-of-the-art placement method based on deep reinforcement learning. Our method achieves runtime reductions of up to 60.6% per training step when applied to models such as Neural Machine Translation.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=Hy7fDog0b",
  "title": "AmbientGAN: Generative models from lossy measurements",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "Generative models provide a way to model structure in complex distributions and have been shown to be useful for many tasks of practical interest. However, current techniques for training generative models require access to fully-observed samples. In many settings, it is expensive or even impossible to obtain fullyobserved samples, but economical to obtain partial, noisy observations. We consider the task of learning an implicit generative model given only lossy measurements of samples from the distribution of interest. We show that the true underlying distribution can be provably recovered even in the presence of per-sample information loss for a class of measurement models. Based on this, we propose a new method of training Generative Adversarial Networks (GANs) which we call AmbientGAN. On three benchmark datasets, and for various measurement models, we demonstrate substantial qualitative and quantitative improvements. Generative models trained with our method can obtain 2-4x higher inception scores than the baselines.",
  "stance": 0.4
 },
 {
  "url": "https://openreview.net/forum?id=S1nQvfgA-",
  "title": "Semantically Decomposing the Latent Spaces of Generative Adversarial Networks",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "We propose a new algorithm for training generative adversarial networks that jointly learns latent codes for both identities (e.g. individual humans) and observations (e.g. specific photographs). By fixing the identity portion of the latent codes, we can generate diverse images of the same subject, and by fixing the observation portion, we can traverse the manifold of subjects while maintaining contingent aspects such as lighting and pose. Our algorithm features a pairwise training scheme in which each sample from the generator consists of two images with a common identity code. Corresponding samples from the real dataset consist of two distinct photographs of the same subject. In order to fool the discriminator, the generator must produce pairs that are photorealistic, distinct, and appear to depict the same individual. We augment both the DCGAN and BEGAN approaches with Siamese discriminators to facilitate pairwise training. Experiments with human judges and an off-the-shelf face verification system demonstrate our algorithm’s ability to generate convincing, identity-matched photographs.",
  "stance": 0.6
 },
 {
  "url": "https://openreview.net/forum?id=S1uxsye0Z",
  "title": "Adaptive Dropout with Rademacher Complexity Regularization",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation. However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space. In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices. Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power. Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm. We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns. Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of-the-art dropout algorithms.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=Sk7KsfW0-",
  "title": "Lifelong Learning with Dynamically Expandable Networks",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efficiently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets under lifelong learning scenarios, on which it not only significantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch counterparts with substantially fewer number of parameters. Further, the obtained network fine-tuned on all tasks obtained siginficantly better performance over the batch models, which shows that it can be used to estimate the optimal network structure even when all tasks are available in the first place.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=SyX0IeWAW",
  "title": "META LEARNING SHARED HIERARCHIES",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "We develop a metalearning approach for learning hierarchically structured policies, improving sample efficiency on unseen tasks through the use of shared primitives—policies that are executed for large numbers of timesteps. Specifically, a set of primitives are shared within a distribution of tasks, and are switched between by task-specific policies. We provide a concrete metric for measuring the strength of such hierarchies, leading to an optimization problem for quickly reaching high reward on unseen tasks. We then present an algorithm to solve this problem end-to-end through the use of any off-the-shelf reinforcement learning method, by repeatedly sampling new tasks and resetting task-specific policies. We successfully discover1 meaningful motor primitives for the directional movement of four-legged robots, solely by interacting with distributions of mazes. We also demonstrate the transferability of primitives to solve long-timescale sparse-reward obstacle courses, and we enable 3D humanoid robots to robustly walk and crawl with the same policy.",
  "stance": 0.7
 },
 {
  "url": "https://openreview.net/forum?id=SyYe6k-CW",
  "title": "Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "Recent advances in deep reinforcement learning have made significant strides in performance on applications such as Go and Atari games. However, developing practical methods to balance exploration and exploitation in complex domains remains largely unsolved. Thompson Sampling and its extension to reinforcement learning provide an elegant approach to exploration that only requires access to posterior samples of the model. At the same time, advances in approximate Bayesian methods have made posterior approximation for flexible neural network models practical. Thus, it is attractive to consider approximate Bayesian neural networks in a Thompson Sampling framework. To understand the impact of using an approximate posterior on Thompson Sampling, we benchmark well-established and recently developed methods for approximate posterior sampling combined with Thompson Sampling over a series of contextual bandit problems. We found that many approaches that have been successful in the supervised learning setting underperformed in the sequential decision-making scenario. In particular, we highlight the challenge of adapting slowly converging uncertainty estimates to the online setting.",
  "stance": -0.5
 },
 {
  "url": "https://openreview.net/forum?id=r1iuQjxCZ",
  "title": "On the importance of single directions for generalization",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "Despite their ability to memorize large datasets, deep neural networks often achieve good generalization performance. However, the differences between the learned solutions of networks which generalize and those which do not remain unclear. Additionally, the tuning properties of single directions (defined as the activation of a single unit or some linear combination of units in response to some input) have been highlighted, but their importance has not been evaluated. Here, we connect these lines of inquiry to demonstrate that a network’s reliance on single directions is a good predictor of its generalization performance, across networks trained on datasets with different fractions of corrupted labels, across ensembles of networks trained on datasets with unmodified labels, across different hyperparameters, and over the course of training. While dropout only regularizes this quantity up to a point, batch normalization implicitly discourages single direction reliance, in part by decreasing the class selectivity of individual units. Finally, we find that class selectivity is a poor predictor of task importance, suggesting not only that networks which generalize well minimize their dependence on individual units by reducing their selectivity, but also that individually selective units may not be necessary for strong network performance.",
  "stance": -0.5
 },
 {
  "url": "https://openreview.net/forum?id=rJvJXZb0W",
  "title": "An efficient framework for learning sentence representations",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. Given a sentence and the context in which it appears, a classifier distinguishes context sentences from other contrastive sentences based on their vector representations. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform stateof-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=rkHVZWZAZ",
  "title": "The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "In this work, we present a new agent architecture, called Reactor, which combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN (Wang et al., 2017) and Categorical DQN (Bellemare et al., 2017), while giving better run-time performance than A3C (Mnih et al., 2016). Our first contribution is a new policy evaluation algorithm called Distributional Retrace, which brings multi-step off-policy updates to the distributional reinforcement learning setting. The same approach can be used to convert several classes of multi-step policy evaluation algorithms, designed for expected value evaluation, into distributional algorithms. Next, we introduce the β-leave-one-out policy gradient algorithm, which improves the trade-off between variance and bias by using action values as a baseline. Our final algorithmic contribution is a new prioritized replay algorithm for sequences, which exploits the temporal locality of neighboring observations for more efficient replay prioritization. Using the Atari 2600 benchmarks, we show that each of these innovations contribute to both sample efficiency and final agent performance. Finally, we demonstrate that Reactor reaches state-of-the-art performance after 200 million frames and less than a day of training.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=rkgOLb-0W",
  "title": "Neural Language Modeling by Jointly Learning Syntax and Lexicon",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "We propose a neural language model capable of unsupervised syntactic structure induction. The model leverages the structure information to form better semantic representations and better language modeling. Standard recurrent neural networks are limited by their structure and fail to efficiently use syntactic information. On the other hand, tree-structured recursive networks usually require additional structural supervision at the cost of human expert annotation. In this paper, We propose a novel neural language model, called the Parsing-Reading-Predict Networks (PRPN), that can simultaneously induce the syntactic structure from unannotated sentences and leverage the inferred structure to learn a better language model. In our model, the gradient can be directly back-propagated from the language model loss into the neural parsing network. Experiments show that the proposed model can discover the underlying syntactic structure and achieve state-of-the-art performance on word/character-level language model tasks.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=rkhlb8lCZ",
  "title": "Wavelet Pooling for Convolutional Neural Networks",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "Convolutional Neural Networks continuously advance the progress of 2D and 3D image and object classification. The steadfast usage of this algorithm requires constant evaluation and upgrading of foundational concepts to maintain progress. Network regularization techniques typically focus on convolutional layer operations, while leaving pooling layer operations without suitable options. We introduce Wavelet Pooling as another alternative to traditional neighborhood pooling. This method decomposes features into a second level decomposition, and discards the first-level subbands to reduce feature dimensions. This method addresses the overfitting problem encountered by max pooling, while reducing features in a more structurally compact manner than pooling via neighborhood regions. Experimental results on four benchmark classification datasets demonstrate our proposed method outperforms or performs comparatively with methods like max, mean, mixed, and stochastic pooling.",
  "stance": 0.7
 },
 {
  "url": "https://openreview.net/forum?id=ryQu7f-RZ",
  "title": "On the Convergence of Adam and Beyond",
  "year": 2018,
  "venue": "ICLR",
  "abstract": "Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSPROP, ADAM, ADADELTA, NADAM are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where ADAM does not converge to the optimal solution, and describe the precise problems with the previous analysis of ADAM algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with “long-term memory” of past gradients, and propose new variants of the ADAM algorithm which not only fix the convergence issues but often also lead to improved empirical performance.",
  "stance": 0.8
 },
 {
  "url": "https://openreview.net/forum?id=B1fpDsAqt7",
  "title": "Visual Reasoning by Progressive Module Networks",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "Humans learn to solve tasks of increasing complexity by building on top of previously acquired knowledge. Typically, there exists a natural progression in the tasks that we learn – most do not require completely independent solutions, but can be broken down into simpler subtasks. We propose to represent a solver for each task as a neural module that calls existing modules (solvers for simpler tasks) in a functional program-like manner. Lower modules are a black box to the calling module, and communicate only via a query and an output. Thus, a module for a new task learns to query existing modules and composes their outputs in order to produce its own output. Our model effectively combines previous skill-sets, does not suffer from forgetting, and is fully differentiable. We test our model in learning a set of visual reasoning tasks, and demonstrate improved performances in all tasks by learning progressively. By evaluating the reasoning process using human judges, we show that our model is more interpretable than an attention-based baseline.",
  "stance": 0.7
 },
 {
  "url": "https://openreview.net/forum?id=B1gabhRcYX",
  "title": "BA-Net: Dense Bundle Adjustment Networks",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "This paper introduces a network architecture to solve the structure-from-motion (SfM) problem via feature-metric bundle adjustment (BA), which explicitly enforces multi-view geometry constraints in the form of feature-metric error. The whole pipeline is differentiable, so that the network can learn suitable features that make the BA problem more tractable. Furthermore, this work introduces a novel depth parameterization to recover dense per-pixel depth. The network first generates several basis depth maps according to the input image, and optimizes the final depth as a linear combination of these basis depth maps via feature-metric BA. The basis depth maps generator is also learned via end-to-end training. The whole system nicely combines domain knowledge (i.e. hard-coded multi-view geometry constraints) and deep learning (i.e. feature learning and basis depth maps learning) to address the challenging dense SfM problem. Experiments on large scale real data prove the success of the proposed method.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=BkeStsCcKQ",
  "title": "Critical Learning Periods in Deep Networks",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "Similar to humans and animals, deep artificial neural networks exhibit critical periods during which a temporary stimulus deficit can impair the development of a skill. The extent of the impairment depends on the onset and length of the deficit window, as in animal models, and on the size of the neural network. Deficits that do not affect low-level statistics, such as vertical flipping of the images, have no lasting effect on performance and can be overcome with further training. To better understand this phenomenon, we use the Fisher Information of the weights to measure the effective connectivity between layers of a network during training. Counterintuitively, information rises rapidly in the early phases of training, and then decreases, preventing redistribution of information resources in a phenomenon we refer to as a loss of “Information Plasticity”. Our analysis suggests that the first few epochs are critical for the creation of strong connections that are optimal relative to the input data distribution. Once such strong connections are created, they do not appear to change during additional training. These findings suggest that the initial learning transient, under-scrutinized compared to asymptotic behavior, plays a key role in determining the outcome of the training process. Our findings, combined with recent theoretical results in the literature, also suggest that forgetting (decrease of information in the weights) is critical to achieving invariance and disentanglement in representation learning. Finally, critical periods are not restricted to biological systems, but can emerge naturally in learning systems, whether biological or artificial, due to fundamental constrains arising from learning dynamics and information processing.",
  "stance": 0.3
 },
 {
  "url": "https://openreview.net/forum?id=Bkl-43C9FQ",
  "title": "Spherical CNNs on Unstructured Grids",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "We present an efficient convolution kernel for Convolutional Neural Networks (CNNs) on unstructured grids using parameterized differential operators while focusing on spherical signals such as panorama images or planetary signals. To this end, we replace conventional convolution kernels with linear combinations of differential operators that are weighted by learnable parameters. Differential operators can be efficiently estimated on unstructured grids using one-ring neighbors, and learnable parameters can be optimized through standard back-propagation. As a result, we obtain extremely efficient neural networks that match or outperform state-of-the-art network architectures in terms of performance but with a significantly smaller number of network parameters. We evaluate our algorithm in an extensive series of experiments on a variety of computer vision and climate science tasks, including shape classification, climate pattern segmentation, and omnidirectional image semantic segmentation. Overall, we (1) present a novel CNN approach on unstructured grids using parameterized differential operators for spherical signals, and (2) show that our unique kernel parameterization allows our model to achieve the same or higher accuracy with significantly fewer network parameters.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=H1x-x309tm",
  "title": "On the Convergence of A Class of Adam-Type Algorithms for Non-Convex Optimization",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "This paper studies a class of adaptive gradient based momentum algorithms that update the search directions and learning rates simultaneously using past gradients. This class, which we refer to as the “Adam-type”, includes the popular algorithms such as Adam (Kingma & Ba, 2014) , AMSGrad (Reddi et al., 2018) , AdaGrad (Duchi et al., 2011). Despite their popularity in training deep neural networks (DNNs), the convergence of these algorithms for solving non-convex problems remains an open question. In this paper, we develop an analysis framework and a set of mild sufficient conditions that guarantee the convergence of the Adam-type methods, with a convergence rate of order O(log T/ √ T ) for non-convex stochastic optimization. Our convergence analysis applies to a new algorithm called AdaFom (AdaGrad with First Order Momentum). We show that the conditions are essential, by identifying concrete examples in which violating the conditions makes an algorithm diverge. Besides providing one of the first comprehensive analysis for Adam-type methods in the non-convex setting, our results can also help the practitioners to easily monitor the progress of algorithms and determine their convergence behavior.",
  "stance": 0.5
 },
 {
  "url": "https://openreview.net/forum?id=H1xwNhCcYm",
  "title": "Do Deep Generative Models Know What They Don't Know?",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data. A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong. Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel, out-of-distribution inputs. In this paper we challenge this assumption. We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former. Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN. To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood. We find such behavior persists even when we restrict the flows to constant-volume transformations. These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.",
  "stance": -0.8
 },
 {
  "url": "https://openreview.net/forum?id=Hk4fpoA5Km",
  "title": "Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "Algorithms for imitation learning based on adversarial optimization, such as generative adversarial imitation learning (GAIL) and adversarial inverse reinforcement learning (AIRL), can effectively mimic demonstrated behaviours by employing both reward and reinforcement learning (RL). However, applications of such algorithms are challenged by the inherent instability and poor sample efficiency of on-policy RL. In particular, the inadequate handling of absorbing states in canonical implementations of RL environments causes an implicit bias in reward functions used by these algorithms. While these biases might work well for some environments, they lead to sub-optimal behaviors in others. Moreover, despite the ability of these algorithms to learn from a few demonstrations, they require a prohibitively large number of the environment interactions for many real-world applications. To address these issues, we first propose to extend the environment MDP with absorbing states which leads to task-independent, and more importantly, unbiased rewards. Secondly, we introduce an off-policy learning algorithm, which we refer to as Discriminator-Actor-Critic. We demonstrate the effectiveness of proper handling of absorbing states, while empirically improving the sample efficiency by an average factor of 10. Our implementation is available online 1.",
  "stance": 0.5
 },
 {
  "url": "https://openreview.net/forum?id=HkezXnA9YX",
  "title": "Systematic Generalization: What Is Required and Can It Be Learned?",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "Numerous models for grounded language understanding have been recently proposed, including (i) generic models that can be easily adapted to any given task and (ii) intuitively appealing modular models that require background knowledge to be instantiated. We compare both types of models in how much they lend themselves to a particular form of systematic generalization. Using a synthetic VQA test, we evaluate which models are capable of reasoning about all possible object pairs after training on only a small subset of them. Our findings show that the generalization of modular models is much more systematic and that it is highly sensitive to the module layout, i.e. to how exactly the modules are connected. We furthermore investigate if modular models that generalize well could be made more end-to-end by learning their layout and parametrization. We find that endto-end methods from prior work often learn inappropriate layouts or parametrizations that do not facilitate systematic generalization. Our results suggest that, in addition to modularity, systematic generalization in language understanding may require explicit regularizers or priors.",
  "stance": -1.0
 },
 {
  "url": "https://openreview.net/forum?id=HkfPSh05K7",
  "title": "Multi-step Retriever-Reader Interaction for Scalable Open-domain Question Answering",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "This paper introduces a new framework for open-domain question answering in which the retriever and the reader iteratively interact with each other. The framework is agnostic to the architecture of the machine reading model, only requiring access to the token-level hidden representations of the reader. The retriever uses fast nearest neighbor search to scale to corpora containing millions of paragraphs. A gated recurrent unit updates the query at each step conditioned on the state of the reader and the reformulated query is used to re-rank the paragraphs by the retriever. We conduct analysis and show that iterative interaction helps in retrieving informative paragraphs from the corpus. Finally, we show that our multistep-reasoning framework brings consistent improvement when applied to two widely used reader architectures (DR.QA and BIDAF) on various large open-domain datasets — TRIVIAQA-unfiltered, QUASAR-T, SEARCHQA, and SQUAD-open1.",
  "stance": 0.7
 },
 {
  "url": "https://openreview.net/forum?id=HkgEQnRqYQ",
  "title": "RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links. The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations. In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition. Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space. In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model. Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=HkxKH2AcFm",
  "title": "Towards GAN Benchmarks Which Require Generalization",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "For many evaluation metrics commonly used as benchmarks for unconditional image generation, trivially memorizing the training set attains a better score than models which are considered state-of-the-art; we consider this problematic. We clarify a necessary condition for an evaluation metric not to behave this way: estimating the function must require a large sample from the model. In search of such a metric, we turn to neural network divergences (NNDs), which are defined in terms of a neural network trained to distinguish between distributions. The resulting benchmarks cannot be “won” by training set memorization, while still being perceptually correlated and computable only from samples. We survey past work on using NNDs for evaluation, implement an example black-box metric based on these ideas, and validate experimentally that it can measure a notion of generalization.",
  "stance": 0.4
 },
 {
  "url": "https://openreview.net/forum?id=HkxaFoC9KQ",
  "title": "Deep reinforcement learning with relational inductive biases",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "We introduce an approach for augmenting model-free deep reinforcement learning agents with a mechanism for relational reasoning over structured representations, which improves performance, learning efficiency, generalization, and interpretability. Our architecture encodes an image as a set of vectors, and applies an iterative message-passing procedure to discover and reason about relevant entities and relations in a scene. In six of seven StarCraft II Learning Environment mini-games, our agent achieved state-of-the-art performance, and surpassed human grandmasterlevel on four. In a novel navigation and planning task, our agent’s performance and learning efficiency far exceeded non-relational baselines, it was able to generalize to more complex scenes than it had experienced during training. Moreover, when we examined its learned internal representations, they reflected important structure about the problem and the agent’s intentions. The main contribution of this work is to introduce techniques for representing and reasoning about states in model-free deep reinforcement learning agents via relational inductive biases. Our experiments show this approach can offer advantages in efficiency, generalization, and interpretability, and can scale up to meet some of the most challenging test environments in modern artificial intelligence.",
  "stance": 0.7
 },
 {
  "url": "https://openreview.net/forum?id=HyEtjoCqFX",
  "title": "Soft Q-Learning with Mutual-Information Regularization",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "We propose a reinforcement learning (RL) algorithm that uses mutual-information regularization to optimize a prior action distribution for better performance and exploration. Entropy-based regularization has previously been shown to improve both exploration and robustness in challenging sequential decision-making tasks. It does so by encouraging policies to put probability mass on all actions. However, entropy regularization might be undesirable when actions have significantly different importance. In this paper, we propose a theoretically motivated framework that dynamically weights the importance of actions by using the mutualinformation. In particular, we express the RL problem as an inference problem where the prior probability distribution over actions is subject to optimization. We show that the prior optimization introduces a mutual-information regularizer in the RL objective. This regularizer encourages the policy to be close to a nonuniform distribution that assigns higher probability mass to more important actions. We empirically demonstrate that our method significantly improves over entropy regularization methods and unregularized methods.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=HyeGBj09Fm",
  "title": "Generating Liquid Simulations with Deformation-aware Neural Networks",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "We propose a novel approach for deformation-aware neural networks that learn the weighting and synthesis of dense volumetric deformation fields. Our method specifically targets the space-time representation of physical surfaces from liquid simulations. Liquids exhibit highly complex, non-linear behavior under changing simulation conditions such as different initial conditions. Our algorithm captures these complex phenomena in two stages: a first neural network computes a weighting function for a set of pre-computed deformations, while a second network directly generates a deformation field for refining the surface. Key for successful training runs in this setting is a suitable loss function that encodes the effect of the deformations, and a robust calculation of the corresponding gradients. To demonstrate the effectiveness of our approach, we showcase our method with several complex examples of flowing liquids with topology changes. Our representation makes it possible to rapidly generate the desired implicit surfaces. We have implemented a mobile application to demonstrate that real-time interactions with complex liquid effects are possible with our approach.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=Hyfn2jCcKm",
  "title": "Solving the Rubik's Cube with Approximate Policy Iteration",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "Recently, Approximate Policy Iteration (API) algorithms have achieved superhuman proficiency in two-player zero-sum games such as Go, Chess, and Shogi without human data. These API algorithms iterate between two policies: a slow policy (tree search), and a fast policy (a neural network). In these two-player games, a reward is always received at the end of the game. However, the Rubik’s Cube has only a single solved state, and episodes are not guaranteed to terminate. This poses a major problem for these API algorithms since they rely on the reward received at the end of the game. We introduce Autodidactic Iteration: an API algorithm that overcomes the problem of sparse rewards by training on a distribution of states that allows the reward to propagate from the goal state to states farther away. Autodidactic Iteration is able to learn how to solve the Rubik’s Cube without relying on human data. Our algorithm is able to solve 100% of randomly scrambled cubes while achieving a median solve length of 30 moves — less than or equal to solvers that employ human domain knowledge.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=Hygxb2CqKm",
  "title": "Stable Recurrent Models",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models.",
  "stance": 0.4
 },
 {
  "url": "https://openreview.net/forum?id=HylTBhA5tQ",
  "title": "The Limitations of Adversarial Training and the Blind-Spot Attack",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "The adversarial training procedure proposed by Madry et al. (2018) is one of the most effective methods to defend against adversarial examples in deep neural networks (DNNs). In our paper, we shed some lights on the practicality and the hardness of adversarial training by showing that the effectiveness (robustness on test set) of adversarial training has a strong correlation with the distance between a test point and the manifold of training data embedded by the network. Test examples that are relatively far away from this manifold are more likely to be vulnerable to adversarial attacks. Consequentially, an adversarial training based defense is susceptible to a new class of attacks, the “blind-spot attack”, where the input images reside in “blind-spots” (low density regions) of the empirical distribution of training data but is still on the ground-truth data manifold. For MNIST, we found that these blind-spots can be easily found by simply scaling and shifting image pixel values. Most importantly, for large datasets with high dimensional and complex data manifold (CIFAR, ImageNet, etc), the existence of blind-spots in adversarial training makes defending on any valid test examples difficult due to the curse of dimensionality and the scarcity of training data. Additionally, we find that blind-spots also exist on provable defenses including (Kolter & Wong, 2018) and (Sinha et al., 2018) because these trainable robustness certificates can only be practically optimized on a limited set of training data.",
  "stance": -0.5
 },
 {
  "url": "https://openreview.net/forum?id=S1gUsoR9YX",
  "title": "Multilingual Neural Machine Translation with Knowledge Distillation",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "Multilingual machine translation, which translates multiple languages with a single model, has attracted much attention due to its efficiency of offline training and online serving. However, traditional multilingual translation usually yields inferior accuracy compared with the counterpart using individual models for each language pair, due to language diversity and model capacity limitations. In this paper, we propose a distillation-based approach to boost the accuracy of multilingual machine translation. Specifically, individual models are first trained and regarded as teachers, and then the multilingual model is trained to fit the training data and match the outputs of individual models simultaneously through knowledge distillation. Experiments on IWSLT, WMT and Ted talk translation datasets demonstrate the effectiveness of our method. Particularly, we show that one model is enough to handle multiple languages (up to 44 languages in our experiment), with comparable or even better accuracy than individual models.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=S1x4ghC9tQ",
  "title": "Temporal Difference Variational Auto-Encoder",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "To act and plan in complex environments, we posit that agents should have a mental simulator of the world with three characteristics: (a) it should build an abstract state representing the condition of the world; (b) it should form a belief which represents uncertainty on the world; (c) it should go beyond simple step-by-step simulation, and exhibit temporal abstraction. Motivated by the absence of a model satisfying all these requirements, we propose TD-VAE, a generative sequence model that learns representations containing explicit beliefs about states several steps into the future, and that can be rolled out directly without single-step transitions. TD-VAE is trained on pairs of temporally separated time points, using an analogue of temporal difference learning used in reinforcement learning.",
  "stance": 0.3
 },
 {
  "url": "https://openreview.net/forum?id=S1xNEhR9KX",
  "title": "On the Sensitivity of Adversarial Robustness to Input Data Distributions",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "Neural networks are vulnerable to small adversarial perturbations. Existing literature largely focused on understanding and mitigating the vulnerability of learned models. In this paper, we demonstrate an intriguing phenomenon about the most popular robust training method in the literature, adversarial training: Adversarial robustness, unlike clean accuracy, is sensitive to the input data distribution. Even a semantics-preserving transformations on the input data distribution can cause a significantly different robustness for the adversarial trained model that is both trained and evaluated on the new distribution. Our discovery of such sensitivity on data distribution is based on a study which disentangles the behaviors of clean accuracy and robust accuracy of the Bayes classifier. Empirical investigations further confirm our finding. We construct semantically-identical variants for MNIST and CIFAR10 respectively, and show that standardly trained models achieve comparable clean accuracies on them, but adversarially trained models achieve significantly different robustness accuracies. This counter-intuitive phenomenon indicates that input data distribution alone can affect the adversarial robustness of trained neural networks, not necessarily the tasks themselves. Lastly, we discuss the practical implications on evaluating adversarial robustness, and make initial attempts to understand this complex phenomenon.",
  "stance": -0.5
 },
 {
  "url": "https://openreview.net/forum?id=S1xNb2A9YX",
  "title": "Minimal Images in Deep Neural Networks: Fragile Object Recognition in Natural Images",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "The human ability to recognize objects is impaired when the object is not shown in full. \"Minimal images\" are the smallest regions of an image that remain recognizable for humans. Ullman et al. (2016) show that a slight modification of the location and size of the visible region of the minimal image produces a sharp drop in human recognition accuracy. In this paper, we demonstrate that such drops in accuracy due to changes of the visible region are a common phenomenon between humans and existing state-of-the-art deep neural networks (DNNs), and are much more prominent in DNNs. We found many cases where DNNs classified one region correctly and the other incorrectly, though they only differed by one row or column of pixels, and were often bigger than the average human minimal image size. We show that this phenomenon is independent from previous works that have reported lack of invariance to minor modifications in object location in DNNs. Our results thus reveal a new failure mode of DNNs that also affects humans to a much lesser degree. They expose how fragile DNN recognition ability is for natural images even without adversarial patterns being introduced. Bringing the robustness of DNNs in natural images to the human level remains an open challenge for the community.",
  "stance": -0.8
 },
 {
  "url": "https://openreview.net/forum?id=SJggZnRcFQ",
  "title": "Learning Programmatically Structured Representations with Perceptor Gradients",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "We present the perceptor gradients algorithm – a novel approach to learning symbolic representations based on the idea of decomposing an agent’s policy into i) a perceptor network extracting symbols from raw observation data and ii) a task encoding program which maps the input symbols to output actions. We show that the proposed algorithm is able to learn representations that can be directly fed into a Linear-Quadratic Regulator (LQR) or a general purpose A* planner. Our experimental results confirm that the perceptor gradients algorithm is able to efficiently learn transferable symbolic representations as well as generate new observations according to a semantically meaningful specification.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=SkgEaj05t7",
  "title": "On the Relation Between the Sharpest Directions of DNN Loss and the SGD Step Length",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "Stochastic Gradient Descent (SGD) based training of neural networks with a large learning rate or a small batch-size typically ends in well-generalizing, flat regions of the weight space, as indicated by small eigenvalues of the Hessian of the training loss. However, the curvature along the SGD trajectory is poorly understood. An empirical investigation shows that initially SGD visits increasingly sharp regions, reaching a maximum sharpness determined by both the learning rate and the batch-size of SGD. When studying the SGD dynamics in relation to the sharpest directions in this initial phase, we find that the SGD step is large compared to the curvature and commonly fails to minimize the loss along the sharpest directions. Furthermore, using a reduced learning rate along these directions can improve training speed while leading to both sharper and better generalizing solutions compared to vanilla SGD. In summary, our analysis of the dynamics of SGD in the subspace of the sharpest directions shows that they influence the regions that SGD steers to (where larger learning rate or smaller batch size result in wider regions visited), the overall training speed, and the generalization ability of the final model.",
  "stance": 0.3
 },
 {
  "url": "https://openreview.net/forum?id=SyxAb30cY7",
  "title": "Robustness May Be at Odds with Accuracy",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "We show that there exists an inherent tension between the goal of adversarial robustness and that of standard generalization. Specifically, training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. We demonstrate that this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists even in a fairly simple and natural setting. These findings also corroborate a similar phenomenon observed in practice. Further, we argue that this phenomenon is a consequence of robust classifiers learning fundamentally different feature representations than standard classifiers. These differences, in particular, seem to result in unexpected benefits: the features learned by robust models tend to align better with salient data characteristics and human perception.",
  "stance": 0.0
 },
 {
  "url": "https://openreview.net/forum?id=r14EOsCqKX",
  "title": "A Closer Look at Deep Learning Heuristics: Learning rate restarts, Warmup and Distillation",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "The convergence rate and final performance of common deep learning models have significantly benefited from heuristics such as learning rate schedules, knowledge distillation, skip connections, and normalization layers. In the absence of theoretical underpinnings, controlled experiments aimed at explaining these strategies can aid our understanding of deep learning landscapes and the training dynamics. Existing approaches for empirical analysis rely on tools of linear interpolation and visualizations with dimensionality reduction, each with their limitations. Instead, we revisit such analysis of heuristics through the lens of recently proposed methods for loss surface and representation analysis, viz., mode connectivity and canonical correlation analysis (CCA), and hypothesize reasons for the success of the heuristics. In particular, we explore knowledge distillation and learning rate heuristics of (cosine) restarts and warmup using mode connectivity and CCA. Our empirical analysis suggests that: (a) the reasons often quoted for the success of cosine annealing are not evidenced in practice; (b) that the effect of learning rate warmup is to prevent the deeper layers from creating training instability; and (c) that the latent knowledge shared by the teacher is primarily disbursed to the deeper layers.",
  "stance": 0.4
 },
 {
  "url": "https://openreview.net/forum?id=r1efr3C9Ym",
  "title": "Interpolation-Prediction Networks for Irregularly Sampled Time Series",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "In this paper, we present a new deep learning architecture for addressing the problem of supervised learning with sparse and irregularly sampled multivariate time series. The architecture is based on the use of a semi-parametric interpolation network followed by the application of a prediction network. The interpolation network allows for information to be shared across multiple dimensions of a multivariate time series during the interpolation stage, while any standard deep learning model can be used for the prediction network. This work is motivated by the analysis of physiological time series data in electronic health records, which are sparse, irregularly sampled, and multivariate. We investigate the performance of this architecture on both classification and regression tasks, showing that our approach outperforms a range of baseline and recently proposed models.1",
  "stance": 0.8
 },
 {
  "url": "https://openreview.net/forum?id=r1lWUoA9FQ",
  "title": "Are adversarial examples inevitable?",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "A wide range of defenses have been proposed to harden neural networks against adversarial attacks. However, a pattern has emerged in which the majority of adversarial defenses are quickly broken by new attacks. Given the lack of success at generating robust defenses, we are led to ask a fundamental question: Are adversarial attacks inevitable? This paper analyzes adversarial examples from a theoretical perspective, and identifies fundamental bounds on the susceptibility of a classifier to adversarial attacks. We show that, for certain classes of problems, adversarial examples are inescapable. Using experiments, we explore the implications of theoretical guarantees for real-world problems and discuss how factors such as dimensionality and image complexity limit a classifier’s robustness against adversarial examples.",
  "stance": -1.0
 },
 {
  "url": "https://openreview.net/forum?id=r1lYRjC9F7",
  "title": "Enabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "Generating musical audio directly with neural networks is notoriously difficult because it requires coherently modeling structure at many different timescales. Fortunately, most music is also highly structured and can be represented as discrete note events played on musical instruments. Herein, we show that by using notes as an intermediate representation, we can train a suite of models capable of transcribing, composing, and synthesizing audio waveforms with coherent musical structure on timescales spanning six orders of magnitude (∼0.1 ms to ∼100 s), a process we call Wave2Midi2Wave. This large advance in the state of the art is enabled by our release of the new MAESTRO (MIDI and Audio Edited for Synchronous TRacks and Organization) dataset, composed of over 172 hours of virtuosic piano performances captured with fine alignment (≈3 ms) between note labels and audio waveforms. The networks and the dataset together present a promising approach toward creating new expressive and interpretable neural models of music.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=r1x4BnCqKX",
  "title": "A Generative Model For Electron Paths",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "Chemical reactions can be described as the stepwise redistribution of electrons in molecules. As such, reactions are often depicted using “arrow-pushing” diagrams which show this movement as a sequence of arrows. We propose an electron path prediction model (ELECTRO) to learn these sequences directly from raw reaction data. Instead of predicting product molecules directly from reactant molecules in one shot, learning a model of electron movement has the benefits of (a) being easy for chemists to interpret, (b) incorporating constraints of chemistry, such as balanced atom counts before and after the reaction, and (c) naturally encoding the sparsity of chemical reactions, which usually involve changes in only a small number of atoms in the reactants. We design a method to extract approximate reaction paths from any dataset of atom-mapped reaction SMILES strings. Our model achieves excellent performance on an important subset of the USPTO reaction dataset, comparing favorably to the strongest baselines. Furthermore, we show that our model recovers a basic knowledge of chemistry without being explicitly trained to do so.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=rJe10iC5K7",
  "title": "Unsupervised Discovery of Parts, Structure, and Dynamics",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "Humans easily recognize object parts and their hierarchical structure by watching how they move; they can then predict how each part moves in the future. In this paper, we propose a novel formulation that simultaneously learns a hierarchical, disentangled object representation and a dynamics model for object parts from unlabeled videos. Our Parts, Structure, and Dynamics (PSD) model learns to, first, recognize the object parts via a layered image representation; second, predict hierarchy via a structural descriptor that composes low-level concepts into a hierarchical structure; and third, model the system dynamics by predicting the future. Experiments on multiple real and synthetic datasets demonstrate that our PSD model works well on all three tasks: segmenting object parts, building their hierarchical structure, and capturing their motion distributions.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=rJfW5oA5KQ",
  "title": "Approximability of Discriminators Implies Diversity in GANs",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "While Generative Adversarial Networks (GANs) have empirically produced impressive results on learning complex real-world distributions, recent works have shown that they suffer from lack of diversity or mode collapse. The theoretical work of Arora et al. (2017a) suggests a dilemma about GANs’ statistical properties: powerful discriminators cause overfitting, whereas weak discriminators cannot detect mode collapse. By contrast, we show in this paper that GANs can in principle learn distributions in Wasserstein distance (or KL-divergence in many cases) with polynomial sample complexity, if the discriminator class has strong distinguishing power against the particular generator class (instead of against all possible generators). For various generator classes such as mixture of Gaussians, exponential families, and invertible and injective neural networks generators, we design corresponding discriminators (which are often neural nets of specific architectures) such that the Integral Probability Metric (IPM) induced by the discriminators can provably approximate the Wasserstein distance and/or KL-divergence. This implies that if the training is successful, then the learned distribution is close to the true distribution in Wasserstein distance or KL divergence, and thus cannot drop modes. Our preliminary experiments show that on synthetic datasets the test IPM is well correlated with KL divergence or the Wasserstein distance, indicating that the lack of diversity in GANs may be caused by the sub-optimality in optimization instead of statistical inefficiency.",
  "stance": 0.0
 },
 {
  "url": "https://openreview.net/forum?id=rJlnB3C5Ym",
  "title": "Rethinking the Value of Network Pruning",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "Network pruning is widely used for reducing the heavy inference cost of deep models in low-resource settings. A typical pruning algorithm is a three-stage pipeline, i.e., training (a large model), pruning and fine-tuning. During pruning, according to a certain criterion, redundant weights are pruned and important weights are kept to best preserve the accuracy. In this work, we make several surprising observations which contradict common beliefs. For all state-of-the-art structured pruning algorithms we examined, fine-tuning a pruned model only gives comparable or worse performance than training that model with randomly initialized weights. For pruning algorithms which assume a predefined target network architecture, one can get rid of the full pipeline and directly train the target network from scratch. Our observations are consistent for multiple network architectures, datasets, and tasks, which imply that: 1) training a large, over-parameterized model is often not necessary to obtain an efficient final model, 2) learned “important” weights of the large model are typically not useful for the small pruned model, 3) the pruned architecture itself, rather than a set of inherited “important” weights, is more crucial to the efficiency in the final model, which suggests that in some cases pruning can be useful as an architecture search paradigm. Our results suggest the need for more careful baseline evaluations in future research on structured pruning methods. We also compare with the “Lottery Ticket Hypothesis” (Frankle & Carbin, 2019), and find that with optimal learning rate, the “winning ticket” initialization as used in Frankle & Carbin (2019) does not bring improvement over random initialization.",
  "stance": -0.5
 },
 {
  "url": "https://openreview.net/forum?id=rkMW1hRqKX",
  "title": "Optimal Completion Distillation for Sequence Learning",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance. OCD is efficient, has no hyper-parameters of its own, and does not require pretraining or joint optimization with conditional log-likelihood. Given a partial sequence generated by the model, we first identify the set of optimal suffixes that minimize the total edit distance, using an efficient dynamic programming algorithm. Then, for each position of the generated sequence, we define a target distribution that puts an equal probability on the first token of each optimal suffix. OCD achieves the state-of-theart performance on end-to-end speech recognition, on both Wall Street Journal and Librispeech datasets, achieving 9.3% and 4.5% word error rates, respectively.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=rkxw-hAcFQ",
  "title": "Generating Multi-Agent Trajectories using Programmatic Weak Supervision",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "We study the problem of training sequential generative models for capturing coordinated multi-agent trajectory behavior, such as offensive basketball gameplay. When modeling such settings, it is often beneficial to design hierarchical models that can capture long-term coordination using intermediate variables. Furthermore, these intermediate variables should capture interesting high-level behavioral semantics in an interpretable and manipulatable way. We present a hierarchical framework that can effectively learn such sequential generative models. Our approach is inspired by recent work on leveraging programmatically produced weak labels, which we extend to the spatiotemporal regime. In addition to synthetic settings, we show how to instantiate our framework to effectively model complex interactions between basketball players and generate realistic multi-agent trajectories of basketball gameplay over long time periods. We validate our approach using both quantitative and qualitative evaluations, including a user study comparison conducted with professional sports analysts.1",
  "stance": 0.5
 },
 {
  "url": "https://openreview.net/forum?id=ryfMLoCqtQ",
  "title": "An analytic theory of generalization dynamics and transfer learning in deep linear networks",
  "year": 2019,
  "venue": "ICLR",
  "abstract": "Much attention has been devoted recently to the generalization puzzle in deep learning: large, deep networks can generalize well, but existing theories bounding generalization error are exceedingly loose, and thus cannot explain this striking performance. Furthermore, a major hope is that knowledge may transfer across tasks, so that multi-task learning can improve generalization on individual tasks. However we lack analytic theories that can quantitatively predict how the degree of knowledge transfer depends on the relationship between the tasks. We develop an analytic theory of the nonlinear dynamics of generalization in deep linear networks, both within and across tasks. In particular, our theory provides analytic solutions to the training and testing error of deep networks as a function of training time, number of examples, network size and initialization, and the task structure and SNR. Our theory reveals that deep networks progressively learn the most important task structure first, so that generalization error at the early stopping time primarily depends on task structure and is independent of network size. This suggests any tight bound on generalization error must take into account task structure, and explains observations about real data being learned faster than random data. Intriguingly our theory also reveals the existence of a learning algorithm that proveably out-performs neural network training through gradient descent. Finally, for transfer learning, our theory reveals that knowledge transfer depends sensitively, but computably, on the SNRs and input feature alignments of pairs of tasks.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=B1elCp4KwH",
  "title": "Learning Hierarchical Discrete Linguistic Units from Visually-Grounded Speech",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "In this paper, we present a method for learning discrete linguistic units by incorporating vector quantization layers into neural models of visually grounded speech. We show that our method is capable of capturing both word-level and sub-word units, depending on how it is configured. What differentiates this paper from prior work on speech unit learning is the choice of training objective. Rather than using a reconstruction-based loss, we use a discriminative, multimodal grounding objective which forces the learned units to be useful for semantic image retrieval. We evaluate the sub-word units on the ZeroSpeech 2019 challenge, achieving a 27.3% reduction in ABX error rate over the top-performing submission, while keeping the bitrate approximately the same. We also present experiments demonstrating the noise robustness of these units. Finally, we show that a model with multiple quantizers can simultaneously learn phone-like detectors at a lower layer and word-like detectors at a higher layer. We show that these detectors are highly accurate, discovering 279 words with an F1 score of greater than 0.5.",
  "stance": 0.7
 },
 {
  "url": "https://openreview.net/forum?id=B1g8VkHFPH",
  "title": "Rethinking the Hyperparameters for Fine-tuning",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Fine-tuning from pre-trained ImageNet models has become the de-facto standard for various computer vision tasks. Current practices for fine-tuning typically involve selecting an ad-hoc choice of hyperparameters and keeping them fixed to values normally used for training from scratch. This paper re-examines several common practices of setting hyperparameters for fine-tuning. Our findings are based on extensive empirical evaluation for fine-tuning on various transfer learning benchmarks. (1) While prior works have thoroughly investigated learning rate and batch size, momentum for fine-tuning is a relatively unexplored parameter. We find that the value of momentum also affects fine-tuning performance and connect it with previous theoretical findings. (2) Optimal hyperparameters for fine-tuning, in particular, the effective learning rate, are not only dataset dependent but also sensitive to the similarity between the source domain and target domain. This is in contrast to hyperparameters for training from scratch. (3) Reference-based regularization that keeps models close to the initial model does not necessarily apply for “dissimilar” datasets. Our findings challenge common practices of finetuning and encourages deep learning practitioners to rethink the hyperparameters for fine-tuning.",
  "stance": -0.5
 },
 {
  "url": "https://openreview.net/forum?id=B1l2bp4YwS",
  "title": "What graph neural networks cannot learn: depth vs width",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "This paper studies the expressive power of graph neural networks falling within the message-passing framework (GNNmp). Two results are presented. First, GNNmp are shown to be Turing universal under sufficient conditions on their depth, width, node attributes, and layer expressiveness. Second, it is discovered that GNNmp can lose a significant portion of their power when their depth and width is restricted. The proposed impossibility statements stem from a new technique that enables the repurposing of seminal results from distributed computing and leads to lower bounds for an array of decision, optimization, and estimation problems involving graphs. Strikingly, several of these problems are deemed impossible unless the product of a GNNmp’s depth and width exceeds a polynomial of the graph size; this dependence remains significant even for tasks that appear simple or when considering approximation.",
  "stance": -0.7
 },
 {
  "url": "https://openreview.net/forum?id=B1lDoJSYDH",
  "title": "Lagrangian Fluid Simulation with Continuous Convolutions",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We present an approach to Lagrangian fluid simulation with a new type of convolutional network. Our networks process sets of moving particles, which describe fluids in space and time. Unlike previous approaches, we do not build an explicit graph structure to connect the particles but use spatial convolutions as the main differentiable operation that relates particles to their neighbors. To this end we present a simple, novel, and effective extension of N-D convolutions to the continuous domain. We show that our network architecture can simulate different materials, generalizes to arbitrary collision geometries, and can be used for inverse problems. In addition, we demonstrate that our continuous convolutions outperform prior formulations in terms of accuracy and speed.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=B1lJzyStvS",
  "title": "Self-Supervised Learning of Appliance Usage",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Learning home appliance usage is important for understanding people’s activities and optimizing energy consumption. The problem is modeled as an event detection task, where the objective is to learn when a user turns an appliance on, and which appliance it is (microwave, hair dryer, etc.). Ideally, we would like to solve the problem in an unsupervised way so that the method can be applied to new homes and new appliances without any labels. To this end, we introduce a new deep learning model that takes input from two home sensors: 1) a smart electricity meter that outputs the total energy consumed by the home as a function of time, and 2) a motion sensor that outputs the locations of the residents over time. The model learns the distribution of the residents’ locations conditioned on the home energy signal. We show that this cross-modal prediction task allows us to detect when a particular appliance is used, and the location of the appliance in the home, all in a self-supervised manner, without any labeled data.",
  "stance": 0.7
 },
 {
  "url": "https://openreview.net/forum?id=B1xMEerYvB",
  "title": "Smooth markets: A basic mechanism for organizing gradient-based learners",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "With the success of modern machine learning, it is becoming increasingly important to understand and control how learning algorithms interact. Unfortunately, negative results from game theory show there is little hope of understanding or controlling general n-player games. We therefore introduce smooth markets (SM-games), a class of n-player games with pairwise zero sum interactions. SM-games codify a common design pattern in machine learning that includes (some) GANs, adversarial training, and other recent algorithms. We show that SM-games are amenable to analysis and optimization using first-order methods. “I began to see legibility as a central problem in modern statecraft. The premodern state was, in many respects, partially blind [. . .] It lacked anything like a detailed ‘map’ of its terrain and its people. It lacked, for the most part, a measure, a metric that would allow it to ‘translate’ what it knew into a common standard necessary for a synoptic view. As a result, its interventions were often crude and self-defeating.” – from Seeing like a State by Scott (1999)",
  "stance": -0.8
 },
 {
  "url": "https://openreview.net/forum?id=BJe-91BtvH",
  "title": "Masked Based Unsupervised Content Transfer",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We consider the problem of translating, in an unsupervised manner, between two domains where one contains some additional information compared to the other. The proposed method disentangles the common and separate parts of these domains and, through the generation of a mask, focuses the attention of the underlying network to the desired augmentation alone, without wastefully reconstructing the entire target. This enables state-of-the-art quality and variety of content translation, as demonstrated through extensive quantitative and qualitative evaluation. Our method is also capable of adding the separate content of different guide images and domains as well as remove existing separate content. Furthermore, our method enables weakly-supervised semantic segmentation of the separate part of each domain, where only class labels are provided. Our code is available at https: //github.com/rmokady/mbu-content-tansfer.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=BJeKwTNFvB",
  "title": "Physics-as-Inverse-Graphics: Unsupervised Physical Parameter Estimation from Video",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We propose a model that is able to perform unsupervised physical parameter estimation of systems from video, where the differential equations governing the scene dynamics are known, but labeled states or objects are not available. Existing physical scene understanding methods require either object state supervision, or do not integrate with differentiable physics to learn interpretable system parameters and states. We address this problem through a physics-as-inverse-graphics approach that brings together vision-as-inverse-graphics and differentiable physics engines, enabling objects and explicit state and velocity representations to be discovered. This framework allows us to perform long term extrapolative video prediction, as well as vision-based model-predictive control. Our approach significantly outperforms related unsupervised methods in long-term future frame prediction of systems with interacting objects (such as ball-spring or 3-body gravitational systems), due to its ability to build dynamics into the model as an inductive bias. We further show the value of this tight vision-physics integration by demonstrating data-efficient learning of vision-actuated model-based control for a pendulum system. We also show that the controller’s interpretability provides unique capabilities in goal-driven control and physical reasoning for zero-data adaptation.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=BJg866NFvB",
  "title": "Estimating counterfactual treatment outcomes over time through adversarially balanced representations",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Identifying when to give treatments to patients and how to select among multiple treatments over time are important medical problems with a few existing solutions. In this paper, we introduce the Counterfactual Recurrent Network (CRN), a novel sequence-to-sequence model that leverages the increasingly available patient observational data to estimate treatment effects over time and answer such medical questions. To handle the bias from time-varying confounders, covariates affecting the treatment assignment policy in the observational data, CRN uses domain adversarial training to build balancing representations of the patient history. At each timestep, CRN constructs a treatment invariant representation which removes the association between patient history and treatment assignments and thus can be reliably used for making counterfactual predictions. On a simulated model of tumour growth, with varying degree of time-dependent confounding, we show how our model achieves lower error in estimating counterfactuals and in choosing the correct treatment and timing of treatment than current state-of-the-art methods.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=BJgZGeHFPH",
  "title": "Dynamics-Aware Embeddings",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "In this paper we consider self-supervised representation learning to improve sample efficiency in reinforcement learning (RL). We propose a forward prediction objective for simultaneously learning embeddings of states and action sequences. These embeddings capture the structure of the environment’s dynamics, enabling efficient policy learning. We demonstrate that our action embeddings alone improve the sample efficiency and peak performance of model-free RL on control from low-dimensional states. By combining state and action embeddings, we achieve efficient learning of high-quality policies on goal-conditioned continuous control from pixel observations in only 1-2 million environment steps.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=BJgqQ6NYvB",
  "title": "FasterSeg: Searching for Faster Real-time Semantic Segmentation",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We present FasterSeg, an automatically designed semantic segmentation network with not only state-of-the-art performance but also faster speed than current methods. Utilizing neural architecture search (NAS), FasterSeg is discovered from a novel and broader search space integrating multi-resolution branches, that has been recently found to be vital in manually designed segmentation models. To better calibrate the balance between the goals of high accuracy and low latency, we propose a decoupled and fine-grained latency regularization, that effectively overcomes our observed phenomenons that the searched networks are prone to “collapsing” to low-latency yet poor-accuracy models. Moreover, we seamlessly extend FasterSeg to a new collaborative search (co-searching) framework, simultaneously searching for a teacher and a student network in the same single run. The teacher-student distillation further boosts the student model’s accuracy. Experiments on popular segmentation benchmarks demonstrate the competency of FasterSeg. For example, FasterSeg can run over 30% faster than the closest manually designed competitor on Cityscapes, while maintaining comparable accuracy.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=BJlZ5ySKPH",
  "title": "U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based method which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new AdaLIN (Adaptive Layer-Instance Normalization) function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a fixed network architecture and hyper-parameters. Our code and datasets are available at https://github.com/taki0112/UGATIT or https://github.com/znxlwm/UGATITpytorch.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=Bke89JBtvB",
  "title": "Batch-shaping for learning conditional channel gated networks",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We present a method that trains large capacity neural networks with significantly improved accuracy and lower dynamic computational cost. We achieve this by gating the deep-learning architecture on a fine-grained-level. Individual convolutional maps are turned on/off conditionally on features in the network. To achieve this, we introduce a new residual block architecture that gates convolutional channels in a fine-grained manner. We also introduce a generally applicable tool batch-shaping that matches the marginal aggregate posteriors of features in a neural network to a pre-specified prior distribution. We use this novel technique to force gates to be more conditional on the data. We present results on CIFAR-10 and ImageNet datasets for image classification, and Cityscapes for semantic segmentation. Our results show that our method can slim down large architectures conditionally, such that the average computational cost on the data is on par with a smaller architecture, but with higher accuracy. In particular, on ImageNet, our ResNet50 and ResNet34 gated networks obtain 74.60% and 72.55% top-1 accuracy compared to the 69.76% accuracy of the baseline ResNet18 model, for similar complexity. We also show that the resulting networks automatically learn to use more features for difficult examples and fewer features for simple examples.",
  "stance": 0.8
 },
 {
  "url": "https://openreview.net/forum?id=BkgXT24tDS",
  "title": "Additive Powers-of-Two Quantization: An Efficient Non-uniform Discretization for Neural Networks",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We propose Additive Powers-of-Two (APoT) quantization, an efficient nonuniform quantization scheme for the bell-shaped and long-tailed distribution of weights and activations in neural networks. By constraining all quantization levels as the sum of Powers-of-Two terms, APoT quantization enjoys high computational efficiency and a good match with the distribution of weights. A simple reparameterization of the clipping function is applied to generate a better-defined gradient for learning the clipping threshold. Moreover, weight normalization is presented to refine the distribution of weights to make the training more stable and consistent. Experimental results show that our proposed method outperforms state-of-the-art methods, and is even competitive with the full-precision models, demonstrating the effectiveness of our proposed APoT quantization. For example, our 4-bit quantized ResNet-50 on ImageNet achieves 76.6% top-1 accuracy without bells and whistles; meanwhile, our model is capable to decrease 22% computational cost compared with the uniformly quantized counterpart. 1",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=BkxpMTEtPB",
  "title": "GLAD: Learning Sparse Graph Recovery",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Recovering sparse conditional independence graphs from data is a fundamental problem in machine learning with wide applications. A popular formulation of the problem is an `1 regularized maximum likelihood estimation. Many convex optimization algorithms have been designed to solve this formulation to recover the graph structure. Recently, there is a surge of interest to learn algorithms directly based on data, and in this case, learn to map empirical covariance to the sparse precision matrix. However, it is a challenging task in this case, since the symmetric positive definiteness (SPD) and sparsity of the matrix are not easy to enforce in learned algorithms, and a direct mapping from data to precision matrix may contain many parameters. We propose a deep learning architecture, GLAD, which uses an Alternating Minimization (AM) algorithm as our model inductive bias, and learns the model parameters via supervised learning. We show that GLAD learns a very compact and effective model for recovering sparse graphs from data.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=Byg1v1HKDB",
  "title": "Abductive Commonsense Reasoning",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Abductive reasoning is inference to the most plausible explanation. For example, if Jenny finds her house in a mess when she returns from work, and remembers that she left a window open, she can hypothesize that a thief broke into her house and caused the mess, as the most plausible explanation. While abduction has long been considered to be at the core of how people interpret and read between the lines in natural language (Hobbs et al., 1988), there has been relatively little research in support of abductive natural language inference and generation. We present the first study that investigates the viability of language-based abductive reasoning. We introduce a challenge dataset, ART, that consists of over 20k commonsense narrative contexts and 200k explanations. Based on this dataset, we conceptualize two new tasks – (i) Abductive NLI: a multiple-choice question answering task for choosing the more likely explanation, and (ii) Abductive NLG: a conditional generation task for explaining given observations in natural language. On Abductive NLI, the best model achieves 68.9% accuracy, well below human performance of 91.4%. On Abductive NLG, the current best language generators struggle even more, as they lack reasoning capabilities that are trivial for humans. Our analysis leads to new insights into the types of reasoning that deep pre-trained language models fail to perform—despite their strong performance on the related but more narrowly defined task of entailment NLI—pointing to interesting avenues for future research.",
  "stance": -1.0
 },
 {
  "url": "https://openreview.net/forum?id=Byl8hhNYPS",
  "title": "Neural Machine Translation with Universal Visual Representation",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Though visual information has been introduced for enhancing neural machine translation (NMT), its effectiveness strongly relies on the availability of large amounts of bilingual parallel sentence pairs with manual image annotations. In this paper, we present a universal visual representation learned over the monolingual corpora with image annotations, which overcomes the lack of largescale bilingual sentence-image pairs, thereby extending image applicability in NMT. In detail, a group of images with similar topics to the source sentence will be retrieved from a light topic-image lookup table learned over the existing sentence-image pairs, and then is encoded as image representations by a pretrained ResNet. An attention layer with a gated weighting is to fuse the visual information and text information as input to the decoder for predicting target translations. In particular, the proposed method enables the visual information to be integrated into large-scale text-only NMT in addition to the multimodal NMT. Experiments on four widely used translation datasets, including the WMT’16 English-to-Romanian, WMT’14 English-to-German, WMT’14 Englishto-French, and Multi30K, show that the proposed approach achieves significant improvements over strong baselines.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=ByxxgCEYDS",
  "title": "Inductive Matrix Completion Based on Graph Neural Networks",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We propose an inductive matrix completion model without using side information. By factorizing the (rating) matrix into the product of low-dimensional latent embeddings of rows (users) and columns (items), a majority of existing matrix completion methods are transductive, since the learned embeddings cannot generalize to unseen rows/columns or to new matrices. To make matrix completion inductive, most previous works use content (side information), such as user’s age or movie’s genre, to make predictions. However, high-quality content is not always available, and can be hard to extract. Under the extreme setting where not any side information is available other than the matrix to complete, can we still learn an inductive matrix completion model? In this paper, we propose an Inductive Graph-based Matrix Completion (IGMC) model to address this problem. IGMC trains a graph neural network (GNN) based purely on 1-hop subgraphs around (user, item) pairs generated from the rating matrix and maps these subgraphs to their corresponding ratings. It achieves highly competitive performance with state-of-the-art transductive baselines. In addition, IGMC is inductive – it can generalize to users/items unseen during the training (given that their interactions exist), and can even transfer to new tasks. Our transfer learning experiments show that a model trained out of the MovieLens dataset can be directly used to predict Douban movie ratings with surprisingly good performance. Our work demonstrates that: 1) it is possible to train inductive matrix completion models without using side information while achieving similar or better performances than state-of-the-art transductive methods; 2) local graph patterns around a (user, item) pair are effective predictors of the rating this user gives to the item; and 3) Long-range dependencies might not be necessary for modeling recommender systems.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=H1eCw3EKvH",
  "title": "On the Weaknesses of Reinforcement Learning for Neural Machine Translation",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Reinforcement learning (RL) is frequently used to increase performance in text generation tasks, including machine translation (MT), notably through the use of Minimum Risk Training (MRT) and Generative Adversarial Networks (GAN). However, little is known about what and how these methods learn in the context of MT. We prove that one of the most common RL methods for MT does not optimize the expected reward, as well as show that other methods take an infeasibly long time to converge. In fact, our results suggest that RL practices in MT are likely to improve performance only where the pre-trained parameters are already close to yielding the correct translation. Our findings further suggest that observed gains may be due to effects unrelated to the training signal, concretely, changes in the shape of the distribution curve.",
  "stance": -0.5
 },
 {
  "url": "https://openreview.net/forum?id=H1eqQeHFDS",
  "title": "AdvectiveNet: An Eulerian-Lagrangian Fluidic Reservoir for Point Cloud Processing",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "This paper presents a novel physics-inspired deep learning approach for point cloud processing motivated by the natural flow phenomena in fluid mechanics. Our learning architecture jointly defines data in an Eulerian world space, using a static background grid, and a Lagrangian material space, using moving particles. By introducing this Eulerian-Lagrangian representation, we are able to naturally evolve and accumulate particle features using flow velocities generated from a generalized, high-dimensional force field. We demonstrate the efficacy of this system by solving various point cloud classification and segmentation problems with state-of-the-art performance. The entire geometric reservoir and data flow mimics the pipeline of the classic PIC/FLIP scheme in modeling natural flow, bridging the disciplines of geometric machine learning and physical simulation.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=H1ezFREtwH",
  "title": "Composing Task-Agnostic Policies with Deep Reinforcement Learning",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "The composition of elementary behaviors to solve challenging transfer learning problems is one of the key elements in building intelligent machines. To date, there has been plenty of work on learning task-specific policies or skills but almost no focus on composing necessary, task-agnostic skills to find a solution to new problems. In this paper, we propose a novel deep reinforcement learning-based skill transfer and composition method that takes the agent’s primitive policies to solve unseen tasks. We evaluate our method in difficult cases where training policy through standard reinforcement learning (RL) or even hierarchical RL is either not feasible or exhibits high sample complexity. We show that our method not only transfers skills to new problem settings but also solves the challenging environments requiring both task planning and motion control with high data efficiency.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=H1gBsgBYwH",
  "title": "Generalization of Two-layer Neural Networks: An Asymptotic Viewpoint",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "This paper investigates the generalization properties of two-layer neural networks in high-dimensions, i.e. when the number of samples n, features d, and neurons h tend to infinity at the same rate. Specifically, we derive the exact population risk of the unregularized least squares regression problem with two-layer neural networks when either the first or the second layer is trained using a gradient flow under different initialization setups. When only the second layer coefficients are optimized, we recover the double descent phenomenon: a cusp in the population risk appears at h ≈ n and further overparameterization decreases the risk. In contrast, when the first layer weights are optimized, we highlight how different scales of initialization lead to different inductive bias, and show that the resulting risk is independent of overparameterization. Our theoretical and experimental results suggest that previously studied model setups that provably give rise to double descent might not translate to optimizing two-layer neural networks.",
  "stance": -0.5
 },
 {
  "url": "https://openreview.net/forum?id=H1gmHaEKwB",
  "title": "Data-Independent Neural Pruning via Coresets",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Previous work showed empirically that large neural networks can be significantly reduced in size while preserving their accuracy. Model compression became a central research topic, as it is crucial for deployment of neural networks on devices with limited computational and memory resources. The majority of the compression methods are based on heuristics and offer no worst-case guarantees on the trade-off between the compression rate and the approximation error for an arbitrarily new sample. We propose the first efficient, data-independent neural pruning algorithm with a provable trade-off between its compression rate and the approximation error for any future test sample. Our method is based on the coreset framework, which finds a small weighted subset of points that provably approximates the original inputs. Specifically, we approximate the output of a layer of neurons by a coreset of neurons in the previous layer and discard the rest. We apply this framework in a layer-by-layer fashion from the top to the bottom. Unlike previous works, our coreset is data independent, meaning that it provably guarantees the accuracy of the function for any input x ∈ R, including an adversarial one. We demonstrate the effectiveness of our method on popular network architectures. In particular, our coresets yield 90% compression of the LeNet-300-100 architecture on MNIST while improving classification accuracy.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=H1gzR2VKDH",
  "title": "Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Video prediction models combined with planning algorithms have shown promise in enabling robots to learn to perform many vision-based tasks through only selfsupervision, reaching novel goals in cluttered scenes with unseen objects. However, due to the compounding uncertainty in long horizon video prediction and poor scalability of sampling-based planning optimizers, one significant limitation of these approaches is the ability to plan over long horizons to reach distant goals. To that end, we propose a framework for subgoal generation and planning, hierarchical visual foresight (HVF), which generates subgoal images conditioned on a goal image, and uses them for planning. The subgoal images are directly optimized to decompose the task into easy to plan segments, and as a result, we observe that the method naturally identifies semantically meaningful states as subgoals. Across four simulated vision-based manipulation tasks, we find that our method achieves more than 20% absolute performance improvement over planning without subgoals and model-free RL approaches. Further, our experiments illustrate that our approach extends to real, cluttered visual scenes.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=HJgLZR4KvH",
  "title": "Dynamics-Aware Unsupervised Discovery of Skills",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Conventionally, model-based reinforcement learning (MBRL) aims to learn a global model for the dynamics of the environment. A good model can potentially enable planning algorithms to generate a large variety of behaviors and solve diverse tasks. However, learning an accurate model for complex dynamical systems is difficult, and even then, the model might not generalize well outside the distribution of states on which it was trained. In this work, we combine model-based learning with model-free learning of primitives that make modelbased planning easy. To that end, we aim to answer the question: how can we discover skills whose outcomes are easy to predict? We propose an unsupervised learning algorithm, Dynamics-Aware Discovery of Skills (DADS), which simultaneously discovers predictable behaviors and learns their dynamics. Our method can leverage continuous skill spaces, theoretically, allowing us to learn infinitely many behaviors even for high-dimensional state-spaces. We demonstrate that zero-shot planning in the learned latent space significantly outperforms standard MBRL and model-free goal-conditioned RL, can handle sparsereward tasks, and substantially improves over prior hierarchical RL methods for unsupervised skill discovery. We have open-sourced our implementation at: https://github.com/google-research/dads Figure 1: A humanoid agent discovers diverse locomotion primitives without any reward using DADS. We show zero-shot generalization to downstream tasks by composing the learned primitives using model predictive control, enabling the agent to follow an online sequence of goals (green markers) without any additional training.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=HkgxW0EYDS",
  "title": "Scalable Model Compression by Entropy Penalized Reparameterization",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We describe a simple and general neural network weight compression approach, in which the network parameters (weights and biases) are represented in a “latent” space, amounting to a reparameterization. This space is equipped with a learned probability model, which is used to impose an entropy penalty on the parameter representation during training, and to compress the representation using a simple arithmetic coder after training. Classification accuracy and model compressibility is maximized jointly, with the bitrate–accuracy trade-off specified by a hyperparameter. We evaluate the method on the MNIST, CIFAR-10 and ImageNet classification benchmarks using six distinct model architectures. Our results show that state-of-the-art model compression can be achieved in a scalable and general way without requiring complex procedures such as multi-stage training.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=Hkl1iRNFwS",
  "title": "The Early Phase of Neural Network Training",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Recent studies have shown that many important aspects of neural network learning take place within the very earliest iterations or epochs of training. For example, sparse, trainable sub-networks emerge (Frankle et al., 2019), gradient descent moves into a small subspace (Gur-Ari et al., 2018), and the network undergoes a critical period (Achille et al., 2019). Here we examine the changes that deep neural networks undergo during this early phase of training. We perform extensive measurements of the network state during these early iterations of training and leverage the framework of Frankle et al. (2019) to quantitatively probe the weight distribution and its reliance on various aspects of the dataset. We find that, within this framework, deep networks are not robust to reinitializing with random weights while maintaining signs, and that weight distributions are highly non-independent even after only a few hundred iterations. Despite this behavior, pre-training with blurred inputs or an auxiliary self-supervised task can approximate the changes in supervised networks, suggesting that these changes are not inherently label-dependent, though labels significantly accelerate this process. Together, these results help to elucidate the network changes occurring during this pivotal initial period of learning.",
  "stance": -0.5
 },
 {
  "url": "https://openreview.net/forum?id=Hyg9anEFPS",
  "title": "Image-guided Neural Object Rendering",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We propose a learned image-guided rendering technique that combines the benefits of image-based rendering and GAN-based image synthesis. The goal of our method is to generate photo-realistic re-renderings of reconstructed objects for virtual and augmented reality applications (e.g., virtual showrooms, virtual tours & sightseeing, the digital inspection of historical artifacts). A core component of our work is the handling of view-dependent effects. Specifically, we directly train an object-specific deep neural network to synthesize the view-dependent appearance of an object. As input data we are using an RGB video of the object. This video is used to reconstruct a proxy geometry of the object via multi-view stereo. Based on this 3D proxy, the appearance of a captured view can be warped into a new target view as in classical image-based rendering. This warping assumes diffuse surfaces, in case of view-dependent effects, such as specular highlights, it leads to artifacts. To this end, we propose EffectsNet, a deep neural network that predicts view-dependent effects. Based on these estimations, we are able to convert observed images to diffuse images. These diffuse images can be projected into other views. In the target view, our pipeline reinserts the new view-dependent effects. To composite multiple reprojected images to a final output, we learn a composition network that outputs photo-realistic results. Using this image-guided approach, the network does not have to allocate capacity on “remembering” object appearance, instead it learns how to combine the appearance of captured images. We demonstrate the effectiveness of our approach both qualitatively and quantitatively on synthetic as well as on real data.",
  "stance": 0.7
 },
 {
  "url": "https://openreview.net/forum?id=HygrdpVKvr",
  "title": "NAS evaluation is frustratingly hard",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Neural Architecture Search (NAS) is an exciting new field which promises to be as much as a game-changer as Convolutional Neural Networks were in 2012. Despite many great works leading to substantial improvements on a variety of tasks, comparison between different methods is still very much an open issue. While most algorithms are tested on the same datasets, there is no shared experimental protocol followed by all. As such, and due to the under-use of ablation studies, there is a lack of clarity regarding why certain methods are more effective than others. Our first contribution is a benchmark of 8 NAS methods on 5 datasets. To overcome the hurdle of comparing methods with different search spaces, we propose using a method’s relative improvement over the randomly sampled average architecture, which effectively removes advantages arising from expertly engineered search spaces or training protocols. Surprisingly, we find that many NAS techniques struggle to significantly beat the average architecture baseline. We perform further experiments with the commonly used DARTS search space in order to understand the contribution of each component in the NAS pipeline. These experiments highlight that: (i) the use of tricks in the evaluation protocol has a predominant impact on the reported performance of architectures; (ii) the cell-based search space has a very narrow accuracy range, such that the seed has a considerable impact on architecture rankings; (iii) the hand-designed macrostructure (cells) is more important than the searched micro-structure (operations); and (iv) the depth-gap is a real phenomenon, evidenced by the change in rankings between 8 and 20 cell architectures. To conclude, we suggest best practices, that we hope will prove useful for the community and help mitigate current NAS pitfalls, e.g. difficulties in reproducibility and comparison of search methods. The code used is available at https://github.com/antoyang/NAS-Benchmark.",
  "stance": -0.8
 },
 {
  "url": "https://openreview.net/forum?id=HylsTT4FvB",
  "title": "On the \"steerability\" of generative adversarial networks",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "An open secret in contemporary machine learning is that many models work beautifully on standard benchmarks but fail to generalize outside the lab. This has been attributed to biased training data, which provide poor coverage over real world events. Generative models are no exception, but recent advances in generative adversarial networks (GANs) suggest otherwise – these models can now synthesize strikingly realistic and diverse images. Is generative modeling of photos a solved problem? We show that although current GANs can fit standard datasets very well, they still fall short of being comprehensive models of the visual manifold. In particular, we study their ability to fit simple transformations such as camera movements and color changes. We find that the models reflect the biases of the datasets on which they are trained (e.g., centered objects), but that they also exhibit some capacity for generalization: by “steering” in latent space, we can shift the distribution while still creating realistic images. We hypothesize that the degree of distributional shift is related to the breadth of the training data distribution. Thus, we conduct experiments to quantify the limits of GAN transformations and introduce techniques to mitigate the problem. Code is released on our project page: https://ali-design.github.io/gan_steerability/.",
  "stance": 0.1
 },
 {
  "url": "https://openreview.net/forum?id=HyxjNyrtPr",
  "title": "RGBD-GAN: Unsupervised 3D Representation Learning From Natural Image Datasets via RGBD Image Synthesis",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Understanding three-dimensional (3D) geometries from two-dimensional (2D) images without any labeled information is promising for understanding the real world without incurring annotation cost. We herein propose a novel generative model, RGBD-GAN, which achieves unsupervised 3D representation learning from 2D images. The proposed method enables camera parameter–conditional image generation and depth image generation without any 3D annotations, such as camera poses or depth. We use an explicit 3D consistency loss for two RGBD images generated from different camera parameters, in addition to the ordinal GAN objective. The loss is simple yet effective for any type of image generator such as DCGAN and StyleGAN to be conditioned on camera parameters. Through experiments, we demonstrated that the proposed method could learn 3D representations from 2D images with various generator architectures.",
  "stance": 0.9
 },
 {
  "url": "https://openreview.net/forum?id=HyxyIgHFvr",
  "title": "Truth or backpropaganda? An empirical investigation of deep learning theory",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike. In this work, we: (1) prove the widespread existence of suboptimal local minima in the loss landscape of neural networks, and we use our theory to find examples; (2) show that small-norm parameters are not optimal for generalization; (3) demonstrate that ResNets do not conform to wide-network theories, such as the neural tangent kernel, and that the interaction between skip connections and batch normalization plays a role; (4) find that rank does not correlate with generalization or robustness in a practical setting.",
  "stance": -0.5
 },
 {
  "url": "https://openreview.net/forum?id=S1xnXRVFwH",
  "title": "Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "The lottery ticket hypothesis proposes that over-parameterization of deep neural networks (DNNs) aids training by increasing the probability of a “lucky” sub-network initialization being present rather than by helping the optimization process (Frankle & Carbin, 2019). Intriguingly, this phenomenon suggests that initialization strategies for DNNs can be improved substantially, but the lottery ticket hypothesis has only previously been tested in the context of supervised learning for natural image tasks. Here, we evaluate whether “winning ticket” initializations exist in two different domains: natural language processing (NLP) and reinforcement learning (RL). For NLP, we examined both recurrent LSTM models and large-scale Transformer models (Vaswani et al., 2017). For RL, we analyzed a number of discrete-action space tasks, including both classic control and pixel control. Consistent with work in supervised image classification, we confirm that winning ticket initializations generally outperform parameter-matched random initializations, even at extreme pruning rates for both NLP and RL. Notably, we are able to find winning ticket initializations for Transformers which enable models one-third the size to achieve nearly equivalent performance. Together, these results suggest that the lottery ticket hypothesis is not restricted to supervised learning of natural images, but rather represents a broader phenomenon in DNNs.",
  "stance": 0.3
 },
 {
  "url": "https://openreview.net/forum?id=SJgIPJBFvH",
  "title": "Fantastic Generalization Measures and Where to Find Them",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Generalization of deep networks has lately been of great interest, resulting in a number of theoretically and empirically motivated complexity measures. However, most papers proposing such measures study only a small set of models, leaving open the question of whether the conclusion drawn from those experiments would remain valid in other settings. We present the first large scale study of generalization in deep networks. We investigate more then 40 complexity measures taken from both theoretical bounds and empirical studies. We train over 10,000 convolutional networks by systematically varying commonly used hyperparameters. Hoping to uncover potentially causal relationships between each measure and generalization, we analyze carefully controlled experiments and show surprising failures of some measures as well as promising measures for further research.",
  "stance": 0.0
 },
 {
  "url": "https://openreview.net/forum?id=SJxSOJStPr",
  "title": "A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Despite the growing interest in continual learning, most of its contemporary works have been studied in a rather restricted setting where tasks are clearly distinguishable, and task boundaries are known during training. However, if our goal is to develop an algorithm that learns as humans do, this setting is far from realistic, and it is essential to develop a methodology that works in a task-free manner. Meanwhile, among several branches of continual learning, expansion-based methods have the advantage of eliminating catastrophic forgetting by allocating new resources to learn new data. In this work, we propose an expansion-based approach for task-free continual learning. Our model, named Continual Neural Dirichlet Process Mixture (CN-DPM), consists of a set of neural network experts that are in charge of a subset of the data. CN-DPM expands the number of experts in a principled way under the Bayesian nonparametric framework. With extensive experiments, we show that our model successfully performs task-free continual learning for both discriminative and generative tasks such as image classification and image generation.",
  "stance": 0.9
 },
 {
  "url": "https://openreview.net/forum?id=SJxhNTNYwB",
  "title": "Black-Box Adversarial Attack with Transferable Model-based Embedding",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We present a new method for black-box adversarial attack. Unlike previous methods that combined transfer-based and scored-based methods by using the gradient or initialization of a surrogate white-box model, this new method tries to learn a low-dimensional embedding using a pretrained model, and then performs efficient search within the embedding space to attack an unknown target network. The method produces adversarial perturbations with high level semantic patterns that are easily transferable. We show that this approach can greatly improve the query efficiency of black-box adversarial attack across different target network architectures. We evaluate our approach on MNIST, ImageNet and Google Cloud Vision API, resulting in a significant reduction on the number of queries. We also attack adversarially defended networks on CIFAR10 and ImageNet, where our method not only reduces the number of queries, but also improves the attack success rate.",
  "stance": 0.7
 },
 {
  "url": "https://openreview.net/forum?id=SkgGCkrKvH",
  "title": "Decentralized Deep Learning with Arbitrary Communication Compression",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Decentralized training of deep learning models is a key element for enabling data privacy and on-device learning over networks, as well as for efficient scaling to large compute clusters. As current approaches are limited by network bandwidth, we propose the use of communication compression in the decentralized training context. We show that CHOCO-SGD achieves linear speedup in the number of workers for arbitrary high compression ratios on general non-convex functions, and non-IID training data. We demonstrate the practical performance of the algorithm in two key scenarios: the training of deep learning models (i) over decentralized user devices, connected by a peer-to-peer network and (ii) in a datacenter.",
  "stance": 0.5
 },
 {
  "url": "https://openreview.net/forum?id=Sklgs0NFvr",
  "title": "Learning The Difference That Makes A Difference With Counterfactually-Augmented Data",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Despite alarm over the reliance of machine learning systems on so-called spurious patterns, the term lacks coherent meaning in standard statistical frameworks. However, the language of causality offers clarity: spurious associations are due to confounding (e.g., a common cause), but not direct or indirect causal effects. In this paper, we focus on natural language processing, introducing methods and resources for training models less sensitive to spurious patterns. Given documents and their initial labels, we task humans with revising each document so that it (i) accords with a counterfactual target label; (ii) retains internal coherence; and (iii) avoids unnecessary changes. Interestingly, on sentiment analysis and natural language inference tasks, classifiers trained on original data fail on their counterfactually-revised counterparts and vice versa. Classifiers trained on combined datasets perform remarkably well, just shy of those specialized to either domain. While classifiers trained on either original or manipulated data alone are sensitive to spurious features (e.g., mentions of genre), models trained on the combined data are less sensitive to this signal. Both datasets are publicly available1.",
  "stance": 0.7
 },
 {
  "url": "https://openreview.net/forum?id=SylzhkBtDB",
  "title": "Understanding and Improving Information Transfer in Multi-Task Learning",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We investigate multi-task learning approaches that use a shared feature representation for all tasks. To better understand the transfer of task information, we study an architecture with a shared module for all tasks and a separate output module for each task. We study the theory of this setting on linear and ReLU-activated models. Our key observation is that whether or not tasks’ data are well-aligned can significantly affect the performance of multi-task learning. We show that misalignment between task data can cause negative transfer (or hurt performance) and provide sufficient conditions for positive transfer. Inspired by the theoretical insights, we show that aligning tasks’ embedding layers leads to performance gains for multi-task training and transfer learning on the GLUE benchmark and sentiment analysis tasks; for example, we obtain a 2.35% GLUE score average improvement on 5 GLUE tasks over BERTLARGE using our alignment method. We also design an SVD-based task reweighting scheme and show that it improves the robustness of multi-task training on a multi-label image dataset.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=r1etN1rtPB",
  "title": "Implementation Matters in Deep RL: A Case Study on PPO and TRPO",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We study the roots of algorithmic progress in deep policy gradient algorithms through a case study on two popular algorithms: Proximal Policy Optimization (PPO) and Trust Region Policy Optimization (TRPO). Specifically, we investigate the consequences of “code-level optimizations:” algorithm augmentations found only in implementations or described as auxiliary details to the core algorithm. Seemingly of secondary importance, such optimizations turn out to have a major impact on agent behavior. Our results show that they (a) are responsible for most of PPO’s gain in cumulative reward over TRPO, and (b) fundamentally change how RL methods function. These insights show the difficulty, and importance, of attributing performance gains in deep reinforcement learning.",
  "stance": -0.2
 },
 {
  "url": "https://openreview.net/forum?id=r1evOhEKvH",
  "title": "Graph inference learning for semi-supervised classification",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "In this work, we address semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem via advanced graph convolution in a conventionally supervised manner, but the performance could degrade significantly when labeled data is scarce. To this end, we propose a Graph Inference Learning (GIL) framework to boost the performance of semisupervised node classification by learning the inference of node labels on graph topology. To bridge the connection between two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths, and local topological structures together, which can make the inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted to testing nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed, and NELL) demonstrate the superiority of our proposed GIL when compared against state-of-the-art methods on the semi-supervised node classification task.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=r1genAVKPB",
  "title": "Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Modern deep learning methods provide effective means to learn good representations. However, is a good representation itself sufficient for sample efficient reinforcement learning? This question has largely been studied only with respect to (worst-case) approximation error, in the more classical approximate dynamic programming literature. With regards to the statistical viewpoint, this question is largely unexplored, and the extant body of literature mainly focuses on conditions which permit sample efficient reinforcement learning with little understanding of what are necessary conditions for efficient reinforcement learning. This work shows that, from the statistical viewpoint, the situation is far subtler than suggested by the more traditional approximation viewpoint, where the requirements on the representation that suffice for sample efficient RL are even more stringent. Our main results provide sharp thresholds for reinforcement learning methods, showing that there are hard limitations on what constitutes good function approximation (in terms of the dimensionality of the representation), where we focus on natural representational conditions relevant to value-based, model-based, and policy-based learning. These lower bounds highlight that having a good (valuebased, model-based, or policy-based) representation in and of itself is insufficient for efficient reinforcement learning, unless the quality of this approximation passes certain hard thresholds. Furthermore, our lower bounds also imply exponential separations on the sample complexity between 1) value-based learning with perfect representation and value-based learning with a good-but-not-perfect representation, 2) value-based learning and policy-based learning, 3) policy-based learning and supervised learning and 4) reinforcement learning and imitation learning.",
  "stance": -0.5
 },
 {
  "url": "https://openreview.net/forum?id=r1lF_CEYwS",
  "title": "On the Need for Topology-Aware Generative Models for Manifold-Based Defenses",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Machine-learning (ML) algorithms or models, especially deep neural networks (DNNs), have shown significant promise in several areas. However, researchers have recently demonstrated that ML algorithms, especially DNNs, are vulnerable to adversarial examples (slightly perturbed samples that cause misclassification). The existence of adversarial examples has hindered the deployment of ML algorithms in safety-critical sectors, such as security. Several defenses for adversarial examples exist in the literature. One of the important classes of defenses are manifoldbased defenses, where a sample is “pulled back” into the data manifold before classifying. These defenses rely on the assumption that data lie in a manifold of a lower dimension than the input space. These defenses use a generative model to approximate the input distribution. In this paper, we investigate the following question: do the generative models used in manifold-based defenses need to be topology-aware? We suggest the answer is yes, and we provide theoretical and empirical evidence to support our claim.",
  "stance": 0.0
 },
 {
  "url": "https://openreview.net/forum?id=r1lPleBFvH",
  "title": "Understanding the Limitations of Conditional Generative Models",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Class-conditional generative models hold promise to overcome the shortcomings of their discriminative counterparts. They are a natural choice to solve discriminative tasks in a robust manner as they jointly optimize for predictive performance and accurate modeling of the input distribution. In this work, we investigate robust classification with likelihood-based generative models from a theoretical and practical perspective to investigate if they can deliver on their promises. Our analysis focuses on a spectrum of robustness properties: (1) Detection of worst-case outliers in the form of adversarial examples; (2) Detection of average-case outliers in the form of ambiguous inputs and (3) Detection of incorrectly labeled in-distribution inputs. Our theoretical result reveals that it is impossible to guarantee detectability of adversarially-perturbed inputs even for near-optimal generative classifiers. Experimentally, we find that while we are able to train robust models for MNIST, robustness completely breaks down on CIFAR10. We relate this failure to various undesirable model properties that can be traced to the maximum likelihood training objective. Despite being a common choice in the literature, our results indicate that likelihood-based conditional generative models may are surprisingly ineffective for robust classification.",
  "stance": -0.5
 },
 {
  "url": "https://openreview.net/forum?id=rJe4_xSFDB",
  "title": "Lipschitz constant estimation of Neural Networks via sparse polynomial optimization",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We introduce LiPopt, a polynomial optimization framework for computing increasingly tighter upper bounds on the Lipschitz constant of neural networks. The underlying optimization problems boil down to either linear (LP) or semidefinite (SDP) programming. We show how to use the sparse connectivity of a network, to significantly reduce the complexity of computation. This is specially useful for convolutional as well as pruned neural networks. We conduct experiments on networks with random weights as well as networks trained on MNIST, showing that in the particular case of the `∞-Lipschitz constant, our approach yields superior estimates, compared to baselines available in the literature.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=rJeW1yHYwH",
  "title": "Inductive representation learning on temporal graphs",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Inductive representation learning on temporal graphs is an important step toward salable machine learning on real-world dynamic networks. The evolving nature of temporal dynamic graphs requires handling new nodes as well as capturing temporal patterns. The node embeddings, which are now functions of time, should represent both the static node features and the evolving topological structures. Moreover, node and topological features can be temporal as well, whose patterns the node embeddings should also capture. We propose the temporal graph attention (TGAT) layer to efficiently aggregate temporal-topological neighborhood features as well as to learn the time-feature interactions. For TGAT, we use the self-attention mechanism as building block and develop a novel functional time encoding technique based on the classical Bochner’s theorem from harmonic analysis. By stacking TGAT layers, the network recognizes the node embeddings as functions of time and is able to inductively infer embeddings for both new and observed nodes as the graph evolves. The proposed approach handles both node classification and link prediction task, and can be naturally extended to include the temporal edge features. We evaluate our method with transductive and inductive tasks under temporal settings with two benchmark and one industrial dataset. Our TGAT model compares favorably to state-of-the-art baselines as well as the previous temporal graph embedding approaches.",
  "stance": 0.8
 },
 {
  "url": "https://openreview.net/forum?id=rJeg7TEYwB",
  "title": "Pruned Graph Scattering Transforms",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Graph convolutional networks (GCNs) have achieved remarkable performance in a variety of network science learning tasks. However, theoretical analysis of such approaches is still at its infancy. Graph scattering transforms (GSTs) are non-trainable deep GCN models that are amenable to generalization and stability analyses. The present work addresses some limitations of GSTs by introducing a novel so-termed pruned (p)GST approach. The resultant pruning algorithm is guided by a graph-spectrum-inspired criterion, and retains informative scattering features on-the-fly while bypassing the exponential complexity associated with GSTs. It is further established that pGSTs are stable to perturbations of the input graph signals with bounded energy. Experiments showcase that i) pGST performs comparably to the baseline GST that uses all scattering features, while achieving significant computational savings; ii) pGST achieves comparable performance to state-of-the-art GCNs; and iii) Graph data from various domains lead to different scattering patterns, suggesting domain-adaptive pGST network architectures.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=rkgMkCEtPB",
  "title": "Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "An important research direction in machine learning has centered around developing meta-learning algorithms to tackle few-shot learning. An especially successful algorithm has been Model Agnostic Meta-Learning (MAML), a method that consists of two optimization loops, with the outer loop finding a meta-initialization, from which the inner loop can efficiently learn new tasks. Despite MAML’s popularity, a fundamental open question remains – is the effectiveness of MAML due to the meta-initialization being primed for rapid learning (large, efficient changes in the representations) or due to feature reuse, with the meta-initialization already containing high quality features? We investigate this question, via ablation studies and analysis of the latent representations, finding that feature reuse is the dominant factor. This leads to the ANIL (Almost No Inner Loop) algorithm, a simplification of MAML where we remove the inner loop for all but the (task-specific) head of the underlying neural network. ANIL matches MAML’s performance on benchmark few-shot image classification and RL and offers computational improvements over MAML. We further study the precise contributions of the head and body of the network, showing that performance on the test tasks is entirely determined by the quality of the learned features, and we can remove even the head of the network (the NIL algorithm). We conclude with a discussion of the rapid learning vs feature reuse question for meta-learning algorithms more broadly.",
  "stance": 0.0
 },
 {
  "url": "https://openreview.net/forum?id=rkgOlCVYvB",
  "title": "Pure and Spurious Critical Points: a Geometric Study of Linear Networks",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "The critical locus of the loss function of a neural network is determined by the geometry of the functional space and by the parameterization of this space by the network’s weights. We introduce a natural distinction between pure critical points, which only depend on the functional space, and spurious critical points, which arise from the parameterization. We apply this perspective to revisit and extend the literature on the loss function of linear neural networks. For this type of network, the functional space is either the set of all linear maps from input to output space, or a determinantal variety, i.e., a set of linear maps with bounded rank. We use geometric properties of determinantal varieties to derive new results on the landscape of linear networks with different loss functions and different parameterizations. Our analysis clearly illustrates that the absence of “bad” local minima in the loss landscape of linear networks is due to two distinct phenomena that apply in different settings: it is true for arbitrary smooth convex losses in the case of architectures that can express all linear maps (“filling architectures”) but it holds only for the quadratic loss when the functional space is a determinantal variety (“non-filling architectures”). Without any assumption on the architecture, smooth convex losses may lead to landscapes with many bad minima.",
  "stance": -1.0
 },
 {
  "url": "https://openreview.net/forum?id=rkgfdeBYvH",
  "title": "Effect of Activation Functions on the Training of Overparametrized Neural Nets",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "It is well-known that overparametrized neural networks trained using gradientbased methods quickly achieve small training error with appropriate hyperparameter settings. Recent papers have proved this statement theoretically for highly overparametrized networks under reasonable assumptions. These results either assume that the activation function is ReLU or they depend on the minimum eigenvalue of a certain Gram matrix. In the latter case, existing works only prove that this minimum eigenvalue is non-zero and do not provide quantitative bounds which require that this eigenvalue be large. Empirically, a number of alternative activation functions have been proposed which tend to perform better than ReLU at least in some settings but no clear understanding has emerged. This state of affairs underscores the importance of theoretically understanding the impact of activation functions on training. In the present paper, we provide theoretical results about the effect of activation function on the training of highly overparametrized 2-layer neural networks. A crucial property that governs the performance of an activation is whether or not it is smooth: • For non-smooth activations such as ReLU,SELU,ELU, which are not smooth because there is a point where either the first order or second order derivative is discontinuous, all eigenvalues of the associated Gram matrix are large under minimal assumptions on the data. • For smooth activations such as tanh, swish, polynomial, which have derivatives of all orders at all points, the situation is more complex: if the subspace spanned by the data has small dimension then the minimum eigenvalue of the Gram matrix can be small leading to slow training. But if the dimension is large and the data satisfies another mild condition, then the eigenvalues are large. If we allow deep networks, then the small data dimension is not a limitation provided that the depth is sufficient. We discuss a number of extensions and applications of these results.",
  "stance": 0.0
 },
 {
  "url": "https://openreview.net/forum?id=rkl3m1BFDB",
  "title": "Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Saliency maps are frequently used to support explanations of the behavior of deep reinforcement learning (RL) agents. However, a review of how saliency maps are used in practice indicates that the derived explanations are often unfalsifiable and can be highly subjective. We introduce an empirical approach grounded in counterfactual reasoning to test the hypotheses generated from saliency maps and assess the degree to which they correspond to the semantics of RL environments. We use Atari games, a common benchmark for deep RL, to evaluate three types of saliency maps. Our results show the extent to which existing claims about Atari games can be evaluated and suggest that saliency maps are best viewed as an exploratory tool rather than an explanatory tool.",
  "stance": 0.1
 },
 {
  "url": "https://openreview.net/forum?id=rkl8dlHYvB",
  "title": "Learning to Group: A Bottom-Up Framework for 3D Part Discovery in Unseen Categories",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We address the problem of discovering 3D parts for objects in unseen categories. Being able to learn the geometry prior of parts and transfer this prior to unseen categories pose fundamental challenges on data-driven shape segmentation approaches. Formulated as a contextual bandit problem, we propose a learningbased agglomerative clustering framework which learns a grouping policy to progressively group small part proposals into bigger ones in a bottom-up fashion. At the core of our approach is to restrict the local context for extracting part-level features, which encourages the generalizability to unseen categories. On the largescale fine-grained 3D part dataset, PartNet, we demonstrate that our method can transfer knowledge of parts learned from 3 training categories to 21 unseen testing categories without seeing any annotated samples. Quantitative comparisons against four shape segmentation baselines shows that our approach achieve the state-of-the-art performance.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=rklTmyBKPH",
  "title": "Fast Neural Network Adaptation via Parameter Remapping and Architecture Search",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Deep neural networks achieve remarkable performance in many computer vision tasks. Most state-of-the-art (SOTA) semantic segmentation and object detection approaches reuse neural network architectures designed for image classification as the backbone, commonly pre-trained on ImageNet. However, performance gains can be achieved by designing network architectures specifically for detection and segmentation, as shown by recent neural architecture search (NAS) research for detection and segmentation. One major challenge though, is that ImageNet pre-training of the search space representation (a.k.a. super network) or the searched networks incurs huge computational cost. In this paper, we propose a Fast Neural Network Adaptation (FNA) method, which can adapt both the architecture and parameters of a seed network (e.g. a high performing manually designed backbone) to become a network with different depth, width, or kernels via a Parameter Remapping technique, making it possible to utilize NAS for detection/segmentation tasks a lot more efficiently. In our experiments, we conduct FNA on MobileNetV2 to obtain new networks for both segmentation and detection that clearly out-perform existing networks designed both manually and by NAS. The total computation cost of FNA is significantly less than SOTA segmentation/detection NAS approaches: 1737× less than DPC, 6.8× less than Auto-DeepLab and 7.4× less than DetNAS. The code is available at https://github.com/JaminFong/FNA.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=rkxDoJBYPB",
  "title": "Reinforced Genetic Algorithm Learning for Optimizing Computation Graphs",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We present a deep reinforcement learning approach to minimizing the execution cost of neural network computation graphs in an optimizing compiler. Unlike earlier learning-based works that require training the optimizer on the same graph to be optimized, we propose a learning approach that trains an optimizer offline and then generalizes to previously unseen graphs without further training. This allows our approach to produce high-quality execution decisions on real-world TensorFlow graphs in seconds instead of hours. We consider two optimization tasks for computation graphs: minimizing running time and peak memory usage. In comparison to an extensive set of baselines, our approach achieves significant improvements over classical and other learning-based methods on these two tasks.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=ryeFY0EFwS",
  "title": "Coherent Gradients: An Approach to Understanding Generalization in Gradient Descent-based Optimization",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "An open question in the Deep Learning community is why neural networks trained with Gradient Descent generalize well on real datasets even though they are capable of fitting random data. We propose an approach to answering this question based on a hypothesis about the dynamics of gradient descent that we call Coherent Gradients: Gradients from similar examples are similar and so the overall gradient is stronger in certain directions where these reinforce each other. Thus changes to the network parameters during training are biased towards those that (locally) simultaneously benefit many examples when such similarity exists. We support this hypothesis with heuristic arguments and perturbative experiments and outline how this can explain several common empirical observations about Deep Learning. Furthermore, our analysis is not just descriptive, but prescriptive. It suggests a natural modification to gradient descent that can greatly reduce overfitting.",
  "stance": 0.6
 },
 {
  "url": "https://openreview.net/forum?id=rygGQyrFvH",
  "title": "The Curious Case of Neural Text Degeneration",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "Despite considerable advances in neural language modeling, it remains an open question what the best decoding strategy is for text generation from a language model (e.g. to generate a story). The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, maximization-based decoding methods such as beam search lead to degeneration — output text that is bland, incoherent, or gets stuck in repetitive loops. To address this we propose Nucleus Sampling, a simple but effective method to draw considerably higher quality text out of neural language models than previous decoding strategies. Our approach avoids text degeneration by truncating the unreliable tail of the probability distribution, sampling from the dynamic nucleus of tokens containing the vast majority of the probability mass. To properly examine current maximization-based and stochastic decoding methods, we compare generations from each of these methods to the distribution of human text along several axes such as likelihood, diversity, and repetition. Our results show that (1) maximization is an inappropriate decoding objective for openended text generation, (2) the probability distributions of the best current language models have an unreliable tail which needs to be truncated during generation and (3) Nucleus Sampling is currently the best available decoding strategy for generating long-form text that is both high-quality — as measured by human evaluation — and as diverse as human-written text. Context: In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. Beam Search, b=32: \"The study, published in the Proceedings of the National Academy of Sciences of the United States of America (PNAS), was conducted by researchers from the Universidad Nacional Autónoma de México (UNAM) and the Universidad Nacional Autónoma de México (UNAM/Universidad Nacional Autónoma de México/Universidad Nacional Autónoma de México/Universidad Nacional Autónoma de México/Universidad Nacional Autónoma de ...” Pure Sampling: They were cattle called Bolivian Cavalleros; they live in a remote desert uninterrupted by town, and they speak huge, beautiful, paradisiacal Bolivian linguistic thing. They say, 'Lunch, marge.' They don't tell what the lunch is,\" director Professor Chuperas Omwell told Sky News. \"They've only been talking to scientists, like we're being interviewed by TV reporters. We don't even stick around to be interviewed by TV reporters. Maybe that's how they figured out that they're cosplaying as the Bolivian Cavalleros.\" Figure 1: Even with substantial human context and the powerful GPT-2 Large language model, Beam Search (size 32) leads to degenerate repetition (highlighted in blue) while pure sampling leads to incoherent gibberish (highlighted in red). When b ≥ 64, both GPT-2 Large and XL (774M and 1542M parameters, respectively) prefer to stop generating immediately after the given context.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=ryxdEkHtPS",
  "title": "A Closer Look at Deep Policy Gradients",
  "year": 2020,
  "venue": "ICLR",
  "abstract": "We study how the behavior of deep policy gradient algorithms reflects the conceptual framework motivating their development. To this end, we propose a finegrained analysis of state-of-the-art methods based on key elements of this framework: gradient estimation, value prediction, and optimization landscapes. Our results show that the behavior of deep policy gradient algorithms often deviates from what their motivating framework would predict: the surrogate objective does not match the true reward landscape, learned value estimators fail to fit the true value function, and gradient estimates poorly correlate with the “true” gradient. The mismatch between predicted and empirical behavior we uncover highlights our poor understanding of current methods, and indicates the need to move beyond current benchmark-centric evaluation methods.",
  "stance": -1.0
 },
 {
  "url": "https://openreview.net/forum?id=-N7PBXqOUJZ",
  "title": "Lipschitz Recurrent Neural Networks",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Viewing recurrent neural networks (RNNs) as continuous-time dynamical systems, we propose a recurrent unit that describes the hidden state’s evolution with two parts: a well-understood linear component plus a Lipschitz nonlinearity. This particular functional form facilitates stability analysis of the long-term behavior of the recurrent unit using tools from nonlinear systems theory. In turn, this enables architectural design decisions before experimentation. Sufficient conditions for global stability of the recurrent unit are obtained, motivating a novel scheme for constructing hidden-to-hidden matrices. Our experiments demonstrate that the Lipschitz RNN can outperform existing recurrent units on a range of benchmark tasks, including computer vision, language modeling and speech prediction tasks. Finally, through Hessian-based analysis we demonstrate that our Lipschitz recurrent unit is more robust with respect to input and parameter perturbations as compared to other continuous-time RNNs.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=1rxHOBjeDUW",
  "title": "Drop-Bottleneck: Learning Discrete Compressed Representation for Noise-Robust Exploration",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "We propose a novel information bottleneck (IB) method named Drop-Bottleneck, which discretely drops features that are irrelevant to the target variable. DropBottleneck not only enjoys a simple and tractable compression objective but also additionally provides a deterministic compressed representation of the input variable, which is useful for inference tasks that require consistent representation. Moreover, it can jointly learn a feature extractor and select features considering each feature dimension’s relevance to the target task, which is unattainable by most neural network-based IB methods. We propose an exploration method based on Drop-Bottleneck for reinforcement learning tasks. In a multitude of noisy and reward sparse maze navigation tasks in VizDoom (Kempka et al., 2016) and DMLab (Beattie et al., 2016), our exploration method achieves state-of-the-art performance. As a new IB framework, we demonstrate that Drop-Bottleneck outperforms Variational Information Bottleneck (VIB) (Alemi et al., 2017) in multiple aspects including adversarial robustness and dimensionality reduction.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=30EvkP2aQLD",
  "title": "What are the Statistical Limits of Offline RL with Linear Function Approximation?",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Offline reinforcement learning seeks to utilize offline (observational) data to guide the learning of (causal) sequential decision making strategies. The hope is that offline reinforcement learning coupled with function approximation methods (to deal with the curse of dimensionality) can provide a means to help alleviate the excessive sample complexity burden in modern sequential decision making problems. However, the extent to which this broader approach can be effective is not well understood, where the literature largely consists of sufficient conditions. This work focuses on the basic question of what are necessary representational and distributional conditions that permit provable sample-efficient offline reinforcement learning. Perhaps surprisingly, our main result shows that even if: i) we have realizability in that the true value function of every policy is linear in a given set of features and 2) our off-policy data has good coverage over all features (under a strong spectral condition), any algorithm still (information-theoretically) requires a number of offline samples that is exponential in the problem horizon to nontrivially estimate the value of any given policy. Our results highlight that sampleefficient offline policy evaluation is not possible unless significantly stronger conditions hold; such conditions include either having low distribution shift (where the offline data distribution is close to the distribution of the policy to be evaluated) or significantly stronger representational conditions (beyond realizability).",
  "stance": -1.0
 },
 {
  "url": "https://openreview.net/forum?id=42kiJ7n_8xO",
  "title": "The geometry of integration in text classification RNNs",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Despite the widespread application of recurrent neural networks (RNNs), a unified understanding of how RNNs solve particular tasks remains elusive. In particular, it is unclear what dynamical patterns arise in trained RNNs, and how those patterns depend on the training dataset or task. This work addresses these questions in the context of text classification, building on earlier work studying the dynamics of binary sentiment-classification networks (Maheswaranathan et al., 2019). We study text-classification tasks beyond the binary case, exploring the dynamics of RNNs trained on both natural and synthetic datasets. These dynamics, which we find to be both interpretable and low-dimensional, share a common mechanism across architectures and datasets: specifically, these text-classification networks use low-dimensional attractor manifolds to accumulate evidence for each class as they process the text. The dimensionality and geometry of the attractor manifold are determined by the structure of the training dataset, with the dimensionality reflecting the number of scalar quantities the network remembers in order to classify. In categorical classification, for example, we show that this dimensionality is one less than the number of classes. Correlations in the dataset, such as those induced by ordering, can further reduce the dimensionality of the attractor manifold; we show how to predict this reduction using simple word-count statistics computed on the training dataset. To the degree that integration of evidence towards a decision is a common computational primitive, this work continues to lay the foundation for using dynamical systems techniques to study the inner workings of RNNs.",
  "stance": 0.0
 },
 {
  "url": "https://openreview.net/forum?id=71zCSP_HuBN",
  "title": "Individually Fair Rankings",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "We develop an algorithm to train individually fair learning-to-rank (LTR) models. The proposed approach ensures items from minority groups appear alongside similar items from majority groups. This notion of fair ranking is based on the definition of individual fairness from supervised learning and is more nuanced than prior fair LTR approaches that simply ensure the ranking model provides underrepresented items with a basic level of exposure. The crux of our method is an optimal transport-based regularizer that enforces individual fairness and an efficient algorithm for optimizing the regularizer. We show that our approach leads to certifiably individually fair LTR models and demonstrate the efficacy of our method on ranking tasks subject to demographic biases.",
  "stance": 0.8
 },
 {
  "url": "https://openreview.net/forum?id=8nl0k08uMi",
  "title": "Selectivity considered harmful: evaluating the causal impact of class selectivity in DNNs",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "The properties of individual neurons are often analyzed in order to understand the biological and artificial neural networks in which they’re embedded. Class selectivity—typically defined as how different a neuron’s responses are across different classes of stimuli or data samples—is commonly used for this purpose. However, it remains an open question whether it is necessary and/or sufficient for deep neural networks (DNNs) to learn class selectivity in individual units. We investigated the causal impact of class selectivity on network function by directly regularizing for or against class selectivity. Using this regularizer to reduce class selectivity across units in convolutional neural networks increased test accuracy by over 2% in ResNet18 and 1% in ResNet50 trained on Tiny ImageNet. For ResNet20 trained on CIFAR10 we could reduce class selectivity by a factor of 2.5 with no impact on test accuracy, and reduce it nearly to zero with only a small (∼2%) drop in test accuracy. In contrast, regularizing to increase class selectivity significantly decreased test accuracy across all models and datasets. These results indicate that class selectivity in individual units is neither sufficient nor strictly necessary, and can even impair DNN performance. They also encourage caution when focusing on the properties of single units as representative of the mechanisms by which DNNs function.",
  "stance": -1.0
 },
 {
  "url": "https://openreview.net/forum?id=9FWas6YbmB3",
  "title": "DrNAS: Dirichlet Neural Architecture Search",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "This paper proposes a novel differentiable architecture search method by formulating it into a distribution learning problem. We treat the continuously relaxed architecture mixing weight as random variables, modeled by Dirichlet distribution. With recently developed pathwise derivatives, the Dirichlet parameters can be easily optimized with gradient-based optimizer in an end-to-end manner. This formulation improves the generalization ability and induces stochasticity that naturally encourages exploration in the search space. Furthermore, to alleviate the large memory consumption of differentiable NAS, we propose a simple yet effective progressive learning scheme that enables searching directly on large-scale tasks, eliminating the gap between search and evaluation phases. Extensive experiments demonstrate the effectiveness of our method. Specifically, we obtain a test error of 2.46% for CIFAR-10, 23.7% for ImageNet under the mobile setting. On NASBench-201, we also achieve state-of-the-art results on all three datasets and provide insights for the effective design of neural architecture search algorithms.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=9SS69KwomAM",
  "title": "Solving Compositional Reinforcement Learning Problems via Task Reduction",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "We propose a novel learning paradigm, Self-Imitation via Reduction (SIR), for solving compositional reinforcement learning problems. SIR is based on two core ideas: task reduction and self-imitation. Task reduction tackles a hard-to-solve task by actively reducing it to an easier task whose solution is known by the RL agent. Once the original hard task is successfully solved by task reduction, the agent naturally obtains a self-generated solution trajectory to imitate. By continuously collecting and imitating such demonstrations, the agent is able to progressively expand the solved subspace in the entire task space. Experiment results show that SIR can significantly accelerate and improve learning on a variety of challenging sparse-reward continuous-control problems with compositional structures. Code and videos are available at https://sites.google.com/ view/sir-compositional.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=BbNIbVPJ-42",
  "title": "The Risks of Invariant Risk Minimization",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Invariant Causal Prediction (Peters et al., 2016) is a technique for out-of-distribution generalization which assumes that some aspects of the data distribution vary across the training set but that the underlying causal mechanisms remain constant. Recently, Arjovsky et al. (2019) proposed Invariant Risk Minimization (IRM), an objective based on this idea for learning deep, invariant features of data which are a complex function of latent variables; many alternatives have subsequently been suggested. However, formal guarantees for all of these works are severely lacking. In this paper, we present the first analysis of classification under the IRM objective—as well as these recently proposed alternatives—under a fairly natural and general model. In the linear case, we give simple conditions under which the optimal solution succeeds or, more often, fails to recover the optimal invariant predictor. We furthermore present the very first results in the non-linear regime: we demonstrate that IRM can fail catastrophically unless the test data are sufficiently similar to the training distribution—this is precisely the issue that it was intended to solve. Thus, in this setting we find that IRM and its alternatives fundamentally do not improve over standard Empirical Risk Minimization.",
  "stance": -1.0
 },
 {
  "url": "https://openreview.net/forum?id=Cb54AMqHQFP",
  "title": "Network Pruning That Matters: A Case Study on Retraining Variants",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Network pruning is an effective method to reduce the computational expense of over-parameterized neural networks for deployment on low-resource systems. Recent state-of-the-art techniques for retraining pruned networks such as weight rewinding and learning rate rewinding have been shown to outperform the traditional fine-tuning technique in recovering the lost accuracy (Renda et al., 2020), but so far it is unclear what accounts for such performance. In this work, we conduct extensive experiments to verify and analyze the uncanny effectiveness of learning rate rewinding. We find that the reason behind the success of learning rate rewinding is the usage of a large learning rate. Similar phenomenon can be observed in other learning rate schedules that involve large learning rates, e.g., the 1-cycle learning rate schedule (Smith & Topin, 2019). By leveraging the right learning rate schedule in retraining, we demonstrate a counter-intuitive phenomenon in that randomly pruned networks could even achieve better performance than methodically pruned networks (fine-tuned with the conventional approach). Our results emphasize the cruciality of the learning rate schedule in pruned network retraining – a detail often overlooked by practioners during the implementation of network pruning.",
  "stance": 0.5
 },
 {
  "url": "https://openreview.net/forum?id=Cz3dbFm5u-",
  "title": "SAFENet: A Secure, Accurate and Fast Neural Network Inference",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "The advances in neural networks have driven many companies to provide prediction services to users in a wide range of applications. However, current prediction systems raise privacy concerns regarding the user’s private data. A cryptographic neural network inference service is an efficient way to allow two parties to execute neural network inference without revealing either party’s data or model. Nevertheless, existing cryptographic neural network inference services suffer from enormous running latency; in particular, the latency of communication-expensive cryptographic activation function is 3 orders of magnitude higher than plaintextdomain activation function. And activations are the necessary components of the modern neural networks. Therefore, slow cryptographic activation has become the primary obstacle of efficient cryptographic inference. In this paper, we propose a new technique, called SAFENet, to enable a Secure, Accurate and Fast nEural Network inference service. To speedup secure inference and guarantee inference accuracy, SAFENet includes channel-wise activation approximation with multiple-degree options. This is implemented by keeping the most useful activation channels and replacing the remaining, less useful, channels with variousdegree polynomials. SAFENet also supports mixed-precision activation approximation by automatically assigning different replacement ratios to various layer; further increasing the approximation ratio and reducing inference latency. Our experimental results show SAFENet obtains the state-of-the-art inference latency and performance, reducing latency by 38% ∼ 61% or improving accuracy by 1.8% ∼ 4% over prior techniques on various encrypted datasets.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=EGdFhBzmAwB",
  "title": "Generalization bounds via distillation",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "This paper theoretically investigates the following empirical phenomenon: given a high-complexity network with poor generalization bounds, one can distill it into a network with nearly identical predictions but low complexity and vastly smaller generalization bounds. The main contribution is an analysis showing that the original network inherits this good generalization bound from its distillation, assuming the use of well-behaved data augmentation. This bound is presented both in an abstract and in a concrete form, the latter complemented by a reduction technique to handle modern computation graphs featuring convolutional layers, fullyconnected layers, and skip connections, to name a few. To round out the story, a (looser) classical uniform convergence analysis of compression is also presented, as well as a variety of experiments on cifar10 and mnist demonstrating similar generalization performance between the original network and its distillation. 1 OVERVIEW AND MAIN RESULTS Generalization bounds are statistical tools which take as input various measurements of a predictor on training data, and output a performance estimate for unseen data — that is, they estimate how well the predictor generalizes to unseen data. Despite extensive development spanning many decades (Anthony & Bartlett, 1999), there is growing concern that these bounds are not only disastrously loose (Dziugaite & Roy, 2017), but worse that they do not correlate with the underlying phenomena (Jiang et al., 2019b), and even that the basic method of proof is doomed (Zhang et al., 2016; Nagarajan & Kolter, 2019). As an explicit demonstration of the looseness of these bounds, Figure 1 calculates bounds for a standard ResNet architecture achieving test errors of respectively 0.008 and 0.067 on mnist and cifar10; the observed generalization gap is 10−1, while standard generalization techniques upper bound it with 10. Contrary to this dilemma, there is evidence that these networks can often be compressed or distilled into simpler networks, while still preserving their output values and low test error. Meanwhile, these simpler networks exhibit vastly better generalization bounds: again referring to Figure 1, those same networks from before can be distilled with hardly any change to their outputs, while their bounds reduce by a factor of roughly 10. Distillation is widely studied (Buciluŭ et al., 2006; Hinton et al., 2015), but usually the original network is discarded and only the final distilled network is preserved. The purpose of this work is to carry the good generalization bounds of the distilled network back to the original network; in a sense, the explicit simplicity of the distilled network is used as a witness to implicit simplicity of the original network. The main contributions are as follows. • The main theoretical contribution is a generalization bound for the original, undistilled network which scales primarily with the generalization properties of its distillation, assuming that wellbehaved data augmentation is used to measure the distillation distance. An abstract version of this bound is stated in Lemma 1.1, along with a sufficient data augmentation technique in Lemma 1.2. A concrete version of the bound, suitable to handle the ResNet architecture in Figure 1, is described in Theorem 1.3. Handling sophisticated architectures with only minor proof alterations is another contribution of this work, and is described alongside Theorem 1.3. This abstract and concrete analysis is sketched in Section 3, with full proofs deferred to appendices. • Rather than using an assumption on the distillation process (e.g., the aforementioned “wellbehaved data augmentation”), this work also gives a direct uniform convergence analysis, culminating in Theorem 1.4. This is presented partially as an open problem or cautionary tale, as",
  "stance": 0.0
 },
 {
  "url": "https://openreview.net/forum?id=EMHoBG0avc1",
  "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "We propose a simple and efficient multi-hop dense retrieval approach for answering complex open-domain questions, which achieves state-of-the-art performance on two multi-hop datasets, HotpotQA and multi-evidence FEVER. Contrary to previous work, our method does not require access to any corpus-specific information, such as inter-document hyperlinks or human-annotated entity markers, and can be applied to any unstructured text corpus. Our system also yields a much better efficiency-accuracy trade-off, matching the best published accuracy on HotpotQA while being 10 times faster at inference time.1",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=FOyuZ26emy",
  "title": "A Critique of Self-Expressive Deep Subspace Clustering",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Subspace clustering is an unsupervised clustering technique designed to cluster data that is supported on a union of linear subspaces, with each subspace defining a cluster with dimension lower than the ambient space. Many existing formulations for this problem are based on exploiting the self-expressive property of linear subspaces, where any point within a subspace can be represented as linear combination of other points within the subspace. To extend this approach to data supported on a union of non-linear manifolds, numerous studies have proposed learning an embedding of the original data using a neural network which is regularized by a self-expressive loss function on the data in the embedded space to encourage a union of linear subspaces prior on the data in the embedded space. Here we show that there are a number of potential flaws with this approach which have not been adequately addressed in prior work. In particular, we show the model formulation is often ill-posed in that it can lead to a degenerate embedding of the data, which need not correspond to a union of subspaces at all and is poorly suited for clustering. We validate our theoretical results experimentally and also repeat prior experiments reported in the literature, where we conclude that a significant portion of the previously claimed performance benefits can be attributed to an ad-hoc post processing step rather than the deep subspace clustering model.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=HHiiQKWsOcV",
  "title": "Explaining the Efficacy of Counterfactually Augmented Data",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "In attempts to produce machine learning models less reliant on spurious patterns in NLP datasets, researchers have recently proposed curating counterfactually augmented data (CAD) via a human-in-the-loop process in which given some documents and their (initial) labels, humans must revise the text to make a counterfactual label applicable. Importantly, edits that are not necessary to flip the applicable label are prohibited. Models trained on the augmented (original and revised) data appear, empirically, to rely less on semantically irrelevant words and to generalize better out of domain. While this work draws loosely on causal thinking, the underlying causal model (even at an abstract level) and the principles underlying the observed out-of-domain improvements remain unclear. In this paper, we introduce a toy analog based on linear Gaussian models, observing interesting relationships between causal models, measurement noise, out-of-domain generalization, and reliance on spurious signals. Our analysis provides some insights that help to explain the efficacy of CAD. Moreover, we develop the hypothesis that while adding noise to causal features should degrade both in-domain and out-ofdomain performance, adding noise to non-causal features should lead to relative improvements in out-of-domain performance. This idea inspires a speculative test for determining whether a feature attribution technique has identified the causal spans. If adding noise (e.g., by random word flips) to the highlighted spans degrades both in-domain and out-of-domain performance on a battery of challenge datasets, but adding noise to the complement gives improvements out-of-domain, this suggests we have identified causal spans. Thus, we present a large-scale empirical study comparing spans edited to create CAD to those selected by attention and saliency maps. Across numerous challenge domains and models, we find that the hypothesized phenomenon is pronounced for CAD.",
  "stance": 0.5
 },
 {
  "url": "https://openreview.net/forum?id=Ig-VyQc-MLK",
  "title": "Pruning Neural Networks at Initialization: Why Are We Missing the Mark?",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Recent work has explored the possibility of pruning neural networks at initialization. We assess proposals for doing so: SNIP (Lee et al., 2019), GraSP (Wang et al., 2020), SynFlow (Tanaka et al., 2020), and magnitude pruning. Although these methods surpass the trivial baseline of random pruning, we find that they remain below the accuracy of magnitude pruning after training. We show that, unlike magnitude pruning after training, randomly shuffling the weights these methods prune within each layer or sampling new initial values preserves or improves accuracy. As such, the per-weight pruning decisions made by these methods can be replaced by a per-layer choice of the fraction of weights to prune. This property suggests broader challenges with the underlying pruning heuristics, the desire to prune at initialization, or both.",
  "stance": 0.6
 },
 {
  "url": "https://openreview.net/forum?id=IrM64DGB21",
  "title": "On the role of planning in model-based deep reinforcement learning",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Model-based planning is often thought to be necessary for deep, careful reasoning and generalization in artificial agents. While recent successes of model-based reinforcement learning (MBRL) with deep function approximation have strengthened this hypothesis, the resulting diversity of model-based methods has also made it difficult to track which components drive success and why. In this paper, we seek to disentangle the contributions of recent methods by focusing on three questions: (1) How does planning benefit MBRL agents? (2) Within planning, what choices drive performance? (3) To what extent does planning improve generalization? To answer these questions, we study the performance of MuZero [58], a state-of-the-art MBRL algorithm with strong connections and overlapping components with many other MBRL algorithms. We perform a number of interventions and ablations of MuZero across a wide range of environments, including control tasks, Atari, and 9x9 Go. Our results suggest the following: (1) Planning is most useful in the learning process, both for policy updates and for providing a more useful data distribution. (2) Using shallow trees with simple Monte-Carlo rollouts is as performant as more complex methods, except in the most difficult reasoning tasks. (3) Planning alone is insufficient to drive strong generalization. These results indicate where and how to utilize planning in reinforcement learning settings, and highlight a number of open questions for future MBRL research. Model-based reinforcement learning (MBRL) has seen much interest in recent years, with advances yielding impressive gains over model-free methods in data efficiency [12, 15, 25, 76], zeroand few-shot learning [16, 37, 60], and strategic thinking [3, 62, 63, 64, 58]. These methods combine planning and learning in a variety of ways, with planning specifically referring to the process of using a learned or given model of the world to construct imagined future trajectories or plans. Some have suggested that models will play a key role in generally intelligent artificial agents [14, 50, 55, 56, 57, 67], with such arguments often appealing to model-based aspects of human cognition as proof of their importance [24, 26, 28, 41]. While the recent successes of MBRL methods lend evidence to this hypothesis, there is huge variance in the algorithmic choices made to support such advances. For example, planning can be used to select actions at evaluation time [e.g., 12] and/or for policy learning [e.g., 34]; models can be used within discrete search [e.g., 58] or gradient-based planning [e.g., 25, 29]; and models can be given [e.g., 45] or learned [e.g., 12]. Worryingly, some works even come to contradictory conclusions, such as that long rollouts can hurt performance due to compounding model errors in some settings [e.g., 34], while performance continues to increase with search depth in others [58]. Given the inconsistencies and non-overlapping choices across the literature, it can be hard to get a clear picture of the full MBRL space. This in turn makes it difficult for practitioners to decide which form of MBRL is best for a given problem (if any). The aim of this paper is to assess the strengths and weaknesses of recent advances in MBRL to help clarify the state of the field. We systematically study the role of planning and its algorithmic design choices in a recent state-of-the-art MBRL algorithm, MuZero [58]. Beyond its strong performance, MuZero’s use of multiple canonical MBRL components (e.g., search-based planning, a learned model, value estimation, and policy optimization) make it a good candidate for building intuition about the roles of these components and other methods that use them. Moreover, as discussed in the ∗Correspondence addressed to: {jhamrick,theophane}@google.com",
  "stance": 0.0
 },
 {
  "url": "https://openreview.net/forum?id=LGgdb4TS4Z",
  "title": "Topology-Aware Segmentation Using Discrete Morse Theory",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "In the segmentation of fine-scale structures from natural and biomedical images, per-pixel accuracy is not the only metric of concern. Topological correctness, such as vessel connectivity and membrane closure, is crucial for downstream analysis tasks. In this paper, we propose a new approach to train deep image segmentation networks for better topological accuracy. In particular, leveraging the power of discrete Morse theory (DMT), we identify global structures, including 1D skeletons and 2D patches, which are important for topological accuracy. Trained with a novel loss based on these global structures, the network performance is significantly improved especially near topologically challenging locations (such as weak spots of connections and membranes). On diverse datasets, our method achieves superior performance on both the DICE score and topological metrics.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=LVotkZmYyDi",
  "title": "Proximal Gradient Descent-Ascent: Variable Convergence under KŁ Geometry",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "The gradient descent-ascent (GDA) algorithm has been widely applied to solve minimax optimization problems. In order to achieve convergent policy parameters for minimax optimization, it is important that GDA generates convergent variable sequences rather than convergent sequences of function values or gradient norms. However, the variable convergence of GDA has been proved only under convexity geometries, and there lacks understanding for general nonconvex minimax optimization. This paper fills such a gap by studying the convergence of a more general proximal-GDA for regularized nonconvex-strongly-concave minimax optimization. Specifically, we show that proximal-GDA admits a novel Lyapunov function, which monotonically decreases in the minimax optimization process and drives the variable sequence to a critical point. By leveraging this Lyapunov function and the KŁ geometry that parameterizes the local geometries of general nonconvex functions, we formally establish the variable convergence of proximal-GDA to a critical point x∗, i.e., xt → x∗, yt → y∗(x∗). Furthermore, over the full spectrum of the KŁ-parameterized geometry, we show that proximal-GDA achieves different types of convergence rates ranging from sublinear convergence up to finite-step convergence, depending on the geometry associated with the KŁ parameter. This is the first theoretical result on the variable convergence for nonconvex minimax optimization.",
  "stance": 0.5
 },
 {
  "url": "https://openreview.net/forum?id=NQbnPjPYaG6",
  "title": "On the Impossibility of Global Convergence in Multi-Loss Optimization",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Under mild regularity conditions, gradient-based methods converge globally to a critical point in the single-loss setting. This is known to break down for vanilla gradient descent when moving to multi-loss optimization, but can we hope to build some algorithm with global guarantees? We negatively resolve this open problem by proving that desirable convergence properties cannot simultaneously hold for any algorithm. Our result has more to do with the existence of games with no satisfactory outcomes, than with algorithms per se. More explicitly we construct a two-player game with zero-sum interactions whose losses are both coercive and analytic, but whose only simultaneous critical point is a strict maximum. Any ‘reasonable’ algorithm, defined to avoid strict maxima, will therefore fail to converge. This is fundamentally different from single losses, where coercivity implies existence of a global minimum. Moreover, we prove that a wide range of existing gradient-based methods almost surely have bounded but non-convergent iterates in a constructed zero-sum game for suitably small learning rates. It nonetheless remains an open question whether such behavior can arise in high-dimensional games of interest to ML practitioners, such as GANs or multi-agent RL.",
  "stance": -1.0
 },
 {
  "url": "https://openreview.net/forum?id=Naqw7EHIfrv",
  "title": "Representation Learning for Sequence Data with Deep Autoencoding Predictive Components",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "We propose Deep Autoencoding Predictive Components (DAPC) – a selfsupervised representation learning method for sequence data, based on the intuition that useful representations of sequence data should exhibit a simple structure in the latent space. We encourage this latent structure by maximizing an estimate of predictive information of latent feature sequences, which is the mutual information between the past and future windows at each time step. In contrast to the mutual information lower bound commonly used by contrastive learning, the estimate of predictive information we adopt is exact under a Gaussian assumption. Additionally, it can be computed without negative sampling. To reduce the degeneracy of the latent space extracted by powerful encoders and keep useful information from the inputs, we regularize predictive information learning with a challenging masked reconstruction loss. We demonstrate that our method recovers the latent space of noisy dynamical systems, extracts predictive features for forecasting tasks, and improves automatic speech recognition when used to pretrain the encoder on large amounts of unlabeled data. 1",
  "stance": 0.8
 },
 {
  "url": "https://openreview.net/forum?id=O3Y56aqpChA",
  "title": "Self-training For Few-shot Transfer Across Extreme Task Differences",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Most few-shot learning techniques are pre-trained on a large, labeled “base dataset”. In problem domains where such large labeled datasets are not available for pre-training (e.g., X-ray, satellite images), one must resort to pre-training in a different “source” problem domain (e.g., ImageNet), which can be very different from the desired target task. Traditional few-shot and transfer learning techniques fail in the presence of such extreme differences between the source and target tasks. In this paper, we present a simple and effective solution to tackle this extreme domain gap: self-training a source domain representation on unlabeled data from the target domain. We show that this improves one-shot performance on the target domain by 2.9 points on average on the challenging BSCD-FSL benchmark consisting of datasets from multiple domains. Our code is available at https://github.com/cpphoo/STARTUP.",
  "stance": 0.9
 },
 {
  "url": "https://openreview.net/forum?id=OPyWRrcjVQw",
  "title": "Shapley explainability on the data manifold",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Explainability in AI is crucial for model development, compliance with regulation, and providing operational nuance to predictions. The Shapley framework for explainability attributes a model’s predictions to its input features in a mathematically principled and model-agnostic way. However, general implementations of Shapley explainability make an untenable assumption: that the model’s features are uncorrelated. In this work, we demonstrate unambiguous drawbacks of this assumption and develop two solutions to Shapley explainability that respect the data manifold. One solution, based on generative modelling, provides flexible access to data imputations; the other directly learns the Shapley value-function, providing performance and stability at the cost of flexibility. While “off-manifold” Shapley values can (i) give rise to incorrect explanations, (ii) hide implicit model dependence on sensitive attributes, and (iii) lead to unintelligible explanations in higher-dimensional data, on-manifold explainability overcomes these problems.",
  "stance": 0.5
 },
 {
  "url": "https://openreview.net/forum?id=PS3IMnScugk",
  "title": "Learning to Recombine and Resample Data For Compositional Generalization",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Flexible neural sequence models outperform grammarand automaton-based counterparts on a variety of tasks. However, neural models perform poorly in settings requiring compositional generalization beyond the training data—particularly to rare or unseen subsequences. Past work has found symbolic scaffolding (e.g. grammars or automata) essential in these settings. We describe R&R, a learned data augmentation scheme that enables a large category of compositional generalizations without appeal to latent symbolic structure. R&R has two components: recombination of original training examples via a prototype-based generative model and resampling of generated examples to encourage extrapolation. Training an ordinary neural sequence model on a dataset augmented with recombined and resampled examples significantly improves generalization in two language processing problems—instruction following (SCAN) and morphological analysis (SIGMORPHON 2018)—where R&R enables learning of new constructions and tenses from as few as eight initial examples.",
  "stance": 0.6
 },
 {
  "url": "https://openreview.net/forum?id=RSU17UoKfJF",
  "title": "R-GAP: Recursive Gradient Attack on Privacy",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Federated learning frameworks have been regarded as a promising approach to break the dilemma between demands on privacy and the promise of learning from large collections of distributed data. Many such frameworks only ask collaborators to share their local update of a common model, i.e. gradients, instead of exposing their raw data to other collaborators. However, recent optimization-based gradient attacks show that raw data can often be accurately recovered from gradients. It has been shown that minimizing the Euclidean distance between true gradients and those calculated from estimated data is often effective in fully recovering private data. However, there is a fundamental lack of theoretical understanding of how and when gradients can lead to unique recovery of original data. Our research fills this gap by providing a closed-form recursive procedure to recover data from gradients in deep neural networks. We name it Recursive Gradient Attack on Privacy (R-GAP). Experimental results demonstrate that R-GAP works as well as or even better than optimization-based approaches at a fraction of the computation under certain conditions. Additionally, we propose a Rank Analysis method, which can be used to estimate the risk of gradient attacks inherent in certain network architectures, regardless of whether an optimization-based or closed-form-recursive attack is used. Experimental results demonstrate the utility of the rank analysis towards improving the network’s security. Source code is available for download from https://github.com/JunyiZhu-AI/R-GAP.",
  "stance": 0.7
 },
 {
  "url": "https://openreview.net/forum?id=US-TP-xnXI",
  "title": "Structured Prediction as Translation between Augmented Natural Languages",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "We propose a new framework, Translation between Augmented Natural Languages (TANL), to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Instead of tackling the problem by training task-specific discriminative classifiers, we frame it as a translation task between augmented natural languages, from which the task-relevant information can be easily extracted. Our approach can match or outperform task-specific models on all tasks, and in particular, achieves new state-of-the-art results on joint entity and relation extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic role labeling (CoNLL-2005 and CoNLL2012). We accomplish this while using the same architecture and hyperparameters for all tasks and even when training a single model to solve all tasks at the same time (multi-task learning). Finally, we show that our framework can also significantly improve the performance in a low-resource regime, thanks to better use of label semantics.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=Xb8xvrtB8Ce",
  "title": "Bag of Tricks for Adversarial Training",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Adversarial training (AT) is one of the most effective strategies for promoting model robustness. However, recent benchmarks show that most of the proposed improvements on AT are less effective than simply early stopping the training procedure. This counter-intuitive fact motivates us to investigate the implementation details of tens of AT methods. Surprisingly, we find that the basic settings (e.g., weight decay, training schedule, etc.) used in these methods are highly inconsistent. In this work, we provide comprehensive evaluations on CIFAR-10, focusing on the effects of mostly overlooked training tricks and hyperparameters for adversarially trained models. Our empirical observations suggest that adversarial robustness is much more sensitive to some basic training settings than we thought. For example, a slightly different value of weight decay can reduce the model robust accuracy by more than 7%, which is probable to override the potential promotion induced by the proposed methods. We conclude a baseline training setting and re-implement previous defenses to achieve new state-of-the-art results1. These facts also appeal to more concerns on the overlooked confounders when benchmarking defenses.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=Y9McSeEaqUh",
  "title": "Predicting Classification Accuracy When Adding New Unobserved Classes",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Multiclass classifiers are often designed and evaluated only on a sample from the classes on which they will eventually be applied. Hence, their final accuracy remains unknown. In this work we study how a classifier’s performance over the initial class sample can be used to extrapolate its expected accuracy on a larger, unobserved set of classes. For this, we define a measure of separation between correct and incorrect classes that is independent of the number of classes: the reversed ROC (rROC), which is obtained by replacing the roles of classes and data-points in the common ROC. We show that the classification accuracy is a function of the rROC in multiclass classifiers, for which the learned representation of data from the initial class sample remains unchanged when new classes are added. Using these results we formulate a robust neural-network-based algorithm, CleaneX, which learns to estimate the accuracy of such classifiers on arbitrarily large sets of classes. Unlike previous methods, our method uses both the observed accuracies of the classifier and densities of classification scores, and therefore achieves remarkably better predictions than current state-of-the-art methods on both simulations and real datasets of object detection, face recognition, and brain decoding.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=YtMG5ex0ou",
  "title": "Tomographic Auto-Encoder: Unsupervised Bayesian Recovery of Corrupted Data",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "We propose a new probabilistic method for unsupervised recovery of corrupted data. Given a large ensemble of degraded samples, our method recovers accurate posteriors of clean values, allowing the exploration of the manifold of possible reconstructed data and hence characterising the underlying uncertainty. In this setting, direct application of classical variational methods often gives rise to collapsed densities that do not adequately explore the solution space. Instead, we derive our novel reduced entropy condition approximate inference method that results in rich posteriors. We test our model in a data recovery task under the common setting of missing values and noise, demonstrating superior performance to existing variational methods for imputation and de-noising with different real data sets. We further show higher classification accuracy after imputation, proving the advantage of propagating uncertainty to downstream tasks with our model.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=_WnwtieRHxM",
  "title": "Understanding the role of importance weighting for deep learning",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "The recent paper by Byrd & Lipton (2019), based on empirical observations, raises a major concern on the impact of importance weighting for the over-parameterized deep learning models. They observe that as long as the model can separate the training data, the impact of importance weighting diminishes as the training proceeds. Nevertheless, there lacks a rigorous characterization of this phenomenon. In this paper, we provide formal characterizations and theoretical justifications on the role of importance weighting with respect to the implicit bias of gradient descent and margin-based learning theory. We reveal both the optimization dynamics and generalization performance under deep learning models. Our work not only explains the various novel phenomenons observed for importance weighting in deep learning, but also extends to the studies where the weights are being optimized as part of the model, which applies to a number of topics under active research.",
  "stance": 0.5
 },
 {
  "url": "https://openreview.net/forum?id=_i3ASPp12WS",
  "title": "Online Adversarial Purification based on Self-supervised Learning",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Deep neural networks are known to be vulnerable to adversarial examples, where a perturbation in the input space leads to an amplified shift in the latent network representation. In this paper, we combine canonical supervised learning with selfsupervised representation learning, and present Self-supervised Online Adversarial Purification (SOAP), a novel defense strategy that uses a self-supervised loss to purify adversarial examples at test-time. Our approach leverages the labelindependent nature of self-supervised signals, and counters the adversarial perturbation with respect to the self-supervised tasks. SOAP yields competitive robust accuracy against state-of-the-art adversarial training and purification methods, with considerably less training complexity. In addition, our approach is robust even when adversaries are given knowledge of the purification defense strategy. To the best of our knowledge, our paper is the first that generalizes the idea of using self-supervised signals to perform online test-time purification.",
  "stance": 0.9
 },
 {
  "url": "https://openreview.net/forum?id=b7g3_ZMHnT0",
  "title": "Learning to Deceive Knowledge Graph Augmented Models via Targeted Perturbation",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Knowledge graphs (KGs) have helped neural models improve performance on various knowledge-intensive tasks, like question answering and item recommendation. By using attention over the KG, such KG-augmented models can also “explain” which KG information was most relevant for making a given prediction. In this paper, we question whether these models are really behaving as we expect. We show that, through a reinforcement learning policy (or even simple heuristics), one can produce deceptively perturbed KGs, which maintain the downstream performance of the original KG while significantly deviating from the original KG’s semantics and structure. Our findings raise doubts about KG-augmented models’ ability to reason about KG information and give sensible explanations.",
  "stance": -1.0
 },
 {
  "url": "https://openreview.net/forum?id=bM3L3I_853",
  "title": "AdaFuse: Adaptive Temporal Fusion Network for Efficient Action Recognition",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Temporal modelling is the key for efficient video action recognition. While understanding temporal information can improve recognition accuracy for dynamic actions, removing temporal redundancy and reusing past features can significantly save computation leading to efficient action recognition. In this paper, we introduce an adaptive temporal fusion network, called AdaFuse, that dynamically fuses channels from current and past feature maps for strong temporal modelling. Specifically, the necessary information from the historical convolution feature maps is fused with current pruned feature maps with the goal of improving both recognition accuracy and efficiency. In addition, we use a skipping operation to further reduce the computation cost of action recognition. Extensive experiments on Something V1&V2, Jester and Mini-Kinetics show that our approach can achieve about 40% computation savings with comparable accuracy to state-of-the-art methods. The project page can be found at https://mengyuest.github.io/AdaFuse/",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=c9-WeM-ceB",
  "title": "Saliency is a Possible Red Herring When Diagnosing Poor Generalization",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Poor generalization is one symptom of models that learn to predict target variables using spuriously-correlated image features present only in the training distribution instead of the true image features that denote a class. It is often thought that this can be diagnosed visually using attribution (aka saliency) maps. We study if this assumption is correct. In some prediction tasks, such as for medical images, one may have some images with masks drawn by a human expert, indicating a region of the image containing relevant information to make the prediction. We study multiple methods that take advantage of such auxiliary labels, by training networks to ignore distracting features which may be found outside of the region of interest. This mask information is only used during training and has an impact on generalization accuracy depending on the severity of the shift between the training and test distributions. Surprisingly, while these methods improve generalization performance in the presence of a covariate shift, there is no strong correspondence between the correction of attribution towards the features a human expert has labelled as important and generalization performance. These results suggest that the root cause of poor generalization may not always be spatially defined, and raise questions about the utility of masks as “attribution priors” as well as saliency maps for explainable predictions.",
  "stance": 0.0
 },
 {
  "url": "https://openreview.net/forum?id=g-wu9TMPODo",
  "title": "How Benign is Benign Overfitting ?",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "We investigate two causes for adversarial vulnerability in deep neural networks: bad data and (poorly) trained models. When trained with SGD, deep neural networks essentially achieve zero training error, even in the presence of label noise, while also exhibiting good generalization on natural test data, something referred to as benign overfitting (Bartlett et al., 2020; Chatterji & Long, 2020). However, these models are vulnerable to adversarial attacks. We identify label noise as one of the causes for adversarial vulnerability, and provide theoretical and empirical evidence in support of this. Surprisingly, we find several instances of label noise in datasets such as MNIST and CIFAR, and that robustly trained models incur training error on some of these, i.e. they don’t fit the noise. However, removing noisy labels alone does not suffice to achieve adversarial robustness. We conjecture that in part sub-optimal representation learning is also responsible for adversarial vulnerability. By means of simple theoretical setups, we show how the choice of representation can drastically affect adversarial robustness.",
  "stance": -0.5
 },
 {
  "url": "https://openreview.net/forum?id=gl3D-xY7wLq",
  "title": "Noise or Signal: The Role of Image Backgrounds in Object Recognition",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "We assess the tendency of state-of-the-art object recognition models to depend on signals from image backgrounds. We create a toolkit for disentangling foreground and background signal on ImageNet images, and find that (a) models can achieve non-trivial accuracy by relying on the background alone, (b) models often misclassify images even in the presence of correctly classified foregrounds—up to 88% of the time with adversarially chosen backgrounds, and (c) more accurate models tend to depend on backgrounds less. Our analysis of backgrounds brings us closer to understanding which correlations machine learning models use, and how they determine models’ out of distribution performance.",
  "stance": -1.0
 },
 {
  "url": "https://openreview.net/forum?id=hPWj1qduVw8",
  "title": "Learning Reasoning Paths over Semantic Graphs for Video-grounded Dialogues",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Compared to traditional visual question answering, video-grounded dialogues require additional reasoning over dialogue context to answer questions in a multiturn setting. Previous approaches to video-grounded dialogues mostly use dialogue context as a simple text input without modelling the inherent information flows at the turn level. In this paper, we propose a novel framework of Reasoning Paths in Dialogue Context (PDC). PDC model discovers information flows among dialogue turns through a semantic graph constructed based on lexical components in each question and answer. PDC model then learns to predict reasoning paths over this semantic graph. Our path prediction model predicts a path from the current turn through past dialogue turns that contain additional visual cues to answer the current question. Our reasoning model sequentially processes both visual and textual information through this reasoning path and the propagated features are used to generate the answer. Our experimental results demonstrate the effectiveness of our method and provide additional insights on how models use semantic dependencies in a dialogue context to retrieve visual cues.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=hsFN92eQEla",
  "title": "EVALUATION OF NEURAL ARCHITECTURES TRAINED WITH SQUARE LOSS VS CROSS-ENTROPY IN CLASSIFICATION TASKS",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Modern neural architectures for classification tasks are trained using the crossentropy loss, which is widely believed to be empirically superior to the square loss. In this work we provide evidence indicating that this belief may not be wellfounded. We explore several major neural architectures and a range of standard benchmark datasets for NLP, automatic speech recognition (ASR) and computer vision tasks to show that these architectures, with the same hyper-parameter settings as reported in the literature, perform comparably or better when trained with the square loss, even after equalizing computational resources. Indeed, we observe that the square loss produces better results in the dominant majority of NLP and ASR experiments. Cross-entropy appears to have a slight edge on computer vision tasks. We argue that there is little compelling empirical or theoretical evidence indicating a clear-cut advantage to the cross-entropy loss. Indeed, in our experiments, performance on nearly all non-vision tasks can be improved, sometimes significantly, by switching to the square loss. Furthermore, training with square loss appears to be less sensitive to the randomness in initialization. We posit that training using the square loss for classification needs to be a part of best practices of modern deep learning on equal footing with cross-entropy.",
  "stance": 0.5
 },
 {
  "url": "https://openreview.net/forum?id=hvdKKV2yt7T",
  "title": "Dataset Inference: Ownership Resolution in Machine Learning",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "With increasingly more data and computation involved in their training, machine learning models constitute valuable intellectual property. This has spurred interest in model stealing, which is made more practical by advances in learning with partial, little, or no supervision. Existing defenses focus on inserting unique watermarks in a model’s decision surface, but this is insufficient: the watermarks are not sampled from the training distribution and thus are not always preserved during model stealing. In this paper, we make the key observation that knowledge contained in the stolen model’s training set is what is common to all stolen copies. The adversary’s goal, irrespective of the attack employed, is always to extract this knowledge or its by-products. This gives the original model’s owner a strong advantage over the adversary: model owners have access to the original training data. We thus introduce dataset inference, the process of identifying whether a suspected model copy has private knowledge from the original model’s dataset, as a defense against model stealing. We develop an approach for dataset inference that combines statistical testing with the ability to estimate the distance of multiple data points to the decision boundary. Our experiments on CIFAR10, SVHN, CIFAR100 and ImageNet show that model owners can claim with confidence greater than 99% that their model (or dataset as a matter of fact) was stolen, despite only exposing 50 of the stolen model’s training points. Dataset inference defends against state-of-the-art attacks even when the adversary is adaptive. Unlike prior work, it does not require retraining or overfitting the defended model.1",
  "stance": 0.7
 },
 {
  "url": "https://openreview.net/forum?id=i80OPhOCVH2",
  "title": "On the Bottleneck of Graph Neural Networks and its Practical Implications",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Since the proposal of the graph neural network (GNN) by Gori et al. (2005) and Scarselli et al. (2008), one of the major problems in training GNNs was their struggle to propagate information between distant nodes in the graph. We propose a new explanation for this problem: GNNs are susceptible to a bottleneck when aggregating messages across a long path. This bottleneck causes the over-squashing of exponentially growing information into fixed-size vectors. As a result, GNNs fail to propagate messages originating from distant nodes and perform poorly when the prediction task depends on long-range interaction. In this paper, we highlight the inherent problem of over-squashing in GNNs: we demonstrate that the bottleneck hinders popular GNNs from fitting long-range signals in the training data; we further show that GNNs that absorb incoming edges equally, such as GCN and GIN, are more susceptible to over-squashing than GAT and GGNN; finally, we show that prior work, which extensively tuned GNN models of long-range problems, suffer from over-squashing, and that breaking the bottleneck improves their state-of-the-art results without any tuning or additional weights. Our code is available at https://github.com/tech-srl/bottleneck/ .",
  "stance": 0.8
 },
 {
  "url": "https://openreview.net/forum?id=iQQK02mxVIT",
  "title": "Why resampling outperforms reweighting for correcting sampling bias with stochastic gradients",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "A data set sampled from a certain population is biased if the subgroups of the population are sampled at proportions that are significantly different from their underlying proportions. Training machine learning models on biased data sets requires correction techniques to compensate for the bias. We consider two commonlyused techniques, resampling and reweighting, that rebalance the proportions of the subgroups to maintain the desired objective function. Though statistically equivalent, it has been observed that resampling outperforms reweighting when combined with stochastic gradient algorithms. By analyzing illustrative examples, we explain the reason behind this phenomenon using tools from dynamical stability and stochastic asymptotics. We also present experiments from regression, classification, and off-policy prediction to demonstrate that this is a general phenomenon. We argue that it is imperative to consider the objective function design and the optimization algorithm together while addressing the sampling bias.",
  "stance": 0.2
 },
 {
  "url": "https://openreview.net/forum?id=iWLByfvUhN",
  "title": "Decoupling Global and Local Representations via Invertible Generative Flows",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "In this work, we propose a new generative model that is capable of automatically decoupling global and local representations of images in an entirely unsupervised setting, by embedding a generative flow in the VAE framework to model the decoder. Specifically, the proposed model utilizes the variational auto-encoding framework to learn a (low-dimensional) vector of latent variables to capture the global information of an image, which is fed as a conditional input to a flow-based invertible decoder with architecture borrowed from style transfer literature. Experimental results on standard image benchmarks demonstrate the effectiveness of our model in terms of density estimation, image generation and unsupervised representation learning. Importantly, this work demonstrates that with only architectural inductive biases, a generative model with a likelihood-based objective is capable of learning decoupled representations, requiring no explicit supervision. The code for our model is available at https://github.com/XuezheMax/wolf.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=jh-rTtvkGeM",
  "title": "Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "We empirically demonstrate that full-batch gradient descent on neural network training objectives typically operates in a regime we call the Edge of Stability. In this regime, the maximum eigenvalue of the training loss Hessian hovers just above the value 2/(step size), and the training loss behaves non-monotonically over short timescales, yet consistently decreases over long timescales. Since this behavior is inconsistent with several widespread presumptions in the field of optimization, our findings raise questions as to whether these presumptions are relevant to neural network training. We hope that our findings will inspire future efforts aimed at rigorously understanding optimization at the Edge of Stability.",
  "stance": -0.5
 },
 {
  "url": "https://openreview.net/forum?id=kDnal_bbb-E",
  "title": "DialoGraph: Incorporating Interpretable Strategy-Graph Networks into Negotiation Dialogues",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "To successfully negotiate a deal, it is not enough to communicate fluently: pragmatic planning of persuasive negotiation strategies is essential. While modern dialogue agents excel at generating fluent sentences, they still lack pragmatic grounding and cannot reason strategically. We present DIALOGRAPH, a negotiation system that incorporates pragmatic strategies in a negotiation dialogue using graph neural networks. DIALOGRAPH explicitly incorporates dependencies between sequences of strategies to enable improved and interpretable prediction of next optimal strategies, given the dialogue context. Our graph-based method outperforms prior state-of-the-art negotiation models both in the accuracy of strategy/dialogue act prediction and in the quality of downstream dialogue response generation. We qualitatively show further benefits of learned strategy-graphs in providing explicit associations between effective negotiation strategies over the course of the dialogue, leading to interpretable and strategic dialogues.1",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=kyaIeYj4zZ",
  "title": "GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "We present GRAPPA, an effective pre-training approach for table semantic parsing that learns a compositional inductive bias in the joint representations of textual and tabular data. We construct synthetic question-SQL pairs over high-quality tables via a synchronous context-free grammar (SCFG). We pre-train GRAPPA on the synthetic data to inject important structural properties commonly found in table semantic parsing into the pre-training language model. To maintain the model’s ability to represent real-world data, we also include masked language modeling (MLM) on several existing table-and-language datasets to regularize our pre-training process. Our proposed pre-training strategy is much data-efficient. When incorporated with strong base semantic parsers, GRAPPA achieves new state-of-the-art results on four popular fully supervised and weakly supervised table semantic parsing tasks. The pre-trained embeddings can be downloaded at https://huggingface.co/Salesforce/grappa_large_jnt.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=l0V53bErniB",
  "title": "Combining Physics and Machine Learning for Network Flow Estimation",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "The flow estimation problem consists of predicting missing edge flows in a network (e.g., traffic, power, and water) based on partial observations. These missing flows depend both on the underlying physics (edge features and a flow conservation law) as well as the observed edge flows. This paper introduces an optimization framework for computing missing edge flows and solves the problem using bilevel optimization and deep learning. More specifically, we learn regularizers that depend on edge features (e.g., number of lanes in a road, resistance of a power line) using neural networks. Empirical results show that our method accurately predicts missing flows, outperforming the best baseline, and is able to capture relevant physical properties in traffic and power networks.",
  "stance": 0.8
 },
 {
  "url": "https://openreview.net/forum?id=lvRTC669EY_",
  "title": "Discovering Diverse Multi-Agent Strategic Behavior via Reward Randomization",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "We propose a simple, general and effective technique, Reward Randomization for discovering diverse strategic policies in complex multi-agent games. Combining reward randomization and policy gradient, we derive a new algorithm, RewardRandomized Policy Gradient (RPG). RPG is able to discover multiple distinctive human-interpretable strategies in challenging temporal trust dilemmas, including grid-world games and a real-world game Agar.io, where multiple equilibria exist but standard multi-agent policy gradient algorithms always converge to a fixed one with a sub-optimal payoff for every player even using state-of-the-art exploration techniques. Furthermore, with the set of diverse strategies from RPG, we can (1) achieve higher payoffs by fine-tuning the best policy from the set; and (2) obtain an adaptive agent by using this set of strategies as its training opponents. The source code and example videos can be found in our website: https://sites.google. com/view/staghuntrpg.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=m1CD7tPubNy",
  "title": "Mind the Pad -- CNNs Can Develop Blind Spots",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "We show how feature maps in convolutional networks are susceptible to spatial bias. Due to a combination of architectural choices, the activation at certain locations is systematically elevated or weakened. The major source of this bias is the padding mechanism. Depending on several aspects of convolution arithmetic, this mechanism can apply the padding unevenly, leading to asymmetries in the learned weights. We demonstrate how such bias can be detrimental to certain tasks such as small object detection: the activation is suppressed if the stimulus lies in the impacted area, leading to blind spots and misdetection. We propose solutions to mitigate spatial bias and demonstrate how they can improve model accuracy. 1 MOTIVATION Convolutional neural networks (CNNs) serve as feature extractors for a wide variety of machinelearning tasks. Little attention has been paid to the spatial distribution of activation in the feature maps a CNN computes. Our interest in analyzing this distribution is triggered by mysterious failure cases of a traffic light detector: The detector successfully detects a small but visible traffic light in a road scene. However, it fails completely in detecting the same traffic light in the next frame captured by the ego-vehicle. The major difference between both frames is a limited shift along the vertical dimension as the vehicle moves forward. Therefore, the drastic difference in object detection is surprising given that CNNs are often assumed to have a high degree of translation invariance [8; 17]. The spatial distribution of activation in feature maps varies with the input. Nevertheless, by closely examining this distribution for a large number of samples, we found consistent patterns among them, often in the form of artifacts that do not resemble any input features. This work aims to analyze the root cause of such artifacts and their impact on CNNs. We show that these artifacts are responsible for the mysterious failure cases mentioned earlier, as they can induce ‘blind spots’ for the object detection head. Our contributions are: • Demonstrating how the padding mechanism can induce spatial bias in CNNs (Section 2). • Demonstrating how spatial bias can impair downstream tasks (Section 3). • Identifying uneven application of 0-padding as a resolvable source of bias (Section 5). • Relating the padding mechanism with the foveation behavior of CNNs (Section 6). • Providing recommendations to mitigate spatial bias and demonstrating how this can prevent blind spots and boost model accuracy. 2 THE EMERGENCE OF SPATIAL BIAS IN CNNS Our aim is to determine to which extent activation magnitude in CNN feature maps is influenced by location. We demonstrate our analysis on a publicly-available traffic-light detection model [36]. This model implements the SSD architecture [26] in TensorFlow [1], using MobileNet-v1 [13] as a feature extractor. The model is trained on the BSTLD dataset [4] which annotates traffic lights in road scenes. Figure 1 shows two example scenes from the dataset. For each scene, we show two feature maps computed by two filters in the 11th convolutional layer. This layer contains 512 filters whose feature maps are used directly by the first box predictor in the SSD to detect small objects. 1 Published as a conference paper at ICLR 2021 Figure 1: Averaging feature maps per input (column marginal) and per filter (row marginal) in the last convolutional layer of a traffic light detector. Color indicates activation strength (the brighter, the higher), revealing line artifacts in the maps. These artifacts are the manifestation of spatial bias. The bottom row in Figure 1 shows the average response of each of the two aforementioned filters, computed over the test set in BSTLD. The first filter seems to respond mainly to features in the top half of the input, while the second filter responds mainly to street areas. There are visible lines in the two average maps that do not seem to resemble any scene features and are consistently present in the individual feature maps. We analyzed the prevalence of these line artifacts in the feature maps of all 512 filters. The right column in Figure 1 shows the average of these maps per scene, as well as over the entire test set (see supplemental for all 512 maps). The artifacts are largely visible in the average maps, with variations per scene depending on which individual maps are dominant. A useful way to make the artifacts stand out is to neutralize scene features by computing the feature maps for a zero-valued input. Figure 2 depicts the resulting average map for each convolutional layer after applying ReLU units. The first average map is constant as we expect with a 0-valued input. The second map is also constant except for a 1-pixel boundary where the value is lower at the left border and higher at the other three borders. We magnify the corners to make these deviations visible. The border deviations increase in thickness and in variance at subsequent layers, creating multiple line artifacts at each border. These artifacts become quite pronounced at ReLU 8 where they start to propagate inwards, resembling the ones in Figure 1. Figure 2: Activation maps for a 0 input, averaged over each layer’s filters (title format: H⇥W⇥C). 2 Published as a conference paper at ICLR 2021 It is evident that the 1-pixel border variations in the second map are caused by the padding mechanism in use. This mechanism pads the output of the previous layer with a 1-pixel 0-valued border in order to maintain the size of the feature map after applying 3x3 convolutional. The maps in the first layer are not impacted because the input we feed is zero valued. Subsequent layers, however, are increasingly impacted by the padding, as preceding bias terms do not warrant 0-valued input. It is noticeable in Figure 2 that the artifacts caused by the padding differ across the four borders. To investigate this asymmetry, we analyze the convolutional kernels (often called filters) that produce the feature maps. Figure 3 depicts a per-layer mean of these 3x3 kernels. These mean kernels exhibit different degrees of asymmetry in the spatial distribution of their weights. For example, the kernels in L1 assign (on average) a negative weight at the left border, and a positive weight at the bottom. This directly impacts the padding-induced variation at each border. Such asymmetries are related to uneven application of padding as we explain in Section 5. Figure 3: Mean kernel per convolutional layer. All kernels are 3⇥ 3, the titles show their counts. 3 IMPLICATIONS OF SPATIAL BIAS We demonstrate how feature-map artifacts can cause blind spots for the SSD model. Similar issues arise in several small-object detectors, e.g., for faces and masks, as well as in pixel-oriented tasks such as semantic segmentation and image inpainting (see supplemental for examples). Figure 4 illustrates how the SSD predicts small objects based on the feature maps of the 11-th convolutional layer. The SSD uses the pixel positions in these maps as anchors of object proposals. Each proposal is scored by the SSD to represent a target category, with ”background“ being an implicit category that is crucial to exclude irrelevant parts of the input. In addition to these scores, the SSD computes a bounding box to localize the predicted object at each anchor. We examine Figure 4: The formation of blind spots in SSD, illustrated via its box predictor internals with a zero-valued input. The predictor uses spatial anchors to detect and localize the target object at 45 ⇥ 80 possible locations based on 512 feature maps. Certain anchors are predisposed to predict background due to feature-map artifacts, as evident in the logit maps. Traffic lights at the corresponding location cannot be detected as demonstrated with a real scene (middle one in the bottom). 3 Published as a conference paper at ICLR 2021 Figure 5: (a) A map showing via color the detection score the SSD computes for a traffic light when present at various locations. The detection is muted when the stimulus lies in the area impacted by the artifacts. (b) The same map after changing the padding method to SYMMETRIC. The detection scores are rather constant except for periodic variations due to the SSD’s reliance on anchors. object proposals computed at 1:2 aspect ratio, as they resemble the shape of most traffic lights in the dataset. We visualize the resulting score maps both for the background category and for traffic lights, when feeding a 0-valued input to the SSD. We also visualize the bounding boxes of these proposals in the image space. The SSD predicts the image content to be of background category at all anchor locations, as evident from the value range in both score maps. Such predictions are expected with an input that contains no traffic lights. However, the line artifacts in the feature maps have a strong impact on the score maps. These artifacts elevate the likelihood of anchors closer to the top to be classified as background (see the yellow band in the background score map). Conversely, these anchors have significantly lower scores for the traffic light category, compared with other anchors in the feature map. Such difference in the impact on the target categories is due to the different weights the SSD assigns to the feature maps for each target. As a result, the artifacts lead to potential blind spots in which the scores for certain categories are artificially muted. To validate whether or not the blind spots hinder object detection, we examine road scenes that contain highly-visible traffic light instances in the impacted area. Figure 4-bottom shows an example of such a scene. The SSD computes a low detection score of 7% when the traffic light lies in the blind spot (see middle image), far below the detection false-positive cutoff. Shifting the scene image upwards or downwards makes the instance detectable with a high score as long as it lies outside the blind spot. This explains the failure cases mentioned in Section 1. To further validate this effect, we run the SSD on baseline images that each contains one traffic light instance at a specific location in the input. We store the detection score for each instance. Figure 5a depicts the computed scores in a 2D map. It is evident that the model fails to detect the traffic light instance exactly when it is located within the “blind spot” band. The artifacts further disrupt the localization of the objects as evident in the top-right plot in Figure 4 which shows per-anchor object proposals computed for a 0 input. 4 REMINDER: WHY IS PADDING NEEDED IN CNNS? Padding is applied at most convolutional layers in CNNs to serve two fundamental purposes: Maintaining feature map size A padding that satisfies this property is often described as SAME or HALF padding. FULL padding expands the maps by kernel size 1 along each dimension. VALID padding performs no padding, eroding the maps by the same amount. SAME padding is important to (1) design deep networks that can handle arbitrary input size (a challenge in the presence of gradual erosion), (2) maintain the aspect ratio of non-square input, and (3) concatenate feature maps from different layers as in Inception [39] and ResNet [12] models. Reducing information bias against the boundary Consider a 3⇥3 kernel applied to a 2D input. An input location at least 2 pixels away from the boundary contributes to nine local convolution operations when computing the feature map. On the other hand, the corner is involved only one time under VALID padding, four times under a 1-pixel SAME 0-padding, and nine times under a 2-pixel FULL 0-padding. With SAME 0-padding, the cumulative contribution differences among the input pixels grow exponentially over the CNN layers. We refer to such uneven treatment of input pixels as the foveation behavior of the padding mechanism and elaborate on this in Section 6. We next explore solutions to the issues that cause padding to induce spatial bias. 4 Published as a conference paper at ICLR 2021 Figure 6: (a) Illustrating the problem of uneven padding when down-sampling at a stride of 2. The padding along x-axis is consumed only at the left side. (b) Mean 3⇥3 filters in three ResNet models, trained on ImageNet with two input sizes. Color encodes average weight (green is positive). A size that induces uneven padding (top row) can lead to asymmetries, esp. around down-sampling layers. These asymmetries are mitigated when the input size induces no uneven padding (bottom row). 5 ELIMINATING UNEVEN APPLICATION OF PADDING While useful to reduce bias against the boundary, applying padding at down-sampling layers can lead to asymmetry in CNN internals. Figure 6a illustrates the source of this asymmetry when strided convolution is used for downsampling: At one side of the feature map, the padding is consumed by the kernel while at the other side it is not. To warrant even application of padding throughout the CNN, the following must hold at all d down-sampling layers, where (hi, wi) is the output shape at the i-th layer with kh i ⇥ kw i as kernel size, (si , si ) as strides, and = (pi , pi ) as padding amount (refer to appendix A for a proof): 8i 2 {1, . . , d} : hi 1 = si · (hi 1) + k i 2 · pi ^ wi 1 = si · (wi 1) + k i 2 · pi (1) The values h0 and w0 represent the CNN input dimensions. The above constraints are not always satisfied during training or inference with arbitrary input dimensions. For example, ImageNet classifiers based on ResNet [12] and MobileNet [13] contain five down-sampling layers (d = 5) that apply 1-pixel 0-padding before performing 2-strided convolution. To avoid uneven application of padding, the input to these CNNs must satisfy the following, as explained in appendix A: h0 = a1⇥2+1 = 32 ·a1+1 and w0 = a2⇥2+1 = 32 ·a2+1 where a1, a2 2 N (2) The traditional 1 and prevalent input size for training ImageNet models is 224⇥224. This size violates Eq. 2, leading to uneven padding at every down-sampling layer in ResNet and MobileNet models where 0-padding is effectively applied only at the left and top sides of layer input. This over-represents zeros at the top and left sides of 3⇥ 3 feature-map patches the filters are convolved with during training. The top row of Figure 6b shows per-layer mean filters in three ResNet models in PyTorch [33], pre-trained on ImageNet with 224⇥224 images. In all of these models, a few of the mean filters, adjacent to down-sampling layers, exhibit stark asymmetry about their centers. We increase the image size to 225⇥225 without introducing additional image information2. This size satisfies Eq. 2, warranting even application of padding at every downsampling layer in the above models. Retraining the models with this size strongly reduces this asymmetry as evident in the bottom row of Figure 6b. This, in turn, visibly boosts the accuracy in all models we experimented with as we report in Table 1. The accuracy did not improve further when we retrained two of the models, ResNet-18 and ResNet-34, on 226 ⇥ 226 images. This provides evidence that the boost is due to eliminating uneven padding and not merely due to increasing the input size. 1 This size has been used to facilitate model comparison on ImageNet, since the inception of AlexNet. 2 This is done via constant padding. The side to pad with one pixel is chosen at random to balance out the application of padding at both sides over the training set. No additional padding is applied at further layers. 5 Published as a conference paper at ICLR 2021 Replacing 0-padding with a padding method that reuses feature map values can alleviate the asymmetry in the learned filters in the presence of unevenly applied padding. Another possibility is to use a rigid downsampling kernel, such as max-pooling, instead of a learned one. Appendix C demonstrates both possibilities. Finally, antialiasing before downsampling [43] can strongly reduce the asymmetry as we elaborate in Section 8 and in Appendix E. Table 1: Top-1 (and top-5) accuracy of five ImageNet classifiers trained with different input sizes. Input Size 2 MobileNet ResNet-18 ResNet-34 ResNet-50 ResNet-101 224⇥224 68.19 (88.44) 69.93 (89.22) 73.30 (91.42) 75.65 (92.47) 77.37 (93.56) 225⇥225 68.80 (88.78) 70.27 (89.52) 73.72 (91.58) 76.01 (92.90) 77.67 (93.81) Even when no padding is applied (pi = 0 or pi = 0), an input size that does no satisfy Eq. 1 can lead to uneven erosion of feature maps, in turn, reducing the contribution of pixels from the impacted sides (Fig 7e. Satisfying Eq 1 imposes a restriction on input size, e.g., to values in increments of 2d = 32 with the above models (193⇥193, 225⇥225, 257⇥257, ...). Depending on the application domain, this can be guaranteed either by resizing an input to the closest increment, or by padding it accordingly with suited values. 6 PADDING MECHANISM AND FOVEATION By foveation we mean the unequal involvement of input pixels in convolutional operations throughout the CNN. Padding plays a fundamental role in the foveation behavior of CNNs. We visualize this behavior by means of a foveation map that counts for each input pixel the number of convolutional paths through which it can propagate information to the CNN output. We obtain these counts by computing the effective receptive field [28] for the sum of the final convolutional layer after assigning all weights in the network to 1 (code in supplemental). Neutralizing the weights is essential to obtain per-pixel counts of input-output paths that reflect the foveation behavior. f Figure 7: Foveation behavior of different padding methods applied to VGG-19 [37], and illustrated in a 512 ⇥ 512 input space (unless otherwise stated). Color represents the number of paths to the output for each input pixel. (a) The difference between VALID, FULL, and SAME 0-padding. (b) SAME alternatives to 0-padding. (c) Dilation amplifies foveation of SAME 0-padding. (d) Strides can lead to checkerboard patterns. (e) Foveation effects are more extensive in smaller inputs (relative to input size) and are sensitive to uneven padding. Figure 7a shows the extensive foveation effect when no padding is applied. The diminishing contribution of vast areas of the input explains the drastic drop in accuracy recently observed under VALID padding [16]. In contrast, FULL 0-padding does not incur foveation, however, at the cost of increasing the output size after each layer, making it impractical as explained in Section 4. SAME 0-padding incurs moderate foveation at the periphery, whose absolute extent depends on the number of convolutional layers and their filter sizes. Its relative extent depends on the input size: the larger the input, the larger the ratio of the constant area in yellow (refer to appendix B for a detailed example). 6 Published as a conference paper at ICLR 2021 Figure 7b shows the foveation behavior of alternatives to SAME 0-padding that have roots in wavelet analysis [19] and image processing [27]. Mirror padding mirrors pixels at the boundary to fill the padding area. When the border is included (SYMMETRIC mode in TensorFlow) all input pixels have an equal number of input-output paths 3, resulting in a uniform foveation map. When the border is not included (REFLECT mode both in PyTorch and in TensorFlow), the map exhibits bias against the border and towards a contour in its proximity. This bias is amplified over multiple layers. Replication padding exhibits the opposite bias when the padding area is wider than 1 pixel. This is because it replicates the outer 1-pixel border multiple times to fill this area 3. The method is equivalent to SYMMETRIC if the padding area is 1-pixel wide. Circular padding wraps opposing borders, enabling the kernels to seamlessly operate on the boundary and resulting in a uniform map. Partial Convolution [22] has been proposed as a padding method that treats pixels outside the original image as missing values and rescales the computed convolutions accordingly [23]. Its foveation behavior resembles reflective padding 3. Distribution padding [30] resizes the input to fill the padding area around the original feature map, aiming at preserving the distribution of the map. Its foveation map is largely uniform, except for the corners and edges. Impact of input size Besides influencing the relative extent of foveation effects, the input size also determines the presence of uneven padding (or uneven feature-map erosion), as we discussed in Section 5. Figure 7e shows the foveation map for VGG-19 with a 127⇥127 input. This input violates Eq. 1 at every downsampling layer (appendix A), leading to successive feature map erosion at the bottom and right sides which is reflected in the foveation map (see appendix B for a detailed example). The bottom-right part of the input is hence less involved in the CNN computations. Impact of dilation We assign a dilation factor of 2 to all VGG-19 convolutional layers. While this exponentially increases the receptive field of the neurons at deeper layers [42], dilation doubles the extent of the non-uniform peripheral areas that emerge with SAME 0-padding as evident in Figure 7c. SYMMETRIC and circular padding maintain uniform foveation maps regardless of dilation 3. In contrast, dilation increases the complexity of these maps for REFLECT and replication padding. Impact of strides Whether learned on based on pooling, downsampling layers can amplify the impact of succeeding convolutional layers on foveation behaviour. Furthermore, these layers can cause input pixels to vary in the count of their input-output paths. This can happen when the kernel size is not divisible by the stride, leading to a checkerboard pattern in the foveation maps. This manifests in ResNet models as we illustrate in appendix B. In VGG-19, all max-pooling layers use a stride of 2 and kernel size of 2. Changing the kernel size to 3 leads to a checkerboard pattern as evident in Figure 7d. Such effects were shown to impact pixel-oriented tasks [32]. The padding technique and its foveation behaviour have direct impact on feature-map artifacts (Section 7), and on the ability of CNNs to encode spatial information (Section 8). Understanding the foveation behavior is key to determine how suited a padding method is for a given task. For example, small object detection is known to be challenging close to the boundary [26], in part due to the foveation behavior of SAME 0-padding. In Figure 5b, we change the padding method in the SSD to SYMMETRIC. The stimulus is noticeably more detectable at the boundary, compared with 0-padding 4. In contrast, ImageNet classification is less sensitive to foveation effects because the target objects are mostly located away from the periphery. Nevertheless, the padding method was shown to impact classification accuracy [23] because it still affects feature map artifacts. 7 PADDING METHODS AND FEATURE MAP ARTIFACTS It is also noticeable that the score map in Figure 5b is more uniform than in Figure 5a. In particular, under SYMMETRIC padding the model is able to detect traffic lights placed in the blind spots of the original 0-padded model. To verify whether the line artifacts in Figure 2 are mitigated, we inspect the mean feature maps of the adapted model. With a constant input, SYMMETRIC padding warrants constant maps throughout the CNN because it reuses the border to fill the padding area. Instead, we average these maps over 30 samples generated uniformly at random. Figure 8 depicts the mean maps which are largely uniform, unlike the case with 0-padding. 3 Refer to appendix F or to http://mind-the-pad.github.io for visual illustration and further theoretical analysis of the foveation behavior. Since the input size causes uneven application of padding, the right and bottom borders are still challenging. 7 Published as a conference paper at ICLR 2021 Figure 8: The same feature maps in Figure 2, generated under mirror padding and averaged over 30 randomly-generated input samples. The line artifacts induced by 0-padding are largely mitigated. To further analyze the impact of SYMMETRIC padding, we retrain the adapted model following the original training protocol. This significantly improves the average precision (AP) as reported in Table 2 under different overlap thresholds (matching IoU), confirming that small object detection is particularly sensitive to feature-map artifacts. Table 2: Performance of the SSD traffic light detector, trained under two different padding schemes. Average Precision (AP) AP@.20IOU AP@.50IOU AP@.75IOU AP@.90IOU Zero Padding 80.24% 49.58% 3.7% 0.007% Mirror Padding 83.20% 57% 8.44% 0.02% Of the padding methods listed in Section 6, mirror padding in both SYMMETRIC and REFLECT modes, PartialConv, and circular padding are generally effective at reducing feature map artifacts that emerge under zero padding, in particular salient line patterns. In contrast, distribution padding can induce significant artifacts. Refer to appendix D for comparative examples of artifacts under the aforementioned padding schemes. Artifact magnitude and propagation While feature-map artifacts are induced by the padding mechanism at the boundary, their magnitude and inward propagation in the maps are impacted by several architectural aspects of CNNs. In particular, certain normalization schemes such as batchnorm [15] tend to limit the range of variation within a feature map and to relatively harmonize this range across different maps. This, in turn, impacts how possible artifacts in these maps accumulate when they are processed by the next convolutional layer. Similarly, artifacts that manifest after applying ReLU units are of a positive sign. These factors were instrumental in the formation of potential blind spots described in Section 3. We hence recommend to involve non-convolutional layers when inspecting the feature maps. Besides having possible impact on artifact magnitude, several aspects of convolution arithmetic, such as filter size and dilation factors, can also impact the spatial propagation of these artifacts. 8 RELATED FINDINGS AND TAKEAWAYS Handling the boundary is an inherent challenge when dealing with spatial data [9]. Mean padding is known to cause visual artifacts in traditional image processing, with alternative methods proposed to mitigate them [24]. CNNs have been often assumed to deal with such effects implicitly. Innamorati et al [14] propose learning separate sets of filters dedicated to the boundaries to avoid impacting the weights learned by regular filters. A grouped padding strategy, proposed to support 2⇥2 filters [41], offers avenues to mitigate uneven padding and corresponding skewness in foveation maps without restrictions on input size (see our note in appendix B for explanation). Finally, insights from signal and image processing [10; 11] could inspire further CNN padding schemes. Zero padding has been recently linked to CNNs’ ability to encode position information [7; 16; 18; 29]. In contrast, circular padding was shown to limit this ability [7] and to boost shift invariance [35]. The input sizes in those studies do induce uneven padding. This can be, in part, the underlying mechanism behind the aforementioned ability. Whether or not this ability is desirable depends on the task, with several methods proposed to explicitly encode spatial information [5; 6; 20; 25; 29; 31]. 8 Published as a conference paper at ICLR 2021 Downsampling using max-pooling or strided convolution has been shown to impact shift invariance in CNNs by incurring aliasing effects [3; 38; 43]. These effects can manifest in the same symptoms we reported in Section 1, albeit for a different reason. Zhang [43] demonstrated how blurring the feature maps before subsampling mitigates aliasing effects and improves ImageNet classification accuracy of various popular CNNs. We analyzed the mean filters in antialiased MobileNet and ResNet models pre-trained on ImageNet under 0-padding, with 224⇥224 as input size (refer to Appendix E). We found that antialiasing can also mitigate the asymmetry of mean filters that exhibited high asymmetry in the baseline models, especially at deeper layers. This is remarkable given that these models are trained on 224⇥224 images, which incurs one-sided zero padding at every downsampling layer. This could, in part, be attributed to the ability of the BlurPool operator used in antialiased CNN to smoothen the acuity of zero-padded borders, in turn, reducing the value imbalance incurred by one-sided padding. Further analysis is needed to examine the interaction between padding and aliasing effects in CNNs and to establish possible synergy between antialiasing and eliminating uneven application of padding. Luo et al [28] drew connections between effective receptive fields and foveated vision. Our analysis links foveation behavior with the padding scheme and suggests that it might occur implicitly in CNNs when using VALID or SAME 0-padding, without the need for explicit mechanisms [2; 21]. Furthermore, it explains the drastic accuracy drop noted by [16] under VALID padding, which is amplified by feature map erosion. Choosing a padding method SAME 0-padding is by far the most widely-used method. Compared with other methods, it can enable as much as 50% faster training and inference. Problem-specific constraints can dictate different choices [34; 35; 40]. In the lack of a universally superior padding method, we recommend considering multiple ones while paying attention to the nature of the data and the task, as well as to the following aspects: • Feature-map statistics: 0-padding can alter the value distribution within the feature maps and can shift their mean value in the presence of ReLU units. The alternatives presented in Section 6 tend to preserve this distribution, thanks to reusing existing values in the maps. • Foveation behavior: 0-padding might not be suited for tasks that require high precision at the periphery, unlike circular and SYMMETRIC mirror padding. • Interference with image semantics (esp. with a padding amount > 1 pixel): For example, circular padding could introduce border discontinuities unless the input is panoramic [35]. • Potential to induce feature map artifacts: All alternatives to 0-padding induce relatively fewer artifacts, except for Distribution padding [30] (see appendix D). We also recommend eliminating uneven padding at downsampling layers both at training and at inference time, as we illustrated in Section 5. This is especially important when zero padding is applied and the downsampling is learned. The scripts used to generate the visualizations in this paper are available in the supplemental as well as at http://mind-the-pad.github.io. Summary We demonstrated how the padding mechanism can induce spatial bias in CNNs, in the form of skewed kernels and feature-map artifacts. These artifacts can be highly pronounced with the widely-used 0-padding when applied unevenly at the four sides of the feature maps. We demonstrated how such uneven padding can inherently take place in state-of-the-art CNNs, and how the artifacts it causes can be detrimental to certain tasks such as small object detection. We provided visualization methods to expose these artifacts and to analyze the implication of various padding schemes on boundary pixels. We further proposed solutions to eliminate uneven padding and to mitigate spatial bias in CNNs. Further work is needed to closely examine the implications of spatial bias and foveation in various applications (see supplementary for examples), as well as padding impact on recurrent models and 1-D CNNs. ACKNOWLEDGEMENT We are thankful to Ross Girshick for providing useful recommendations and experiment ideas, and to Shubham Muttepawar for implementing an interactive tool out of our analysis scripts, guided by our front-end specialist Edward Wang and our AI user-experience designer Sara Zhang. 9 Published as a conference paper at ICLR 2021 REFERENCES [1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, et al. TensorFlow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016. [2] E. Akbas and M. P. Eckstein. Object detection through search with a foveated visual system. PLoS computational biology, 13(10):e1005743, 2017. [3] A. Azulay and Y. Weiss. Why do deep convolutional networks generalize so poorly to small image transformations? Journal of Machine Learning Research (JMLR), 20(184):1–25, 2019. [4] K. Behrendt, L. Novak, and R. Botros. A deep learning approach to traffic lights: Detection, tracking, and classification. In Robotics and Automation (ICRA), 2017 IEEE International Conference on, pp. 1370–1377. IEEE, 2017. [5] C.-A. Brust, S. Sickert, M. Simon, E. Rodner, and J. Denzler. Convolutional patch networks with spatial prior for road detection and urban scene understanding. In International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications (VISAPP), 2015. [6] G. F. Elsayed, P. Ramachandran, J. Shlens, and S. Kornblith. Revisiting spatial invariance with low-rank local connectivity. In International Conference on Machine Learning (ICML), 2020. [7] J. Geiping, H. Bauermeister, H. Dröge, and M. Moeller. Inverting gradients–how easy is it to break privacy in federated learning? arXiv preprint arXiv:2003.14053, 2020. [8] R. Gens and P. M. Domingos. Deep symmetry networks. In Advances in neural information processing systems (NeurIPS), pp. 2537–2545, 2014. [9] D. Griffith and C. Amrhein. An evaluation of correction techniques for boundary effects in spatial statistical analysis: traditional methods. Geographical Analysis, 15(4):352–360, 1983. [10] V. Gupta and N. Ramani. A note on convolution and padding for two-dimensional data. Geophysical Prospecting, 26(1):214–217, 1978. [11] L. Hamey. A functional approach to border handling in image processing. In International Conference on Digital Image Computing: Techniques and Applications, pp. 1–8, 2015. [12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In IEEE conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, 2016. [13] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam. MobileNets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. [14] C. Innamorati, T. Ritschel, T. Weyrich, and N. J. Mitra. Learning on the edge: Investigating boundary filters in CNNs. International Journal of Computer Vision (IJCV), pp. 1–10, 2019. [15] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning (ICML), pp. 448– 456, 2015. [16] M. A. Islam, S. Jia, and N. D. Bruce. How much position information do convolutional neural networks encode? In International Conference on Learning Representations (ICLR), 2020. [17] M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial transformer networks. In Advances in neural information processing systems (NeurIPS), pp. 2017–2025, 2015. [18] O. S. Kayhan and J. C. van Gemert. On translation invariance in CNNs: Convolutional layers can exploit absolute spatial location. In IEEE conference on Computer Vision and Pattern Recognition (CVPR), 2020. [19] T. L. Kijewski-Correa. Full-scale measurements and system identification: A time-frequency perspective. PhD thesis, University of Notre Dame., 2003. 10 Published as a conference paper at ICLR 2021 [20] I. Kim, W. Baek, and S. Kim. Spatially attentive output layer for image classification. In IEEE conference on Computer Vision and Pattern Recognition (CVPR), 2020. [21] H. Larochelle and G. E. Hinton. Learning to combine foveal glimpses with a third-order boltzmann machine. In Advances in neural information processing systems (NeurIPS), pp. 1243–1251, 2010. [22] G. Liu, F. A. Reda, K. J. Shih, T.-C. Wang, A. Tao, and B. Catanzaro. Image inpainting for irregular holes using partial convolutions. In European Conference on Computer Vision, 2018. [23] G. Liu, K. J. Shih, T.-C. Wang, F. A. Reda, K. Sapra, Z. Yu, A. Tao, and B. Catanzaro. Partial convolution based padding. In arXiv preprint arXiv:1811.11718, 2018. [24] R. Liu and J. Jia. Reducing boundary artifacts in image deconvolution. In IEEE International Conference on Image Processing (ICIP), pp. 505–508, 2008. [25] R. Liu, J. Lehman, P. Molino, F. P. Such, E. Frank, A. Sergeev, and J. Yosinski. An intriguing failing of convolutional neural networks and the CoordConv solution. In Advances in Neural Information Processing Systems (NeurIPS), pp. 9605–9616, 2018. [26] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg. SSD: Single shot multibox detector. In European Conference on Computer Vision, pp. 21–37, 2016. [27] S. Lou, X. Jiang, and P. J. Scott. Fast algorithm for morphological filters. Journal of Physics: Conference Series, 311(1):012001, 2011. [28] W. Luo, Y. Li, R. Urtasun, and R. Zemel. Understanding the effective receptive field in deep convolutional neural networks. In Advances in Neural Information Processing Systems (NeurIPS), pp. 4898–4906, 2016. [29] R. Murase, M. Suganuma, and T. Okatani. How can cnns use image position for segmentation? arXiv preprint arXiv:2005.03463, 2020. [30] A.-D. Nguyen, S. Choi, W. Kim, S. Ahn, J. Kim, and S. Lee. Distribution padding in convolutional neural networks. In IEEE International Conference on Image Processing (ICIP), pp. 4275–4279, 2019. [31] D. Novotny, S. Albanie, D. Larlus, and A. Vedaldi. Semi-convolutional operators for instance segmentation. In European Conference on Computer Vision (ECCV), pp. 86–102, 2018. [32] A. Odena, V. Dumoulin, and C. Olah. Deconvolution and checkerboard artifacts. Distill, 1 (10):e3, 2016. [33] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, et al. PyTorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems (NeurIPS), pp. 8024–8035, 2019. [34] P. O. Pinheiro, T.-Y. Lin, R. Collobert, and P. Dollár. Learning to refine object segments. In European Conference on Computer Vision (ECCV), pp. 75–91, 2016. [35] S. Schubert, P. Neubert, J. Pöschmann, and P. Pretzel. Circular convolutional neural networks for panoramic images and laser data. In IEEE Intelligent Vehicles Symposium (IV), pp. 653– 660, 2019. [36] E. Shalnov. BSTLD-demo: A sample project to train and evaluate model on BSTLD. https: //github.com/e-sha/BSTLD_demo, 2019. [37] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations (ICLR), 2015. [38] G. Sundaramoorthi and T. E. Wang. Translation insensitive CNNs. arXiv preprint arXiv:1911.11238, 2019. 11 Published as a conference paper at ICLR 2021 [39] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In IEEE conference on Computer Vision and Pattern Recognition (CVPR), pp. 1–9, 2015. [40] S. Vashishth, S. Sanyal, V. Nitin, N. Agrawal, and P. Talukdar. InteractE: Improving convolution-based knowledge graph embeddings by increasing feature interactions. In AAAI conference on Artifical Intelligence, 2020. [41] S. Wu, G. Wang, P. Tang, F. Chen, and L. Shi. Convolution with even-sized kernels and symmetric padding. In Advances in Neural Information Processing Systems (NeurIPS), pp. 1192–1203, 2019. [42] F. Yu and V. Koltun. Multi-scale context aggregation by dilated convolutions. In International Conference on Learning Representations (ICLR), 2016. [43] R. Zhang. Making convolutional networks shift-invariant again. In International Conference on Machine Learning (ICML), 2019.",
  "stance": 0.5
 },
 {
  "url": "https://openreview.net/forum?id=qZzy5urZw9",
  "title": "Robust Overfitting may be mitigated by properly learned smoothening",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "A recent study (Rice et al., 2020) revealed overfitting to be a dominant phenomenon in adversarially robust training of deep networks, and that appropriate early-stopping of adversarial training (AT) could match the performance gains of most recent algorithmic improvements. This intriguing problem of robust overfitting motivates us to seek more remedies. As a pilot study, this paper investigates two empirical means to inject more learned smoothening during AT: one leveraging knowledge distillation and self-training to smooth the logits, the other performing stochastic weight averaging (Izmailov et al., 2018) to smooth the weights. Despite the embarrassing simplicity, the two approaches are surprisingly effective and hassle-free in mitigating robust overfitting. Experiments demonstrate that by plugging in them to AT, we can simultaneously boost the standard accuracy by 3.72% ∼ 6.68% and robust accuracy by 0.22% ∼ 2.03%, across multiple datasets (STL-10, SVHN, CIFAR-10, CIFAR-100, and Tiny ImageNet), perturbation types (`∞ and `2), and robustified methods (PGD, TRADES, and FSGM), establishing the new state-of-the-art bar in AT. We present systematic visualizations and analyses to dive into their possible working mechanisms. We also carefully exclude the possibility of gradient masking by evaluating our models’ robustness against transfer attacks. Codes are available at https: //github.com/VITA-Group/Alleviate-Robust-Overfitting.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=qbH974jKUVy",
  "title": "The role of Disentanglement in Generalisation",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Combinatorial generalisation — the ability to understand and produce novel combinations of familiar elements — is a core capacity of human intelligence that current AI systems struggle with. Recently, it has been suggested that learning disentangled representations may help address this problem. It is claimed that such representations should be able to capture the compositional structure of the world which can then be combined to support combinatorial generalisation. In this study, we systematically tested how the degree of disentanglement affects various forms of generalisation, including two forms of combinatorial generalisation that varied in difficulty. We trained three classes of variational autoencoders (VAEs) on two datasets on an unsupervised task by excluding combinations of generative factors during training. At test time we ask the models to reconstruct the missing combinations in order to measure generalisation performance. Irrespective of the degree of disentanglement, we found that the models supported only weak combinatorial generalisation. We obtained the same outcome when we directly input perfectly disentangled representations as the latents, and when we tested a model on a more complex task that explicitly required independent generative factors to be controlled. While learning disentangled representations does improve interpretability and sample efficiency in some downstream tasks, our results suggest that they are not sufficient for supporting more difficult forms of generalisation.",
  "stance": -0.5
 },
 {
  "url": "https://openreview.net/forum?id=rABUmU3ulQh",
  "title": "Learning to Generate 3D Shapes with Generative Cellular Automata",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "We present a probabilistic 3D generative model, named Generative Cellular Automata, which is able to produce diverse and high quality shapes. We formulate the shape generation process as sampling from the transition kernel of a Markov chain, where the sampling chain eventually evolves to the full shape of the learned distribution. The transition kernel employs the local update rules of cellular automata, effectively reducing the search space in a high-resolution 3D grid space by exploiting the connectivity and sparsity of 3D shapes. Our progressive generation only focuses on the sparse set of occupied voxels and their neighborhood, thus enabling the utilization of an expressive sparse convolutional network. We propose an effective training scheme to obtain the local homogeneous rule of generative cellular automata with sequences that are slightly different from the sampling chain but converge to the full shapes in the training data. Extensive experiments on probabilistic shape completion and shape generation demonstrate that our method achieves competitive performance against recent methods.",
  "stance": 0.9
 },
 {
  "url": "https://openreview.net/forum?id=tC6iW2UUbJf",
  "title": "What Makes Instance Discrimination Good for Transfer Learning?",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "Contrastive visual pretraining based on the instance discrimination pretext task has made significant progress. Notably, recent work on unsupervised pretraining has shown to surpass the supervised counterpart for finetuning downstream applications such as object detection and segmentation. It comes as a surprise that image annotations would be better left unused for transfer learning. In this work, we investigate the following problems: What makes instance discrimination pretraining good for transfer learning? What knowledge is actually learned and transferred from these models? From this understanding of instance discrimination, how can we better exploit human annotation labels for pretraining? Our findings are threefold. First, what truly matters for the transfer is low-level and mid-level representations, not high-level representations. Second, the intra-category invariance enforced by the traditional supervised model weakens transferability by increasing task misalignment. Finally, supervised pretraining can be strengthened by following an exemplar-based approach without explicit constraints among the instances within the same category.",
  "stance": 0.0
 },
 {
  "url": "https://openreview.net/forum?id=uXl3bZLkr3c",
  "title": "Tent: Fully Test-Time Adaptation by Entropy Minimization",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "A model must adapt itself to generalize to new and different data during testing. In this setting of fully test-time adaptation the model has only the test data and its own parameters. We propose to adapt by test entropy minimization (tent1): we optimize the model for confidence as measured by the entropy of its predictions. Our method estimates normalization statistics and optimizes channel-wise affine transformations to update online on each batch. Tent reduces generalization error for image classification on corrupted ImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on ImageNet-C. Tent handles source-free domain adaptation on digit recognition from SVHN to MNIST/MNIST-M/USPS, on semantic segmentation from GTA to Cityscapes, and on the VisDA-C benchmark. These results are achieved in one epoch of test-time optimization without altering training.",
  "stance": 1.0
 },
 {
  "url": "https://openreview.net/forum?id=wb3wxCObbRT",
  "title": "Growing Efficient Deep Networks by Structured Continuous Sparsification",
  "year": 2021,
  "venue": "ICLR",
  "abstract": "We develop an approach to growing deep network architectures over the course of training, driven by a principled combination of accuracy and sparsity objectives. Unlike existing pruning or architecture search techniques that operate on full-sized models or supernet architectures, our method can start from a small, simple seed architecture and dynamically grow and prune both layers and filters. By combining a continuous relaxation of discrete network structure optimization with a scheme for sampling sparse subnetworks, we produce compact, pruned networks, while also drastically reducing the computational expense of training. For example, we achieve 49.7% inference FLOPs and 47.4% training FLOPs savings compared to a baseline ResNet-50 on ImageNet, while maintaining 75.2% top-1 accuracy — all without any dedicated fine-tuning stage. Experiments across CIFAR, ImageNet, PASCAL VOC, and Penn Treebank, with convolutional networks for image classification and semantic segmentation, and recurrent networks for language modeling, demonstrate that we both train faster and produce more efficient networks than competing architecture pruning or search methods.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/Conferences/2010/papers/202.pdf",
  "title": "Exploiting Data-Independence for Fast Belief-Propagation.",
  "year": 2010,
  "venue": "ICML",
  "abstract": "Maximum a posteriori (MAP) inference in graphical models requires that we maximize the sum of two terms: a data-dependent term, encoding the conditional likelihood of a certain labeling given an observation, and a data-independent term, encoding some prior on labelings. Often, data-dependent factors contain fewer latent variables than dataindependent factors – for instance, many grid and tree-structured models contain only firstorder conditionals despite having pairwise priors. In this paper, we note that MAPinference in such models can be made substantially faster by appropriately preprocessing their data-independent terms. Our main result is to show that message-passing in any such pairwise model has an expected-case exponent of only 1.5 on the number of states per node, leading to significant improvements over existing quadratic-time solutions.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/Conferences/2010/papers/223.pdf",
  "title": "SVM Classifier Estimation from Group Probabilities.",
  "year": 2010,
  "venue": "ICML",
  "abstract": "A learning problem that has only recently gained attention in the machine learning community is that of learning a classifier from group probabilities. It is a learning task that lies somewhere between the well-known tasks of supervised and unsupervised learning, in the sense that for a set of observations we do not know the labels, but for some groups of observations, the frequency distribution of the label is known. This learning problem has important practical applications, for example in privacy-preserving data mining. This paper presents an approach to learn a classifier from group probabilities based on support vector regression and the idea of inverting a classifier calibration process. A detailed analysis will show that this new approach outperforms existing approaches.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/Conferences/2010/papers/227.pdf",
  "title": "Learning Sparse SVM for Feature Selection on Very High Dimensional Datasets.",
  "year": 2010,
  "venue": "ICML",
  "abstract": "A sparse representation of Support Vector Machines (SVMs) with respect to input features is desirable for many applications. In this paper, by introducing a 0-1 control variable to each input feature, l0-norm Sparse SVM (SSVM) is converted to a mixed integer programming (MIP) problem. Rather than directly solving this MIP, we propose an efficient cutting plane algorithm combining with multiple kernel learning to solve its convex relaxation. A global convergence proof for our method is also presented. Comprehensive experimental results on one synthetic and 10 real world datasets show that our proposed method can obtain better or competitive performance compared with existing SVM-based feature selection methods in term of sparsity and generalization performance. Moreover, our proposed method can effectively handle large-scale and extremely high dimensional problems.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/Conferences/2010/papers/233.pdf",
  "title": "Total Variation, Cheeger Cuts.",
  "year": 2010,
  "venue": "ICML",
  "abstract": "In this work, inspired by (Bühler & Hein, 2009), (Strang, 1983), and (Zhang et al., 2009), we give a continuous relaxation of the Cheeger cut problem on a weighted graph. We show that the relaxation is actually equivalent to the original problem. We then describe an algorithm for finding good cuts suggested by the similarities of the energy of the relaxed problem and various well studied energies in image processing. Finally we provide experimental validation of the proposed algorithm, demonstrating its efficiency in finding high quality cuts.",
  "stance": 0.9
 },
 {
  "url": "https://icml.cc/Conferences/2010/papers/352.pdf",
  "title": "Tree-Guided Group Lasso for Multi-Task Regression with Structured Sparsity.",
  "year": 2010,
  "venue": "ICML",
  "abstract": "We consider the problem of learning a sparse multi-task regression, where the structure in the outputs can be represented as a tree with leaf nodes as outputs and internal nodes as clusters of the outputs at multiple granularity. Our goal is to recover the common set of relevant inputs for each output cluster. Assuming that the tree structure is available as prior knowledge, we formulate this problem as a new multi-task regularized regression called tree-guided group lasso. Our structured regularization is based on a grouplasso penalty, where groups are defined with respect to the tree structure. We describe a systematic weighting scheme for the groups in the penalty such that each output variable is penalized in a balanced manner even if the groups overlap. We present an efficient optimization method that can handle a largescale problem. Using simulated and yeast datasets, we demonstrate that our method shows a superior performance in terms of both prediction errors and recovery of true sparsity patterns compared to other methods for multi-task learning.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/Conferences/2010/papers/505.pdf",
  "title": "Transfer Learning for Collective Link Prediction in Multiple Heterogenous Domains.",
  "year": 2010,
  "venue": "ICML",
  "abstract": "Link prediction is a key technique in many applications such as recommender systems, where potential links between users and items need to be predicted. A challenge in link prediction is the data sparsity problem. In this paper, we address this problem by jointly considering multiple heterogeneous link prediction tasks such as predicting links between users and different types of items including books, movies and songs, which we refer to as the collective link prediction (CLP) problem. We propose a nonparametric Bayesian framework for solving the CLP problem, which allows knowledge to be adaptively transferred across heterogeneous tasks while taking into account the similarities between tasks. We learn the inter-task similarity automatically. We also introduce link functions for different tasks to correct their biases and skewness of distributions in their link data. We conduct experiments on several real world datasets and demonstrate significant improvements over several existing state-of-the-art methods.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/Conferences/2010/papers/532.pdf",
  "title": "Learning the Linear Dynamical System with ASOS.",
  "year": 2010,
  "venue": "ICML",
  "abstract": "We develop a new algorithm, based on EM, for learning the Linear Dynamical System model. Called the method of Approximated Second-Order Statistics (ASOS) our approach achieves dramatically superior computational performance over standard EM through its use of approximations, which we justify with both intuitive explanations and rigorous convergence results. In particular, after an inexpensive precomputation phase, the iterations of ASOS can be performed in time independent of the length of the training dataset.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/Conferences/2010/papers/544.pdf",
  "title": "Active Learning for Networked Data.",
  "year": 2010,
  "venue": "ICML",
  "abstract": "We introduce a novel active learning algorithm for classification of network data. In this setting, training instances are connected by a set of links to form a network, the labels of linked nodes are correlated, and the goal is to exploit these dependencies and accurately label the nodes. This problem arises in many domains, including social and biological network analysis and document classification, and there has been much recent interest in methods that collectively classify the nodes in the network. While in many cases labeled examples are expensive, often network information is available. We show how an active learning algorithm can take advantage of network structure. Our algorithm effectively exploits the links between instances and the interaction between the local and collective aspects of a classifier to improve the accuracy of learning from fewer labeled examples. We experiment with two real-world benchmark collective classification domains, and show that we are able to achieve extremely accurate results even when only a small fraction of the data is labeled.",
  "stance": 0.7
 },
 {
  "url": "https://icml.cc/Conferences/2010/papers/556.pdf",
  "title": "A Fast Augmented Lagrangian Algorithm for Learning Low-Rank Matrices.",
  "year": 2010,
  "venue": "ICML",
  "abstract": "We propose a general and efficient algorithm for learning low-rank matrices. The proposed algorithm converges super-linearly and can keep the matrix to be learned in a compact factorized representation without the need of specifying the rank beforehand. Moreover, we show that the framework can be easily generalized to the problem of learning multiple matrices and general spectral regularization. Empirically we show that we can recover a 10,000×10,000 matrix from 1.2 million observations in about 5 minutes. Furthermore, we show that in a brain-computer interface problem, the proposed method can speed-up the optimization by two orders of magnitude against the conventional projected gradient method and produces more reliable solutions.",
  "stance": 0.9
 },
 {
  "url": "https://icml.cc/Conferences/2010/papers/587.pdf",
  "title": "Learning Efficiently with Approximate Inference via Dual Losses.",
  "year": 2010,
  "venue": "ICML",
  "abstract": "Many structured prediction tasks involve complex models where inference is computationally intractable, but where it can be well approximated using a linear programming relaxation. Previous approaches for learning for structured prediction (e.g., cuttingplane, subgradient methods, perceptron) repeatedly make predictions for some of the data points. These approaches are computationally demanding because each prediction involves solving a linear program to optimality. We present a scalable algorithm for learning for structured prediction. The main idea is to instead solve the dual of the structured prediction loss. We formulate the learning task as a convex minimization over both the weights and the dual variables corresponding to each data point. As a result, we can begin to optimize the weights even before completely solving any of the individual prediction problems. We show how the dual variables can be efficiently optimized using coordinate descent. Our algorithm is competitive with state-of-the-art methods such as stochastic subgradient and cutting-plane.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/Conferences/2010/papers/589.pdf",
  "title": "Bayes Optimal Multilabel Classification via Probabilistic Classifier Chains.",
  "year": 2010,
  "venue": "ICML",
  "abstract": "In the realm of multilabel classification (MLC), it has become an opinio communis that optimal predictive performance can only be achieved by learners that explicitly take label dependence into account. The goal of this paper is to elaborate on this postulate in a critical way. To this end, we formalize and analyze MLC within a probabilistic setting. Thus, it becomes possible to look at the problem from the point of view of risk minimization and Bayes optimal prediction. Moreover, inspired by our probabilistic setting, we propose a new method for MLC that generalizes and outperforms another approach, called classifier chains, that was recently introduced in the literature.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/Conferences/2010/papers/601.pdf",
  "title": "On the Interaction between Norm and Dimensionality: Multiple Regimes in Learning.",
  "year": 2010,
  "venue": "ICML",
  "abstract": "A learning problem might have several measures of complexity (e.g., norm and dimensionality) that affect the generalization error. What is the interaction between these complexities? Dimension-free learning theory bounds and parametric asymptotic analyses each provide a partial picture of the full learning curve. In this paper, we use high-dimensional asymptotics on two classical problems—mean estimation and linear regression—to explore the learning curve more completely. We show that these curves exhibit multiple regimes, where in each regime, the excess risk is controlled by a subset of the problem complexities.",
  "stance": 0.0
 },
 {
  "url": "https://icml.cc/Conferences/2010/papers/87.pdf",
  "title": "A Conditional Random Field for Multiple-Instance Learning.",
  "year": 2010,
  "venue": "ICML",
  "abstract": "We present MI-CRF, a conditional random field (CRF) model for multiple instance learning (MIL). MI-CRF models bags as nodes in a CRF with instances as their states. It combines discriminative unary instance classifiers and pairwise dissimilarity measures. We show that both forces improve the classification performance. Unlike other approaches, MI-CRF considers all bags jointly during training as well as during testing. This makes it possible to classify test bags in an imputation setup. The parameters of MI-CRF are learned using constraint generation. Furthermore, we show that MI-CRF can incorporate previous MIL algorithms to improve on their results. MICRF obtains competitive results on five standard MIL datasets.",
  "stance": 0.5
 },
 {
  "url": "https://icml.cc/Conferences/2010/papers/903.pdf",
  "title": "The Role of Machine Learning in Business Optimization.",
  "year": 2010,
  "venue": "ICML",
  "abstract": "In a trend that reflects the increasing demand for intelligent applications driven by business data, IBM today is building out a significant number of applications that leverage machine learning technologies to optimize business process decisions. This talk highlights this trend; and describes the many different ways in which leading edge machine learning concepts are being utilized in business applications developed by IBM for its internal use and for clients.",
  "stance": 0.0
 },
 {
  "url": "https://icml.cc/Conferences/2010/papers/906.pdf",
  "title": "FAB-MAP: Appearance-Based Place Recognition and Mapping using a Learned Visual Vocabulary Model.",
  "year": 2010,
  "venue": "ICML",
  "abstract": "We present an overview of FAB-MAP, an algorithm for place recognition and mapping developed for infrastructure-free mobile robot navigation in large environments. The system allows a robot to identify when it is revisiting a previously seen location, on the basis of imagery captured by the robot’s camera. We outline a complete probabilistic framework for the task, which is applicable even in visually repetitive environments where many locations may appear identical. Our work introduces a number of technical innovations notably we demonstrate that place recognition performance can be improved by learning an approximation to the joint distribution over visual elements. We also investigate several principled approaches to making the system robust in visually repetitive environments, and define an efficient bail-out strategy for multi-hypothesis testing to improve system speed. Our model has been shown to substantially outperform standard tf-idf ranking on our task of interest. We demonstrate the system performing reliable online appearance mapping and loop closure detection over a 1,000 km trajectory, with mean filter update times of 14ms.",
  "stance": 0.9
 },
 {
  "url": "https://icml.cc/2011/papers/108_icmlpaper.pdf",
  "title": "Dynamic Tree Block Coordinate Ascent.",
  "year": 2011,
  "venue": "ICML",
  "abstract": "This paper proposes a novel Linear Programming (LP) based algorithm, called Dynamic Tree-Block Coordinate Ascent (DTBCA), for performing maximum a posteriori (MAP) inference in probabilistic graphical models. Unlike traditional message passing algorithms, which operate uniformly on the whole factor graph, our method dynamically chooses regions of the factor graph on which to focus message-passing efforts. We propose two criteria for selecting regions, including an efficiently computable upperbound on the increase in the objective possible by passing messages in any particular region. This bound is derived from the theory of primal-dual methods from combinatorial optimization, and the forest that maximizes the bounds can be chosen efficiently using a maximum-spanning-tree-like algorithm. Experimental results show that our dynamic schedules significantly speed up state-of-theart LP-based message-passing algorithms on a wide variety of real-world problems.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2011/papers/125_icmlpaper.pdf",
  "title": "Parsing Natural Scenes and Natural Language with Recursive Neural Networks.",
  "year": 2011,
  "venue": "ICML",
  "abstract": "Recursive structure is commonly found in the inputs of different modalities such as natural scene images or natural language sentences. Discovering this recursive structure helps us to not only identify the units that an image or sentence contains but also how they interact to form a whole. We introduce a max-margin structure prediction architecture based on recursive neural networks that can successfully recover such structure both in complex scene images as well as sentences. The same algorithm can be used both to provide a competitive syntactic parser for natural language sentences from the Penn Treebank and to outperform alternative approaches for semantic scene segmentation, annotation and classification. For segmentation and annotation our algorithm obtains a new level of state-of-theart performance on the Stanford background dataset (78.1%). The features from the image parse tree outperform Gist descriptors for scene classification by 4%.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2011/papers/150_icmlpaper.pdf",
  "title": "An Augmented Lagrangian Approach to Constrained MAP Inference.",
  "year": 2011,
  "venue": "ICML",
  "abstract": "We propose a new algorithm for approximate MAP inference on factor graphs, by combining augmented Lagrangian optimization with the dual decomposition method. Each slave subproblem is given a quadratic penalty, which pushes toward faster consensus than in previous subgradient approaches. Our algorithm is provably convergent, parallelizable, and suitable for fine decompositions of the graph. We show how it can efficiently handle problems with (possibly global) structural constraints via simple sort operations. Experiments on synthetic and real-world data show that our approach compares favorably with the state-of-the-art.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2011/papers/210_icmlpaper.pdf",
  "title": "On optimization methods for deep learning.",
  "year": 2011,
  "venue": "ICML",
  "abstract": "The predominant methodology in training deep learning advocates the use of stochastic gradient descent methods (SGDs). Despite its ease of implementation, SGDs are difficult to tune and parallelize. These problems make it challenging to develop, debug and scale up deep learning algorithms with SGDs. In this paper, we show that more sophisticated off-the-shelf optimization methods such as Limited memory BFGS (L-BFGS) and Conjugate gradient (CG) with line search can significantly simplify and speed up the process of pretraining deep algorithms. In our experiments, the difference between LBFGS/CG and SGDs are more pronounced if we consider algorithmic extensions (e.g., sparsity regularization) and hardware extensions (e.g., GPUs or computer clusters). Our experiments with distributed optimization support the use of L-BFGS with locally connected networks and convolutional neural networks. Using L-BFGS, our convolutional network model achieves 0.69% on the standard MNIST dataset. This is a state-of-theart result on MNIST among algorithms that do not use distortions or pretraining.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2011/papers/216_icmlpaper.pdf",
  "title": "On the Necessity of Irrelevant Variables.",
  "year": 2011,
  "venue": "ICML",
  "abstract": "This work explores the effects of relevant and irrelevant boolean variables on the accuracy of classifiers. The analysis uses the assumption that the variables are conditionally independent given the class, and focuses on a natural family of learning algorithms for such sources when the relevant variables have a small advantage over random guessing. The main result is that algorithms relying predominately on irrelevant variables have error probabilities that quickly go to 0 in situations where algorithms that limit the use of irrelevant variables have errors bounded below by a positive constant. We also show that accurate learning is possible even when there are so few examples that one cannot determine with high confidence whether or not any individual variable is relevant.",
  "stance": -0.3
 },
 {
  "url": "https://icml.cc/2011/papers/323_icmlpaper.pdf",
  "title": "PILCO: A Model-Based and Data-Efficient Approach to Policy Search.",
  "year": 2011,
  "venue": "ICML",
  "abstract": "In this paper, we introduce pilco, a practical, data-efficient model-based policy search method. Pilco reduces model bias, one of the key problems of model-based reinforcement learning, in a principled way. By learning a probabilistic dynamics model and explicitly incorporating model uncertainty into long-term planning, pilco can cope with very little data and facilitates learning from scratch in only a few trials. Policy evaluation is performed in closed form using state-ofthe-art approximate inference. Furthermore, policy gradients are computed analytically for policy improvement. We report unprecedented learning efficiency on challenging and high-dimensional control tasks.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2011/papers/374_icmlpaper.pdf",
  "title": "Infinite SVM: a Dirichlet Process Mixture of Large-margin Kernel Machines.",
  "year": 2011,
  "venue": "ICML",
  "abstract": "We present Infinite SVM (iSVM), a Dirichlet process mixture of large-margin kernel machines for multi-way classification. An iSVM enjoys the advantages of both Bayesian nonparametrics in handling the unknown number of mixing components, and large-margin kernel machines in robustly capturing local nonlinearity of complex data. We develop an efficient variational learning algorithm for posterior inference of iSVM, and we demonstrate the advantages of iSVM over Dirichlet process mixture of generalized linear models and other benchmarks on both synthetic and real Flickr image classification datasets.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2011/papers/38_icmlpaper.pdf",
  "title": "A Graphbased Framework for Multi-Task Multi-View Learning.",
  "year": 2011,
  "venue": "ICML",
  "abstract": "Many real-world problems exhibit dualheterogeneity. A single learning task might have features in multiple views (i.e., feature heterogeneity); multiple learning tasks might be related with each other through one or more shared views (i.e., task heterogeneity). Existing multi-task learning or multi-view learning algorithms only capture one type of heterogeneity. In this paper, we introduce Multi-Task MultiView (MTV ) learning for such complicated learning problems with both feature heterogeneity and task heterogeneity. We propose a graph-based framework (GraM) to take full advantage of the dual-heterogeneous nature. Our framework has a natural connection to Reproducing Kernel Hilbert Space (RKHS). Furthermore, we propose an iterative algorithm (IteM) for GraM framework, and analyze its optimality, convergence and time complexity. Experimental results on various real data sets demonstrate its effectiveness.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2011/papers/419_icmlpaper.pdf",
  "title": "Clusterpath: an Algorithm for Clustering using Convex Fusion Penalties.",
  "year": 2011,
  "venue": "ICML",
  "abstract": "We present a new clustering algorithm by proposing a convex relaxation of hierarchical clustering, which results in a family of objective functions with a natural geometric interpretation. We give efficient algorithms for calculating the continuous regularization path of solutions, and discuss relative advantages of the parameters. Our method experimentally gives state-ofthe-art results similar to spectral clustering for non-convex clusters, and has the added benefit of learning a tree structure from the data.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2011/papers/491_icmlpaper.pdf",
  "title": "Large-Scale Learning of Embeddings with Reconstruction Sampling.",
  "year": 2011,
  "venue": "ICML",
  "abstract": "In this paper, we present a novel method to speed up the learning of embeddings for large-scale learning tasks involving very sparse data, as is typically the case for Natural Language Processing tasks. Our speed-up method has been developed in the context of Denoising Auto-encoders, which are trained in a purely unsupervised way to capture the input distribution, and learn embeddings for words and text similar to earlier neural language models. The main contribution is a new method to approximate reconstruction error by a sampling procedure. We show how this approximation can be made to obtain an unbiased estimator of the training criterion, and we show how it can be leveraged to make learning much more computationally efficient. We demonstrate the effectiveness of this method on the Amazon and RCV1 NLP datasets. Instead of reducing vocabulary size to make learning practical, our method allows us to train using very large vocabularies. In particular, reconstruction sampling requires 22x less training time on the full Amazon dataset.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2011/papers/546_icmlpaper.pdf",
  "title": "A Unified Probabilistic Model for Global and Local Unsupervised Feature Selection.",
  "year": 2011,
  "venue": "ICML",
  "abstract": "Existing algorithms for joint clustering and feature selection can be categorized as either global or local approaches. Global methods select a single cluster-independent subset of features, whereas local methods select cluster-specific subsets of features. In this paper, we present a unified probabilistic model that can perform both global and local feature selection for clustering. Our approach is based on a hierarchical beta-Bernoulli prior combined with a Dirichlet process mixture model. We obtain global or local feature selection by adjusting the variance of the beta prior. We provide a variational inference algorithm for our model. In addition to simultaneously learning the clusters and features, this Bayesian formulation allows us to learn both the number of clusters and the number of features to retain. Experiments on synthetic and real data show that our unified model can find global and local features and cluster data as well as competing methods of each type.",
  "stance": 0.5
 },
 {
  "url": "https://icml.cc/2011/papers/54_icmlpaper.pdf",
  "title": "Learning Output Kernels with Block Coordinate Descent.",
  "year": 2011,
  "venue": "ICML",
  "abstract": "We propose a method to learn simultaneously a vector-valued function and a kernel between its components. The obtained kernel can be used both to improve learning performance and to reveal structures in the output space which may be important in their own right. Our method is based on the solution of a suitable regularization problem over a reproducing kernel Hilbert space of vector-valued functions. Although the regularized risk functional is non-convex, we show that it is invex, implying that all local minimizers are global minimizers. We derive a block-wise coordinate descent method that efficiently exploits the structure of the objective functional. Then, we empirically demonstrate that the proposed method can improve classification accuracy. Finally, we provide a visual interpretation of the learned kernel matrix for some well known datasets.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2011/papers/9_icmlpaper.pdf",
  "title": "Efficient Sparse Modeling with Automatic Feature Grouping.",
  "year": 2011,
  "venue": "ICML",
  "abstract": "The grouping of features is highly beneficial in learning with high-dimensional data. It reduces the variance in the estimation and improves the stability of feature selection, leading to improved generalization. Moreover, it can also help in data understanding and interpretation. OSCAR is a recent sparse modeling tool that achieves this by using a `1-regularizer and a pairwise `∞-regularizer. However, its optimization is computationally expensive. In this paper, we propose an efficient solver based on the accelerated gradient methods. We show that its key projection step can be solved by a simple iterative group merging algorithm. It is highly efficient and reduces the empirical time complexity from O(d ∼ d) for the existing solvers to just O(d), where d is the number of features. Experimental results on toy and real-world data sets demonstrate that OSCAR is a competitive sparse modeling approach with the added ability of automatic feature grouping.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2012/papers/25.pdf",
  "title": "Quasi-Newton Methods: A New Direction.",
  "year": 2012,
  "venue": "ICML",
  "abstract": "Four decades after their invention, quasiNewton methods are still state of the art in unconstrained numerical optimization. Although not usually interpreted thus, these are learning algorithms that fit a local quadratic approximation to the objective function. We show that many, including the most popular, quasi-Newton methods can be interpreted as approximations of Bayesian linear regression under varying prior assumptions. This new notion elucidates some shortcomings of classical algorithms, and lights the way to a novel nonparametric quasi-Newton method, which is able to make more efficient use of available information at computational cost similar to its predecessors.",
  "stance": 0.7
 },
 {
  "url": "https://icml.cc/2012/papers/274.pdf",
  "title": "Modeling Latent Variable Uncertainty for Loss-based Learning.",
  "year": 2012,
  "venue": "ICML",
  "abstract": "We consider the problem of parameter estimation using weakly supervised datasets, where a training sample consists of the input and a partially specified annotation, which we refer to as the output. The missing information in the annotation is modeled using latent variables. Previous methods overburden a single distribution with two separate tasks: (i) modeling the uncertainty in the latent variables during training; and (ii) making accurate predictions for the output and the latent variables during testing. We propose a novel framework that separates the demands of the two tasks using two distributions: (i) a conditional distribution to model the uncertainty of the latent variables for a given input-output pair; and (ii) a delta distribution to predict the output and the latent variables for a given input. During learning, we encourage agreement between the two distributions by minimizing a loss-based dissimilarity coefficient. Our approach generalizes latent svm in two important ways: (i) it models the uncertainty over latent variables instead of relying on a pointwise estimate; and (ii) it allows the use of loss functions that depend on latent variables, which greatly increases its applicability. We demonstrate the efficacy of our approach on two challenging problems—object detection and action detection—using publicly available datasets.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2012/papers/349.pdf",
  "title": "Unachievable Region in Precision-Recall Space and Its Effect on Empirical Evaluation.",
  "year": 2012,
  "venue": "ICML",
  "abstract": "Precision-recall (PR) curves and the areas under them are widely used to summarize machine learning results, especially for data sets exhibiting class skew. They are often used analogously to ROC curves and the area under ROC curves. It is known that PR curves vary as class skew changes. What was not recognized before this paper is that there is a region of PR space that is completely unachievable, and the size of this region depends only on the skew. This paper precisely characterizes the size of that region and discusses its implications for empirical evaluation methodology in machine learning.",
  "stance": -0.3
 },
 {
  "url": "https://icml.cc/2012/papers/405.pdf",
  "title": "Evaluating Bayesian and L1 Approaches for Sparse Unsupervised Learning .",
  "year": 2012,
  "venue": "ICML",
  "abstract": "The use of L1 regularisation for sparse learning has generated immense research interest, with many successful applications in diverse areas such as signal acquisition, image coding, genomics and collaborative filtering. While existing work highlights the many advantages of L1 methods, in this paper we find that L1 regularisation often dramatically under-performs in terms of predictive performance when compared to other methods for inferring sparsity. We focus on unsupervised latent variable models, and develop L1 minimising factor models, Bayesian variants of “L1”, and Bayesian models with a stronger L0-like sparsity induced through spike-and-slab distributions. These spikeand-slab Bayesian factor models encourage sparsity while accounting for uncertainty in a principled manner, and avoid unnecessary shrinkage of non-zero values. We demonstrate on a number of data sets that in practice spike-and-slab Bayesian methods outperform L1 minimisation, even on a computational budget. We thus highlight the need to re-assess the wide use of L1 methods in sparsity-reliant applications, particularly when we care about generalising to previously unseen data, and provide an alternative that, over many varying conditions, provides improved generalisation performance. Appearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).",
  "stance": 0.9
 },
 {
  "url": "https://icml.cc/2012/papers/486.pdf",
  "title": "Copula Mixture Model for Dependency-seeking Clustering.",
  "year": 2012,
  "venue": "ICML",
  "abstract": "We introduce a copula mixture model to perform dependency-seeking clustering when cooccurring samples from different data sources are available. The model takes advantage of the great flexibility offered by the copulas framework to extend mixtures of Canonical Correlation Analysis to multivariate data with arbitrary continuous marginal densities. We formulate our model as a non-parametric Bayesian mixture, while providing efficient MCMC inference. Experiments on synthetic and real data demonstrate that the increased flexibility of the copula mixture significantly improves the clustering and the interpretability of the results.",
  "stance": 0.7
 },
 {
  "url": "https://icml.cc/2012/papers/501.pdf",
  "title": "Fast classification using sparse decision DAGs.",
  "year": 2012,
  "venue": "ICML",
  "abstract": "In this paper we propose an algorithm that builds sparse decision DAGs (directed acyclic graphs) from a list of base classifiers provided by an external learning method such as AdaBoost. The basic idea is to cast the DAG design task as a Markov decision process. Each instance can decide to use or to skip each base classifier, based on the current state of the classifier being built. The result is a sparse decision DAG where the base classifiers are selected in a data-dependent way. The method has a single hyperparameter with a clear semantics of controlling the accuracy/speed trade-off. The algorithm is competitive with state-of-the-art cascade detectors on three object-detection benchmarks, and it clearly outperforms them when there is a small number of base classifiers. Unlike cascades, it is also readily applicable for multi-class classification. Using the multi-class setup, we show on a benchmark Web page ranking data set that we can significantly improve the decision speed without harming the performance of the ranker.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2012/papers/507.pdf",
  "title": "Efficient Structured Prediction with Latent Variables for General Graphical Models.",
  "year": 2012,
  "venue": "ICML",
  "abstract": "In this paper we propose a unified framework for structured prediction with latent variables which includes hidden conditional random fields and latent structured support vector machines as special cases. We describe a local entropy approximation for this general formulation using duality, and derive an efficient message passing algorithm that is guaranteed to converge. We demonstrate its effectiveness in the tasks of image segmentation as well as 3D indoor scene understanding from single images, showing that our approach is superior to latent structured support vector machines and hidden conditional random fields.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2012/papers/625.pdf",
  "title": "On causal and anticausal learning.",
  "year": 2012,
  "venue": "ICML",
  "abstract": "We consider the problem of function estimation in the case where an underlying causal model can be inferred. This has implications for popular scenarios such as covariate shift, concept drift, transfer learning and semi-supervised learning. We argue that causal knowledge may facilitate some approaches for a given problem, and rule out others. In particular, we formulate a hypothesis for when semi-supervised learning can help, and corroborate it with empirical results.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2012/papers/665.pdf",
  "title": "Lognormal and Gamma Mixed Negative Binomial Regression.",
  "year": 2012,
  "venue": "ICML",
  "abstract": "In regression analysis of counts, a lack of simple and efficient algorithms for posterior computation has made Bayesian approaches appear unattractive and thus underdeveloped. We propose a lognormal and gamma mixed negative binomial (NB) regression model for counts, and present efficient closed-form Bayesian inference; unlike conventional Poisson models, the proposed approach has two free parameters to include two different kinds of random effects, and allows the incorporation of prior information, such as sparsity in the regression coefficients. By placing a gamma distribution prior on the NB dispersion parameter r, and connecting a lognormal distribution prior with the logit of the NB probability parameter p, efficient Gibbs sampling and variational Bayes inference are both developed. The closed-form updates are obtained by exploiting conditional conjugacy via both a compound Poisson representation and a Polya-Gamma distribution based data augmentation approach. The proposed Bayesian inference can be implemented routinely, while being easily generalizable to more complex settings involving multivariate dependence structures. The algorithms are illustrated using real examples.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2012/papers/690.pdf",
  "title": "Learning Task Grouping and Overlap in Multi-task Learning.",
  "year": 2012,
  "venue": "ICML",
  "abstract": "In the paradigm of multi-task learning, multiple related prediction tasks are learned jointly, sharing information across the tasks. We propose a framework for multi-task learning that enables one to selectively share the information across the tasks. We assume that each task parameter vector is a linear combination of a finite number of underlying basis tasks. The coefficients of the linear combination are sparse in nature and the overlap in the sparsity patterns of two tasks controls the amount of sharing across these. Our model is based on the assumption that task parameters within a group lie in a low dimensional subspace but allows the tasks in different groups to overlap with each other in one or more bases. Experimental results on four datasets show that our approach outperforms competing methods.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2012/papers/718.pdf",
  "title": "Large-Scale Feature Learning With Spike-and-Slab Sparse Coding.",
  "year": 2012,
  "venue": "ICML",
  "abstract": "We consider the problem of object recognition with a large number of classes. In order to overcome the low amount of labeled examples available in this setting, we introduce a new feature learning and extraction procedure based on a factor model we call spike-and-slab sparse coding (S3C). Prior work on S3C has not prioritized the ability to exploit parallel architectures and scale S3C to the enormous problem sizes needed for object recognition. We present a novel inference procedure for appropriate for use with GPUs which allows us to dramatically increase both the training set size and the amount of latent factors that S3C may be trained with. We demonstrate that this approach improves upon the supervised learning capabilities of both sparse coding and the spike-and-slab Restricted Boltzmann Machine (ssRBM) on the CIFAR-10 dataset. We use the CIFAR-100 dataset to demonstrate that our method scales to large numbers of classes better than previous methods. Finally, we use our method to win the NIPS 2011 Workshop on Challenges In Learning Hierarchical Models’ Transfer Learning Challenge.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2012/papers/726.pdf",
  "title": "Discovering Support and Affiliated Features from Very High Dimensions.",
  "year": 2012,
  "venue": "ICML",
  "abstract": "In this paper, a novel learning paradigm is presented to automatically identify groups of informative and correlated features from very high dimensions. Specifically, we explicitly incorporate correlation measures as constraints and then propose an efficient embedded feature selection method using recently developed cutting plane strategy. The benefits of the proposed algorithm are two-folds. First, it can identify the optimal discriminative and uncorrelated feature subset to the output labels, denoted here as Support Features, which brings about significant improvements in prediction performance over other state of the art feature selection methods considered in the paper. Second, during the learning process, the underlying group structures of correlated features associated with each support feature, denoted as Affiliated Features, can also be discovered without any additional cost. These affiliated features serve to improve the interpretations on the learning tasks. Extensive empirical studies on both synthetic and very high dimensional real-world datasets verify the validity and efficiency of the proposed method.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2012/papers/763.pdf",
  "title": "Using CCA to improve CCA: A new spectral method for estimating vector models of words.",
  "year": 2012,
  "venue": "ICML",
  "abstract": "Unlabeled data is often used to learn representations which can be used to supplement baseline features in a supervised learner. For example, for text applications where the words lie in a very high dimensional space (the size of the vocabulary), one can learn a low rank “dictionary” by an eigendecomposition of the word co-occurrence matrix (e.g. using PCA or CCA). In this paper, we present a new spectral method based on CCA to learn an eigenword dictionary. Our improved procedure computes two set of CCAs, the first one between the left and right contexts of the given word and the second one between the projections resulting from this CCA and the word itself. We prove theoretically that this two-step procedure has lower sample complexity than the simple single step procedure and also illustrate the empirical efficacy of our approach and the richness of representations learned by our Two Step CCA (TSCCA) procedure on the tasks of POS tagging and sentiment classification.",
  "stance": 1.0
 },
 {
  "url": "https://icml.cc/2012/papers/81.pdf",
  "title": "Bayesian Nonexhaustive Learning for Online Discovery and Modeling of Emerging Classes.",
  "year": 2012,
  "venue": "ICML",
  "abstract": "We present a framework for online inference in the presence of a nonexhaustively defined set of classes that incorporates supervised classification with class discovery and modeling. A Dirichlet process prior (DPP) model defined over class distributions ensures that both known and unknown class distributions originate according to a common base distribution. In an attempt to automatically discover potentially interesting class formations, the prior model is coupled with a suitably chosen data model, and sequential Monte Carlo sampling is used to perform online inference. Our research is driven by a biodetection application, where a new class of pathogen may suddenly appear, and the rapid increase in the number of samples originating from this class indicates the onset of an outbreak.",
  "stance": 0.3
 },
 {
  "url": "https://icml.cc/2012/papers/863.pdf",
  "title": "Predicting Consumer Behavior in Commerce Search.",
  "year": 2012,
  "venue": "ICML",
  "abstract": "Traditional approaches to ranking in web search follow the paradigm of rank-by-score: a learned function gives each query-URL combination an absolute score and URLs are ranked according to this score. This paradigm ensures that if the score of one URL is better than another then one will always be ranked higher than the other. Scoring contradicts prior work in behavioral economics that preference between items depends not only on the items but also on the presented alternatives. Thus, for the same query, preference between items A and B may depend on the presence or absence of item C. We propose a new model of ranking, the Random Shopper Model, that allows and explains such behavior. In this model, each feature is viewed as a Markov chain over the items to be ranked, and the goal is to find a weighting of the features that best reflects their importance. We show that our model can be learned under the empirical risk minimization framework, and give an efficient learning algorithm. Experiments on commerce search logs demonstrate that our algorithm outperforms scoring-based approaches including regression and listwise ranking.",
  "stance": 0.8
 },
 {
  "url": "https://icml.cc/2012/papers/94.pdf",
  "title": "Manifold Relevance Determination.",
  "year": 2012,
  "venue": "ICML",
  "abstract": "In this paper we present a fully Bayesian latent variable model which exploits conditional nonlinear (in)-dependence structures to learn an efficient latent representation. The latent space is factorized to represent shared and private information from multiple views of the data. In contrast to previous approaches, we introduce a relaxation to the discrete segmentation and allow for a “softly” shared latent space. Further, Bayesian techniques allow us to automatically estimate the dimensionality of the latent spaces. The model is capable of capturing structure underlying extremely high dimensional spaces. This is illustrated by modelling unprocessed images with tenths of thousands of pixels. This also allows us to directly generate novel images from the trained model by sampling from the discovered latent spaces. We also demonstrate the model by prediction of human pose in an ambiguous setting. Our Bayesian framework allows us to perform disambiguation in a principled manner by including latent space priors which incorporate the dynamic nature of the data.",
  "stance": 0.5
 },
 {
  "url": "https://proceedings.mlr.press/v28/balasubramanian13.pdf",
  "title": "Smooth Sparse Coding via Marginal Regression for Learning Sparse Representations",
  "year": 2013,
  "venue": "ICML",
  "abstract": "We propose and analyze a novel framework for learning sparse representations, based on two statistical techniques: kernel smoothing and marginal regression. The proposed approach provides a flexible framework for incorporating feature similarity or temporal information present in data sets, via nonparametric kernel smoothing. We provide generalization bounds for dictionary learning using smooth sparse coding and show how the sample complexity depends on the L1 norm of kernel function used. Furthermore, we propose using marginal regression for obtaining sparse codes, which significantly improves the speed and allows one to scale to large dictionary sizes easily. We demonstrate the advantages of the proposed approach, both in terms of accuracy and speed by extensive experimentation on several real data sets. In addition, we demonstrate how the proposed approach can be used for improving semisupervised sparse coding.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v28/bengio13.pdf",
  "title": "Better Mixing via Deep Representations",
  "year": 2013,
  "venue": "ICML",
  "abstract": "It has been hypothesized, and supported with experimental evidence, that deeper representations, when well trained, tend to do a better job at disentangling the underlying factors of variation. We study the following related conjecture: better representations, in the sense of better disentangling, can be exploited to produce Markov chains that mix faster between modes. Consequently, mixing between modes would be more efficient at higher levels of representation. To better understand this, we propose a secondary conjecture: the higher-level samples fill more uniformly the space they occupy and the highdensity manifolds tend to unfold when represented at higher levels. The paper discusses these hypotheses and tests them experimentally through visualization and measurements of mixing between modes and interpolating between samples.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v28/koppula13.pdf",
  "title": "Learning Spatio-Temporal Structure from RGB-D Videos for Human Activity Detection and Anticipation",
  "year": 2013,
  "venue": "ICML",
  "abstract": "We consider the problem of detecting past activities as well as anticipating which activity will happen in the future and how. We start by modeling the rich spatio-temporal relations between human poses and objects (called affordances) using a conditional random field (CRF). However, because of the ambiguity in the temporal segmentation of the sub-activities that constitute an activity, in the past as well as in the future, multiple graph structures are possible. In this paper, we reason about these alternate possibilities by reasoning over multiple possible graph structures. We obtain them by approximating the graph with only additive features, which lends to efficient dynamic programming. Starting with this proposal graph structure, we then design moves to obtain several other likely graph structures. We then show that our approach improves the state-of-the-art significantly for detecting past activities as well as for anticipating future activities, on a dataset of 120 activity videos collected from four subjects.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v28/nguyen13.pdf",
  "title": "Online Feature Selection for Model-based Reinforcement Learning",
  "year": 2013,
  "venue": "ICML",
  "abstract": "We propose a new framework for learning the world dynamics of feature-rich environments in model-based reinforcement learning. The main idea is formalized as a new, factored state-transition representation that supports efficient online-learning of the relevant features. We construct the transition models through predicting how the actions change the world. We introduce an online sparse coding learning technique for feature selection in high-dimensional spaces. We derive theoretical guarantees for our framework and empirically demonstrate its practicality in both simulated and real robotics domains.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v28/nguyen13a.pdf",
  "title": "Algorithms for Direct 0–1 Loss Optimization in Binary Classification",
  "year": 2013,
  "venue": "ICML",
  "abstract": "While convex losses for binary classification are attractive due to the existence of numerous (provably) efficient methods for finding their global optima, they are sensitive to outliers. On the other hand, while the nonconvex 0–1 loss is robust to outliers, it is NP-hard to optimize and thus rarely directly optimized in practice. In this paper, however, we do just that: we explore a variety of practical methods for direct (approximate) optimization of the 0–1 loss based on branch and bound search, combinatorial search, and coordinate descent on smooth, differentiable relaxations of 0–1 loss. Empirically, we compare our proposed algorithms to logistic regression, SVM, and the Bayes point machine showing that the proposed 0–1 loss optimization algorithms perform at least as well and offer a clear advantage in the presence of outliers. To this end, we believe this work reiterates the importance of 0–1 loss and its robustness properties while challenging the notion that it is difficult to directly optimize.",
  "stance": 0.5
 },
 {
  "url": "https://proceedings.mlr.press/v28/sutskever13.pdf",
  "title": "On the importance of initialization and momentum in deep learning",
  "year": 2013,
  "venue": "ICML",
  "abstract": "Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods su ce for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.",
  "stance": 0.3
 },
 {
  "url": "https://proceedings.mlr.press/v28/wang13b.pdf",
  "title": "Subproblem-Tree Calibration: A Unified Approach to Max-Product Message Passing",
  "year": 2013,
  "venue": "ICML",
  "abstract": "Max-product (max-sum) message passing algorithms are widely used for MAP inference in MRFs. It has many variants sharing a common flavor of passing “messages” over some graph-object. Recent advances revealed that its convergent versions (such as MPLP, MSD, TRW-S) can be viewed as performing block coordinate descent (BCD) in a dual objective. That is, each BCD step achieves dual-optimal w.r.t. a block of dual variables (messages), thereby decreases the dual objective monotonically. However, most existing algorithms are limited to updating blocks selected in rather restricted ways. In this paper, we show a “unified” message passing algorithm that: (a) subsumes MPLP, MSD, and TRW-S as special cases when applied to their respective choices of dual objective and blocks, and (b) is able to perform BCD under much more flexible choices of blocks (including very large blocks) as well as the dual objective itself (that arise from an arbitrary dual decomposition).",
  "stance": 0.4
 },
 {
  "url": "https://proceedings.mlr.press/v28/willemvandemeent13.pdf",
  "title": "Hierarchically-coupled hidden Markov models for learning kinetic rates from single-molecule data",
  "year": 2013,
  "venue": "ICML",
  "abstract": "We address the problem of analyzing sets of noisy time-varying signals that all report on the same process but confound straightforward analyses due to complex inter-signal heterogeneities and measurement artifacts. In particular we consider single-molecule experiments which indirectly measure the distinct steps in a biomolecular process via observations of noisy time-dependent signals such as a fluorescence intensity or bead position. Straightforward hidden Markov model (HMM) analyses attempt to characterize such processes in terms of a set of conformational states, the transitions that can occur between these states, and the associated rates at which those transitions occur; but require ad-hoc post-processing steps to combine multiple signals. Here we develop a hierarchically coupled HMM that allows experimentalists to deal with inter-signal variability in a principled and automatic way. Our approach is a generalized expectation maximization hyperparameter point estimation procedure with variational Bayes at the level of individual time series that learns an single interpretable representation of the overall data generating process.",
  "stance": 0.2
 },
 {
  "url": "https://proceedings.mlr.press/v28/zhai13.pdf",
  "title": "Online Latent Dirichlet Allocation with Infinite Vocabulary",
  "year": 2013,
  "venue": "ICML",
  "abstract": "Topic models based on latent Dirichlet allocation (LDA) assume a predefined vocabulary. This is reasonable in batch settings but not reasonable for streaming and online settings. To address this lacuna, we extend LDA by drawing topics from a Dirichlet process whose base distribution is a distribution over all strings rather than from a finite Dirichlet. We develop inference using online variational inference and—to only consider a finite number of words for each topic—propose heuristics to dynamically order, expand, and contract the set of words we consider in our vocabulary. We show our model can successfully incorporate new words and that it performs better than topic models with finite vocabularies in evaluations of topic quality and classification performance.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v32/eban14.pdf",
  "title": "Discrete Chebyshev Classifiers",
  "year": 2014,
  "venue": "ICML",
  "abstract": "In large scale learning problems it is often easy to collect simple statistics of the data, but hard or impractical to store all the original data. A key question in this setting is how to construct classifiers based on such partial information. One traditional approach to the problem has been to use maximum entropy arguments to induce a complete distribution on variables from statistics. However, this approach essentially makes conditional independence assumptions about the distribution, and furthermore does not optimize prediction loss. Here we present a framework for discriminative learning given a set of statistics. Specifically, we address the case where all variables are discrete and we have access to various marginals. Our approach minimizes the worst case hinge loss in this case, which upper bounds the generalization error. We show that for certain sets of statistics the problem is tractable, and in the general case can be approximated using MAP LP relaxations. Empirical results show that the method is competitive with other approaches that use the same input.",
  "stance": 0.9
 },
 {
  "url": "https://proceedings.mlr.press/v32/gal14.pdf",
  "title": "Pitfalls in the use of Parallel Inference for the Dirichlet Process",
  "year": 2014,
  "venue": "ICML",
  "abstract": "Recent work done by Lovell, Adams, and Mansingka (2012) and Williamson, Dubey, and Xing (2013) has suggested an alternative parametrisation for the Dirichlet process in order to derive non-approximate parallel MCMC inference for it – work which has been picked-up and implemented in several different fields. In this paper we show that the approach suggested is impractical due to an extremely unbalanced distribution of the data. We characterise the requirements of efficient parallel inference for the Dirichlet process and show that the proposed inference fails most of these requirements (while approximate approaches often satisfy most of them). We present both theoretical and experimental evidence, analysing the load balance for the inference and showing that it is independent of the size of the dataset and the number of nodes available in the parallel implementation. We end with suggestions of alternative paths of research for efficient non-approximate parallel inference for the Dirichlet process.",
  "stance": -1.0
 },
 {
  "url": "https://proceedings.mlr.press/v32/korattikara14.pdf",
  "title": "Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget",
  "year": 2014,
  "venue": "ICML",
  "abstract": "Can we make Bayesian posterior MCMC sampling more efficient when faced with very large datasets? We argue that computing the likelihood for N datapoints in the Metropolis-Hastings (MH) test to reach a single binary decision is computationally inefficient. We introduce an approximate MH rule based on a sequential hypothesis test that allows us to accept or reject samples with high confidence using only a fraction of the data required for the exact MH rule. While this method introduces an asymptotic bias, we show that this bias can be controlled and is more than offset by a decrease in variance due to our ability to draw more samples per unit of time.",
  "stance": 0.6
 },
 {
  "url": "https://proceedings.mlr.press/v32/nguyenb14.pdf",
  "title": "Bayesian Nonparametric Multilevel Clustering with Group-Level Contexts",
  "year": 2014,
  "venue": "ICML",
  "abstract": "We present a Bayesian nonparametric framework for multilevel clustering which utilizes grouplevel context information to simultaneously discover low-dimensional structures of the group contents and partitions groups into clusters. Using the Dirichlet process as the building block, our model constructs a product base-measure with a nested structure to accommodate content and context observations at multiple levels. The proposed model possesses properties that link the nested Dirichlet processes (nDP) and the Dirichlet process mixture models (DPM) in an interesting way: integrating out all contents results in the DPM over contexts, whereas integrating out group-specific contexts results in the nDP mixture over content variables. We provide a Polyaurn view of the model and an efficient collapsed Gibbs inference procedure. Extensive experiments on real-world datasets demonstrate the advantage of utilizing context information via our model in both text and image domains.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v32/scherrer14.pdf",
  "title": "Approximate Policy Iteration Schemes: A Comparison",
  "year": 2014,
  "venue": "ICML",
  "abstract": "We consider the infinite-horizon discounted optimal control problem formalized by Markov Decision Processes. We focus on several approximate variations of the Policy Iteration algorithm: Approximate Policy Iteration (API) (Bertsekas & Tsitsiklis, 1996), Conservative Policy Iteration (CPI) (Kakade & Langford, 2002), a natural adaptation of the Policy Search by Dynamic Programming algorithm (Bagnell et al., 2003) to the infinite-horizon case (PSDP∞), and the recently proposed Non-Stationary Policy Iteration (NSPI(m)) (Scherrer & Lesner, 2012). For all algorithms, we describe performance bounds with respect the per-iteration error , and make a comparison by paying a particular attention to the concentrability constants involved, the number of iterations and the memory required. Our analysis highlights the following points: 1) The performance guarantee of CPI can be arbitrarily better than that of API, but this comes at the cost of a relative—exponential in 1 —increase of the number of iterations. 2) PSDP∞ enjoys the best of both worlds: its performance guarantee is similar to that of CPI, but within a number of iterations similar to that of API. 3) Contrary to API that requires a constant memory, the memory needed by CPI and PSDP∞ is proportional to their number of iterations, which may be problematic when the discount factor γ is close to 1 or the approximation error is close to 0; we show that the NSPI(m) algorithm allows to make an overall trade-off between memory and performance. Simulations with these schemes confirm our analysis. Proceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).",
  "stance": 0.0
 },
 {
  "url": "https://proceedings.mlr.press/v32/thomas14.pdf",
  "title": "Bias in Natural Actor-Critic Algorithms",
  "year": 2014,
  "venue": "ICML",
  "abstract": "We show that several popular discounted reward natural actor-critics, including the popular NACLSTD and eNAC algorithms, do not generate unbiased estimates of the natural policy gradient as claimed. We derive the first unbiased discounted reward natural actor-critics using batch and iterative approaches to gradient estimation. We argue that the bias makes the existing algorithms more appropriate for the average reward setting. We also show that, when Sarsa(λ) is guaranteed to converge to an optimal policy, the objective function used by natural actor-critics has only global optima, so policy gradient methods are guaranteed to converge to globally optimal policies as well.",
  "stance": 0.1
 },
 {
  "url": "https://proceedings.mlr.press/v32/zhong14.pdf",
  "title": "Fast Stochastic Alternating Direction Method of Multipliers",
  "year": 2014,
  "venue": "ICML",
  "abstract": "We propose a new stochastic alternating direction method of multipliers (ADMM) algorithm, which incrementally approximates the full gradient in the linearized ADMM formulation. Besides having a low per-iteration complexity as existing stochastic ADMM algorithms, it improves the convergence rate on convex problems fromO(1/ √ T ) toO(1/T ), where T is the number of iterations. This matches the convergence rate of the batch ADMM algorithm, but without the need to visit all the samples in each iteration. Experiments on the graph-guided fused lasso demonstrate that the new algorithm is significantly faster than state-of-the-art stochastic and batch ADMM algorithms.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v37/agarwal15.pdf",
  "title": "A Lower Bound for the Optimization of Finite Sums",
  "year": 2015,
  "venue": "ICML",
  "abstract": "This paper presents a lower bound for optimizing a finite sum of n functions, where each function is L-smooth and the sum is μ-strongly convex. We show that no algorithm can reach an error ε in minimizing all functions from this class in fewer than Ω(n+ √ n(κ−1) log(1/ε)) iterations, where κ = L/μ is a surrogate condition number. We then compare this lower bound to upper bounds for recently developed methods specializing to this setting. When the functions involved in this sum are not arbitrary, but based on i.i.d. random data, then we further contrast these complexity results with those for optimal first-order methods to directly optimize the sum. The conclusion we draw is that a lot of caution is necessary for an accurate comparison, and identify machine learning scenarios where the new methods help computationally.",
  "stance": 0.3
 },
 {
  "url": "https://proceedings.mlr.press/v37/avron15.pdf",
  "title": "Community Detection Using Time-Dependent Personalized PageRank",
  "year": 2015,
  "venue": "ICML",
  "abstract": "Local graph diffusions have proven to be valuable tools for solving various graph clustering problems. As such, there has been much interest recently in efficient local algorithms for computing them. We present an efficient local algorithm for approximating a graph diffusion that generalizes both the celebrated personalized PageRank and its recent competitor/companion the heat kernel. Our algorithm is based on writing the diffusion vector as the solution of an initial value problem, and then using a waveform relaxation approach to approximate the solution. Our experimental results suggest that it produces rankings that are distinct and competitive with the ones produced by high quality implementations of personalized PageRank and localized heat kernel, and that our algorithm is a useful addition to the toolset of localized graph diffusions.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v37/aybat15.pdf",
  "title": "An Asynchronous Distributed Proximal Gradient Method for Composite Convex Optimization",
  "year": 2015,
  "venue": "ICML",
  "abstract": "We propose a distributed first-order augmented Lagrangian (DFAL) algorithm to minimize the sum of composite convex functions, where each term in the sum is a private cost function belonging to a node, and only nodes connected by an edge can directly communicate with each other. This optimization model abstracts a number of applications in distributed sensing and machine learning. We show that any limit point of DFAL iterates is optimal; and for any ǫ > 0, an ǫ-optimal and ǫ-feasible solution can be computed within O(log(ǫ−1)) DFAL iterations, which require O( 1.5 max dmin ǫ−1) proximal gradient computations and communications per node in total, where ψmax denotes the largest eigenvalue of the graph Laplacian, and dmin is the minimum degree of the graph. We also propose an asynchronous version of DFAL by incorporating randomized block coordinate descent methods; and demonstrate the efficiency of DFAL on large scale sparse-group LASSO problems.",
  "stance": 0.9
 },
 {
  "url": "https://proceedings.mlr.press/v37/bai15.pdf",
  "title": "An Aligned Subtree Kernel for Weighted Graphs",
  "year": 2015,
  "venue": "ICML",
  "abstract": "In this paper, we develop a new entropic matching kernel for weighted graphs by aligning depthbased representations. We demonstrate that this kernel can be seen as an aligned subtree kernel that incorporates explicit subtree correspondences, and thus addresses the drawback of neglecting the relative locations between substructures that arises in the R-convolution kernels. Experiments on standard datasets demonstrate that our kernel can easily outperform state-of-the-art graph kernels in terms of classification accuracy.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v37/betancourt15.pdf",
  "title": "The Fundamental Incompatibility of Scalable Hamiltonian Monte Carlo and Naive Data Subsampling",
  "year": 2015,
  "venue": "ICML",
  "abstract": "Leveraging the coherent exploration of Hamiltonian flow, Hamiltonian Monte Carlo produces computationally efficient Monte Carlo estimators, even with respect to complex and highdimensional target distributions. When confronted with data-intensive applications, however, the algorithm may be too expensive to implement, leaving us to consider the utility of approximations such as data subsampling. In this paper I demonstrate how data subsampling fundamentally compromises the scalability of Hamiltonian Monte Carlo. With the preponderance of applications featuring enormous data sets, methods of inference requiring only subsamples of data are becoming more and more appealing. Subsampled Markov Chain Monte Carlo algorithms, (Neiswanger et al., 2013; Welling & Teh, 2011), are particularly desired for their potential applicability to most statistical models. Unfortunately, careful analysis of these algorithms reveals unavoidable biases unless the data are tall, or highly redundant (Bardenet et al., 2014; Teh et al., 2014; Vollmer et al., 2015). Because redundancy can be defined only relative to a given model, the utility of these subsampled algorithms is then a consequence of not only the desired accuracy and also the particular model and data under consideration, severely restricting practicality. Recently (Chen et al., 2014) considered subsampling within Hamiltonian Monte Carlo (Duane et al., 1987; Neal, 2011; Betancourt et al., 2014b) and demonstrated that the biases induced by naive subsampling lead to unacceptably large biases. Ultimately the authors rectified this bias by sacrificing the coherent exploration of Hamiltonian flow for a diffusive correction, fundamentally compromising the scalability of the algorithm with respect to the complexity Proceedings of the 32 International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s). of the target distribution. An algorithm scalable with respect to both the size of the data and the complexity of the target distribution would have to maintain the coherent exploration of Hamiltonian flow while subsampling and, unfortunately, these objectives are mutually exclusive in general. In this paper I review the elements of Hamiltonian Monte Carlo critical to its robust and scalable performance in practice and demonstrate how different subsampling strategies all compromise those properties and consequently induce poor performance. 1. Hamiltonian Monte Carlo in Theory Hamiltonian Monte Carlo utilizes deterministic, measurepreserving maps to generate efficient Markov transitions (Betancourt et al., 2014b). Formally, we begin by complementing a target distribution, π ∝ exp[−V (q)] dq, with a conditional distribution over auxiliary momenta parameters, πq ∝ exp[−T (p, q)] dp. Together these define a joint distribution, $H ∝ exp[− (T (q, p) + V (q))] dq dp ∝ exp[−H(q, p)] dq dp, and a Hamiltonian system corresponding to the Hamiltonian, H(q, p). We refer to T (q, p) and V (q) as the kinetic energy and potential energy, respectively. The Hamiltonian immediately defines a Hamiltonian vector field, ~ H = ∂H ∂p ∂ ∂q − ∂H ∂q ∂ ∂p , and an application of the exponential map yields a Hamiltonian flow on the joint space, φτ = e τ ~ H (Lee, 2013), which exactly preserves the joint distribution under a pullback, ( φt ) ∗ πH = πH . The Fundamental Incompatibility of Scalable Hamiltonian Monte Carlo and Naive Data Subsampling Consequently, we can compose a Markov chain by sampling the auxiliary momenta, q → (q, p), p ∼ πq, applying the Hamiltonian flow, (q, p)→ φt (q, p) and then projecting back down to the target space, (q, p)→ q. By construction, the trajectories generated by the Hamiltonian flow explore the level sets of the Hamiltonian function. Because these level sets can also span large volumes of the joint space, sufficiently-long trajectories can yield transitions far away from the initial state of the Markov chain, drastically reducing autocorrelations and producing computationally efficient Monte Carlo estimators. When the kinetic energy does not depend on position we say that the Hamiltonian is separable, H(q, p) = T (p) + V (q), and the Hamiltonian vector field decouples into a kinetic vector field, ~ T and potential vector field, ~ V , ~ H = ∂H ∂p ∂ ∂q − ∂H ∂q ∂ ∂p = ∂T ∂p ∂ ∂q − ∂V ∂q ∂ ∂p ≡ ~ T + ~ V . In this paper I consider only separable Hamiltonians, although the conclusions also carry over to the non-seperable Hamiltonians, for example those arising in Riemannian Hamiltonian Monte Carlo (Girolami & Calderhead, 2011). 2. Hamiltonian Monte Carlo in Practice The biggest challenge of implementing Hamiltonian Monte Carlo is that the exact Hamiltonian flow is rarely calculable in practice and we must instead resort to approximate integration. Symplectic integrators, which yield numerical trajectories that closely track the true trajectories, are of particular importance to any high-performance implementation. An especially transparent strategy for constructing symplectic integrators is to split the Hamiltonian into terms with soluble flows which can then be composed together (Leimkuhler & Reich, 2004; Hairer et al., 2006). For example, consider the symmetric Strang splitting, φ 2 ◦ φ ◦ φ 2 = e 2 ~ V ◦ e ~ T ◦ e 2 ~ V , where is a small interval of time known as the step size. Appealing to the Baker-Campbell-Hausdorff formula, this symmetric composition yields φ 2 ◦ φ ◦ φ 2 = e 2 ~ V ◦ e ~ T ◦ e 2 ~ V = e 2 ~ V ◦ exp ( ~ T + 2 ~ V + 2 4 [ ~ T , ~ V ]) +O ( 3 )",
  "stance": 0.7
 },
 {
  "url": "https://proceedings.mlr.press/v37/chenb15.pdf",
  "title": "Learning Deep Structured Models",
  "year": 2015,
  "venue": "ICML",
  "abstract": "Many problems in real-world applications involve predicting several random variables that are statistically related. Markov random fields (MRFs) are a great mathematical tool to encode such dependencies. The goal of this paper is to combine MRFs with deep learning to estimate complex representations while taking into account the dependencies between the output random variables. Towards this goal, we propose a training algorithm that is able to learn structured models jointly with deep features that form the MRF potentials. Our approach is efficient as it blends learning and inference and makes use of GPU acceleration. We demonstrate the effectiveness of our algorithm in the tasks of predicting words from noisy images, as well as tagging of Flickr photographs. We show that joint learning of the deep features and the MRF parameters results in significant performance gains.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v37/garbera15.pdf",
  "title": "Faster Rates for the Frank-Wolfe Method over Strongly-Convex Sets",
  "year": 2015,
  "venue": "ICML",
  "abstract": "The Frank-Wolfe method (a.k.a. conditional gradient algorithm) for smooth optimization has regained much interest in recent years in the context of large scale optimization and machine learning. A key advantage of the method is that it avoids projections the computational bottleneck in many applications replacing it by a linear optimization step. Despite this advantage, the known convergence rates of the FW method fall behind standard first order methods for most settings of interest. It is an active line of research to derive faster linear optimization-based algorithms for various settings of convex optimization.",
  "stance": -0.5
 },
 {
  "url": "https://proceedings.mlr.press/v37/gouws15.pdf",
  "title": "BilBOWA: Fast Bilingual Distributed Representations without Word Alignments",
  "year": 2015,
  "venue": "ICML",
  "abstract": "We introduce BilBOWA (Bilingual Bag-ofWords without Alignments), a simple and computationally-efficient model for learning bilingual distributed representations of words which can scale to large monolingual datasets and does not require word-aligned parallel training data. Instead it trains directly on monolingual data and extracts a bilingual signal from a smaller set of raw-text sentence-aligned data. This is achieved using a novel sampled bag-of-words cross-lingual objective, which is used to regularize two noise-contrastive language models for efficient cross-lingual feature learning. We show that bilingual embeddings learned using the proposed model outperform state-of-the-art methods on a cross-lingual document classification task as well as a lexical translation task on WMT11 data.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v37/kandemir15.pdf",
  "title": "Asymmetric Transfer Learning with Deep Gaussian Processes",
  "year": 2015,
  "venue": "ICML",
  "abstract": "We introduce a novel Gaussian process based Bayesian model for asymmetric transfer learning. We adopt a two-layer feed-forward deep Gaussian process as the task learner of source and target domains. The first layer projects the data onto a separate non-linear manifold for each task. We perform knowledge transfer by projecting the target data also onto the source domain and linearly combining its representations on the source and target domain manifolds. Our approach achieves the state-of-the-art in a benchmark real-world image categorization task, and improves on it in cross-tissue tumor detection from histopathology tissue slide images.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v37/lebret15.pdf",
  "title": "Phrase-based Image Captioning",
  "year": 2015,
  "venue": "ICML",
  "abstract": "Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely bilinear model that learns a metric between an image representation (generated from a previously trained Convolutional Neural Network) and phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on caption syntax statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results in two popular datasets for the task: Flickr30k and the recently proposed Microsoft COCO.",
  "stance": 0.7
 },
 {
  "url": "https://proceedings.mlr.press/v37/macdonald15.pdf",
  "title": "Controversy in mechanistic modelling with Gaussian processes",
  "year": 2015,
  "venue": "ICML",
  "abstract": "Parameter inference in mechanistic models based on non-affine differential equations is computationally onerous, and various faster alternatives based on gradient matching have been proposed. A particularly promising approach is based on nonparametric Bayesian modelling with Gaussian processes, which exploits the fact that a Gaussian process is closed under differentiation. However, two alternative paradigms have been proposed. The first paradigm, proposed at NIPS 2008 and AISTATS 2013, is based on a product of experts approach and a marginalization over the derivatives of the state variables. The second paradigm, proposed at ICML 2014, is based on a probabilistic generative model and a marginalization over the state variables. The claim has been made that this leads to better inference results. In the present article, we offer a new interpretation of the second paradigm, which highlights the underlying assumptions, approximations and limitations. In particular, we show that the second paradigm suffers from an intrinsic identifiability problem, which the first paradigm is not affected by.",
  "stance": -0.9
 },
 {
  "url": "https://proceedings.mlr.press/v37/qiu15.pdf",
  "title": "Robust Estimation of Transition Matrices in High Dimensional Heavy-tailed Vector Autoregressive Processes",
  "year": 2015,
  "venue": "ICML",
  "abstract": "Gaussian vector autoregressive (VAR) processes have been extensively studied in the literature. However, Gaussian assumptions are stringent for heavy-tailed time series that frequently arises in finance and economics. In this paper, we develop a unified framework for modeling and estimating heavy-tailed VAR processes. In particular, we generalize the Gaussian VAR model by an elliptical VAR model that naturally accommodates heavy-tailed time series. Under this model, we develop a quantile-based robust estimator for the transition matrix of the VAR process. We show that the proposed estimator achieves parametric rates of convergence in high dimensions. This is the first work in analyzing heavy-tailed high dimensional VAR processes. As an application of the proposed framework, we investigate Granger causality in the elliptical VAR process, and show that the robust transition matrix estimator induces sign-consistent estimators of Granger causality. The empirical performance of the proposed methodology is demonstrated by both synthetic and real data. We show that the proposed estimator is robust to heavy tails, and exhibit superior performance in stock price prediction. Proceedings of the 32 International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v37/sui15.pdf",
  "title": "Safe Exploration for Optimization with Gaussian Processes",
  "year": 2015,
  "venue": "ICML",
  "abstract": "We consider sequential decision problems under uncertainty, where we seek to optimize an unknown function from noisy samples. This requires balancing exploration (learning about the objective) and exploitation (localizing the maximum), a problem well-studied in the multiarmed bandit literature. In many applications, however, we require that the sampled function values exceed some prespecified “safety” threshold, a requirement that existing algorithms fail to meet. Examples include medical applications where patient comfort must be guaranteed, recommender systems aiming to avoid user dissatisfaction, and robotic control, where one seeks to avoid controls causing physical harm to the platform. We tackle this novel, yet rich, set of problems under the assumption that the unknown function satisfies regularity conditions expressed via a Gaussian process prior. We develop an efficient algorithm called SAFEOPT, and theoretically guarantee its convergence to a natural notion of optimum reachable under safety constraints. We evaluate SAFEOPT on synthetic data, as well as two real applications: movie recommendation, and therapeutic spinal cord stimulation. Proceedings of the 32 International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).",
  "stance": 0.5
 },
 {
  "url": "https://proceedings.mlr.press/v37/yogatama15.pdf",
  "title": "Learning Word Representations with Hierarchical Sparse Coding",
  "year": 2015,
  "venue": "ICML",
  "abstract": "We propose a new method for learning word representations using hierarchical regularization in sparse coding inspired by the linguistic study of word meanings. We show an efficient learning algorithm based on stochastic proximal methods that is significantly faster than previous approaches, making it possible to perform hierarchical sparse coding on a corpus of billions of word tokens. Experiments on various benchmark tasks—word similarity ranking, syntactic and semantic analogies, sentence completion, and sentiment analysis—demonstrate that the method outperforms or is competitive with state-of-the-art methods.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v37/zhoub15.pdf",
  "title": "\\ell_1,p-Norm Regularization: Error Bounds and Convergence Rate Analysis of First-Order Methods",
  "year": 2015,
  "venue": "ICML",
  "abstract": "In recent years, the `1,p-regularizer has been widely used to induce structured sparsity in the solutions to various optimization problems. Currently, such `1,p-regularized problems are typically solved by first-order methods. Motivated by the desire to analyze the convergence rates of these methods, we show that for a large class of `1,p-regularized problems, an error bound condition is satisfied when p ∈ [1, 2] or p = ∞ but fails to hold for any p ∈ (2,∞). Based on this result, we show that many first-order methods enjoy an asymptotic linear rate of convergence when applied to `1,p-regularized linear or logistic regression with p ∈ [1, 2] or p = ∞. By contrast, numerical experiments suggest that for the same class of problems with p ∈ (2,∞), the aforementioned methods may not converge linearly.",
  "stance": -0.3
 },
 {
  "url": "https://proceedings.mlr.press/v48/fernando16.pdf",
  "title": "Learning End-to-end Video Classification with Rank-Pooling",
  "year": 2016,
  "venue": "ICML",
  "abstract": "We introduce a new model for representation learning and classification of video sequences. Our model is based on a convolutional neural network coupled with a novel temporal pooling layer. The temporal pooling layer relies on an inner-optimization problem to efficiently encode temporal semantics over arbitrarily long video clips into a fixed-length vector representation. Importantly, the representation and classification parameters of our model can be estimated jointly in an end-to-end manner by formulating learning as a bilevel optimization problem. Furthermore, the model can make use of any existing convolutional neural network architecture (e.g., AlexNet or VGG) without modification or introduction of additional parameters. We demonstrate our approach on action and activity recognition tasks.",
  "stance": 0.6
 },
 {
  "url": "https://proceedings.mlr.press/v48/gea16.pdf",
  "title": "Rich Component Analysis",
  "year": 2016,
  "venue": "ICML",
  "abstract": "In many settings, we have multiple data sets (also called views) that capture different and overlapping aspects of the same phenomenon. We are often interested in finding patterns that are unique to one or to a subset of the views. For example, we might have one set of molecular observations and one set of physiological observations on the same group of individuals, and we want to quantify molecular patterns that are uncorrelated with physiology. Despite being a common problem, this is highly challenging when the correlations come from complex distributions. In this paper, we develop the general framework of Rich Component Analysis (RCA) to model settings where the observations from different views are driven by different sets of latent components, and each component can be a complex, high-dimensional distribution. We introduce algorithms based on cumulant extraction that provably learn each of the components without having to model the other components. We show how to integrate RCA with stochastic gradient descent into a meta-algorithm for learning general models, and demonstrate substantial improvement in accuracy on several synthetic and real datasets in both supervised and unsupervised tasks. Our method makes it possible to learn latent variable models when we don’t have samples from the true model but only samples after complex perturbations.",
  "stance": 0.8
 },
 {
  "url": "https://proceedings.mlr.press/v48/lic16.pdf",
  "title": "Fast k-Nearest Neighbour Search via Dynamic Continuous Indexing",
  "year": 2016,
  "venue": "ICML",
  "abstract": "Existing methods for retrieving k-nearest neighbours suffer from the curse of dimensionality. We argue this is caused in part by inherent deficiencies of space partitioning, which is the underlying strategy used by most existing methods. We devise a new strategy that avoids partitioning the vector space and present a novel randomized algorithm that runs in time linear in dimensionality of the space and sub-linear in the intrinsic dimensionality and the size of the dataset and takes space constant in dimensionality of the space and linear in the size of the dataset. The proposed algorithm allows fine-grained control over accuracy and speed on a per-query basis, automatically adapts to variations in data density, supports dynamic updates to the dataset and is easy-toimplement. We show appealing theoretical properties and demonstrate empirically that the proposed algorithm outperforms locality-sensitivity hashing (LSH) in terms of approximation quality, speed and space efficiency.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v48/papakonstantinou16.pdf",
  "title": "On the Power and Limits of Distance-Based Learning",
  "year": 2016,
  "venue": "ICML",
  "abstract": "We initiate the study of low-distortion finite metric embeddings in multi-class (and multi-label) classification where (i) both the space of input instances and the space of output classes have combinatorial metric structure, and (ii) the concepts we wish to learn are low-distortion embeddings. We develop new geometric techniques and prove strong learning lower bounds. These provable limits hold even when we allow learners and classifiers to get advice by one or more experts. Our study overwhelmingly indicates that post-geometry assumptions are necessary in multi-class classification, as in natural language processing (NLP). Technically, the mathematical tools we developed in this work could be of independent interest to NLP. To the best of our knowledge, this is the first work which formally studies classification problems in combinatorial spaces and where the concepts are low-distortion embeddings.",
  "stance": 0.6
 },
 {
  "url": "https://proceedings.mlr.press/v48/simsek16.pdf",
  "title": "Why Most Decisions Are Easy in Tetris—And Perhaps in Other Sequential Decision Problems, As Well",
  "year": 2016,
  "venue": "ICML",
  "abstract": "We examined the sequence of decision problems that are encountered in the game of Tetris and found that most of the problems are easy in the following sense: One can choose well among the available actions without knowing an evaluation function that scores well in the game. This is a consequence of three conditions that are prevalent in the game: simple dominance, cumulative dominance, and noncompensation. These conditions can be exploited to develop faster and more effective learning algorithms. In addition, they allow certain types of domain knowledge to be incorporated with ease into a learning algorithm. Among the sequential decision problems we encounter, it is unlikely that Tetris is unique or rare in having these properties.",
  "stance": 0.1
 },
 {
  "url": "https://proceedings.mlr.press/v48/ustinovskiy16.pdf",
  "title": "Meta–Gradient Boosted Decision Tree Model for Weight and Target Learning",
  "year": 2016,
  "venue": "ICML",
  "abstract": "Labeled training data is an essential part of any supervised machine learning framework. In practice, there is a trade-off between the quality of a label and its cost. In this paper, we consider a problem of learning to rank on a large-scale dataset with low-quality relevance labels aiming at maximizing the quality of a trained ranker on a small validation dataset with high-quality ground truth relevance labels. Motivated by the classical Gauss-Markov theorem for the linear regression problem, we formulate the problems of (1) reweighting training instances and (2) remapping learning targets. We propose meta– gradient decision tree learning framework for optimizing weight and target functions by applying gradient-based hyperparameter optimization. Experiments on a large-scale real-world dataset demonstrate that we can significantly improve state-of-the-art machine-learning algorithms by incorporating our framework.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v70/agarwal17a/agarwal17a.pdf",
  "title": "The Price of Differential Privacy for Online Learning",
  "year": 2017,
  "venue": "ICML",
  "abstract": "In the paradigm of online learning, a learning algorithm makes a sequence of predictions given the (possibly incomplete) knowledge of the correct answers for the past queries. In contrast to statistical learning, online learning algorithms typically offer distribution-free guarantees. Consequently, online learning algorithms are well suited to dynamic and adversarial environments, where real-time learning from changing data is essential making them ubiquitous in practical applications such as servicing search advertisements. In these settings often these algorithms interact with sensitive user data, making privacy a natural concern for these algorithms. A natural notion of privacy in such settings is differential privacy (Dwork et al., 2006) which ensures that the outputs of an algorithm are indistinguishable in the case when a user’s data is present as opposed to when it is absent in a dataset.",
  "stance": -0.2
 },
 {
  "url": "https://proceedings.mlr.press/v70/allen-zhu17a/allen-zhu17a.pdf",
  "title": "Natasha: Faster Non-Convex Stochastic Optimization via Strongly Non-Convex Parameter",
  "year": 2017,
  "venue": "ICML",
  "abstract": "Given a non-convex function f(x) that is an average of n smooth functions, we design stochastic first-order methods to find its approximate stationary points. The performance of our new methods depend on the smallest (negative) eigenvalue −σ of the Hessian. This parameter σ captures how strongly non-convex f(x) is, and is analogous to the strong convexity parameter for convex optimization. At least in theory, our methods outperform known results for a range of parameter σ, and can also be used to find approximate local minima. Our result implies an interesting dichotomy: there exists a threshold σ0 so that the (currently) fastest methods for σ > σ0 and for σ < σ0 have different behaviors: the former scales with n and the latter scales with n.",
  "stance": 0.8
 },
 {
  "url": "https://proceedings.mlr.press/v70/andreas17a/andreas17a.pdf",
  "title": "Modular Multitask Reinforcement Learning with Policy Sketches",
  "year": 2017,
  "venue": "ICML",
  "abstract": "We describe a framework for multitask deep reinforcement learning guided by policy sketches. Sketches annotate tasks with sequences of named subtasks, providing information about high-level structural relationships among tasks but not how to implement them—specifically not providing the detailed guidance used by much previous work on learning policy abstractions for RL (e.g. intermediate rewards, subtask completion signals, or intrinsic motivations). To learn from sketches, we present a model that associates every subtask with a modular subpolicy, and jointly maximizes reward over full task-specific policies by tying parameters across shared subpolicies. Optimization is accomplished via a decoupled actor–critic training objective that facilitates learning common behaviors from multiple dissimilar reward functions. We evaluate the effectiveness of our approach in three environments featuring both discrete and continuous control, and with sparse rewards that can be obtained only after completing a number of high-level subgoals. Experiments show that using our approach to learn policies guided by sketches gives better performance than existing techniques for learning task-specific or shared policies, while naturally inducing a library of interpretable primitive behaviors that can be recombined to rapidly adapt to new tasks.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v70/arik17a/arik17a.pdf",
  "title": "Deep Voice: Real-time Neural Text-to-Speech",
  "year": 2017,
  "venue": "ICML",
  "abstract": "We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-tophoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-tospeech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v70/arjevani17a/arjevani17a.pdf",
  "title": "Oracle Complexity of Second-Order Methods for Finite-Sum Problems",
  "year": 2017,
  "venue": "ICML",
  "abstract": "Finite-sum optimization problems are ubiquitous in machine learning, and are commonly solved using first-order methods which rely on gradient computations. Recently, there has been growing interest in second-order methods, which rely on both gradients and Hessians. In principle, second-order methods can require much fewer iterations than first-order methods, and hold the promise for more efficient algorithms. Although computing and manipulating Hessians is prohibitive for high-dimensional problems in general, the Hessians of individual functions in finite-sum problems can often be efficiently computed, e.g. because they possess a low-rank structure. Can second-order information indeed be used to solve such problems more efficiently? In this paper, we provide evidence that the answer – perhaps surprisingly – is negative, at least in terms of worst-case guarantees. We also discuss what additional assumptions and algorithmic approaches might potentially circumvent this negative result.",
  "stance": -0.2
 },
 {
  "url": "https://proceedings.mlr.press/v70/arpit17a/arpit17a.pdf",
  "title": "A Closer Look at Memorization in Deep Networks",
  "year": 2017,
  "venue": "ICML",
  "abstract": "We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization.",
  "stance": 0.1
 },
 {
  "url": "https://proceedings.mlr.press/v70/balog17a/balog17a.pdf",
  "title": "Lost Relatives of the Gumbel Trick",
  "year": 2017,
  "venue": "ICML",
  "abstract": "The Gumbel trick is a method to sample from a discrete probability distribution, or to estimate its normalizing partition function. The method relies on repeatedly applying a random perturbation to the distribution in a particular way, each time solving for the most likely configuration. We derive an entire family of related methods, of which the Gumbel trick is one member, and show that the new methods have superior properties in several settings with minimal additional computational cost. In particular, for the Gumbel trick to yield computational benefits for discrete graphical models, Gumbel perturbations on all configurations are typically replaced with socalled low-rank perturbations. We show how a subfamily of our new methods adapts to this setting, proving new upper and lower bounds on the log partition function and deriving a family of sequential samplers for the Gibbs distribution. Finally, we balance the discussion by showing how the simpler analytical form of the Gumbel trick enables additional theoretical results.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v70/bamler17a/bamler17a.pdf",
  "title": "Dynamic Word Embeddings",
  "year": 2017,
  "venue": "ICML",
  "abstract": "We present a probabilistic language model for time-stamped text data which tracks the semantic evolution of individual words over time. The model represents words and contexts by latent trajectories in an embedding space. At each moment in time, the embedding vectors are inferred from a probabilistic version of word2vec (Mikolov et al., 2013b). These embedding vectors are connected in time through a latent diffusion process. We describe two scalable variational inference algorithms—skipgram smoothing and skip-gram filtering—that allow us to train the model jointly over all times; thus learning on all data while simultaneously allowing word and context vectors to drift. Experimental results on three different corpora demonstrate that our dynamic model infers word embedding trajectories that are more interpretable and lead to higher predictive likelihoods than competing methods that are based on static models trained separately on time slices.",
  "stance": 0.9
 },
 {
  "url": "https://proceedings.mlr.press/v70/brown17a/brown17a.pdf",
  "title": "Reduced Space and Faster Convergence in Imperfect-Information Games via Pruning",
  "year": 2017,
  "venue": "ICML",
  "abstract": "Iterative algorithms such as Counterfactual Regret Minimization (CFR) are the most popular way to solve large zero-sum imperfect-information games. In this paper we introduce Best-Response Pruning (BRP), an improvement to iterative algorithms such as CFR that allows poorly-performing actions to be temporarily pruned. We prove that when using CFR in zero-sum games, adding BRP will asymptotically prune any action that is not part of a best response to some Nash equilibrium. This leads to provably faster convergence and lower space requirements. Experiments show that BRP results in a factor of 7 reduction in space, and the reduction factor increases with game size.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v70/czarnecki17a/czarnecki17a.pdf",
  "title": "Understanding Synthetic Gradients and Decoupled Neural Interfaces",
  "year": 2017,
  "venue": "ICML",
  "abstract": "When training neural networks, the use of Synthetic Gradients (SG) allows layers or modules to be trained without update locking – without waiting for a true error gradient to be backpropagated – resulting in Decoupled Neural Interfaces (DNIs). This unlocked ability of being able to update parts of a neural network asynchronously and with only local information was demonstrated to work empirically in Jaderberg et al. (2016). However, there has been very little demonstration of what changes DNIs and SGs impose from a functional, representational, and learning dynamics point of view. In this paper, we study DNIs through the use of synthetic gradients on feed-forward networks to better understand their behaviour and elucidate their effect on optimisation. We show that the incorporation of SGs does not affect the representational strength of the learning system for a neural network, and prove the convergence of the learning system for linear and deep linear models. On practical problems we investigate the mechanism by which synthetic gradient estimators approximate the true loss, and, surprisingly, how that leads to drastically different layer-wise representations. Finally, we also expose the relationship of using synthetic gradients to other error approximation techniques and find a unifying language for discussion and comparison.",
  "stance": 0.3
 },
 {
  "url": "https://proceedings.mlr.press/v70/dembczynski17a/dembczynski17a.pdf",
  "title": "Consistency Analysis for Binary Classification Revisited",
  "year": 2017,
  "venue": "ICML",
  "abstract": "Statistical learning theory is at an inflection point enabled by recent advances in understanding and optimizing a wide range of metrics. Of particular interest are non-decomposable metrics such as the F-measure and the Jaccard measure which cannot be represented as a simple average over examples. Non-decomposability is the primary source of difficulty in theoretical analysis, and interestingly has led to two distinct settings and notions of consistency. In this manuscript we analyze both settings, from statistical and algorithmic points of view, to explore the connections and to highlight differences between them for a wide range of metrics. The analysis complements previous results on this topic, clarifies common confusions around both settings, and provides guidance to the theory and practice of binary classification with complex metrics.",
  "stance": 0.0
 },
 {
  "url": "https://proceedings.mlr.press/v70/dinh17b/dinh17b.pdf",
  "title": "Sharp Minima Can Generalize For Deep Nets",
  "year": 2017,
  "venue": "ICML",
  "abstract": "Despite their overwhelming capacity to overfit, deep learning architectures tend to generalize relatively well to unseen data, allowing them to be deployed in practice. However, explaining why this is the case is still an open area of research. One standing hypothesis that is gaining popularity, e.g. Hochreiter & Schmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization. This paper argues that most notions of flatness are problematic for deep models and can not be directly applied to explain generalization. Specifically, when focusing on deep networks with rectifier units, we can exploit the particular geometry of parameter space induced by the inherent symmetries that these architectures exhibit to build equivalent models corresponding to arbitrarily sharper minima. Furthermore, if we allow to reparametrize a function, the geometry of its parameters can change drastically without affecting its generalization properties.",
  "stance": -0.7
 },
 {
  "url": "https://proceedings.mlr.press/v70/finn17a/finn17a.pdf",
  "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
  "year": 2017,
  "venue": "ICML",
  "abstract": "We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two fewshot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v70/grave17a/grave17a.pdf",
  "title": "Efficient softmax approximation for GPUs",
  "year": 2017,
  "venue": "ICML",
  "abstract": "We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computation time. Our approach further reduces the computational time by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax. The code of our method is available at https://github.com/ facebookresearch/adaptive-softmax.",
  "stance": 0.9
 },
 {
  "url": "https://proceedings.mlr.press/v70/graves17a/graves17a.pdf",
  "title": "Automated Curriculum Learning for Neural Networks",
  "year": 2017,
  "venue": "ICML",
  "abstract": "We introduce a method for automatically selecting the path, or syllabus, that a neural network follows through a curriculum so as to maximise learning efficiency. A measure of the amount that the network learns from each data sample is provided as a reward signal to a nonstationary multiarmed bandit algorithm, which then determines a stochastic syllabus. We consider a range of signals derived from two distinct indicators of learning progress: rate of increase in prediction accuracy, and rate of increase in network complexity. Experimental results for LSTM networks on three curricula demonstrate that our approach can significantly accelerate learning, in some cases halving the time required to attain a satisfactory performance level.",
  "stance": 0.9
 },
 {
  "url": "https://proceedings.mlr.press/v70/guo17a/guo17a.pdf",
  "title": "On Calibration of Modern Neural Networks",
  "year": 2017,
  "venue": "ICML",
  "abstract": "Confidence calibration – the problem of predicting probability estimates representative of the true correctness likelihood – is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-ofthe-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling – a singleparameter variant of Platt Scaling – is surprisingly effective at calibrating predictions.",
  "stance": 0.1
 },
 {
  "url": "https://proceedings.mlr.press/v70/ingraham17a/ingraham17a.pdf",
  "title": "Variational Inference for Sparse and Undirected Models",
  "year": 2017,
  "venue": "ICML",
  "abstract": "Undirected graphical models are applied in genomics, protein structure prediction, and neuroscience to identify sparse interactions that underlie discrete data. Although Bayesian methods for inference would be favorable in these contexts, they are rarely used because they require doubly intractable Monte Carlo sampling. Here, we develop a framework for scalable Bayesian inference of discrete undirected models based on two new methods. The first is Persistent VI, an algorithm for variational inference of discrete undirected models that avoids doubly intractable MCMC and approximations of the partition function. The second is Fadeout, a reparameterization approach for variational inference under sparsity-inducing priors that captures a posteriori correlations between parameters and hyperparameters with noncentered parameterizations. We find that, together, these methods for variational inference substantially improve learning of sparse undirected graphical models in simulated and real problems from physics and biology.",
  "stance": 0.0
 },
 {
  "url": "https://proceedings.mlr.press/v70/katt17a/katt17a.pdf",
  "title": "Learning in POMDPs with Monte Carlo Tree Search",
  "year": 2017,
  "venue": "ICML",
  "abstract": "The POMDP is a powerful framework for reasoning under outcome and information uncertainty, but constructing an accurate POMDP model is difficult. Bayes-Adaptive Partially Observable Markov Decision Processes (BA-POMDPs) extend POMDPs to allow the model to be learned during execution. BA-POMDPs are a Bayesian RL approach that, in principle, allows for an optimal trade-off between exploitation and exploration. Unfortunately, BA-POMDPs are currently impractical to solve for any non-trivial domain. In this paper, we extend the Monte-Carlo Tree Search method POMCP to BA-POMDPs and show that the resulting method, which we call BA-POMCP, is able to tackle problems that previous solution methods have been unable to solve. Additionally, we introduce several techniques that exploit the BA-POMDP structure to improve the efficiency of BA-POMCP along with proof of their convergence.",
  "stance": 0.9
 },
 {
  "url": "https://proceedings.mlr.press/v70/levy17a/levy17a.pdf",
  "title": "Learning to Align the Source Code to the Compiled Object Code",
  "year": 2017,
  "venue": "ICML",
  "abstract": "We propose a new neural network architecture and use it for the task of statement-by-statement alignment of source code and its compiled object code. Our architecture learns the alignment between the two sequences – one being the translation of the other – by mapping each statement to a context-dependent representation vector and aligning such vectors using a grid of the two sequence domains. Our experiments include short C functions, both artificial and human-written, and show that our neural network architecture is able to predict the alignment with high accuracy, outperforming known baselines. We also demonstrate that our model is general and can learn to solve graph problems such as the Traveling Salesman Problem.",
  "stance": 0.9
 },
 {
  "url": "https://proceedings.mlr.press/v70/maystre17b/maystre17b.pdf",
  "title": "ChoiceRank: Identifying Preferences from Node Traffic in Networks",
  "year": 2017,
  "venue": "ICML",
  "abstract": "Understanding how users navigate in a network is of high interest in many applications. We consider a setting where only aggregate node-level traffic is observed and tackle the task of learning edge transition probabilities. We cast it as a preference learning problem, and we study a model where choices follow Luce’s axiom. In this case, the O(n) marginal counts of node visits are a sufficient statistic for the O(n) transition probabilities. We show how to make the inference problem well-posed regardless of the network’s structure, and we present ChoiceRank, an iterative algorithm that scales to networks that contains billions of nodes and edges. We apply the model to two clickstream datasets and show that it successfully recovers the transition probabilities using only the network structure and marginal (nodelevel) traffic data. Finally, we also consider an application to mobility networks and apply the model to one year of rides on New York City’s bicycle-sharing system.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v70/mishra17a/mishra17a.pdf",
  "title": "Prediction and Control with Temporal Segment Models",
  "year": 2017,
  "venue": "ICML",
  "abstract": "We introduce a method for learning the dynamics of complex nonlinear systems based on deep generative models over temporal segments of states and actions. Unlike dynamics models that operate over individual discrete timesteps, we learn the distribution over future state trajectories conditioned on past state, past action, and planned future action trajectories, as well as a latent prior over action trajectories. Our approach is based on convolutional autoregressive models and variational autoencoders. It makes stable and accurate predictions over long horizons for complex, stochastic systems, effectively expressing uncertainty and modeling the effects of collisions, sensory noise, and action delays. The learned dynamics model and action prior can be used for end-to-end, fully differentiable trajectory optimization and model-based policy optimization, which we use to evaluate the performance and sample-efficiency of our method.",
  "stance": 0.8
 },
 {
  "url": "https://proceedings.mlr.press/v70/shalev-shwartz17a/shalev-shwartz17a.pdf",
  "title": "Failures of Gradient-Based Deep Learning",
  "year": 2017,
  "venue": "ICML",
  "abstract": "In recent years, Deep Learning has become the go-to solution for a broad range of applications, often outperforming state-of-the-art. However, it is important, for both theoreticians and practitioners, to gain a deeper understanding of the difficulties and limitations associated with common approaches and algorithms. We describe four types of simple problems, for which the gradientbased algorithms commonly used in deep learning either fail or suffer from significant difficulties. We illustrate the failures through practical experiments, and provide theoretical insights explaining their source, and how they might be remedied.",
  "stance": -1.0
 },
 {
  "url": "https://proceedings.mlr.press/v70/sun17c/sun17c.pdf",
  "title": "meProp: Sparsified Back Propagation for Accelerated Deep Learning with Reduced Overfitting",
  "year": 2017,
  "venue": "ICML",
  "abstract": "We propose a simple yet effective technique for neural network learning. The forward propagation is computed as usual. In back propagation, only a small subset of the full gradient is computed to update the model parameters. The gradient vectors are sparsified in such a way that only the top-k elements (in terms of magnitude) are kept. As a result, only k rows or columns (depending on the layout) of the weight matrix are modified, leading to a linear reduction (k divided by the vector dimension) in the computational cost. Surprisingly, experimental results demonstrate that we can update only 1–4% of the weights at each back propagation pass. This does not result in a larger number of training iterations. More interestingly, the accuracy of the resulting models is actually improved rather than degraded, and a detailed analysis is given.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v70/ye17b/ye17b.pdf",
  "title": "A Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization",
  "year": 2017,
  "venue": "ICML",
  "abstract": "Learning under a Wasserstein loss, a.k.a. Wasserstein loss minimization (WLM), is an emerging research topic for gaining insights from a large set of structured objects. Despite being conceptually simple, WLM problems are computationally challenging because they involve minimizing over functions of quantities (i.e. Wasserstein distances) that themselves require numerical algorithms to compute. In this paper, we introduce a stochastic approach based on simulated annealing for solving WLMs. Particularly, we have developed a Gibbs sampler to approximate effectively and efficiently the partial gradients of a sequence of Wasserstein losses. Our new approach has the advantages of numerical stability and readiness for warm starts. These characteristics are valuable for WLM problems that often require multiple levels of iterations in which the oracle for computing the value and gradient of a loss function is embedded. We applied the method to optimal transport with Coulomb cost and the Wasserstein non-negative matrix factorization problem, and made comparisons with the existing method of entropy regularization.",
  "stance": 0.4
 },
 {
  "url": "https://proceedings.mlr.press/v80/kamnitsas18a/kamnitsas18a.pdf",
  "title": "Semi-Supervised Learning via Compact Latent Space Clustering",
  "year": 2018,
  "venue": "ICML",
  "abstract": "We present a novel cost function for semisupervised learning of neural networks that encourages compact clustering of the latent space to facilitate separation. The key idea is to dynamically create a graph over embeddings of labeled and unlabeled samples of a training batch to capture underlying structure in feature space, and use label propagation to estimate its high and low density regions. We then devise a cost function based on Markov chains on the graph that regularizes the latent space to form a single compact cluster per class, while avoiding to disturb existing clusters during optimization. We evaluate our approach on three benchmarks and compare to state-of-the art with promising results. Our approach combines the benefits of graph-based regularization with efficient, inductive inference, does not require modifications to a network architecture, and can thus be easily applied to existing networks to enable an effective use of unlabeled data.",
  "stance": 0.8
 },
 {
  "url": "https://proceedings.mlr.press/v80/lake18a/lake18a.pdf",
  "title": "Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks",
  "year": 2018,
  "venue": "ICML",
  "abstract": "Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb “dax,” he or she can immediately understand the meaning of “dax twice” or “sing and dax.” In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can make successful zero-shot generalizations when the differences between training and test commands are small, so that they can apply “mix-and-match” strategies to solve the task. However, when generalization requires systematic compositional skills (as in the “dax” example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks’ notorious training data thirst.",
  "stance": -0.5
 },
 {
  "url": "https://proceedings.mlr.press/v80/li18d/li18d.pdf",
  "title": "On the Limitations of First-Order Approximation in GAN Dynamics",
  "year": 2018,
  "venue": "ICML",
  "abstract": "While Generative Adversarial Networks (GANs) have demonstrated promising performance on multiple vision tasks, their learning dynamics are not yet well understood, both in theory and in practice. To address this issue, we study GAN dynamics in a simple yet rich parametric model that exhibits several of the common problematic convergence behaviors such as vanishing gradients, mode collapse, and diverging or oscillatory behavior. In spite of the non-convex nature of our model, we are able to perform a rigorous theoretical analysis of its convergence behavior. Our analysis reveals an interesting dichotomy: a GAN with an optimal discriminator provably converges, while first order approximations of the discriminator steps lead to unstable GAN dynamics and mode collapse. Our result suggests that using first order discriminator steps (the de-facto standard in most existing GAN setups) might be one of the factors that makes GAN training challenging in practice.",
  "stance": 0.0
 },
 {
  "url": "https://proceedings.mlr.press/v80/liang18a/liang18a.pdf",
  "title": "Understanding the Loss Surface of Neural Networks for Binary Classification",
  "year": 2018,
  "venue": "ICML",
  "abstract": "It is widely conjectured that training algorithms for neural networks are successful because all local minima lead to similar performance; for example, see (LeCun et al., 2015; Choromanska et al., 2015; Dauphin et al., 2014). Performance is typically measured in terms of two metrics: training performance and generalization performance. Here we focus on the training performance of neural networks for binary classification, and provide conditions under which the training error is zero at all local minima of appropriately chosen surrogate loss functions. Our conditions are roughly in the following form: the neurons have to be increasing and strictly convex, the neural network should either be single-layered or is multi-layered with a shortcutlike connection, and the surrogate loss function should be a smooth version of hinge loss. We also provide counterexamples to show that, when these conditions are relaxed, the result may not hold.",
  "stance": -0.5
 },
 {
  "url": "https://proceedings.mlr.press/v80/liu18c/liu18c.pdf",
  "title": "Delayed Impact of Fair Machine Learning",
  "year": 2018,
  "venue": "ICML",
  "abstract": "Fairness in machine learning has predominantly been studied in static classification settings without concern for how decisions change the underlying population over time. Conventional wisdom suggests that fairness criteria promote the longterm well-being of those groups they aim to protect. We study how static fairness criteria interact with temporal indicators of well-being, such as long-term improvement, stagnation, and decline in a variable of interest. We demonstrate that even in a one-step feedback model, common fairness criteria in general do not promote improvement over time, and may in fact cause harm in cases where an unconstrained objective would not. We completely characterize the delayed impact of three standard criteria, contrasting the regimes in which these exhibit qualitatively different behavior. In addition, we find that a natural form of measurement error broadens the regime in which fairness criteria perform favorably. Our results highlight the importance of measurement and temporal modeling in the evaluation of fairness criteria, suggesting a range of new challenges and trade-offs.",
  "stance": 0.0
 },
 {
  "url": "https://proceedings.mlr.press/v80/loukas18a/loukas18a.pdf",
  "title": "Spectrally Approximating Large Graphs with Smaller Graphs",
  "year": 2018,
  "venue": "ICML",
  "abstract": "How does coarsening affect the spectrum of a general graph? We provide conditions such that the principal eigenvalues and eigenspaces of a coarsened and original graph Laplacian matrices are close. The achieved approximation is shown to depend on standard graph-theoretic properties, such as the degree and eigenvalue distributions, as well as on the ratio between the coarsened and actual graph sizes. Our results carry implications for learning methods that utilize coarsening. For the particular case of spectral clustering, they imply that coarse eigenvectors can be used to derive good quality assignments even without refinement—this phenomenon was previously observed, but lacked formal justification.",
  "stance": 0.3
 },
 {
  "url": "https://proceedings.mlr.press/v80/madras18a/madras18a.pdf",
  "title": "Learning Adversarially Fair and Transferable Representations",
  "year": 2018,
  "venue": "ICML",
  "abstract": "In this paper, we advocate for representation learning as the key to mitigating unfair prediction outcomes downstream. Motivated by a scenario where learned representations are used by third parties with unknown objectives, we propose and explore adversarial representation learning as a natural method of ensuring those parties act fairly. We connect group fairness (demographic parity, equalized odds, and equal opportunity) to different adversarial objectives. Through worst-case theoretical guarantees and experimental validation, we show that the choice of this objective is crucial to fair prediction. Furthermore, we present the first in-depth experimental demonstration of fair transfer learning and demonstrate empirically that our learned representations admit fair predictions on new tasks while maintaining utility, an essential goal of fair representation learning.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v80/nguyen18b/nguyen18b.pdf",
  "title": "Neural Networks Should Be Wide Enough to Learn Disconnected Decision Regions",
  "year": 2018,
  "venue": "ICML",
  "abstract": "In the recent literature the important role of depth in deep learning has been emphasized. In this paper we argue that sufficient width of a feedforward network is equally important by answering the simple question under which conditions the decision regions of a neural network are connected. It turns out that for a class of activation functions including leaky ReLU, neural networks having a pyramidal structure, that is no layer has more hidden units than the input dimension, produce necessarily connected decision regions. This implies that a sufficiently wide hidden layer is necessary to guarantee that the network can produce disconnected decision regions. We discuss the implications of this result for the construction of neural networks, in particular the relation to the problem of adversarial manipulation of classifiers.",
  "stance": 0.0
 },
 {
  "url": "https://proceedings.mlr.press/v80/reagan18a/reagan18a.pdf",
  "title": "Weightless: Lossy weight encoding for deep neural network compression",
  "year": 2018,
  "venue": "ICML",
  "abstract": "The large memory requirements of deep neural networks limit their deployment and adoption on many devices. Model compression methods effectively reduce the memory requirements of these models, usually through applying transformations such as weight pruning or quantization. In this paper, we present a novel scheme for lossy weight encoding co-designed with weight simplification techniques. The encoding is based on the Bloomier filter, a probabilistic data structure that can save space at the cost of introducing random errors. Leveraging the ability of neural networks to tolerate these imperfections and by re-training around the errors, the proposed technique, named Weightless, can compress weights by up to 496× without loss of model accuracy. This results in up to a 1.51× improvement over the state-of-the-art.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v97/allen19a/allen19a.pdf",
  "title": "Analogies Explained: Towards Understanding Word Embeddings",
  "year": 2019,
  "venue": "ICML",
  "abstract": "Word embeddings generated by neural network methods such as word2vec (W2V) are well known to exhibit seemingly linear behaviour, e.g. the embeddings of analogy “woman is to queen as man is to king” approximately describe a parallelogram. This property is particularly intriguing since the embeddings are not trained to achieve it. Several explanations have been proposed, but each introduces assumptions that do not hold in practice. We derive a probabilistically grounded definition of paraphrasing that we re-interpret as word transformation, a mathematical description of “wx is to wy”. From these concepts we prove existence of linear relationships between W2V-type embeddings that underlie the analogical phenomenon, identifying explicit error terms.",
  "stance": -0.5
 },
 {
  "url": "https://proceedings.mlr.press/v97/amin19a/amin19a.pdf",
  "title": "Bounding User Contributions: A Bias-Variance Trade-off in Differential Privacy",
  "year": 2019,
  "venue": "ICML",
  "abstract": "Differentially private learning algorithms protect individual participants in the training dataset by guaranteeing that their presence does not significantly change the resulting model. In order to make this promise, such algorithms need to know the maximum contribution that can be made by a single user: the more data an individual can contribute, the more noise will need to be added to protect them. While most existing analyses assume that the maximum contribution is known and fixed in advance—indeed, it is often assumed that each user contributes only a single example— we argue that in practice there is a meaningful choice to be made. On the one hand, if we allow users to contribute large amounts of data, we may end up adding excessive noise to protect a few outliers, even when the majority contribute only modestly. On the other hand, limiting users to small contributions keeps noise levels low at the cost of potentially discarding significant amounts of excess data, thus introducing bias. Here, we characterize this trade-off for an empirical risk minimization setting, showing that in general there is a “sweet spot” that depends on measurable properties of the dataset, but that there is also a concrete cost to privacy that cannot be avoided simply by collecting more data.",
  "stance": 0.0
 },
 {
  "url": "https://proceedings.mlr.press/v97/bouthillier19a/bouthillier19a.pdf",
  "title": "Unreproducible Research is Reproducible",
  "year": 2019,
  "venue": "ICML",
  "abstract": "The apparent contradiction in the title is a wordplay on the different meanings attributed to the word reproducible across different scientific fields. What we imply is that unreproducible findings can be built upon reproducible methods. Without denying the importance of facilitating the reproduction of methods, we deem important to reassert that reproduction of findings is a fundamental step of the scientific inquiry. We argue that the commendable quest towards easy deterministic reproducibility of methods and numerical results should not have us forget the even more important necessity of ensuring the reproducibility of empirical findings and conclusions by properly accounting for essential sources of variations. We provide experiments to exemplify the brittleness of current common practice in the evaluation of models in the field of deep learning, showing that even if the results could be reproduced, a slightly different experiment would not support the findings. We hope to help clarify the distinction between exploratory and empirical research in the field of deep learning and believe more energy should be devoted to proper empirical research in our community. This work is an attempt to promote the use of more rigorous and diversified methodologies. It is not an attempt to impose a new methodology and it is not a critique on the nature of exploratory research.",
  "stance": 0.0
 },
 {
  "url": "https://proceedings.mlr.press/v97/bubeck19a/bubeck19a.pdf",
  "title": "Adversarial examples from computational constraints",
  "year": 2019,
  "venue": "ICML",
  "abstract": "Why are classifiers in high dimension vulnerable to “adversarial” perturbations? We show that it is likely not due to information theoretic limitations, but rather it could be due to computational constraints. First we prove that, for a broad set of classification tasks, the mere existence of a robust classifier implies that it can be found by a possibly exponential-time algorithm with relatively few training examples. Then we give two particular classification tasks where learning a robust classifier is computationally intractable. More precisely we construct two binary classifications task in high dimensional space which are (i) information theoretically easy to learn robustly for large perturbations, (ii) efficiently learnable (nonrobustly) by a simple linear separator, (iii) yet are not efficiently robustly learnable, even for small perturbations. Specifically, for the first task hardness holds for any efficient algorithm in the statistical query (SQ) model, while for the second task we rule out any efficient algorithm under a cryptographic assumption. These examples give an exponential separation between classical learning and robust learning in the statistical query model or under a cryptographic assumption. It suggests that adversarial examples may be an unavoidable byproduct of computational limitations of learning algorithms.",
  "stance": -0.4
 },
 {
  "url": "https://proceedings.mlr.press/v97/byrd19a/byrd19a.pdf",
  "title": "What is the Effect of Importance Weighting in Deep Learning?",
  "year": 2019,
  "venue": "ICML",
  "abstract": "Importance-weighted risk minimization is a key ingredient in many machine learning algorithms for causal inference, domain adaptation, class imbalance, and off-policy reinforcement learning. While the effect of importance weighting is wellcharacterized for low-capacity misspecified models, little is known about how it impacts overparameterized, deep neural networks. Inspired by recent theoretical results showing that on (linearly) separable data, deep linear networks optimized by SGD learn weight-agnostic solutions, we ask, for realistic deep networks, for which many practical datasets are separable, what is the effect of importance weighting? We present the surprising finding that while importance weighting impacts deep nets early in training, so long as the nets are able to separate the training data, its effect diminishes over successive epochs. Moreover, while L2 regularization and batch normalization (but not dropout), restore some of the impact of importance weighting, they express the effect via (seemingly) the wrong abstraction: why should practitioners tweak the L2 regularization, and by how much, to produce the correct weighting effect? We experimentally confirm these findings across a range of architectures and datasets.",
  "stance": 0.0
 },
 {
  "url": "https://proceedings.mlr.press/v97/chai19a/chai19a.pdf",
  "title": "Automated Model Selection with Bayesian Quadrature",
  "year": 2019,
  "venue": "ICML",
  "abstract": "We present a novel technique for tailoring Bayesian quadrature (BQ) to model selection. The state-of-the-art for comparing the evidence of multiple models relies on Monte Carlo methods, which converge slowly and are unreliable for computationally expensive models. Although previous research has shown that BQ offers sample efficiency superior to Monte Carlo in computing the evidence of an individual model, applying BQ directly to model comparison may waste computation producing an overly-accurate estimate for the evidence of a clearly poor model. We propose an automated and efficient algorithm for computing the most-relevant quantity for model selection: the posterior model probability. Our technique maximizes the mutual information between this quantity and observations of the models’ likelihoods, yielding efficient sample acquisition across disparate model spaces when likelihood observations are limited. Our method produces moreaccurate posterior estimates using fewer likelihood evaluations than standard Bayesian quadrature and Monte Carlo estimators, as we demonstrate on synthetic and real-world examples.",
  "stance": 0.9
 },
 {
  "url": "https://proceedings.mlr.press/v97/chen19e/chen19e.pdf",
  "title": "Information-Theoretic Considerations in Batch Reinforcement Learning",
  "year": 2019,
  "venue": "ICML",
  "abstract": "Value-function approximation methods that operate in batch mode have foundational importance to reinforcement learning (RL). Finite sample guarantees for these methods often crucially rely on two types of assumptions: (1) mild distribution shift, and (2) representation conditions that are stronger than realizability. However, the necessity (“why do we need them?”) and the naturalness (“when do they hold?”) of such assumptions have largely eluded the literature. In this paper, we revisit these assumptions and provide theoretical results towards answering the above questions, and make steps towards a deeper understanding of value-function approximation.",
  "stance": 0.5
 },
 {
  "url": "https://proceedings.mlr.press/v97/cortes19b/cortes19b.pdf",
  "title": "Active Learning with Disagreement Graphs",
  "year": 2019,
  "venue": "ICML",
  "abstract": "We present two novel enhancements of an online importance-weighted active learning algorithm IWAL, using the properties of disagreements among hypotheses. The first enhancement, IWALD, prunes the hypothesis set with a more aggressive strategy based on the disagreement graph. We show that IWAL-D improves the generalization performance and the label complexity of the original IWAL, and quantify the improvement in terms of a disagreement graph coefficient. The second enhancement, IZOOM, further improves IWAL-D by adaptively zooming into the current version space and thus reducing the best-in-class error. We show that IZOOM admits favorable theoretical guarantees with the changing hypothesis set. We report experimental results on multiple datasets and demonstrate that the proposed algorithms achieve better test performances than IWAL given the same amount of labeling budget.",
  "stance": 0.8
 },
 {
  "url": "https://proceedings.mlr.press/v97/cranko19a/cranko19a.pdf",
  "title": "Monge blunts Bayes: Hardness Results for Adversarial Training",
  "year": 2019,
  "venue": "ICML",
  "abstract": "The last few years have seen a staggering number of empirical studies of the robustness of neural networks in a model of adversarial perturbations of their inputs. Most rely on an adversary which carries out local modifications within prescribed balls. None however has so far questioned the broader picture: how to frame a resource-bounded adversary so that it can be severely detrimental to learning, a non-trivial problem which entails at a minimum the choice of loss and classifiers. We suggest a formal answer for losses that satisfy the minimal statistical requirement of being proper. We pin down a simple sufficient property for any given class of adversaries to be detrimental to learning, involving a central measure of “harmfulness” which generalizes the well-known class of integral probability metrics. A key feature of our result is that it holds for all proper losses, and for a popular subset of these, the optimisation of this central measure appears to be independent of the loss. When classifiers are Lipschitz – a now popular approach in adversarial training –, this optimisation resorts to optimal transport to make a low-budget compression of class marginals. Toy experiments reveal a finding recently separately observed: training against a sufficiently budgeted adversary of this kind improves generalization.",
  "stance": 0.4
 },
 {
  "url": "https://proceedings.mlr.press/v97/crawford19a/crawford19a.pdf",
  "title": "Submodular Cost Submodular Cover with an Approximate Oracle",
  "year": 2019,
  "venue": "ICML",
  "abstract": "In this work, we study the Submodular Cost Submodular Cover problem, which is to minimize the submodular cost required to ensure that the submodular benefit function exceeds a given threshold. Existing approximation ratios for the greedy algorithm assume a value oracle to the benefit function. However, access to a value oracle is not a realistic assumption for many applications of this problem, where the benefit function is difficult to compute. We present two incomparable approximation ratios for this problem with an approximate value oracle and demonstrate that the ratios take on empirically relevant values through a case study with the Influence Threshold problem in online social networks.",
  "stance": 0.5
 },
 {
  "url": "https://proceedings.mlr.press/v97/dereli19a/dereli19a.pdf",
  "title": "A Multitask Multiple Kernel Learning Algorithm for Survival Analysis with Application to Cancer Biology",
  "year": 2019,
  "venue": "ICML",
  "abstract": "Predictive performance of machine learning algorithms on related problems can be improved using multitask learning approaches. Rather than performing survival analysis on each data set to predict survival times of cancer patients, we developed a novel multitask approach based on multiple kernel learning (MKL). Our multitask MKL algorithm both works on multiple cancer data sets and integrates cancer-related pathways/gene sets into survival analysis. We tested our algorithm, which is named as Path2MSurv, on the Cancer Genome Atlas data sets analyzing gene expression profiles of 7,655 patients from 20 cancer types together with cancer-specific pathway/gene set collections. Path2MSurv obtained better or comparable predictive performance when benchmarked against random survival forest, survival support vector machine, and single-task variant of our algorithm. Path2MSurv has the ability to identify key pathways/gene sets in predicting survival times of patients from different cancer types.",
  "stance": 0.9
 },
 {
  "url": "https://proceedings.mlr.press/v97/du19e/du19e.pdf",
  "title": "Task-Agnostic Dynamics Priors for Deep Reinforcement Learning",
  "year": 2019,
  "venue": "ICML",
  "abstract": "While model-based deep reinforcement learning (RL) holds great promise for sample efficiency and generalization, learning an accurate dynamics model is often challenging and requires substantial interaction with the environment. A wide variety of domains have dynamics that share common foundations like the laws of classical mechanics, which are rarely exploited by existing algorithms. In fact, humans continuously acquire and use such dynamics priors to easily adapt to operating in new environments. In this work, we propose an approach to learn task-agnostic dynamics priors from videos and incorporate them into an RL agent. Our method involves pre-training a frame predictor on task-agnostic physics videos to initialize dynamics models (and fine-tune them) for unseen target environments. Our frame prediction architecture, SpatialNet, is designed specifically to capture localized physical phenomena and interactions. Our approach allows for both faster policy learning and convergence to better policies, outperforming competitive approaches on several different environments. We also demonstrate that incorporating this prior allows for more effective transfer between environments.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v97/duncker19a/duncker19a.pdf",
  "title": "Learning interpretable continuous-time models of latent stochastic dynamical systems",
  "year": 2019,
  "venue": "ICML",
  "abstract": "We develop an approach to learn an interpretable semi-parametric model of a latent continuoustime stochastic dynamical system, assuming noisy high-dimensional outputs sampled at uneven times. The dynamics are described by a nonlinear stochastic differential equation (SDE) driven by a Wiener process, with a drift evolution function drawn from a Gaussian process (GP) conditioned on a set of learnt fixed points and corresponding local Jacobian matrices. This form yields a flexible nonparametric model of the dynamics, with a representation corresponding directly to the interpretable portraits routinely employed in the study of nonlinear dynamical systems. The learning algorithm combines inference of continuous latent paths underlying observed data with a sparse variational description of the dynamical process. We demonstrate our approach on simulated data from different nonlinear dynamical systems.",
  "stance": 0.4
 },
 {
  "url": "https://proceedings.mlr.press/v97/edwards19a/edwards19a.pdf",
  "title": "Imitating Latent Policies from Observation",
  "year": 2019,
  "venue": "ICML",
  "abstract": "In this paper, we describe a novel approach to imitation learning that infers latent policies directly from state observations. We introduce a method that characterizes the causal effects of latent actions on observations while simultaneously predicting their likelihood. We then outline an action alignment procedure that leverages a small amount of environment interactions to determine a mapping between the latent and real-world actions. We show that this corrected labeling can be used for imitating the observed behavior, even though no expert actions are given. We evaluate our approach within classic control environments and a platform game and demonstrate that it performs better than standard approaches. Code for this work is available at https://github. com/ashedwards/ILPO.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v97/engstrom19a/engstrom19a.pdf",
  "title": "Exploring the Landscape of Spatial Robustness",
  "year": 2019,
  "venue": "ICML",
  "abstract": "The study of adversarial robustness has so far largely focused on perturbations bound in `pnorms. However, state-of-the-art models turn out to be also vulnerable to other, more natural classes of perturbations such as translations and rotations. In this work, we thoroughly investigate the vulnerability of neural network–based classifiers to rotations and translations. While data augmentation offers relatively small robustness, we use ideas from robust optimization and testtime input aggregation to significantly improve robustness. Finally we find that, in contrast to the `p-norm case, first-order methods cannot reliably find worst-case perturbations. This highlights spatial robustness as a fundamentally different setting requiring additional study.",
  "stance": -0.8
 },
 {
  "url": "https://proceedings.mlr.press/v97/etmann19a/etmann19a.pdf",
  "title": "On the Connection Between Adversarial Robustness and Saliency Map Interpretability",
  "year": 2019,
  "venue": "ICML",
  "abstract": "Recent studies on the adversarial vulnerability of neural networks have shown that models trained to be more robust to adversarial attacks exhibit more interpretable saliency maps than their nonrobust counterparts. We aim to quantify this behavior by considering the alignment between input image and saliency map. We hypothesize that as the distance to the decision boundary grows, so does the alignment. This connection is strictly true in the case of linear models. We confirm these theoretical findings with experiments based on models trained with a local Lipschitz regularization and identify where the non-linear nature of neural networks weakens the relation.",
  "stance": 0.0
 },
 {
  "url": "https://proceedings.mlr.press/v97/gao19b/gao19b.pdf",
  "title": "Deep Generative Learning via Variational Gradient Flow",
  "year": 2019,
  "venue": "ICML",
  "abstract": "We propose a framework to learn deep generative models via Variational Gradient Flow (VGrow) on probability spaces. The evolving distribution that asymptotically converges to the target distribution is governed by a vector field, which is the negative gradient of the first variation of the f -divergence between them. We prove that the evolving distribution coincides with the pushforward distribution through the infinitesimal time composition of residual maps that are perturbations of the identity map along the vector field. The vector field depends on the density ratio of the pushforward distribution and the target distribution, which can be consistently learned from a binary classification problem. Connections of our proposed VGrow method with other popular methods, such as VAE, GAN and flow-based methods, have been established in this framework, gaining new insights of deep generative learning. We also evaluated several commonly used divergences, including Kullback-Leibler, Jensen-Shannon, Jeffreys divergences as well as our newly discovered “logD” divergence which serves as the objective function of the logD-trick GAN. Experimental results on benchmark datasets demonstrate that VGrow can generate high-fidelity images in a stable and efficient manner, achieving competitive performance with state-of-the-art GANs. School of Mathematics and Statistics, Xi’an Jiaotong University, China School of Statistics and Mathematics, Zhongnan University of Economics and Law, China and KLATASDSMOE, School of Statistics, East China Normal University, China Department of Mathematics, The Hong Kong University of Science and Technology, Hong Kong School of Management, Xi’an Jiaotong University, China. Correspondence to: Yuling Jiao <yulingjiaomath@whu.edu.cn>, Can Yang <macyang@ust.hk>. Proceedings of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v97/ghorbani19a/ghorbani19a.pdf",
  "title": "An Instability in Variational Inference for Topic Models",
  "year": 2019,
  "venue": "ICML",
  "abstract": "Naive mean field variational methods are the stateof-the-art approach to inference in topic models. We show that these methods suffer from an instability that can produce misleading conclusions. Namely, for certain regimes of the model parameters, variational inference outputs a non-trivial decomposition into topics. However -–for the same parameter values-– the data contain no actual information about the true topic decomposition, and the output of the algorithm is uncorrelated with it. In particular, the estimated posterior mean is wrong, and estimated credible regions do not achieve the nominal coverage. We discuss how this instability is remedied by more accurate mean field approximations.",
  "stance": -1.0
 },
 {
  "url": "https://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf",
  "title": "Adversarial Examples Are a Natural Consequence of Test Error in Noise",
  "year": 2019,
  "venue": "ICML",
  "abstract": "Over the last few years, the phenomenon of adversarial examples — maliciously constructed inputs that fool trained machine learning models — has captured the attention of the research community, especially when the adversary is restricted to small modifications of a correctly handled input. Less surprisingly, image classifiers also lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this paper we provide both empirical and theoretical evidence that these are two manifestations of the same underlying phenomenon. We establish close connections between the adversarial robustness and corruption robustness research programs, with the strongest connection in the case of additive Gaussian noise. This suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. Based on our results we recommend that future adversarial defenses consider evaluating the robustness of their methods to distributional shift with benchmarks such as ImageNet-C.",
  "stance": -1.0
 },
 {
  "url": "https://proceedings.mlr.press/v97/greenfeld19a/greenfeld19a.pdf",
  "title": "Learning to Optimize Multigrid PDE Solvers",
  "year": 2019,
  "venue": "ICML",
  "abstract": "Constructing fast numerical solvers for partial differential equations (PDEs) is crucial for many scientific disciplines. A leading technique for solving large-scale PDEs is using multigrid methods. At the core of a multigrid solver is the prolongation matrix, which relates between different scales of the problem. This matrix is strongly problem-dependent, and its optimal construction is critical to the efficiency of the solver. In practice, however, devising multigrid algorithms for new problems often poses formidable challenges. In this paper we propose a framework for learning multigrid solvers. Our method learns a (single) mapping from a family of parameterized PDEs to prolongation operators. We train a neural network once for the entire class of PDEs, using an efficient and unsupervised loss function. Experiments on a broad class of 2D diffusion problems demonstrate improved convergence rates compared to the widely used Black-Box multigrid scheme, suggesting that our method successfully learned rules for constructing prolongation matrices.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v97/hanin19a/hanin19a.pdf",
  "title": "Complexity of Linear Regions in Deep Networks",
  "year": 2019,
  "venue": "ICML",
  "abstract": "It is well-known that the expressivity of a neural network depends on its architecture, with deeper networks expressing more complex functions. In the case of networks that compute piecewise linear functions, such as those with ReLU activation, the number of distinct linear regions is a natural measure of expressivity. It is possible to construct networks with merely a single region, or for which the number of linear regions grows exponentially with depth; it is not clear where within this range most networks fall in practice, either before or after training. In this paper, we provide a mathematical framework to count the number of linear regions of a piecewise linear network and measure the volume of the boundaries between these regions. In particular, we prove that for networks at initialization, the average number of regions along any one-dimensional subspace grows linearly in the total number of neurons, far below the exponential upper bound. We also find that the average distance to the nearest region boundary at initialization scales like the inverse of the number of neurons. Our theory suggests that, even after training, the number of linear regions is far below exponential, an intuition that matches our empirical observations. We conclude that the practical expressivity of neural networks is likely far below that of the theoretical maximum, and that this gap can be quantified.",
  "stance": 0.3
 },
 {
  "url": "https://proceedings.mlr.press/v97/heidari19a/heidari19a.pdf",
  "title": "On the Long-term Impact of Algorithmic Decision Policies: Effort Unfairness and Feature Segregation through Social Learning",
  "year": 2019,
  "venue": "ICML",
  "abstract": "Most existing notions of algorithmic fairness are one-shot: they ensure some form of allocative equality at the time of decision making, but do not account for the adverse impact of the algorithmic decisions today on the long-term welfare and prosperity of certain segments of the population. We take a broader perspective on algorithmic fairness. We propose an effort-based measure of fairness and present a data-driven framework for characterizing the long-term impact of algorithmic policies on reshaping the underlying population. Motivated by the psychological literature on social learning and the economic literature on equality of opportunity, we propose a micro-scale model of how individuals may respond to decision making algorithms. We employ existing measures of segregation from sociology and economics to quantify the resulting macroscale population-level change. Importantly, we observe that different models may shift the groupconditional distribution of qualifications in different directions. Our findings raise a number of important questions regarding the formalization of fairness for decision-making models.",
  "stance": 0.2
 },
 {
  "url": "https://proceedings.mlr.press/v97/hendrycks19a/hendrycks19a.pdf",
  "title": "Using Pre-Training Can Improve Model Robustness and Uncertainty",
  "year": 2019,
  "venue": "ICML",
  "abstract": "He et al. (2018) have called into question the utility of pre-training by showing that training from scratch can often yield similar performance to pre-training. We show that although pre-training may not improve performance on traditional classification metrics, it improves model robustness and uncertainty estimates. Through extensive experiments on label corruption, class imbalance, adversarial examples, out-of-distribution detection, and confidence calibration, we demonstrate large gains from pre-training and complementary effects with task-specific methods. We show approximately a 10% absolute improvement over the previous state-of-the-art in adversarial robustness. In some cases, using pre-training without task-specific methods also surpasses the stateof-the-art, highlighting the need for pre-training when evaluating future methods on robustness and uncertainty tasks.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v97/jaber19a/jaber19a.pdf",
  "title": "Causal Identification under Markov Equivalence: Completeness Results",
  "year": 2019,
  "venue": "ICML",
  "abstract": "Causal effect identification is the task of determining whether a causal distribution is computable from the combination of an observational distribution and substantive knowledge about the domain under investigation. One of the most studied versions of this problem assumes that knowledge is articulated in the form of a fully known causal diagram, which is arguably a strong assumption in many settings. In this paper, we relax this requirement and consider that the knowledge is articulated in the form of an equivalence class of causal diagrams, in particular, a partial ancestral graph (PAG). This is attractive because a PAG can be learned directly from data, and the scientist does not need to commit to a particular, unique diagram. There are different sufficient conditions for identification in PAGs, but none is complete. We derive a complete algorithm for identification given a PAG. This implies that whenever the causal effect is identifiable, the algorithm returns a valid identification expression; alternatively, it will throw a failure condition, which means that the effect is provably not identifiable. We further provide a graphical characterization of nonidentifiability of causal effects in PAGs.",
  "stance": 0.5
 },
 {
  "url": "https://proceedings.mlr.press/v97/jeong19d/jeong19d.pdf",
  "title": "Learning Discrete and Continuous Factors of Data via Alternating Disentanglement",
  "year": 2019,
  "venue": "ICML",
  "abstract": "We address the problem of unsupervised disentanglement of discrete and continuous explanatory factors of data. We first show a simple procedure for minimizing the total correlation of the continuous latent variables without having to use a discriminator network or perform importance sampling, via cascading the information flow in the β-vae framework. Furthermore, we propose a method which avoids offloading the entire burden of jointly modeling the continuous and discrete factors to the variational encoder by employing a separate discrete inference procedure. This leads to an interesting alternating minimization problem which switches between finding the most likely discrete configuration given the continuous factors and updating the variational encoder based on the computed discrete factors. Experiments show that the proposed method clearly disentangles discrete factors and significantly outperforms current disentanglement methods based on the disentanglement score and inference network classification score. The source code is available at https://github.com/snumllab/DisentanglementICML19.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v97/lawrence19a/lawrence19a.pdf",
  "title": "DP-GP-LVM: A Bayesian Non-Parametric Model for Learning Multivariate Dependency Structures",
  "year": 2019,
  "venue": "ICML",
  "abstract": "We present a non-parametric Bayesian latent variable model capable of learning dependency structures across dimensions in a multivariate setting. Our approach is based on flexible Gaussian process priors for the generative mappings and interchangeable Dirichlet process priors to learn the structure. The introduction of the Dirichlet process as a specific structural prior allows our model to circumvent issues associated with previous Gaussian process latent variable models. Inference is performed by deriving an efficient variational bound on the marginal log-likelihood of the model. We demonstrate the efficacy of our approach via analysis of discovered structure and superior quantitative performance on missing data imputation.",
  "stance": 0.8
 },
 {
  "url": "https://proceedings.mlr.press/v97/li19c/li19c.pdf",
  "title": "LGM-Net: Learning to Generate Matching Networks for Few-Shot Learning",
  "year": 2019,
  "venue": "ICML",
  "abstract": "In this work, we propose a novel meta-learning approach for few-shot classification, which learns transferable prior knowledge across tasks and directly produces network parameters for similar unseen tasks with training samples. Our approach, called LGM-Net, includes two key modules, namely, TargetNet and MetaNet. The TargetNet module is a neural network for solving a specific task and the MetaNet module aims at learning to generate functional weights for TargetNet by observing training samples. We also present an intertask normalization strategy for the training process to leverage common information shared across different tasks. The experimental results on Omniglot and miniImageNet datasets demonstrate that LGM-Net can effectively adapt to similar unseen tasks and achieve competitive performance, and the results on synthetic datasets show that transferable prior knowledge is learned by the MetaNet module via mapping training data to functional weights. LGM-Net enables fast learning and adaptation since no further tuning steps are required compared to other metalearning approaches.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v97/li19n/li19n.pdf",
  "title": "Alternating Minimizations Converge to Second-Order Optimal Solutions",
  "year": 2019,
  "venue": "ICML",
  "abstract": "This work studies the second-order convergence for both standard alternating minimization and proximal alternating minimization. We show that under mild assumptions on the (nonconvex) objective function, both algorithms avoid strict saddles almost surely from random initialization. Together with known first-order convergence results, this implies that both algorithms converge to a second-order stationary point. This solves an open problem for the second-order convergence of alternating minimization algorithms that have been widely used in practice to solve large-scale nonconvex problems due to their simple implementation, fast convergence, and superb empirical performance.",
  "stance": 0.5
 },
 {
  "url": "https://proceedings.mlr.press/v119/aitchison20a/aitchison20a.pdf",
  "title": "Why bigger is not always better: on finite and infinite neural networks",
  "year": 2020,
  "venue": "ICML",
  "abstract": "Recent work has argued that neural networks can be understood theoretically by taking the number of channels to infinity, at which point the outputs become Gaussian process (GP) distributed. However, we note that infinite Bayesian neural networks lack a key facet of the behaviour of real neural networks: the fixed kernel, determined only by network hyperparameters, implies that they cannot do any form of representation learning. The lack of representation or equivalently kernel learning leads to less flexibility and hence worse performance, giving a potential explanation for the inferior performance of infinite networks observed in the literature (e.g. Novak et al. 2019). We give analytic results characterising the prior over representations and representation learning in finite deep linear networks. We show empirically that the representations in SOTA architectures such as ResNets trained with SGD are much closer to those suggested by our deep linear results than by the corresponding infinite network. This motivates the introduction of a new class of network: infinite networks with bottlenecks, which inherit the theoretical tractability of infinite networks while at the same time allowing representation learning. One approach to understanding and improving neural networks is to perform Bayesian inference in an infinitely wide network (Lee et al., 2018; Matthews et al., 2018; GarrigaAlonso et al., 2019; Novak et al., 2019). In this limit the outputs become Gaussian process distributed, enabling efficient and exact reasoning about uncertainty, and giving a means of interpretation using the parameter-free kernel function (which depends only on network hyperparameters such as depth). However, the performance of Bayesian infinite networks lags considerably behind state-of-the-art finite University of Bristol, Bristol, UK. Correspondence to: Laurence Aitchison <laurence.aitchison@gmail.com>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). networks trained using SGD (e.g. compare performance in Garriga-Alonso et al. (2019), Novak et al. (2019) and Arora et al. (2019) against He et al. (2016) and Chen et al. (2018)). This seems surprising, because, to our knowledge, there are no reports of wider networks degrading classification performance (indeed, the opposite is sometimes argued; see Zagoruyko & Komodakis, 2016), and because exact Bayesian inference is provably optimal, if the prior accurately describes our beliefs (Ramsey, 1926). Indeed, recent work on the Neural Tangent Kernel (NTK) (Li et al., 2019) has suggested that deterministic gradient descent in an infinite network gives slighly lower performance than Bayesian inference in the same network. Our hypothesis is that the poor performance of Bayesian infinite networks arises because the top-layer representation (equivalent to the kernel), is fixed by the network hyperparameters, and thus cannot be learned from data. This breaks many of our key intuitions about why deep networks are effective. For instance in transfer learning (Huh et al., 2016) we use a large-scale dataset such as ImageNet to a learn a good high-level representation, then apply this representation to other tasks where less data is available. However, transfer learning is impossible in infinite Bayesian neural networks, because the top-layer representation is fixed by the network hyperparameters and so cannot be learned using e.g. ImageNet. To understand these issues, we analysed finite networks using tools from the infinite network literature (Lee et al., 2018; Matthews et al., 2018; Garriga-Alonso et al., 2019; Novak et al., 2019). We begin by giving a toy, two-layer example, contrasting the flexibility of finite networks with the inflexibility of infinite networks, showing that flexible finite networks offer benefits under conditions of model-mismatch. We then introduce infinite networks with bottlenecks, which combine the theoretical tractability of infinite networks with the flexibility of finite networks. To obtain an analytic understanding of kernel/representation flexibility and learning in such networks, we consider linear infinite networks with bottlenecks, which are equivalent to finite deep linear networks. We took two approaches to characterising these networks. First, we considered the prior viewpoint, i.e. the covariance in the top-layer kernel induced by randomness in the lower-layer weights. In particular, we showed that narrower, deeper networks offer more flexibility, and that Why bigger is not always better: on finite and infinite neural networks CNNs offer more flexibility than locally connected networks (LCNs) when the input is spatially structured. Second, we considered the posterior viewpoint, showing that under both MAP inference and posterior sampling, the representations in learned neural networks slowly transition from being similar to the input kernel (i.e. the inner product of the inputs) to being similar to the output kernel (i.e. the inner product of one-hot vectors representing the labels). We found an important difference between MAP inference and sampling: for MAP inference, the learned representations transition from the input to the output kernel, irrespective of the network width. Bayesian networks behave similarly when the network width and the number of output channels are equal, but as the network width increases, the learned representations become increasingly dominated by the prior, and insensitive to the outputs. Remarkably, we find that in a ResNet trained using SGD on CIFAR-10, the representation differs dramatically from the corresponding infinite network and is instead very close to the output kernel, as suggested by our deep linear results. This confirms the importance of working with a theoretical model, such as infinite networks with bottlenecks, that is capable of capturing representation learning.",
  "stance": -0.5
 },
 {
  "url": "https://proceedings.mlr.press/v119/anokhin20a/anokhin20a.pdf",
  "title": "Low-loss connection of weight vectors: distribution-based approaches",
  "year": 2020,
  "venue": "ICML",
  "abstract": "Recent research shows that sublevel sets of the loss surfaces of overparameterized networks are connected, exactly or approximately. We describe and compare experimentally a panel of methods used to connect two low-loss points by a low-loss curve on this surface. Our methods vary in accuracy and complexity. Most of our methods are based on “macroscopic” distributional assumptions, and some are insensitive to the detailed properties of the points to be connected. Some methods require a prior training of a “global connection model” which can then be applied to any pair of points. The accuracy of the method generally correlates with its complexity and sensitivity to the endpoint detail.",
  "stance": 0.0
 },
 {
  "url": "https://proceedings.mlr.press/v119/assran20a/assran20a.pdf",
  "title": "On the Convergence of Nesterov’s Accelerated Gradient Method in Stochastic Settings",
  "year": 2020,
  "venue": "ICML",
  "abstract": "We study Nesterov’s accelerated gradient method with constant step-size and momentum parameters in the stochastic approximation setting (unbiased gradients with bounded variance) and the finite-sum setting (where randomness is due to sampling mini-batches). To build better insight into the behavior of Nesterov’s method in stochastic settings, we focus throughout on objectives that are smooth, strongly-convex, and twice continuously differentiable. In the stochastic approximation setting, Nesterov’s method converges to a neighborhood of the optimal point at the same accelerated rate as in the deterministic setting. Perhaps surprisingly, in the finite-sum setting, we prove that Nesterov’s method may diverge with the usual choice of step-size and momentum, unless additional conditions on the problem related to conditioning and data coherence are satisfied. Our results shed light as to why Nesterov’s method may fail to converge or achieve acceleration in the finite-sum setting.",
  "stance": -0.5
 },
 {
  "url": "https://proceedings.mlr.press/v119/ayoub20a/ayoub20a.pdf",
  "title": "Model-Based Reinforcement Learning with Value-Targeted Regression",
  "year": 2020,
  "venue": "ICML",
  "abstract": "This paper studies model-based reinforcement learning (RL) for regret minimization. We focus on finite-horizon episodic RL where the transition model P belongs to a known family of models P , a special case of which is when models in P take the form of linear mixtures: P✓ = P d i=1 ✓iPi. We propose a model based RL algorithm that is based on the optimism principle: In each episode, the set of models that are ‘consistent’ with the data collected is constructed. The criterion of consistency is based on the total squared error that the model incurs on the task of predicting state values as determined by the last value estimate along the transitions. The next value function is then chosen by solving the optimistic planning problem with the constructed set of models. We derive a bound on the regret, which, in the special case of linear mixtures, takes the form Õ(d p H3T ), where H , T and d are the horizon, the total number of steps and the dimension of ✓, respectively. In particular, this regret bound is independent of the total number of states or actions, and is close to a lower bound ⌦( p HdT ). For a general model family P , the regret bound is derived based on the Eluder dimension.",
  "stance": 0.0
 },
 {
  "url": "https://proceedings.mlr.press/v119/bengio20a/bengio20a.pdf",
  "title": "Interference and Generalization in Temporal Difference Learning",
  "year": 2020,
  "venue": "ICML",
  "abstract": "We study the link between generalization and interference in temporal-difference (TD) learning. Interference is defined as the inner product of two different gradients, representing their alignment; this quantity emerges as being of interest from a variety of observations about neural networks, parameter sharing and the dynamics of learning. We find that TD easily leads to low-interference, under-generalizing parameters, while the effect seems reversed in supervised learning. We hypothesize that the cause can be traced back to the interplay between the dynamics of interference and bootstrapping. This is supported empirically by several observations: the negative relationship between the generalization gap and interference in TD, the negative effect of bootstrapping on interference and the local coherence of targets, and the contrast between the propagation rate of information in TD(0) versus TD(λ) and regression tasks such as Monte-Carlo policy evaluation. We hope that these new findings can guide the future discovery of better bootstrapping methods.",
  "stance": 0.0
 },
 {
  "url": "https://proceedings.mlr.press/v119/blumenfeld20a/blumenfeld20a.pdf",
  "title": "Beyond Signal Propagation: Is Feature Diversity Necessary in Deep Neural Network Initialization?",
  "year": 2020,
  "venue": "ICML",
  "abstract": "Deep neural networks are typically initialized with random weights, with variances chosen to facilitate signal propagation and stable gradients. It is also believed that diversity of features is an important property of these initializations. We construct a deep convolutional network with identical features by initializing almost all the weights to 0. The architecture also enables perfect signal propagation and stable gradients, and achieves high accuracy on standard benchmarks. This indicates that random, diverse initializations are not necessary for training neural networks. An essential element in training this network is a mechanism of symmetry breaking; we study this phenomenon and find that standard GPU operations, which are non-deterministic, can serve as a sufficient source of symmetry breaking to enable training.",
  "stance": 0.9
 },
 {
  "url": "https://proceedings.mlr.press/v119/braverman20a/braverman20a.pdf",
  "title": "Calibration, Entropy Rates, and Memory in Language Models",
  "year": 2020,
  "venue": "ICML",
  "abstract": "Building accurate language models that capture meaningful long-term dependencies is a core challenge in natural language processing. Towards this end, we present a calibration-based approach to measure long-term discrepancies between a generative sequence model and the true distribution, and use these discrepancies to improve the model. Empirically, we show that stateof-the-art language models, including LSTMs and Transformers, are miscalibrated: the entropy rates of their generations drift dramatically upward over time. We then provide provable methods to mitigate this phenomenon. Furthermore, we show how this calibration-based approach can also be used to measure the amount of memory that language models use for prediction.",
  "stance": -0.5
 },
 {
  "url": "https://proceedings.mlr.press/v119/chatterjee20a/chatterjee20a.pdf",
  "title": "Circuit-Based Intrinsic Methods to Detect Overfitting",
  "year": 2020,
  "venue": "ICML",
  "abstract": "The focus of this paper is on intrinsic methods to detect overfitting. By intrinsic methods, we mean methods that rely only on the model and the training data, as opposed to traditional methods (we call them extrinsic methods) that rely on performance on a test set or on bounds from model complexity. We propose a family of intrinsic methods called Counterfactual Simulation (CFS) which analyze the flow of training examples through the model by identifying and perturbing rare patterns. By applying CFS to logic circuits we get a method that has no hyper-parameters and works uniformly across different types of models such as neural networks, random forests and lookup tables. Experimentally, CFS can separate models with different levels of overfit using only their logic circuit representations without any access to the high level structure. By comparing lookup tables, neural networks, and random forests using CFS, we get insight into why neural networks generalize. In particular, we find that stochastic gradient descent in neural nets does not lead to “brute force” memorization, but finds common patterns (whether we train with actual or randomized labels), and neural networks are not unlike forests in this regard. Finally, we identify a limitation with our proposal that makes it unsuitable in an adversarial setting, but points the way to future work on robust intrinsic methods.",
  "stance": -0.2
 },
 {
  "url": "https://proceedings.mlr.press/v119/chen20q/chen20q.pdf",
  "title": "More Data Can Expand The Generalization Gap Between Adversarially Robust and Standard Models",
  "year": 2020,
  "venue": "ICML",
  "abstract": "Despite remarkable success in practice, modern machine learning models have been found to be susceptible to adversarial attacks that make human-imperceptible perturbations to the data, but result in serious and potentially dangerous prediction errors. To address this issue, practitioners often use adversarial training to learn models that are robust against such attacks at the cost of higher generalization error on unperturbed test sets. The conventional wisdom is that more training data should shrink the gap between the generalization error of adversarially-trained models and standard models. However, we study the training of robust classifiers for both Gaussian and Bernoulli models under `∞ attacks, and we prove that more data may actually increase this gap. Furthermore, our theoretical results identify if and when additional data will finally begin to shrink the gap. Lastly, we experimentally demonstrate that our results also hold for linear regression models, which may indicate that this phenomenon occurs more broadly.",
  "stance": 0.0
 },
 {
  "url": "https://proceedings.mlr.press/v119/claici20a/claici20a.pdf",
  "title": "Model Fusion with Kullback-Leibler Divergence",
  "year": 2020,
  "venue": "ICML",
  "abstract": "We propose a method to fuse posterior distributions learned from heterogeneous datasets. Our algorithm relies on a mean field assumption for both the fused model and the individual dataset posteriors and proceeds using a simple assign-andaverage approach. The components of the dataset posteriors are assigned to the proposed global model components by solving a regularized variant of the assignment problem. The global components are then updated based on these assignments by their mean under a KL divergence. For exponential family variational distributions, our formulation leads to an efficient non-parametric algorithm for computing the fused model. Our algorithm is easy to describe and implement, efficient, and competitive with state-of-the-art on motion capture analysis, topic modeling, and federated learning of Bayesian neural networks.1",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v119/dai20c/dai20c.pdf",
  "title": "The Usual Suspects? Reassessing Blame for VAE Posterior Collapse",
  "year": 2020,
  "venue": "ICML",
  "abstract": "In narrow asymptotic settings Gaussian VAE models of continuous data have been shown to possess global optima aligned with ground-truth distributions. Even so, it is well known that poor solutions whereby the latent posterior collapses to an uninformative prior are sometimes obtained in practice. However, contrary to conventional wisdom that largely assigns blame for this phenomena on the undue influence of KL-divergence regularization, we will argue that posterior collapse is, at least in part, a direct consequence of bad local minima inherent to the loss surface of deep autoencoder networks. In particular, we prove that even small nonlinear perturbations of affine VAE decoder models can produce such minima, and in deeper models, analogous minima can force the VAE to behave like an aggressive truncation operator, provably discarding information along all latent dimensions in certain circumstances. Regardless, the underlying message here is not meant to undercut valuable existing explanations of posterior collapse, but rather, to refine the discussion and elucidate alternative risk factors that may have been previously underappreciated.",
  "stance": -0.7
 },
 {
  "url": "https://proceedings.mlr.press/v119/dong20e/dong20e.pdf",
  "title": "Collapsed Amortized Variational Inference for Switching Nonlinear Dynamical Systems",
  "year": 2020,
  "venue": "ICML",
  "abstract": "We propose an efficient inference method for switching nonlinear dynamical systems. The key idea is to learn an inference network which can be used as a proposal distribution for the continuous latent variables, while performing exact marginalization of the discrete latent variables. This allows us to use the reparameterization trick, and apply end-to-end training with stochastic gradient descent. We show that the proposed method can successfully segment time series data, including videos and 3D human pose, into meaningful “regimes” by using the piece-wise nonlinear dynamics.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v119/dupont20a/dupont20a.pdf",
  "title": "Equivariant Neural Rendering",
  "year": 2020,
  "venue": "ICML",
  "abstract": "We propose a framework for learning neural scene representations directly from images, without 3D supervision. Our key insight is that 3D structure can be imposed by ensuring that the learned representation transforms like a real 3D scene. Specifically, we introduce a loss which enforces equivariance of the scene representation with respect to 3D transformations. Our formulation allows us to infer and render scenes in real time while achieving comparable results to models requiring minutes for inference. In addition, we introduce two challenging new datasets for scene representation and neural rendering, including scenes with complex lighting and backgrounds. Through experiments, we show that our model achieves compelling results on these datasets as well as on standard ShapeNet benchmarks.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v119/dutta20a/dutta20a.pdf",
  "title": "Is There a Trade-Off Between Fairness and Accuracy? A Perspective Using Mismatched Hypothesis Testing",
  "year": 2020,
  "venue": "ICML",
  "abstract": "A trade-off between accuracy and fairness is almost taken as a given in the existing literature on fairness in machine learning. Yet, it is not preordained that accuracy should decrease with increased fairness. Novel to this work, we examine fair classification through the lens of mismatched hypothesis testing: trying to find a classifier that distinguishes between two ideal distributions when given two mismatched distributions that are biased. Using Chernoff information, a tool in information theory, we theoretically demonstrate that, contrary to popular belief, there always exist ideal distributions such that optimal fairness and accuracy (with respect to the ideal distributions) are achieved simultaneously: there is no trade-off. Moreover, the same classifier yields the lack of a trade-off with respect to ideal distributions while yielding a trade-off when accuracy is measured with respect to the given (possibly biased) dataset. To complement our main result, we formulate an optimization to find ideal distributions and derive fundamental limits to explain why a trade-off exists on the given biased dataset. We also derive conditions under which active data collection can alleviate the fairness-accuracy trade-off in the real world. Our results lead us to contend that it is problematic to measure accuracy with respect to data that reflects bias, and instead, we should be considering accuracy with respect to ideal, unbiased data.",
  "stance": 0.4
 },
 {
  "url": "https://proceedings.mlr.press/v119/eftekhari20a/eftekhari20a.pdf",
  "title": "Training Linear Neural Networks: Non-Local Convergence and Complexity Results",
  "year": 2020,
  "venue": "ICML",
  "abstract": "Linear networks provide valuable insights into the workings of neural networks in general. This paper identifies conditions under which the gradient flow provably trains a linear network, in spite of the non-strict saddle points present in the optimization landscape. This paper also provides the computational complexity of training linear networks with gradient flow. To achieve these results, this work develops a machinery to provably identify the stable set of gradient flow, which then enables us to improve over the state of the art in the literature of linear networks (Bah et al., 2019; Arora et al., 2018a). Crucially, our results appear to be the first to break away from the lazy training regime which has dominated the literature of neural networks. This work requires the network to have a layer with one neuron, which subsumes the networks with a scalar output, but extending the results of this theoretical work to all linear networks remains a challenging open problem.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v119/engstrom20a/engstrom20a.pdf",
  "title": "Identifying Statistical Bias in Dataset Replication",
  "year": 2020,
  "venue": "ICML",
  "abstract": "Dataset replication is a useful tool for assessing whether improvements in test accuracy on a specific benchmark correspond to improvements in models’ ability to generalize reliably. In this work, we present unintuitive yet significant ways in which standard approaches to dataset replication introduce statistical bias, skewing the resulting observations. We study ImageNet-v2, a replication of the ImageNet dataset on which models exhibit a significant (11-14%) drop in accuracy, even after controlling for selection frequency, a human-in-the-loop measure of data quality. We show that after remeasuring selection frequencies and correcting for statistical bias, only an estimated 3.6%±1.5% of the original 11.7%±1.0% accuracy drop remains unaccounted for. We conclude with concrete recommendations for recognizing and avoiding bias in dataset replication. Code for our study is publicly available.",
  "stance": 0.2
 },
 {
  "url": "https://proceedings.mlr.press/v119/fedus20a/fedus20a.pdf",
  "title": "Revisiting Fundamentals of Experience Replay",
  "year": 2020,
  "venue": "ICML",
  "abstract": "Experience replay is central to off-policy algorithms in deep reinforcement learning (RL), but there remain significant gaps in our understanding. We therefore present a systematic and extensive analysis of experience replay in Q-learning methods, focusing on two fundamental properties: the replay capacity and the ratio of learning updates to experience collected (replay ratio). Our additive and ablative studies upend conventional wisdom around experience replay — greater capacity is found to substantially increase the performance of certain algorithms, while leaving others unaffected. Counterintuitively we show that theoretically ungrounded, uncorrected n-step returns are uniquely beneficial while other techniques confer limited benefit for sifting through larger memory. Separately, by directly controlling the replay ratio we contextualize previous observations in the literature and empirically measure its importance across a variety of deep RL algorithms. Finally, we conclude by testing a set of hypotheses on the nature of these performance benefits.",
  "stance": 0.4
 },
 {
  "url": "https://proceedings.mlr.press/v119/feng20b/feng20b.pdf",
  "title": "Global Concavity and Optimization in a Class of Dynamic Discrete Choice Models",
  "year": 2020,
  "venue": "ICML",
  "abstract": "Discrete choice models with unobserved heterogeneity are commonly used Econometric models for dynamic Economic behavior which have been adopted in practice to predict behavior of individuals and firms from schooling and job choices to strategic decisions in market competition. These models feature optimizing agents who choose among a finite set of options in a sequence of periods and receive choice-specific payoffs that depend on both variables that are observed by the agent and recorded in the data and variables that are only observed by the agent but not recorded in the data. Existing work in Econometrics assumes that optimizing agents are fully rational and requires finding a functional fixed point to find the optimal policy. We show that in an important class of discrete choice models the value function is globally concave in the policy. That means that simple algorithms that do not require fixed point computation, such as the policy gradient algorithm, globally converge to the optimal policy. This finding can both be used to relax behavioral assumption regarding the optimizing agents and to facilitate Econometric analysis of dynamic behavior. In particular, we demonstrate significant computational advantages in using a simple implementation policy gradient algorithm over existing “nested fixed point” algorithms used in Econometrics. Equal contribution Department of Computer Science, Northwestern University, Evanston, IL, USA Department of Economics, University of Virginia, Charlottesville, VA, USA. Correspondence to: Y.F. <yidingfeng2021@u.northwestern.edu>, E.K. <eak5rf@virginia.edu>, D.N. <denis@virginia.edu>. Proceedings of the 37 th International Conference on Machine Learning, Online, PMLR 119, 2020. Copyright 2020 by the author(s).",
  "stance": 0.2
 },
 {
  "url": "https://proceedings.mlr.press/v119/ghari20a/ghari20a.pdf",
  "title": "Online Multi-Kernel Learning with Graph-Structured Feedback",
  "year": 2020,
  "venue": "ICML",
  "abstract": "is more powerful, as it learns the optimal kernel from a dicMulti-kernel learning (MKL) exhibits reliable performance in nonlinear function approximation tasks. Instead of using one kernel, it learns the optimal kernel from a pre-selected dictionary of kernels. The selection of the dictionary has crucial impact on both the performance and complexity of MKL. Specifcally, inclusion of a large number of irrelevant kernels may impair the accuracy, and increase the complexity of MKL algorithms. To enhance the accuracy, and alleviate the computational burden, the present paper develops a novel scheme which actively chooses relevant kernels. The proposed framework models the pruned kernel combination as feedback collected from a graph, that is refned ‘on the fy.’ Leveraging the random feature approximation, we propose an online scalable multi-kernel learning approach with graph feedback, and prove that the proposed algorithm enjoys sublinear regret. Numerical tests on real datasets demonstrate the effectiveness of the novel approach.",
  "stance": 0.7
 },
 {
  "url": "https://proceedings.mlr.press/v119/gottesman20a/gottesman20a.pdf",
  "title": "Interpretable Off-Policy Evaluation in Reinforcement Learning by Highlighting Influential Transitions",
  "year": 2020,
  "venue": "ICML",
  "abstract": "Off-policy evaluation in reinforcement learning offers the chance of using observational data to improve future outcomes in domains such as healthcare and education, but safe deployment in high stakes settings requires ways of assessing its validity. Traditional measures such as confidence intervals may be insufficient due to noise, limited data and confounding. In this paper we develop a method that could serve as a hybrid human-AI system, to enable human experts to analyze the validity of policy evaluation estimates. This is accomplished by highlighting observations in the data whose removal will have a large effect on the OPE estimate, and formulating a set of rules for choosing which ones to present to domain experts for validation. We develop methods to compute exactly the influence functions for fitted Q-evaluation with two different function classes: kernel-based and linear least squares, as well as importance sampling methods. Experiments on medical simulations and real-world intensive care unit data demonstrate that our method can be used to identify limitations in the evaluation process and make evaluation more robust.",
  "stance": 0.6
 },
 {
  "url": "https://proceedings.mlr.press/v119/goyal20a/goyal20a.pdf",
  "title": "PoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Elimination",
  "year": 2020,
  "venue": "ICML",
  "abstract": "We develop a novel method, called PoWER-BERT, for improving the inference time of the popular BERT model, while maintaining the accuracy. It works by: a) exploiting redundancy pertaining to word-vectors (intermediate transformer block outputs) and eliminating the redundant vectors. b) determining which wordvectors to eliminate by developing a strategy for measuring their significance, based on the selfattention mechanism. c) learning how many word-vectors to eliminate by augmenting the BERT model and the loss function. Experiments on the standard GLUE benchmark shows that PoWER-BERT achieves up to 4.5x reduction in inference time over BERT with < 1% loss in accuracy. We show that PoWER-BERT offers significantly better trade-off between accuracy and inference time compared to prior methods. We demonstrate that our method attains up to 6.8x reduction in inference time with < 1% loss in accuracy when applied over ALBERT, a highly compressed version of BERT. The code for PoWER-BERT is publicly available at https: //github.com/IBM/PoWER-BERT.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v119/goyal20b/goyal20b.pdf",
  "title": "PackIt: A Virtual Environment for Geometric Planning",
  "year": 2020,
  "venue": "ICML",
  "abstract": "The ability to jointly understand the geometry of objects and plan actions for manipulating them is crucial for intelligent agents. We refer to this ability as geometric planning. Recently, many interactive environments have been proposed to evaluate intelligent agents on various skills, however, none of them cater to the needs of geometric planning. We present PackIt, a virtual environment to evaluate and potentially learn the ability to do geometric planning, where an agent needs to take a sequence of actions to pack a set of objects into a box with limited space. We also construct a set of challenging packing tasks using an evolutionary algorithm. Further, we study various baselines for the task that include model-free learning-based and heuristic-based methods, as well as search-based optimization methods that assume access to the model of the environment.2",
  "stance": 0.6
 },
 {
  "url": "https://proceedings.mlr.press/v119/hacohen20a/hacohen20a.pdf",
  "title": "Let’s Agree to Agree: Neural Networks Share Classification Order on Real Datasets",
  "year": 2020,
  "venue": "ICML",
  "abstract": "We report a series of robust empirical observations, demonstrating that deep Neural Networks learn the examples in both the training and test sets in a similar order. This phenomenon is observed in all the commonly used benchmarks we evaluated, including many image classification benchmarks, and one text classification benchmark. While this phenomenon is strongest for models of the same architecture, it also crosses architectural boundaries – models of different architectures start by learning the same examples, after which the more powerful model may continue to learn additional examples. We further show that this pattern of results reflects the interplay between the way neural networks learn benchmark datasets. Specifically, when fixing the architecture, we describe synthetic datasets for which this pattern is no longer observed. When fixing the dataset, we show that other learning paradigms may learn the data in a different order. We hypothesize that our results reflect how neural networks discover structure in natural datasets.",
  "stance": 0.2
 },
 {
  "url": "https://proceedings.mlr.press/v119/hanzely20a/hanzely20a.pdf",
  "title": "Stochastic Subspace Cubic Newton Method",
  "year": 2020,
  "venue": "ICML",
  "abstract": "In this paper, we propose a new randomized second-order optimization algorithm—Stochastic Subspace Cubic Newton (SSCN)—for minimizing a high dimensional convex function f . Our method can be seen both as a stochastic extension of the cubically-regularized Newton method of Nesterov and Polyak (2006), and a second-order enhancement of stochastic subspace descent of Kozak et al. (2019). We prove that as we vary the minibatch size, the global convergence rate of SSCN interpolates between the rate of stochastic coordinate descent (CD) and the rate of cubic regularized Newton, thus giving new insights into the connection between first and second-order methods. Remarkably, the local convergence rate of SSCN matches the rate of stochastic subspace descent applied to the problem of minimizing the quadratic function 12 (x−x ∗)>∇2f(x∗)(x−x∗), where x∗ is the minimizer of f , and hence depends on the properties of f at the optimum only. Our numerical experiments show that SSCN outperforms non-accelerated first-order CD algorithms while being competitive to their accelerated variants.",
  "stance": 1.0
 },
 {
  "url": "https://proceedings.mlr.press/v119/hasanzadeh20a/hasanzadeh20a.pdf",
  "title": "Bayesian Graph Neural Networks with Adaptive Connection Sampling",
  "year": 2020,
  "venue": "ICML",
  "abstract": "We propose a unified framework for adaptive connection sampling in graph neural networks (GNNs) that generalizes existing stochastic regularization methods for training GNNs. The proposed framework not only alleviates oversmoothing and over-fitting tendencies of deep GNNs, but also enables learning with uncertainty in graph analytic tasks with GNNs. Instead of using fixed sampling rates or hand-tuning them as model hyperparameters as in existing stochastic regularization methods, our adaptive connection sampling can be trained jointly with GNN model parameters in both global and local fashions. GNN training with adaptive connection sampling is shown to be mathematically equivalent to an efficient approximation of training Bayesian GNNs. Experimental results with ablation studies on benchmark datasets validate that adaptively learning the sampling rate given graph training data is the key to boosting the performance of GNNs in semi-supervised node classification, making them less prone to over-smoothing and over-fitting with more robust prediction.",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2000/file/04df4d434d481c5bb723be1b6df1ee65-Paper.pdf",
  "title": "Reinforcement Learning with Function Approximation Converges to a Region",
  "year": 2000,
  "venue": "NeurIPS",
  "abstract": "Many algorithms for approximate reinforcement learning are not known to converge. In fact, there are counterexamples showing that the adjustable weights in some algorithms may oscillate within a region rather than converging to a point. This paper shows that, for two popular algorithms, such oscillation is the worst that can happen: the weights cannot diverge, but instead must converge to a bounded region. The algorithms are SARSA(O) and V(O); the latter algorithm was used in the well-known TD-Gammon program.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2000/file/0950ca92a4dcf426067cfd2246bb5ff3-Paper.pdf",
  "title": "Occam's Razor",
  "year": 2000,
  "venue": "NeurIPS",
  "abstract": "The Bayesian paradigm apparently only sometimes gives rise to Occam's Razor; at other times very large models perform well. We give simple examples of both kinds of behaviour. The two views are reconciled when measuring complexity of functions, rather than of the machinery used to implement them. We analyze the complexity of functions for some linear in the parameter models that are equivalent to Gaussian Processes, and always find Occam's Razor at work.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2000/file/09b15d48a1514d8209b192a8b8f34e48-Paper.pdf",
  "title": "Exact Solutions to Time-Dependent MDPs",
  "year": 2000,
  "venue": "NeurIPS",
  "abstract": "We describe an extension of the Markov decision process model in which a continuous time dimension is included in the state space. This allows for the representation and exact solution of a wide range of problems in which transitions or rewards vary over time. We examine problems based on route planning with public transportation and telescope observation scheduling.",
  "stance": -0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2000/file/82cadb0649a3af4968404c9f6031b233-Paper.pdf",
  "title": "Stability and Noise in Biochemical Switches",
  "year": 2000,
  "venue": "NeurIPS",
  "abstract": "Many processes in biology, from the regulation of gene expression in bacteria to memory in the brain, involve switches constructed from networks of biochemical reactions. Crucial molecules are present in small numbers, raising questions about noise and stability. Analysis of noise in simple reaction schemes indicates that switches stable for years and switchable in milliseconds can be built from fewer than one hundred molecules. Prospects for direct tests of this prediction, as well as implications, are discussed.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2000/file/f0fcf351df4eb6786e9bb6fc4e2dee02-Paper.pdf",
  "title": "Whence Sparseness?",
  "year": 2000,
  "venue": "NeurIPS",
  "abstract": "It has been shown that the receptive fields of simple cells in VI can be explained by assuming optimal encoding, provided that an extra constraint of sparseness is added. This finding suggests that there is a reason, independent of optimal representation, for sparseness. However this work used an ad hoc model for the noise. Here I show that, if a biologically more plausible noise model, describing neurons as Poisson processes, is used sparseness does not have to be added as a constraint. Thus I conclude that sparseness is not a feature that evolution has striven for, but is simply the result of the evolutionary pressure towards an optimal representation.",
  "stance": -0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2001/file/2d579dc29360d8bbfbb4aa541de5afa9-Paper.pdf",
  "title": "Classifying Single Trial EEG: Towards Brain Computer Interfacing",
  "year": 2001,
  "venue": "NeurIPS",
  "abstract": "Driven by the progress in the field of single-trial analysis of EEG, there is a growing interest in brain computer interfaces (BCIs), i.e., systems that enable human subjects to control a computer only by means of their brain signals. In a pseudo-online simulation our BCI detects upcoming finger movements in a natural keyboard typing condition and predicts their laterality. This can be done on average 100–230ms before the respective key is actually pressed, i.e., long before the onset of EMG. Our approach is appealing for its short response time and high classification accuracy (>96%) in a binary decision where no human training is involved. We compare discriminative classifiers like Support Vector Machines (SVMs) and different variants of Fisher Discriminant that possess favorable regularization properties for dealing with high noise cases (inter-trial variablity).",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2001/file/7b7a53e239400a13bd6be6c91c4f6c4e-Paper.pdf",
  "title": "On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes",
  "year": 2001,
  "venue": "NeurIPS",
  "abstract": "We compare discriminative and generative learning as typified by logistic regression and naive Bayes. We show, contrary to a widelyheld belief that discriminative classifiers are almost always to be preferred, that there can often be two distinct regimes of performance as the training set size is increased, one in which each algorithm does better. This stems from the observationwhich is borne out in repeated experimentsthat while discriminative learning has lower asymptotic error, a generative classifier may also approach its (higher) asymptotic error much faster.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2001/file/a96d3afec184766bfeca7a9f989fc7e7-Paper.pdf",
  "title": "Associative memory in realistic neuronal networks",
  "year": 2001,
  "venue": "NeurIPS",
  "abstract": "Almost two decades ago, Hopfield [1] showed that networks of highly reduced model neurons can exhibit multiple attracting fixed points, thus providing a substrate for associative memory. It is still not clear, however, whether realistic neuronal networks can support multiple attractors. The main difficulty is that neuronal networks in vivo exhibit a stable background state at low firing rate, typically a few Hz. Embedding attractor is easy; doing so without destabilizing the background is not. Previous work [2 , 3] focused on the sparse coding limit, in which a vanishingly small number of neurons are involved in any memory. Here we investigate the case in which the number of neurons involved in a memory scales with the number of neurons in the network. In contrast to the sparse coding limit, we find that multiple attractors can co-exist robustly with a stable background state. Mean field theory is used to understand how the behavior of the network scales with its parameters, and simulations with analog neurons are presented. One of the most important features of the nervous system is its ability to perform associative memory. It is generally believed that associative memory is implemented using attractor networks experimental studies point in that direction [47], and there are virtually no competing theoretical models. Perhaps surprisingly, however, it is still an open theoretical question whether attractors can exist in realistic neuronal networks. The \"realistic\" feature that is probably hardest to capture is the steady firing at low rates the background state that is observed throughout the intact nervous system [813]. The reason it is difficult to build an attractor network that is stable at low firing rates, at least in the sparse coding limit, is as follows [2,3]: Attractor networks are constructed by strengthening recurrent connections among sub-populations of neurons. The strengthening must be large enough that neurons within a sub-population can sustain a high firing rate state, but not so large that the sub-population can be spontaneously active. This implies that the neuronal gain functions the firing rate of the post-synaptic neurons as a function of the average • http) / culture.neurobio.ucla.edu/ \"'pel firing rate of the pre-synaptic neurons must be sigmoidal: small at low firing rate to provide stability, high at intermediate firing rate to provide a threshold (at an unstable equilibrium), and low again at high firing rate to provide saturation and a stable attractor. In other words, a requirement for the co-existence of a stable background state and multiple attractors is that the gain function of the excitatory neurons be super linear at the observed background rates of a few Hz [2,3]. However and this is where the problem lies above a few Hz most realistic gain function are nearly linear or sublinear (see, for example, Fig. Bl of [14]). The superlinearity requirement rests on the implicit assumption that the activity of the sub-population involved in a memory does not affect the other neurons in the network. While this assumption is valid in the sparse coding limit , it breaks down in realistic networks containing both excitatory and inhibitory neurons. In such networks, activity among excitatory cells results in inhibitory feedback. This feedback, if powerful enough, can stabilize attractors even without a saturating nonlinearity, essentially by stabilizing the equilibrium (above considered unstable) on the steep part of the gain function. The price one pays, though, is that a reasonable fraction of the neurons must be involved in each of the memories, which takes us away from the sparse coding limit and thus reduces network capacity [15].",
  "stance": 0.4
 },
 {
  "url": "https://papers.neurips.cc/paper/2001/file/d46e1fcf4c07ce4a69ee07e4134bcef1-Paper.pdf",
  "title": "Entropy and Inference, Revisited",
  "year": 2001,
  "venue": "NeurIPS",
  "abstract": "We study properties of popular near–uniform (Dirichlet) priors for learning undersampled probability distributions on discrete nonmetric spaces and show that they lead to disastrous results. However, an Occam–style phase space argument expands the priors into their infinite mixture and resolves most of the observed problems. This leads to a surprisingly good estimator of entropies of discrete distributions. Learning a probability distribution from examples is one of the basic problems in data analysis. Common practical approaches introduce a family of parametric models, leading to questions about model selection. In Bayesian inference, computing the total probability of the data arising from a model involves an integration over parameter space, and the resulting “phase space volume” automatically discriminates against models with larger numbers of parameters—hence the description of these volume terms as Occam factors [1, 2]. As we move from finite parameterizations to models that are described by smooth functions, the integrals over parameter space become functional integrals and methods from quantum field theory allow us to do these integrals asymptotically; again the volume in model space consistent with the data is larger for models that are smoother and hence less complex [3]. Further, at least under some conditions the relevant degree of smoothness can be determined self–consistently from the data, so that we approach something like a model independent method for learning a distribution [4]. The results emphasizing the importance of phase space factors in learning prompt us to look back at a seemingly much simpler problem, namely learning a distribution on a discrete, nonmetric space. Here the probability distribution is just a list of numbers {qi}, i = 1, 2, · · · ,K, where K is the number of bins or possibilities. We do not assume any metric on the space, so that a priori there is no reason to believe that any qi and qj should be similar. The task is to learn this distribution from a set of examples, which we can describe as the number of times ni each possibility is observed in a set of N = ∑K i=1 ni samples. This problem arises in the context of language, where the index i might label words or phrases, so that there is no natural way to place a metric on the space, nor is it even clear that our intuitions about similarity are consistent with the constraints of a metric space. Similarly, in bioinformatics the index i might label n–mers of the the DNA or amino acid sequence, and although most work in the field is based on metrics for sequence comparison one might like an alternative approach that does not rest on such assumptions. In the analysis of neural responses, once we fix our time resolution the response becomes a set of discrete “words,” and estimates of the information content in the response are determined by the probability distribution on this discrete space. What all of these examples have in common is that we often need to draw some conclusions with data sets that are not in the asymptotic limit N K. Thus, while we might use a large corpus to sample the distribution of words in English by brute force (reaching N K with K the size of the vocabulary), we can hardly do the same for three or four word phrases. In models described by continuous functions, the infinite number of “possibilities” can never be overwhelmed by examples; one is saved by the notion of smoothness. Is there some nonmetric analog of this notion that we can apply in the discrete case? Our intuition is that information theoretic quantities may play this role. If we have a joint distribution of two variables, the analog of a smooth distribution would be one which does not have too much mutual information between these variables. Even more simply, we might say that smooth distributions have large entropy. While the idea of “maximum entropy inference” is common [5], the interplay between constraints on the entropy and the volume in the space of models seems not to have been considered. As we shall explain, phase space factors alone imply that seemingly sensible, more or less uniform priors on the space of discrete probability distributions correspond to disastrously singular prior hypotheses about the entropy of the underlying distribution. We argue that reliable inference outside the asymptotic regimeN K requires a more uniform prior on the entropy, and we offer one way of doing this. While many distributions are consistent with the data when N ≤ K, we provide empirical evidence that this flattening of the entropic prior allows us to make surprisingly reliable statements about the entropy itself in this regime. At the risk of being pedantic, we state very explicitly what we mean by uniform or nearly uniform priors on the space of distributions. The natural “uniform” prior is given by Pu({qi}) = 1 Zu δ ( 1 − K ∑",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2002/file/0233f3bb964cf325a30f8b1c2ed2da93-Paper.pdf",
  "title": "\"Name That Song!\" A Probabilistic Approach to Querying on Music and Text",
  "year": 2002,
  "venue": "NeurIPS",
  "abstract": "We present a novel, flexible statistical approach for modelling music and text jointly. The approach is based on multi-modal mixture models and maximum a posteriori estimation using EM. The learned models can be used to browse databases with documents containing music and text, to search for music using queries consisting of music and text (lyrics and other contextual information), to annotate text documents with music, and to automatically recommend or identify similar songs.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2002/file/04ad5632029cbfbed8e136e5f6f7ddfa-Paper.pdf",
  "title": "How the Poverty of the Stimulus Solves the Poverty of the Stimulus",
  "year": 2002,
  "venue": "NeurIPS",
  "abstract": "Language acquisition is a special kind of learning problem because the outcome of learning of one generation is the input for the next. That makes it possible for languages to adapt to the particularities of the learner. In this paper, I show that this type of language change has important consequences for models of the evolution and acquisition of syntax. 1 The Language Acquisition Problem For both artificial systems and non-human animals, learning the syntax of natural languages is a notoriously hard problem. All healthy human infants, in contrast, learn any of the approximately 6000 human languages rapidly, accurately and spontaneously. Any explanation of how they accomplish this difficult task must specify the (innate) inductive bias that human infants bring to bear, and the input data that is available to them. Traditionally, the inductive bias is termed somewhat unfortunately \"Universal Grammar\", and the input data \"primary linguistic data\". Over the last 30 years or so, a view on the acquisition of the syntax of natural language has become popular that has put much emphasis on the innate machinery. In this view, that one can call the \"Principles and Parameters\" model, the Universal Grammar specifies most aspects of syntax in great detail [e.g. 1]. The role of experience is reduced to setting a limited number (30 or so) of parameters. The main argument for this view is the argument from the poverty of the stimulus [2]. This argument states that children have insufficient evidence in the primary linguistic data to induce the grammar of their native language. Mark Gold [3] provides the most well-known formal basis to this argument. Gold introduced the criterion \"identification in the limit\" for evaluating the success of a learning algorithm: with an infinite number of training samples all hypotheses of the algorithm should be identical, and equivalent to the target. Gold showed that the class of context-free grammars is not learnable in this sense by any algorithm from positive samples alone (and neither are other super'-jinite classes). This proof is based on the fact that no matter how many samples from an infinite language a learning algorithm has seen, the algorithm can not decide with certainty that the samples are drawn from the infinite language or from a finite language that contains all samples. Because natural languages are thought to be at least as complex as context-free grammars, and negative feedback is assumed to be absent in the primary linguistic data, Gold's analysis, and subsequent work in learn ability theory [1] , is usually interpreted as strong support for the argument from the poverty of the stimulus, and, in the extreme, for the view that grammar induction is fundamentally impossible (a claim that Gold would not subscribe to). Critics of this \"nativist\" approach [e.g. 4, 5] have argued for different assumptions on the appropriate grammar formalism (e.g. stochastic context-free grammars), the available primary data (e.g. semantic information) or the appropriate learnability criterion. In this paper I will take a different approach. I will present a model that induces context-free grammars without a-priori restrictions on the search space, semantic information or negative evidence. Gold's negative results thus apply. Nevertheless, acquisition of grammar is successful in my model, because another process is taken into account as well: the cultural evolution of language. 2 The Language Evolution Problem Whereas in language acquisition research the central question is how a child acquires an existing language, in language evolution research the central question is how this language and its properties have emerged in the first place. Within the nativist paradigm, some have suggested that the answer to this question is that Universal Grammar is the product of evolution under selection pressures for communication [e.g. 6]. Recently, several formal models have been presented to evaluate this view. For this paper, the most relevant of those is the model of Nowak et al. [7]. In that model it is assumed that there is a finite number of grammars, that newcomers (infants) learn their grammar from the population, that more successful grammars have a higher probability of being learned and that mistakes are made in learning. The system can thus be described in terms of the changes in the relative frequencies Xi of each grammar type i in the population. The first result that Nowak et al. obtain is a \"coherence threshold\". This threshold is the necessary condition for grammatical coherence in a population, i.e. for a majority of individuals to use the same grammar. They show that this coherence depends on the chances that a child has to correctly acquire its parents' grammar. This probability is described with the parameter q. Nowak et al. show analytically that there is a minimum value for q to keep coherence in the population. If q is lower than this value, all possible grammar types are equally frequent in the population and the communicative success in minimal. If q is higher than this value, one grammar type is dominant; the communicative success is much higher than before and reaches 100% if q = l. The second result relates this required fidelity (called qd to a lower bound (be) on the number of sample sentences that a child needs. Nowak et al. make the crucial assumption that all languages are equally expressive and equally different from each other. With that assumption they can show that be is proportional to the total number of possible grammars N. Of course, the actual number of sample sentences b is finite; Nowak et al. conclude that only if N is relatively small can a stable grammar emerge in a population. I.e. the population dynamics require a restrictive Universal Grammar. The models of Gold and Nowak et al. have in common that they implicitly assume that every possible grammar is equally likely to become the target grammar for learning. If even the best possible learning algorithm cannot learn such a grammar, the set of allowed grammars must be restricted. There is, however, reason to believe that this assumption is not the most useful for language learning. Language learning is a very particular type of learning problem, because the outcome of the learning process at one generation is the input for the next. The samples from which a child learns with its learning procedure, are therefore biased by the learning of previous generations that used the same procedure[8]. In [9] and other papers, Kirby, Hurford and students have developed a framework to study the consequences of that fact. In this framework, called the \"Iterated Learning Model\" (ILM), a population of individuals is modeled that can each produce and interpret sentences, and have a language acquisition procedure to learn grammar from each other. In the ILM one individual (the parent) presents a relatively small number of examples of form-meaning pairs to the next individual (the child). The child then uses these examples to induce his own granunar. In the next iteration the child becomes the parent, and a new individual becomes the child. This process is repeated many times. Interestingly, Kirby and Hurford have found that in these iterated transmission steps the language becomes easier and easier to learn, because the language adapts to the learning algorithm by becoming more and more structured. The structure of language in these models thus emerges from the iteration of learning. The role of biological evolution, in this view, is to shape the learning algorithms, such that the complex results of the iterated learning is biologically adaptive [10]. In this paper I will show that if one adopts this view on the interactions between learning, cultural evolution and biological evolution, the models such as those of Gold [3] and Nowak et al. [7] can no longer be taken as evidence for an extensive, innate pr~specification of human language. 3 A Simple Model of Grammar Induction To study the interactions between language adaptation and language acquisition, I have first designed a grammar induction algorithm that is simple, but can nevertheless deal with some non-trivial induction problems. The model uses context-free grammars to represent linguistic abilities. In particular, the representation is limited to grammars G where all rules are of one of the following forms: (1) A 1-+ t, (2) A 1-+ BC, (3) A 1-+ Bt. The nontenninals A, B, C are elements of the non-terminal alphabet Vnt , which includes the start symbol S. t is a string of tenninal symbols from the terminal alphabet Vt 1• For determining the language L of a certain grammar G I use simple depth-first exhaustive search of the derivation tree. For computational reasons, the depth of the search is limited to a certain depth d, and the string length is limited to length l. The set of sentences (L' ~ L) used in training and in communication is therefore finite (and strictly speaking not context-free, but regular); in production, strings are drawn from a uniform distribution over L'. The grammar induction algorithm learns from a set of sample strings (sentences) that are provided by a teacher. The design of the learning algorithm is originally inspired by [11] and is similar to the algorithm in [12]. The algorithm fits within a tradition of algorithms that search for compact descriptions of the input data [e.g. 13, 14, 15]. It consists of three operations: Incorporation: extend the language, such that it includes the encountered string; if string s is not already part of the language, add a rule S 1-+ s to the grammar. INote that the restrictions on the rule-types above do not limit the scope of languages that can be represented (they are essentially equivalent to Chomsky Normal Form). They are, however, relevant for the language acquisition algorithm. Compression: substitute frequent and long substrings with a nonterminal, such that the gmmmar becomes smaller and the language remains unchangedj for every valid substring z of the right-hand sides of all rules, calculate the compression effect v(z) of substituting z with a nonterminal Aj replace all valid occurrences of the substring z, = arymaxzv(z) with A if v(z') > 0, and add a rule A f-+ Zl to the grammar. \"Valid substrings\" are those substrings which can be replaced while keeping all rules of the forms 13 described above. The compression effect is measured as the difference between the number of symbols in the grammar before and after the substitution. The compression step is repeated until the grammar does not change anymore. Generalization: equate two nonterminals, such that the grammar becomes smaller and the language laryerj for every combination of two nonterminals A and B (B :f S), calculate the compression effect v of equating A and B. Equate the combination (A',B') = arymaxABv(A,B) ifv(A',B') > OJ i.e. replace all occurrences of B with A. The compression effect is measured as the difference between the number of symbols before and after replacing and deleting redundant rules. The generalization step is repeated until the grammar does not change anymore. 4 Learnable and U nlearnable Classes The algorithm described above is implemented in C++ and tested on a variety of target grammars2 • I will not present a detailed analysis of the learning behavior here, but limit myself to a simple example that shows that the algorithm can learn some (recursive) grammars, while it can not learn others. The induction algorithm receives three sentences (abed, abcabcd, abcabcabcd). The incorporation, compression (repeated twice) and generalization steps yield subsequently the following grammars: (a) Incorporation (b) Compression (c) Generalization S f-+ abed S f-+ Yd S f-+ Xd S f-+ abcabcd S f-+ Xd S f-+ Xabcd S f-+ abcabcabcd S f-+ Xabcd X f-+ XX X f-+ yy X f-+ abc Y f-+ abc In (b) the substrings \"abcabc\" and \"abc\" are subsequently replaced by the nonterminals X and Y. In (c) the non-terminals X and Y are equated, which leads to the deletion of the second rule in (b). One can check that the total size of the grammar reduces from 24, to 19 and further down to 16 characters. From this example it is also clear that learning is not always successful. Any of the three grammars above «a) and (b) are equivalent) could have generated the training data, but with these three input strings the algorithm always yields grammar (c). Consistent with Gold's general proof [3], many target grammars will never be learned correctly, no matter how many input strings are generated. In practice, each finite set of randomly generated strings from some target grammar, might yield a different result. Thus, for some number of input strings T, some set of target grammars are always acquired, some are never acquired, and some are some of the time acquired. H we can enumerate all possible grammars, we can describe this with a matrix Q, where each entry Qij describes the probability that the algorithm learning from sample strings from a target grammar i, will end up with grammar 2The source code is available at http://wvv.ling.ed.ac . uk/ \"\" j elle of type j. Qii is the probability that the algorithm finds the target grammar. To make learning successful, the target grammars that are presented to the algorithm have to be biased. The following section will show that for this we need nothing more than to assume that the output of one learner is the input for the next. 5 Iterated Learning: the Emergence of Learnability To study the effects of iterated learning, we extend the model with a population structure. In the new version of the model individuals (agents, that each represent a generation) are placed in a chain. The first agent induces its grammar from a number E of randomly generated strings. Every subsequent agent (the child) learns its grammar from T sample sentences that are generated by the previous one (the parent). To avoid insufficient expressivenes:,;, we al:,;o extend the generalization step with a check if the number EG of different strings the grammar G can recognize is larger than or equal to E. If not, E EG random new strings are generated and incorporated in the grammar. Using the matrix Q from the previou:,; section, we can formalize this iterated learning model with the following general equation, where Xi is the probability that grammar i is the grammar of the current generation:",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2002/file/43e4e6a6f341e00671e123714de019a8-Paper.pdf",
  "title": "An Impossibility Theorem for Clustering",
  "year": 2002,
  "venue": "NeurIPS",
  "abstract": "Although the study of clustering is centered around an intuitively compelling goal, it has been very difficult to develop a unified framework for reasoning about it at a technical level, and profoundly diverse approaches to clustering abound in the research community. Here we suggest a formal perspective on the difficulty in finding such a unification, in the form of an impossibility theorem: for a set of three simple properties, we show that there is no clustering function satisfying all three. Relaxations of these properties expose some of the interesting (and unavoidable) trade-offs at work in well-studied clustering techniques such as single-linkage, sum-of-pairs, k-means, and k-median.",
  "stance": -0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2002/file/635440afdfc39fe37995fed127d7df4f-Paper.pdf",
  "title": "Categorization Under Complexity: A Unified MDL Account of Human Learning of Regular and Irregular Categories",
  "year": 2002,
  "venue": "NeurIPS",
  "abstract": "We present an account of human concept learning-that is, learning of categories from examples-based on the principle of minimum description length (MDL). In support of this theory, we tested a wide range of two-dimensional concept types, including both regular (simple) and highly irregular (complex) structures, and found the MDL theory to give a good account of subjects' performance. This suggests that the intrinsic complexity of a concept (that is, its description -length) systematically influences its leamability. 1The Structure of Categories A number of different principles have been advanced to explain the manner in which humans learn to categorize objects. It has been variously suggested that the underlying principle might be the similarity structure of objects [1], the manipulability of decision bound~ aries [2], or Bayesian inference [3][4]. While many of these theories are mathematically well-grounded and have been successful in explaining a range of experimental findings, they have commonly only been tested on a narrow collection of concept types similar to the simple unimodal categories of Figure 1(a-e).",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2002/file/6e5025ccc7d638ae4e724da8938450a6-Paper.pdf",
  "title": "One-Class LP Classifiers for Dissimilarity Representations",
  "year": 2002,
  "venue": "NeurIPS",
  "abstract": "Problems in which abnormal or novel situations should be detected can be approached by describing the domain of the class of typical examples. These applications come from the areas of machine diagnostics, fault detection, illness identification or, in principle, refer to any problem where little knowledge is available outside the typical class. In this paper we explain why proximities are natural representations for domain descriptors and we propose a simple one-class classifier for dissimilarity representations. By the use of linear programming an efficient one-class description can be found, based on a small number of prototype objects. This classifier can be made (1) more robust by transforming the dissimilarities and (2) cheaper to compute by using a reduced representation set. Finally, a comparison to a comparable one-class classifier by Campbell and Bennett is given.",
  "stance": 0.7
 },
 {
  "url": "https://papers.neurips.cc/paper/2002/file/716e1b8c6cd17b771da77391355749f3-Paper.pdf",
  "title": "Modeling Midazolam's Effect on the Hippocampus and Recognition Memory",
  "year": 2002,
  "venue": "NeurIPS",
  "abstract": "The benz.odiaze:pine '~1idazolam causes dense,but temporary ~ anterograde amnesia, similar to that produced byhippocampal damage~Does the action of M'idazola:m on the hippocanlpus cause less storage, or less accurate storage, .of information in episodic. long-term menlory?\\rVe used a sinlple variant of theREJv1. JD.odel [18] to fit data collected. by IIirsbnla.n~Fisher, .IIenthorn,Arndt} and Passa.nnante [9] on the effects of Midazola.m, study time~ and normative \\vQrd... frequenc:y on both yes-no and remember-k.novv recognition m.emory. That a: simple strength. 'model fit well \\\\tas cont.rary to the expectations of 'flirshman et aLMore important,within the Bayesian based R.EM modeling frame\\vork, the data were consistentw'ith the view that Midazolam causes less accurate storage~ rather than less storage, of infornlation in episodic mcm.ory..",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2002/file/806fec5af7f5b48b8a31a003e171f3fb-Paper.pdf",
  "title": "Dynamical Constraints on Computing with Spike Timing in the Cortex",
  "year": 2002,
  "venue": "NeurIPS",
  "abstract": "If the cortex uses spike timing to compute, the timing of the spikes must be robust to perturbations. Based on a recent framework that provides a simple criterion to determine whether a spike sequence produced by a generic network is sensitive to initial conditions, and numerical simulations of a variety of network architectures, we argue within the limits set by our model of the neuron, that it is unlikely that precise sequences of spike timings are used for computation under conditions typically found in the cortex.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2002/file/8c5f6ecd29a0eb234459190ca51c16dd-Paper.pdf",
  "title": "A Minimal Intervention Principle for Coordinated Movement",
  "year": 2002,
  "venue": "NeurIPS",
  "abstract": "Behavioral goals are achieved reliably and repeatedly with movements rarely reproducible in their detail. Here we offer an explanation: we show that not only are variability and goal achievement compatible, but indeed that allowing variability in redundant dimensions is the optimal control strategy in the face of uncertainty. The optimal feedback control laws for typical motor tasks obey a “minimal intervention” principle: deviations from the average trajectory are only corrected when they interfere with the task goals. The resulting behavior exhibits task-constrained variability, as well as synergetic coupling among actuators—which is another unexplained empirical phenomenon.",
  "stance": -0.2
 },
 {
  "url": "https://papers.neurips.cc/paper/2002/file/ce6c92303f38d297e263c7180f03d402-Paper.pdf",
  "title": "Automatic Acquisition and Efficient Representation of Syntactic Structures",
  "year": 2002,
  "venue": "NeurIPS",
  "abstract": "The distributional principle according to which morphemes that occur in identical contexts belong, in some sense, to the same category [1] has been advanced as a means for extracting syntactic structures from corpus data. We extend this principle by applying it recursively, and by using mutual information for estimating category coherence. The resulting model learns, in an unsupervised fashion, highly structured, distributed representations of syntactic knowledge from corpora. It also exhibits promising behavior in tasks usually thought to require representations anchored in a grammar, such as systematicity. 1 Motivation Models dealing with the acquisition of syntactic knowledge are sharply divided into two classes, depending on whether they subscribe to some variant of the classical generative theory of syntax, or operate within the framework of “general-purpose” statistical or distributional learning. An example of the former is the model of [2], which attempts to learn syntactic structures such as Functional Category, as stipulated by the Government and Binding theory. An example of the latter model is Elman’s widely used Simple Recursive Network (SRN) [3]. We believe that polarization between statistical and classical (generative, rule-based) approaches to syntax is counterproductive, because it hampers the integration of the stronger aspects of each method into a common powerful framework. Indeed, on the one hand, the statistical approach is geared to take advantage of the considerable progress made to date in the areas of distributed representation, probabilistic learning, and “connectionist” modeling. Yet, generic connectionist architectures are ill-suited to the abstraction and processing of symbolic information. On the other hand, classical rule-based systems excel in just those tasks, yet are brittle and difficult to train. We present a scheme that acquires “raw” syntactic information construed in a distributional sense, yet also supports the distillation of rule-like regularities out of the accrued statistical knowledge. Our research is motivated by linguistic theories that postulate syntactic structures (and transformations) rooted in distributional data, as exemplified by the work of Zellig Harris [1]. 2 The ADIOS model The ADIOS (Automatic DIstillation Of Structure) model constructs syntactic representations of a sample of language from unlabeled corpus data. The model consists of two elements: (1) a Representational Data Structure (RDS) graph, and (2) a Pattern Acquisition (PA) algorithm that learns the RDS in an unsupervised fashion. The PA algorithm aims to detect patterns — repetitive sequences of “significant” strings of primitives occurring in the corpus (Figure 1). In that, it is related to prior work on alignment-based learning [4] and regular expression (“local grammar”) extraction [5] from corpora. We stress, however, that our algorithm requires no pre-judging either of the scope of the primitives or of their classification, say, into syntactic categories: all the information needed for its operation is extracted from the corpus in an unsupervised fashion. In the initial phase of the PA algorithm the text is segmented down to the smallest possible morphological constituents (e.g., ed is split off both walked and bed; the algorithm later discovers that bed should be left whole, on statistical grounds).1 This initial set of unique constituents is the vertex set of the newly formed RDS (multi-)graph. A directed edge is inserted between two vertices whenever the corresponding transition exists in the corpus (Figure 2(a)); the edge is labeled by the sentence number and by its within-sentence index. Thus, corpus sentences initially correspond to paths in the graph, a path being a sequence of edges that share the same sentence number. mi ml mj mk i{j,k}l mh mn (a) mi ml j k ... (b) u v Figure 1: (a) Two sequences mi,mj ,ml and mi,mk,ml form a pattern ci{j,k}l . = mi, {mj ,mk},ml, which allows mj and mk to be attributed to the same equivalence class, following the principle of complementary distributions [1]. Both the length of the shared context and the cohesiveness of the equivalence class need to be taken into account in estimating the goodness of the candidate pattern (see eq. 1). (b) Patterns can serve as constituents in their own right; recursively abstracting patterns from a corpus allows us to capture the syntactic regularities concisely, yet expressively. Abstraction also supports generalization: in this schematic illustration, two new paths (dashed lines) emerge from the formation of equivalence classes associated with cu and cv . In the second phase, the PA algorithm repeatedly scans the RDS graph for Significant Patterns (sequences of constituents) (SP), which are then used to modify the graph (Algorithm 1). For each path pi, the algorithm constructs a list of candidate constituents, ci1, . . . , cik. Each of these consists of a “prefix” (sequence of graph edges), an equivalence class of vertices, and a “suffix” (another sequence of edges; cf. Figure 2(b)). The criterion I ′ for judging pattern significance combines a syntagmatic consideration (the pattern must be long enough) with a paradigmatic one (its constituents c1, . . . , ck must have high mutual information): I (c1, c2, . . . , ck) = e −(L/k)2P (c1, c2, . . . , ck) log P (c1, c2, . . . , ck) Πj=1P (cj) (1) where L is the typical context length and k is the length of the candidate pattern; the probabilities associated with a cj are estimated from frequencies that are immediately available We remark that the algorithm can work in any language, with any set of tokens, including individual characters – or phonemes, if applied to speech. Algorithm 1 PA (pattern acquisition), phase 2 1: while patterns exist do 2: for all path ∈ graph do {path=sentence; graph=corpus} 3: for all source node ∈ path do 4: for all sink node ∈ path do {source and sink can be equivalence classes} 5: degree of separation = path index(sink) − path index(source); 6: pattern table ⇐ detect patterns(source, sink, degree of separation, equivalence table); 7: end for 8: end for 9: winner ⇐ get most significant pattern(pattern table); 10: equivalence table ⇐ detect equivalences(graph, winner); 11: graph ⇐ rewire graph(graph, winner); 12: end for 13: end while in the graph (e.g., the out-degree of a node is related to the marginal probability of the corresponding cj). Equation 1 balances two opposing “forces” in pattern formation: (1) the length of the pattern, and (2) the number and the cohesiveness of the set of examples that support it. On the one hand, shorter patterns are likely to be supported by more examples; on the other hand, they are also more likely to lead to over-generalization, because shorter patterns mean less context. A pattern tagged as significant is added as a new vertex to the RDS graph, replacing the constituents and edges it subsumes (Figure 2). Note that only those edges of the multigraph that belong to the detected pattern are rewired; edges that belong to sequences not subsumed by the pattern are untouched. This highly context-sensitive approach to pattern abstraction, which is unique to our model, allows ADIOS to achieve a high degree of representational parsimony without sacrificing generalization power. During the pass over the corpus the list of equivalence sets is updated continuously; the identification of new significant patterns is done using thecurrent equivalence sets (Figure 3(d)). Thus, as the algorithm processes more and more text, it “bootstraps” itself and enriches the RDS graph structure with new SPs and their accompanying equivalence sets. The recursive nature of this process enables the algorithm to form more and more complex patterns, in a hierarchical manner. The relationships among these can be visualized recursively in a tree format, with tree depth corresponding to the level of recursion (e.g., Figure 3(c)). The PA algorithm halts if it processes a given amount of text without finding a new SP or equivalence set (in real-life language acquisition this process may never stop). Generalization. A collection of patterns distilled from a corpus can be seen as an empirical grammar of sorts; cf. [6], p.63: “the grammar of a language is simply an inventory of linguistic units.” The patterns can eventually become highly abstract, thus endowing the model with an ability to generalize to unseen inputs. Generalization is possible, for example, when two equivalence classes are placed next to each other in a pattern, creating new paths among the members of the equivalence classes (dashed lines in Figure 1(b)). Generalization can also ensue from partial activation of existing patterns by novel inputs. This function is supported by the input module, designed to process a novel sentence by forming its distributed representation in terms of activities of existing patterns (Figure 6). These are computed by propagating activation from bottom (the terminals) to top (the patterns) of the RDS. The initial activities wj of the terminals cj are calculated given the novel input s1, . . . , sk as follows: wj = max m=1..k {I(sk, cj)} (2) PATTERN 230: the cat is {eat, play, stay} -ing s h o w E N D h e r in g p la y is P a m c a t th e B E G IN e a t 102: do you see the cat? 101: the cat is eating 103: are you sure? 101_1 101_2 101_3 101_4 101_5 101_6 Sentence Number Within-Sentence Index s ta y in g e a t is c a t th e p la y 101_1 101_6",
  "stance": 0.6
 },
 {
  "url": "https://papers.neurips.cc/paper/2002/file/d2a27e83d429f0dcae6b937cf440aeb1-Paper.pdf",
  "title": "Stable Fixed Points of Loopy Belief Propagation Are Local Minima of the Bethe Free Energy",
  "year": 2002,
  "venue": "NeurIPS",
  "abstract": "We extend recent work on the connection between loopy belief propagation and the Bethe free energy. Constrained minimization of the Bethe free energy can be turned into an unconstrained saddle-point problem. Both converging double-loop algorithms and standard loopy belief propagation can be interpreted as attempts to solve this saddle-point problem. Stability analysis then leads us to conclude that stable fixed points of loopy belief propagation must be (local) minima of the Bethe free energy. Perhaps surprisingly, the converse need not be the case: minima can be unstable fixed points. We illustrate this with an example and discuss implications.",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2002/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf",
  "title": "Replay, Repair and Consolidation",
  "year": 2002,
  "venue": "NeurIPS",
  "abstract": "A standard view of memory consolidation is that episodes are stored temporarily in the hippocampus, and are transferred to the neocortex through replay. Various recent experimental challenges to the idea of transfer, particularly for human memory, are forcing its re-evaluation. However, although there is independent neurophysiological evidence for replay, short of transfer, there are few theoretical ideas for what it might be doing. We suggest and demonstrate two important computational roles associated with neocortical indices.",
  "stance": 0.4
 },
 {
  "url": "https://papers.neurips.cc/paper/2002/file/e0f7a4d0ef9b84b83b693bbf3feb8e6e-Paper.pdf",
  "title": "Circuit Model of Short-Term Synaptic Dynamics",
  "year": 2002,
  "venue": "NeurIPS",
  "abstract": "We describe a model of short-term synaptic depression that is derived from a silicon circuit implementation. The dynamics of this circuit model are similar to the dynamics of some present theoretical models of shortterm depression except that the recovery dynamics of the variable describing the depression is nonlinear and it also depends on the presynaptic frequency. The equations describing the steady-state and transient responses of this synaptic model fit the experimental results obtained from a fabricated silicon network consisting of leaky integrate-and-fire neurons and different types of synapses. We also show experimental data demonstrating the possible computational roles of depression. One possible role of a depressing synapse is that the input can quickly bring the neuron up to threshold when the membrane potential is close to the resting potential.",
  "stance": 0.4
 },
 {
  "url": "https://papers.neurips.cc/paper/2003/file/063e26c670d07bb7c4d30e6fc69fe056-Paper.pdf",
  "title": "Information Dynamics and Emergent Computation in Recurrent Circuits of Spiking Neurons",
  "year": 2003,
  "venue": "NeurIPS",
  "abstract": "We employ an efficient method using Bayesian and linear classifiers for analyzing the dynamics of information in high-dimensional states of generic cortical microcircuit models. It is shown that such recurrent circuits of spiking neurons have an inherent capability to carry out rapid computations on complex spike patterns, merging information contained in the order of spike arrival with previously acquired context information.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2003/file/0bf727e907c5fc9d5356f11e4c45d613-Paper.pdf",
  "title": "A Model for Learning the Semantics of Pictures",
  "year": 2003,
  "venue": "NeurIPS",
  "abstract": "We propose an approach to learning the semantics of images which allows us to automatically annotate an image with keywords and to retrieve images based on text queries. We do this using a formalism that models the generation of annotated images. We assume that every image is divided into regions, each described by a continuous-valued feature vector. Given a training set of images with annotations, we compute a joint probabilistic model of image features and words which allow us to predict the probability of generating a word given the image regions. This may be used to automatically annotate and retrieve images given a word as a query. Experiments show that our model significantly outperforms the best of the previously reported results on the tasks of automatic image annotation and retrieval.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2003/file/1f34004ebcb05f9acda6016d5cc52d5e-Paper.pdf",
  "title": "Human and Ideal Observers for Detecting Image Curves",
  "year": 2003,
  "venue": "NeurIPS",
  "abstract": "This paper compares the ability of human observers to detect target image curves with that of an ideal observer. The target curves are sampled from a generative model which specifies (probabilistically) the geometry and local intensity properties of the curve. The ideal observer performs Bayesian inference on the generative model using MAP estimation. Varying the probability model for the curve geometry enables us investigate whether human performance is best for target curves that obey specific shape statistics, in particular those observed on natural shapes. Experiments are performed with data on both rectangular and hexagonal lattices. Our results show that human observers’ performance approaches that of the ideal observer and are, in general, closest to the ideal for conditions where the target curve tends to be straight or similar to natural statistics on curves. This suggests a bias of human observers towards straight curves and natural statistics.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2003/file/31c97cbb941d3e92d0e6f9925e9bc4d7-Paper.pdf",
  "title": "Sensory Modality Segregation",
  "year": 2003,
  "venue": "NeurIPS",
  "abstract": "Why are sensory modalities segregated the way they are? In this paper we show that sensory modalities are well designed for self-supervised cross-modal learning. Using the Minimizing-Disagreement algorithm on an unsupervised speech categorization task with visual (moving lips) and auditory (sound signal) inputs, we show that very informative auditory dimensions actually harm performance when moved to the visual side of the network. It is better to throw them away than to consider them part of the “visual input”. We explain this finding in terms of the statistical structure in sensory inputs.",
  "stance": 0.2
 },
 {
  "url": "https://papers.neurips.cc/paper/2003/file/8bdb5058376143fa358981954e7626b8-Paper.pdf",
  "title": "Ambiguous Model Learning Made Unambiguous with 1/f Priors",
  "year": 2003,
  "venue": "NeurIPS",
  "abstract": "What happens to the optimal interpretation of noisy data when there exists more than one equally plausible interpretation of the data? In a Bayesian model-learning framework the answer depends on the prior expectations of the dynamics of the model parameter that is to be inferred from the data. Local time constraints on the priors are insufficient to pick one interpretation over another. On the other hand, nonlocal time constraints, induced by a 1/f noise spectrum of the priors, is shown to permit learning of a specific model parameter even when there are infinitely many equally plausible interpretations of the data. This transition is inferred by a remarkable mapping of the model estimation problem to a dissipative physical system, allowing the use of powerful statistical mechanical methods to uncover the transition from indeterminate to determinate model learning.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2003/file/e82c4b19b8151ddc25d4d93baf7b908f-Paper.pdf",
  "title": "No Unbiased Estimator of the Variance of K-Fold Cross-Validation",
  "year": 2003,
  "venue": "NeurIPS",
  "abstract": "Most machine learning researchers perform quantitative experiments to estimate generalization error and compare algorithm performances. In order to draw statistically convincing conclusions, it is important to estimate the uncertainty of such estimates. This paper studies the estimation of uncertainty around the K-fold cross-validation estimator. The main theorem shows that there exists no universal unbiased estimator of the variance of K-fold cross-validation. An analysis based on the eigendecomposition of the covariance matrix of errors helps to better understand the nature of the problem and shows that naive estimators may grossly underestimate variance, as con£rmed by numerical experiments.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2003/file/f8b932c70d0b2e6bf071729a4fa68dfc-Paper.pdf",
  "title": "Learning Curves for Stochastic Gradient Descent in Linear Feedforward Networks",
  "year": 2003,
  "venue": "NeurIPS",
  "abstract": "Gradient-following learning methods can encounter problems of implementation in many applications, and stochastic variants are frequently used to overcome these difficulties. We derive quantitative learning curves for three online training methods used with a linear perceptron: direct gradient descent, node perturbation, and weight perturbation. The maximum learning rate for the stochastic methods scales inversely with the first power of the dimensionality of the noise injected into the system; with sufficiently small learning rate, all three methods give identical learning curves. These results suggest guidelines for when these stochastic methods will be limited in their utility, and considerations for architectures in which they will be effective.",
  "stance": 0.2
 },
 {
  "url": "https://papers.neurips.cc/paper/2004/file/1680e9fa7b4dd5d62ece800239bb53bd-Paper.pdf",
  "title": "Outlier Detection with One-class Kernel Fisher Discriminants",
  "year": 2004,
  "venue": "NeurIPS",
  "abstract": "The problem of detecting “atypical objects” or “outliers” is one of the classical topics in (robust) statistics. Recently, it has been proposed to address this problem by means of one-class SVM classifiers. The main conceptual shortcoming of most one-class approaches, however, is that in a strict sense they are unable to detect outliers, since the expected fraction of outliers has to be specified in advance. The method presented in this paper overcomes this problem by relating kernelized one-class classification to Gaussian density estimation in the induced feature space. Having established this relation, it is possible to identify “atypical objects” by quantifying their deviations from the Gaussian model. For RBF kernels it is shown that the Gaussian model is “rich enough” in the sense that it asymptotically provides an unbiased estimator for the true density. In order to overcome the inherent model selection problem, a cross-validated likelihood criterion for selecting all free model parameters is applied.",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2004/file/36e729ec173b94133d8fa552e4029f8b-Paper.pdf",
  "title": "Hierarchical Clustering of a Mixture Model",
  "year": 2004,
  "venue": "NeurIPS",
  "abstract": "In this paper we propose an efficient algorithm for reducing a large mixture of Gaussians into a smaller mixture while still preserving the component structure of the original model; this is achieved by clustering (grouping) the components. The method minimizes a new, easily computed distance measure between two Gaussian mixtures that can be motivated from a suitable stochastic model and the iterations of the algorithm use only the model parameters, avoiding the need for explicit resampling of datapoints. We demonstrate the method by performing hierarchical clustering of scenery images and handwritten digits.",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2004/file/921c2dc40d0b979c2910298d2f880152-Paper.pdf",
  "title": "An Auditory Paradigm for Brain-Computer Interfaces",
  "year": 2004,
  "venue": "NeurIPS",
  "abstract": "Motivated by the particular problems involved in communicating with “locked-in” paralysed patients, we aim to develop a braincomputer interface that uses auditory stimuli. We describe a paradigm that allows a user to make a binary decision by focusing attention on one of two concurrent auditory stimulus sequences. Using Support Vector Machine classification and Recursive Channel Elimination on the independent components of averaged eventrelated potentials, we show that an untrained user’s EEG data can be classified with an encouragingly high level of accuracy. This suggests that it is possible for users to modulate EEG signals in a single trial by the conscious direction of attention, well enough to be useful in BCI.",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2005/file/045cf83ab0722e782cf72d14e44adf98-Paper.pdf",
  "title": "Fixing two weaknesses of the Spectral Method",
  "year": 2005,
  "venue": "NeurIPS",
  "abstract": "We discuss two intrinsic weaknesses of the spectral graph partitioning method, both of which have practical consequences. The first is that spectral embeddings tend to hide the best cuts from the commonly used hyperplane rounding method. Rather than cleaning up the resulting suboptimal cuts with local search, we recommend the adoption of flow-based rounding. The second weakness is that for many “power law” graphs, the spectral method produces cuts that are highly unbalanced, thus decreasing the usefulness of the method for visualization (see figure 4(b)) or as a basis for divide-and-conquer algorithms. These balance problems, which occur even though the spectral method’s quotient-style objective function does encourage balance, can be fixed with a stricter balance constraint that turns the spectral mathematical program into an SDP that can be solved for million-node graphs by a method of Burer and Monteiro.",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2005/file/6775a0635c302542da2c32aa19d86be0-Paper.pdf",
  "title": "Fast Gaussian Process Regression using KD-Trees",
  "year": 2005,
  "venue": "NeurIPS",
  "abstract": "The computation required for Gaussian process regression with n training examples is about O(n) during training and O(n) for each prediction. This makes Gaussian process regression too slow for large datasets. In this paper, we present a fast approximation method, based on kd-trees, that significantly reduces both the prediction and the training times of Gaussian process regression.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2005/file/e6af401c28c1790eaef7d55c92ab6ab6-Paper.pdf",
  "title": "Noise and the two-thirds power Law",
  "year": 2005,
  "venue": "NeurIPS",
  "abstract": "The two-thirds power law, an empirical law stating an inverse non-linear relationship between the tangential hand speed and the curvature of its trajectory during curved motion, is widely acknowledged to be an invariant of upper-limb movement. It has also been shown to exist in eyemotion, locomotion and was even demonstrated in motion perception and prediction. This ubiquity has fostered various attempts to uncover the origins of this empirical relationship. In these it was generally attributed either to smoothness in handor joint-space or to the result of mechanisms that damp noise inherent in the motor system to produce the smooth trajectories evident in healthy human motion. We show here that white Gaussian noise also obeys this power-law. Analysis of signal and noise combinations shows that trajectories that were synthetically created not to comply with the power-law are transformed to power-law compliant ones after combination with low levels of noise. Furthermore, there exist colored noise types that drive non-power-law trajectories to power-law compliance and are not affected by smoothing. These results suggest caution when running experiments aimed at verifying the power-law or assuming its underlying existence without proper analysis of the noise. Our results could also suggest that the power-law might be derived not from smoothness or smoothness-inducing mechanisms operating on the noise inherent in our motor system but rather from the correlated noise which is inherent in this motor system.",
  "stance": 0.3
 },
 {
  "url": "https://papers.neurips.cc/paper/2005/file/f6c9dc70ecfd8f90ba8598aa2401cd1a-Paper.pdf",
  "title": "Learning Influence among Interacting Markov Chains",
  "year": 2005,
  "venue": "NeurIPS",
  "abstract": "We present a model that learns the influence of interacting Markov chains within a team. The proposed model is a dynamic Bayesian network (DBN) with a two-level structure: individual-level and group-level. Individual level models actions of each player, and the group-level models actions of the team as a whole. Experiments on synthetic multi-player games and a multi-party meeting corpus show the effectiveness of the proposed model.",
  "stance": 0.7
 },
 {
  "url": "https://papers.neurips.cc/paper/2006/file/1091660f3dff84fd648efe31391c5524-Paper.pdf",
  "title": "Modeling Human Motion Using Binary Latent Variables",
  "year": 2006,
  "venue": "NeurIPS",
  "abstract": "We propose a non-linear generative model for human motion data that uses an undirected model with binary latent variables and real-valued “visible” variables that represent joint angles. The latent and visible variables at each time step receive directed connections from the visible variables at the last few time-steps. Such an architecture makes on-line inference efficient and allows us to use a simple approximate learning procedure. After training, the model finds a single set of parameters that simultaneously capture several different kinds of motion. We demonstrate the power of our approach by synthesizing various motion sequences and by performing on-line filling in of data lost during motion capture. Website: http://www.cs.toronto.edu/∼gwtaylor/publications/nips2006mhmublv/",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2006/file/2bb0502c80b7432eee4c5847a5fd077b-Paper.pdf",
  "title": "Blind Motion Deblurring Using Image Statistics",
  "year": 2006,
  "venue": "NeurIPS",
  "abstract": "We address the problem of blind motion deblurring from a single image, caused by a few moving objects. In such situations only part of the image may be blurred, and the scene consists of layers blurred in different degrees. Most of of existing blind deconvolution research concentrates at recovering a single blurring kernel for the entire image. However, in the case of different motions, the blur cannot be modeled with a single kernel, and trying to deconvolve the entire image with the same kernel will cause serious artifacts. Thus, the task of deblurring needs to involve segmentation of the image into regions with different blurs. Our approach relies on the observation that the statistics of derivative filters in images are significantly changed by blur. Assuming the blur results from a constant velocity motion, we can limit the search to one dimensional box filter blurs. This enables us to model the expected derivatives distributions as a function of the width of the blur kernel. Those distributions are surprisingly powerful in discriminating regions with different blurs. The approach produces convincing deconvolution results on real world images with rich texture.",
  "stance": 0.7
 },
 {
  "url": "https://papers.neurips.cc/paper/2006/file/2bd2e3373dce441c6c3bfadd1daa953e-Paper.pdf",
  "title": "Online Clustering of Moving Hyperplanes",
  "year": 2006,
  "venue": "NeurIPS",
  "abstract": "We propose a recursive algorithm for clustering trajectories lying in multiple moving hyperplanes. Starting from a given or random initial condition, we use normalized gradient descent to update the coefficients of a time varying polynomial whose degree is the number of hyperplanes and whose derivatives at a trajectory give an estimate of the vector normal to the hyperplane containing that trajectory. As time proceeds, the estimates of the hyperplane normals are shown to track their true values in a stable fashion. The segmentation of the trajectories is then obtained by clustering their associated normal vectors. The final result is a simple recursive algorithm for segmenting a variable number of moving hyperplanes. We test our algorithm on the segmentation of dynamic scenes containing rigid motions and dynamic textures, e.g., a bird floating on water. Our method not only segments the bird motion from the surrounding water motion, but also determines patterns of motion in the scene (e.g., periodic motion) directly from the temporal evolution of the estimated polynomial coefficients. Our experiments also show that our method can deal with appearing and disappearing motions in the scene.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2006/file/663fd3c5144fd10bd5ca6611a9a5b92d-Paper.pdf",
  "title": "Learning annotated hierarchies from relational data",
  "year": 2006,
  "venue": "NeurIPS",
  "abstract": "The objects in many real-world domains can be organized into hierarchies, where each internal node picks out a category of objects. Given a collection of features and relations defined over a set of objects, an annotated hierarchy includes a specification of the categories that are most useful for describing each individual feature and relation. We define a generative model for annotated hierarchies and the features and relations that they describe, and develop a Markov chain Monte Carlo scheme for learning annotated hierarchies. We show that our model discovers interpretable structure in several real-world data sets.",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2006/file/748d6b6ed8e13f857ceaa6cfbdca14b8-Paper.pdf",
  "title": "Inducing Metric Violations in Human Similarity Judgements",
  "year": 2006,
  "venue": "NeurIPS",
  "abstract": "Attempting to model human categorization and similarity judgements is both a very interesting but also an exceedingly difficult challenge. Some of the difficulty arises because of conflicting evidence whether human categorization and similarity judgements should or should not be modelled as to operate on a mental representation that is essentially metric. Intuitively, this has a strong appeal as it would allow (dis)similarity to be represented geometrically as distance in some internal space. Here we show how a single stimulus, carefully constructed in a psychophysical experiment, introduces l2 violations in what used to be an internal similarity space that could be adequately modelled as Euclidean. We term this one influential data point a conflictual judgement. We present an algorithm of how to analyse such data and how to identify the crucial point. Thus there may not be a strict dichotomy between either a metric or a non-metric internal space but rather degrees to which potentially large subsets of stimuli are represented metrically with a small subset causing a global violation of metricity.",
  "stance": -0.6
 },
 {
  "url": "https://papers.neurips.cc/paper/2006/file/b8af7d0fbf094517781e0382102d7b27-Paper.pdf",
  "title": "Cross-Validation Optimization for Large Scale Hierarchical Classification Kernel Methods",
  "year": 2006,
  "venue": "NeurIPS",
  "abstract": "We propose a highly efficient framework for kernel multi-class models with a large and structured set of classes. Kernel parameters are learned automatically by maximizing the cross-validation log likelihood, and predictive probabilities are estimated. We demonstrate our approach on large scale text classification tasks with hierarchical class structure, achieving state-of-the-art results in an order of magnitude less time than previous work.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2009/file/0f96613235062963ccde717b18f97592-Paper.pdf",
  "title": "Differential Use of Implicit Negative Evidence in Generative and Discriminative Language Learning",
  "year": 2009,
  "venue": "NeurIPS",
  "abstract": "A classic debate in cognitive science revolves around understanding how children learn complex linguistic rules, such as those governing restrictions on verb alternations, without negative evidence. Traditionally, formal learnability arguments have been used to claim that such learning is impossible without the aid of innate language-specific knowledge. However, recently, researchers have shown that statistical models are capable of learning complex rules from only positive evidence. These two kinds of learnability analyses differ in their assumptions about the distribution from which linguistic input is generated. The former analyses assume that learners seek to identify grammatical sentences in a way that is robust to the distribution from which the sentences are generated, analogous to discriminative approaches in machine learning. The latter assume that learners are trying to estimate a generative model, with sentences being sampled from that model. We show that these two learning approaches differ in their use of implicit negative evidence – the absence of a sentence – when learning verb alternations, and demonstrate that human learners can produce results consistent with the predictions of both approaches, depending on how the learning problem is presented.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2009/file/22fb0cee7e1f3bde58293de743871417-Paper.pdf",
  "title": "Which graphical models are difficult to learn?",
  "year": 2009,
  "venue": "NeurIPS",
  "abstract": "We consider the problem of learning the structure of Ising models (pairwise binary Markov random fields) from i.i.d. samples. While several methods have been proposed to accomplish this task, their relative merits and limitations remain somewhat obscure. By analyzing a number of concrete examples, we show that low-complexity algorithms systematically fail when the Markov random field develops long-range correlations. More precisely, this phenomenon appears to be related to the Ising model phase transition (although it does not coincide with it).",
  "stance": -1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2009/file/2b24d495052a8ce66358eb576b8912c8-Paper.pdf",
  "title": "No evidence for active sparsification in the visual cortex",
  "year": 2009,
  "venue": "NeurIPS",
  "abstract": "The proposal that cortical activity in the visual cortex is optimized for sparse neural activity is one of the most established ideas in computational neuroscience. However, direct experimental evidence for optimal sparse coding remains inconclusive, mostly due to the lack of reference values on which to judge the measured sparseness. Here we analyze neural responses to natural movies in the primary visual cortex of ferrets at different stages of development and of rats while awake and under different levels of anesthesia. In contrast with prediction from a sparse coding model, our data shows that population and lifetime sparseness decrease with visual experience, and increase from the awake to anesthetized state. These results suggest that the representation in the primary visual cortex is not actively optimized to maximize sparseness.",
  "stance": -0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2009/file/3435c378bb76d4357324dd7e69f3cd18-Paper.pdf",
  "title": "Bayesian Belief Polarization",
  "year": 2009,
  "venue": "NeurIPS",
  "abstract": "Empirical studies have documented cases of belief polarization, where two people with opposing prior beliefs both strengthen their beliefs after observing the same evidence. Belief polarization is frequently offered as evidence of human irrationality, but we demonstrate that this phenomenon is consistent with a fully Bayesian approach to belief revision. Simulation results indicate that belief polarization is not only possible but relatively common within the set of Bayesian models that we consider. Suppose that Carol has requested a promotion at her company and has received a score of 50 on an aptitude test. Alice, one of the company’s managers, began with a high opinion of Carol and became even more confident of her abilities after seeing her test score. Bob, another manager, began with a low opinion of Carol and became even less confident about her qualifications after seeing her score. On the surface, it may appear that either Alice or Bob is behaving irrationally, since the same piece of evidence has led them to update their beliefs about Carol in opposite directions. This situation is an example of belief polarization [1, 2], a widely studied phenomenon that is often taken as evidence of human irrationality [3, 4]. In some cases, however, belief polarization may appear much more sensible when all the relevant information is taken into account. Suppose, for instance, that Alice was familiar with the aptitude test and knew that it was scored out of 60, but that Bob was less familiar with the test and assumed that the score was a percentage. Even though only one interpretation of the score can be correct, Alice and Bob have both made rational inferences given their assumptions about the test. Some instances of belief polarization are almost certain to qualify as genuine departures from rational inference, but we argue in this paper that others will be entirely compatible with a rational approach. Distinguishing between these cases requires a precise normative standard against which human inferences can be compared. We suggest that Bayesian inference provides this normative standard, and present a set of Bayesian models that includes cases where polarization can and cannot emerge. Our work is in the spirit of previous studies that use careful rational analyses in order to illuminate apparently irrational human behavior (e.g. [5, 6, 7]). Previous studies of belief polarization have occasionally taken a Bayesian approach, but often the goal is to show how belief polarization can emerge as a consequence of approximate inference in a Bayesian model that is subject to memory constraints or processing limitations [8]. In contrast, we demonstrate that some examples of polarization are compatible with a fully Bayesian approach. Other formal accounts of belief polarization have relied on complex versions of utility theory [9], or have focused on continuous hypothesis spaces [10] unlike the discrete hypothesis spaces usually considered by psychological studies of belief polarization. We focus on discrete hypothesis spaces and require no additional machinery beyond the basics of Bayesian inference. We begin by introducing the belief revision phenomena considered in this paper and developing a Bayesian approach that clarifies whether and when these phenomena should be considered irrational. We then consider several Bayesian models that are capable of producing belief polarization and illustrate them with concrete examples. Having demonstrated that belief polarization is compatible",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2009/file/fec8d47d412bcbeece3d9128ae855a7a-Paper.pdf",
  "title": "Complexity of Decentralized Control: Special Cases",
  "year": 2009,
  "venue": "NeurIPS",
  "abstract": "The worst-case complexity of general decentralized POMDPs, which are equivalent to partially observable stochastic games (POSGs) is very high, both for the cooperative and competitive cases. Some reductions in complexity have been achieved by exploiting independence relations in some models. We show that these results are somewhat limited: when these independence assumptions are relaxed in very small ways, complexity returns to that of the general case.",
  "stance": -1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2010/file/182be0c5cdcd5072bb1864cdee4d3d6e-Paper.pdf",
  "title": "Universal Consistency of Multi-Class Support Vector Classification",
  "year": 2010,
  "venue": "NeurIPS",
  "abstract": "Steinwart was the first to prove universal consistency of support vector machine classification. His proof analyzed the ‘standard’ support vector machine classifier, which is restricted to binary classification problems. In contrast, recent analysis has resulted in the common belief that several extensions of SVM classification to more than two classes are inconsistent. Countering this belief, we prove the universal consistency of the multi-class support vector machine by Crammer and Singer. Our proof extends Steinwart’s techniques to the multi-class case. Erratum, 20.01.2011 Unfortunately this paper contains a subtle flaw in the proof of Lemma 5. Furthermore it turns out the statement itself is wrong: The multi-class SVM by Crammer&Singer is not universally consistent.",
  "stance": -1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2010/file/a9b7ba70783b617e9998dc4dd82eb3c5-Paper.pdf",
  "title": "An analysis on negative curvature induced by singularity in multi-layer neural-network learning",
  "year": 2010,
  "venue": "NeurIPS",
  "abstract": "In the neural-network parameter space, an attractive field is likely to be induced by singularities. In such a singularity region, first-order gradient learning typically causes a long plateau with very little change in the objective function value E (hence, a flat region). Therefore, it may be confused with “attractive” local minima. Our analysis shows that the Hessian matrix of E tends to be indefinite in the vicinity of (perturbed) singular points, suggesting a promising strategy that exploits negative curvature so as to escape from the singularity plateaus. For numerical evidence, we limit the scope to small examples (some of which are found in journal papers) that allow us to confirm singularities and the eigenvalues of the Hessian matrix, and for which computation using a descent direction of negative curvature encounters no plateau. Even for those small problems, no efficient methods have been previously developed that avoided plateaus.",
  "stance": -1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2010/file/e00406144c1e7e35240afed70f34166a-Paper.pdf",
  "title": "Online Learning: Random Averages, Combinatorial Parameters, and Learnability",
  "year": 2010,
  "venue": "NeurIPS",
  "abstract": "We develop a theory of online learning by defining several complexity measures. Among them are analogues of Rademacher complexity, covering numbers and fatshattering dimension from statistical learning theory. Relationship among these complexity measures, their connection to online learning, and tools for bounding them are provided. We apply these results to various learning problems. We provide a complete characterization of online learnability in the supervised setting.",
  "stance": 0.2
 },
 {
  "url": "https://papers.neurips.cc/paper/2011/file/1019c8091693ef5c5f55970346633f92-Paper.pdf",
  "title": "Learning Patient-Specific Cancer Survival Distributions as a Sequence of Dependent Regressors",
  "year": 2011,
  "venue": "NeurIPS",
  "abstract": "An accurate model of patient survival time can help in the treatment and care of cancer patients. The common practice of providing survival time estimates based only on population averages for the site and stage of cancer ignores many important individual differences among patients. In this paper, we propose a local regression method for learning patient-specific survival time distribution based on patient attributes such as blood tests and clinical assessments. When tested on a cohort of more than 2000 cancer patients, our method gives survival time predictions that are much more accurate than popular survival analysis models such as the Cox and Aalen regression models. Our results also show that using patient-specific attributes can reduce the prediction error on survival time by as much as 20% when compared to using cancer site and stage only.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2011/file/182be0c5cdcd5072bb1864cdee4d3d6e-Paper.pdf",
  "title": "Video Annotation and Tracking with Active Learning",
  "year": 2011,
  "venue": "NeurIPS",
  "abstract": "We introduce a novel active learning framework for video annotation. By judiciously choosing which frames a user should annotate, we can obtain highly accurate tracks with minimal user effort. We cast this problem as one of active learning, and show that we can obtain excellent performance by querying frames that, if annotated, would produce a large expected change in the estimated object track. We implement a constrained tracker and compute the expected change for putative annotations with efficient dynamic programming algorithms. We demonstrate our framework on four datasets, including two benchmark datasets constructed with key frame annotations obtained by Amazon Mechanical Turk. Our results indicate that we could obtain equivalent labels for a small fraction of the original cost.",
  "stance": 0.7
 },
 {
  "url": "https://papers.neurips.cc/paper/2011/file/218a0aefd1d1a4be65601cc6ddc1520e-Paper.pdf",
  "title": "Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent",
  "year": 2011,
  "venue": "NeurIPS",
  "abstract": "Stochastic Gradient Descent (SGD) is a popular algorithm that can achieve stateof-the-art performance on a variety of machine learning tasks. Several researchers have recently proposed schemes to parallelize SGD, but all require performancedestroying memory locking and synchronization. This work aims to show using novel theoretical analysis, algorithms, and implementation that SGD can be implemented without any locking. We present an update scheme called HOGWILD! which allows processors access to shared memory with the possibility of overwriting each other’s work. We show that when the associated optimization problem is sparse, meaning most gradient updates only modify small parts of the decision variable, then HOGWILD! achieves a nearly optimal rate of convergence. We demonstrate experimentally that HOGWILD! outperforms alternative schemes that use locking by an order of magnitude.",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2011/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf",
  "title": "Active Classification based on Value of Classifier",
  "year": 2011,
  "venue": "NeurIPS",
  "abstract": "Modern classification tasks usually involve many class labels and can be informed by a broad range of features. Many of these tasks are tackled by constructing a set of classifiers, which are then applied at test time and then pieced together in a fixed procedure determined in advance or at training time. We present an active classification process at the test time, where each classifier in a large ensemble is viewed as a potential observation that might inform our classification process. Observations are then selected dynamically based on previous observations, using a value-theoretic computation that balances an estimate of the expected classification gain from each observation as well as its computational cost. The expected classification gain is computed using a probabilistic model that uses the outcome from previous observations. This active classification process is applied at test time for each individual test instance, resulting in an efficient instance-specific decision path. We demonstrate the benefit of the active scheme on various real-world datasets, and show that it can achieve comparable or even higher classification accuracy at a fraction of the computational costs of traditional methods.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2011/file/46072631582fc240dd2674a7d063b040-Paper.pdf",
  "title": "Variance Penalizing AdaBoost",
  "year": 2011,
  "venue": "NeurIPS",
  "abstract": "This paper proposes a novel boosting algorithm called VadaBoost which is motivated by recent empirical Bernstein bounds. VadaBoost iteratively minimizes a cost function that balances the sample mean and the sample variance of the exponential loss. Each step of the proposed algorithm minimizes the cost efficiently by providing weighted data to a weak learner rather than requiring a brute force evaluation of all possible weak learners. Thus, the proposed algorithm solves a key limitation of previous empirical Bernstein boosting methods which required brute force enumeration of all possible weak learners. Experimental results confirm that the new algorithm achieves the performance improvements of EBBoost yet goes beyond decision stumps to handle any weak learner. Significant performance gains are obtained over AdaBoost for arbitrary weak learners including decision trees (CART).",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2011/file/5a4b25aaed25c2ee1b74de72dc03c14e-Paper.pdf",
  "title": "Fast and Balanced: Efficient Label Tree Learning for Large Scale Object Recognition",
  "year": 2011,
  "venue": "NeurIPS",
  "abstract": "We present a novel approach to efficiently learn a label tree for large scale classification with many classes. The key contribution of the approach is a technique to simultaneously determine the structure of the tree and learn the classifiers for each node in the tree. This approach also allows fine grained control over the efficiency vs accuracy trade-off in designing a label tree, leading to more balanced trees. Experiments are performed on large scale image classification with 10184 classes and 9 million images. We demonstrate significant improvements in test accuracy and efficiency with less training time and more balanced trees compared to the previous state of the art by Bengio et al.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2011/file/8b5040a8a5baf3e0e67386c2e3a9b903-Paper.pdf",
  "title": "The Fast Convergence of Boosting",
  "year": 2011,
  "venue": "NeurIPS",
  "abstract": "This manuscript considers the convergence rate of boosting under a large class of losses, including the exponential and logistic losses, where the best previous rate of convergence was O(exp(1/✏2)). First, it is established that the setting of weak learnability aids the entire class, granting a rate O(ln(1/✏)). Next, the (disjoint) conditions under which the infimal empirical risk is attainable are characterized in terms of the sample and weak learning class, and a new proof is given for the known rate O(ln(1/✏)). Finally, it is established that any instance can be decomposed into two smaller instances resembling the two preceding special cases, yielding a rate O(1/✏), with a matching lower bound for the logistic loss. The principal technical hurdle throughout this work is the potential unattainability of the infimal empirical risk; the technique for overcoming this barrier may be of general interest.",
  "stance": 0.1
 },
 {
  "url": "https://papers.neurips.cc/paper/2011/file/ab541d874c7bc19ab77642849e02b89f-Paper.pdf",
  "title": "High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity",
  "year": 2011,
  "venue": "NeurIPS",
  "abstract": "Although the standard formulations of prediction problems involve fully-observed and noiseless data drawn in an i.i.d. manner, many applications involve noisy and/or missing data, possibly involving dependencies. We study these issues in the context of high-dimensional sparse linear regression, and propose novel estimators for the cases of noisy, missing, and/or dependent data. Many standard approaches to noisy or missing data, such as those using the EM algorithm, lead to optimization problems that are inherently non-convex, and it is difficult to establish theoretical guarantees on practical algorithms. While our approach also involves optimizing non-convex programs, we are able to both analyze the statistical error associated with any global optimum, and prove that a simple projected gradient descent algorithm will converge in polynomial time to a small neighborhood of the set of global minimizers. On the statistical side, we provide non-asymptotic bounds that hold with high probability for the cases of noisy, missing, and/or dependent data. On the computational side, we prove that under the same types of conditions required for statistical consistency, the projected gradient descent algorithm will converge at geometric rates to a near-global minimizer. We illustrate these theoretical predictions with simulations, showing agreement with the predicted scalings.",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2011/file/c9e1074f5b3f9fc8ea15d152add07294-Paper.pdf",
  "title": "Environmental statistics and the trade-off between model-based and TD learning in humans",
  "year": 2011,
  "venue": "NeurIPS",
  "abstract": "There is much evidence that humans and other animals utilize a combination of model-based and model-free RL methods. Although it has been proposed that these systems may dominate according to their relative statistical efficiency in different circumstances, there is little specific evidence — especially in humans — as to the details of this trade-off. Accordingly, we examine the relative performance of different RL approaches under situations in which the statistics of reward are differentially noisy and volatile. Using theory and simulation, we show that model-free TD learning is relatively most disadvantaged in cases of high volatility and low noise. We present data from a decision-making experiment manipulating these parameters, showing that humans shift learning strategies in accord with these predictions. The statistical circumstances favoring model-based RL are also those that promote a high learning rate, which helps explain why, in psychology, the distinction between these strategies is traditionally conceived in terms of rulebased vs. incremental learning.",
  "stance": -0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2011/file/c9f95a0a5af052bffce5c89917335f67-Paper.pdf",
  "title": "A Model for Temporal Dependencies in Event Streams",
  "year": 2011,
  "venue": "NeurIPS",
  "abstract": "We introduce the Piecewise-Constant Conditional Intensity Model, a model for learning temporal dependencies in event streams. We describe a closed-form Bayesian approach to learning these models, and describe an importance sampling algorithm for forecasting future events using these models, using a proposal distribution based on Poisson superposition. We then use synthetic data, supercomputer event logs, and web search query logs to illustrate that our learning algorithm can efficiently learn nonlinear temporal dependencies, and that our importance sampling algorithm can effectively forecast future events.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2011/file/e1e32e235eee1f970470a3a6658dfdd5-Paper.pdf",
  "title": "From Bandits to Experts: On the Value of Side-Observations",
  "year": 2011,
  "venue": "NeurIPS",
  "abstract": "We consider an adversarial online learning setting where a decision maker can choose an action in every stage of the game. In addition to observing the reward of the chosen action, the decision maker gets side observations on the reward he would have obtained had he chosen some of the other actions. The observation structure is encoded as a graph, where node i is linked to node j if sampling i provides information on the reward of j. This setting naturally interpolates between the well-known “experts” setting, where the decision maker can view all rewards, and the multi-armed bandits setting, where the decision maker can only view the reward of the chosen action. We develop practical algorithms with provable regret guarantees, which depend on non-trivial graph-theoretic properties of the information feedback structure. We also provide partially-matching lower bounds.",
  "stance": 0.3
 },
 {
  "url": "https://papers.neurips.cc/paper/2011/file/e6b4b2a746ed40e1af829d1fa82daa10-Paper.pdf",
  "title": "Maximal Cliques that Satisfy Hard Constraints with Application to Deformable Object Model Learning",
  "year": 2011,
  "venue": "NeurIPS",
  "abstract": "We propose a novel inference framework for finding maximal cliques in a weighted graph that satisfy hard constraints. The constraints specify the graph nodes that must belong to the solution as well as mutual exclusions of graph nodes, i.e., sets of nodes that cannot belong to the same solution. The proposed inference is based on a novel particle filter algorithm with state permeations. We apply the inference framework to a challenging problem of learning part-based, deformable object models. Two core problems in the learning framework, matching of image patches and finding salient parts, are formulated as two instances of the problem of finding maximal cliques with hard constraints. Our learning framework yields discriminative part based object models that achieve very good detection rate, and outperform other methods on object classes with large deformation.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2011/file/eddb904a6db773755d2857aacadb1cb0-Paper.pdf",
  "title": "Probabilistic Joint Image Segmentation and Labeling",
  "year": 2011,
  "venue": "NeurIPS",
  "abstract": "We present a joint image segmentation and labeling model (JSL) which, given a bag of figure-ground segment hypotheses extracted at multiple image locations and scales, constructs a joint probability distribution over both the compatible image interpretations (tilings or image segmentations) composed from those segments, and over their labeling into categories. The process of drawing samples from the joint distribution can be interpreted as first sampling tilings, modeled as maximal cliques, from a graph connecting spatially non-overlapping segments in the bag [1], followed by sampling labels for those segments, conditioned on the choice of a particular tiling. We learn the segmentation and labeling parameters jointly, based on Maximum Likelihood with a novel Incremental Saddle Point estimation procedure. The partition function over tilings and labelings is increasingly more accurately approximated by including incorrect configurations that a not-yet-competent model rates probable during learning. We show that the proposed methodology matches the current state of the art in the Stanford dataset [2], as well as in VOC2010, where 41.7% accuracy on the test set is achieved.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2011/file/effc299a1addb07e7089f9b269c31f2f-Paper.pdf",
  "title": "Non-parametric Group Orthogonal Matching Pursuit for Sparse Learning with Multiple Kernels",
  "year": 2011,
  "venue": "NeurIPS",
  "abstract": "We consider regularized risk minimization in a large dictionary of Reproducing kernel Hilbert Spaces (RKHSs) over which the target function has a sparse representation. This setting, commonly referred to as Sparse Multiple Kernel Learning (MKL), may be viewed as the non-parametric extension of group sparsity in linear models. While the two dominant algorithmic strands of sparse learning, namely convex relaxations using l1 norm (e.g., Lasso) and greedy methods (e.g., OMP), have both been rigorously extended for group sparsity, the sparse MKL literature has so far mainly adopted the former with mild empirical success. In this paper, we close this gap by proposing a Group-OMP based framework for sparse MKL. Unlike l1-MKL, our approach decouples the sparsity regularizer (via a direct l0 constraint) from the smoothness regularizer (via RKHS norms), which leads to better empirical performance and a simpler optimization procedure that only requires a black-box single-kernel solver. The algorithmic development and empirical studies are complemented by theoretical analyses in terms of Rademacher generalization bounds and sparse recovery conditions analogous to those for OMP [27] and Group-OMP [16].",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2011/file/f85454e8279be180185cac7d243c5eb3-Paper.pdf",
  "title": "Solving Decision Problems with Limited Information",
  "year": 2011,
  "venue": "NeurIPS",
  "abstract": "We present a new algorithm for exactly solving decision-making problems represented as an influence diagram. We do not require the usual assumptions of no forgetting and regularity, which allows us to solve problems with limited information. The algorithm, which implements a sophisticated variable elimination procedure, is empirically shown to outperform a state-of-the-art algorithm in randomly generated problems of up to 150 variables and 10 strategies.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2011/file/fc3cf452d3da8402bebb765225ce8c0e-Paper.pdf",
  "title": "Joint 3D Estimation of Objects and Scene Layout",
  "year": 2011,
  "venue": "NeurIPS",
  "abstract": "We propose a novel generative model that is able to reason jointly about the 3D scene layout as well as the 3D location and orientation of objects in the scene. In particular, we infer the scene topology, geometry as well as traffic activities from a short video sequence acquired with a single camera mounted on a moving car. Our generative model takes advantage of dynamic information in the form of vehicle tracklets as well as static information coming from semantic labels and geometry (i.e., vanishing points). Experiments show that our approach outperforms a discriminative baseline based on multiple kernel learning (MKL) which has access to the same image information. Furthermore, as we reason about objects in 3D, we are able to significantly increase the performance of state-of-the-art object detectors in their ability to estimate object orientation.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2012/file/4fac9ba115140ac4f1c22da82aa0bc7f-Paper.pdf",
  "title": "Phoneme Classification using Constrained Variational Gaussian Process Dynamical System",
  "year": 2012,
  "venue": "NeurIPS",
  "abstract": "For phoneme classification, this paper describes an acoustic model based on the variational Gaussian process dynamical system (VGPDS). The nonlinear and nonparametric acoustic model is adopted to overcome the limitations of classical hidden Markov models (HMMs) in modeling speech. The Gaussian process prior on the dynamics and emission functions respectively enable the complex dynamic structure and long-range dependency of speech to be better represented than that by an HMM. In addition, a variance constraint in the VGPDS is introduced to eliminate the sparse approximation error in the kernel matrix. The effectiveness of the proposed model is demonstrated with three experimental results, including parameter estimation and classification performance, on the synthetic and benchmark datasets.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2012/file/50f3f8c42b998a48057e9d33f4144b8b-Paper.pdf",
  "title": "On the (Non-)existence of Convex, Calibrated Surrogate Losses for Ranking",
  "year": 2012,
  "venue": "NeurIPS",
  "abstract": "We study surrogate losses for learning to rank, in a framework where the rankings are induced by scores and the task is to learn the scoring function. We focus on the calibration of surrogate losses with respect to a ranking evaluation metric, where the calibration is equivalent to the guarantee that near-optimal values of the surrogate risk imply near-optimal values of the risk defined by the evaluation metric. We prove that if a surrogate loss is a convex function of the scores, then it is not calibrated with respect to two evaluation metrics widely used for search engine evaluation, namely the Average Precision and the Expected Reciprocal Rank. We also show that such convex surrogate losses cannot be calibrated with respect to the Pairwise Disagreement, an evaluation metric used when learning from pairwise preferences. Our results cast lights on the intrinsic difficulty of some ranking problems, as well as on the limitations of learning-to-rank algorithms based on the minimization of a convex surrogate risk.",
  "stance": -0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2012/file/56468d5607a5aaf1604ff5e15593b003-Paper.pdf",
  "title": "Efficient and direct estimation of a neural subunit model for sensory coding",
  "year": 2012,
  "venue": "NeurIPS",
  "abstract": "Many visual and auditory neurons have response properties that are well explained by pooling the rectified responses of a set of spatially shifted linear filters. These filters cannot be estimated using spike-triggered averaging (STA). Subspace methods such as spike-triggered covariance (STC) can recover multiple filters, but require substantial amounts of data, and recover an orthogonal basis for the subspace in which the filters reside rather than the filters themselves. Here, we assume a linear-nonlinear–linear-nonlinear (LN-LN) cascade model in which the first linear stage is a set of shifted (‘convolutional’) copies of a common filter, and the first nonlinear stage consists of rectifying scalar nonlinearities that are identical for all filter outputs. We refer to these initial LN elements as the ‘subunits’ of the receptive field. The second linear stage then computes a weighted sum of the responses of the rectified subunits. We present a method for directly fitting this model to spike data, and apply it to both simulated and real neuronal data from primate V1. The subunit model significantly outperforms STA and STC in terms of cross-validated accuracy and efficiency.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2012/file/84d9ee44e457ddef7f2c4f25dc8fa865-Paper.pdf",
  "title": "Non-parametric Approximate Dynamic Programming via the Kernel Method",
  "year": 2012,
  "venue": "NeurIPS",
  "abstract": "This paper presents a novel non-parametric approximate dynamic programming (ADP) algorithm that enjoys graceful approximation and sample complexity guarantees. In particular, we establish both theoretically and computationally that our proposal can serve as a viable alternative to state-of-the-art parametric ADP algorithms, freeing the designer from carefully specifying an approximation architecture. We accomplish this by developing a kernel-based mathematical program for ADP. Via a computational study on a controlled queueing network, we show that our procedure is competitive with parametric ADP approaches.",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2013/file/0bb4aec1710521c12ee76289d9440817-Paper.pdf",
  "title": "Multi-Prediction Deep Boltzmann Machines",
  "year": 2013,
  "venue": "NeurIPS",
  "abstract": "We introduce the multi-prediction deep Boltzmann machine (MP-DBM). The MPDBM can be seen as a single probabilistic model trained to maximize a variational approximation to the generalized pseudolikelihood, or as a family of recurrent nets that share parameters and approximately solve different inference problems. Prior methods of training DBMs either do not perform well on classification tasks or require an initial learning pass that trains the DBM greedily, one layer at a time. The MP-DBM does not require greedy layerwise pretraining, and outperforms the standard DBM at classification, classification with missing inputs, and mean field prediction tasks.1",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2013/file/2dace78f80bc92e6d7493423d729448e-Paper.pdf",
  "title": "Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result",
  "year": 2013,
  "venue": "NeurIPS",
  "abstract": "Approximate dynamic programming approaches to the reinforcement learning problem are often categorized into greedy value function methods and value-based policy gradient methods. As our first main result, we show that an important subset of the latter methodology is, in fact, a limiting special case of a general formulation of the former methodology; optimistic policy iteration encompasses not only most of the greedy value function methods but also natural actor-critic methods, and permits one to directly interpolate between them. The resulting continuum adjusts the strength of the Markov assumption in policy improvement and, as such, can be seen as dual in spirit to the continuum in TD(λ)-style algorithms in policy evaluation. As our second main result, we show for a substantial subset of softgreedy value function approaches that, while having the potential to avoid policy oscillation and policy chattering, this subset can never converge toward an optimal policy, except in a certain pathological case. Consequently, in the context of approximations (either in state estimation or in value function representation), the majority of greedy value function methods seem to be deemed to suffer either from the risk of oscillation/chattering or from the presence of systematic sub-optimality.",
  "stance": -0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2013/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf",
  "title": "Bellman Error Based Feature Generation using Random Projections on Sparse Spaces",
  "year": 2013,
  "venue": "NeurIPS",
  "abstract": "This paper addresses the problem of automatic generation of features for value function approximation in reinforcement learning. Bellman Error Basis Functions (BEBFs) have been shown to improve policy evaluation, with a convergence rate similar to that of value iteration. We propose a simple, fast and robust algorithm based on random projections, which generates BEBFs for sparse feature spaces. We provide a finite sample analysis of the proposed method, and prove that projections logarithmic in the dimension of the original space guarantee a contraction in the error. Empirical results demonstrate the strength of this method in domains in which choosing a good state representation is challenging.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2013/file/a01a0380ca3c61428c26a231f0e49a09-Paper.pdf",
  "title": "Which Space Partitioning Tree to Use for Search?",
  "year": 2013,
  "venue": "NeurIPS",
  "abstract": "We consider the task of nearest-neighbor search with the class of binary-spacepartitioning trees, which includes kd-trees, principal axis trees and random projection trees, and try to rigorously answer the question “which tree to use for nearestneighbor search?” To this end, we present the theoretical results which imply that trees with better vector quantization performance have better search performance guarantees. We also explore another factor affecting the search performance – margins of the partitions in these trees. We demonstrate, both theoretically and empirically, that large margin partitions can improve tree search performance. 1 Nearest-neighbor search Nearest-neighbor search is ubiquitous in computer science. Several techniques exist for nearestneighbor search, but most algorithms can be categorized into two following groups based on the indexing scheme used – (1) search with hierarchical tree indices, or (2) search with hash-based indices. Although multidimensional binary space-partitioning trees (or BSP-trees), such as kd-trees [1], are widely used for nearest-neighbor search, it is believed that their performances degrade with increasing dimensions. Standard worst-case analyses of search with BSP-trees in high dimensions usually lead to trivial guarantees (such as, an Ω(n) search time guarantee for a single nearest-neighbor query in a set of n points). This is generally attributed to the “curse of dimensionality” – in the worst case, the high dimensionality can force the search algorithm to visit every node in the BSP-tree. However, these BSP-trees are very simple and intuitive, and still used in practice with success. The occasional favorable performances of BSP-trees in high dimensions are attributed to the low “intrinsic” dimensionality of real data. However, no clear relationship between the BSP-tree search performance and the intrinsic data properties is known. We present theoretical results which link the search performance of BSP-trees to properties of the data and the tree. This allows us to identify implicit factors influencing BSP-tree search performance — knowing these driving factors allows us to develop successful heuristics for BSP-trees with improved search performance. Algorithm 1 BSP-tree search Input: BSP-tree T on set S, Query q, Desired depth l Output: Candidate neighbor p current tree depth lc ← 0 current tree node Tc ← T while lc < l do if 〈Tc.w, q〉+ Tc.b ≤ 0 then Tc ← Tc.left child else Tc ← Tc.right child end if Increment depth lc ← lc + 1 end while p← argminr∈Tc∩S ‖q − r‖. Each node in a BSP-tree represents a region of the space and each non-leaf node has a left and right child representing a disjoint partition of this region with some separating hyperplane and threshold (w, b). A search query on this tree is usually answered with a depth-first branch-and-bound algorithm. Algorithm 1 presents a simplified version where a search query is answered with a small set of neighbor candidates of any desired size by performing a greedy depth-first tree traversal to a specified depth. This is known as defeatist tree search. We are not aware of any data-dependent analysis of the quality of the results from defeatist BSP-tree search. However, Verma et al. (2009) [2] presented adaptive data-dependent analyses of some BSP-trees for the task of vector quantization. These results show precise connections between the quantization performance of the BSP-trees and certain properties of the data (we will present these data properties in Section 2).",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2013/file/a1d50185e7426cbb0acad1e6ca74b9aa-Paper.pdf",
  "title": "Submodular Optimization with Submodular Cover and Submodular Knapsack Constraints",
  "year": 2013,
  "venue": "NeurIPS",
  "abstract": "We investigate two new optimization problems — minimizing a submodular function subject to a submodular lower bound constraint (submodular cover) and maximizing a submodular function subject to a submodular upper bound constraint (submodular knapsack). We are motivated by a number of real-world applications in machine learning including sensor placement and data subset selection, which require maximizing a certain submodular function (like coverage or diversity) while simultaneously minimizing another (like cooperative cost). These problems are often posed as minimizing the difference between submodular functions [9, 25] which is in the worst case inapproximable. We show, however, that by phrasing these problems as constrained optimization, which is more natural for many applications, we achieve a number of bounded approximation guarantees. We also show that both these problems are closely related and an approximation algorithm solving one can be used to obtain an approximation guarantee for the other. We provide hardness results for both problems thus showing that our approximation factors are tight up to log-factors. Finally, we empirically demonstrate the performance and good scalability properties of our algorithms.",
  "stance": 0.9
 },
 {
  "url": "https://papers.neurips.cc/paper/2013/file/b4d168b48157c623fbd095b4a565b5bb-Paper.pdf",
  "title": "What do row and column marginals reveal about your dataset?",
  "year": 2013,
  "venue": "NeurIPS",
  "abstract": "Numerous datasets ranging from group memberships within social networks to purchase histories on e-commerce sites are represented by binary matrices. While this data is often either proprietary or sensitive, aggregated data, notably row and column marginals, is often viewed as much less sensitive, and may be furnished for analysis. Here, we investigate how these data can be exploited to make inferences about the underlying matrix H . Instead of assuming a generative model for H , we view the input marginals as constraints on the dataspace of possible realizations of H and compute the probability density function of particular entries H(i, j) of interest. We do this for all the cells of H simultaneously, without generating realizations, but rather via implicitly sampling the datasets that satisfy the input marginals. The end result is an efficient algorithm with asymptotic running time the same as that required by standard sampling techniques to generate a single dataset from the same dataspace. Our experimental evaluation demonstrates the efficiency and the efficacy of our framework in multiple settings.",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2013/file/c042f4db68f23406c6cecf84a7ebb0fe-Paper.pdf",
  "title": "Non-Linear Domain Adaptation with Boosting",
  "year": 2013,
  "venue": "NeurIPS",
  "abstract": "A common assumption in machine vision is that the training and test samples are drawn from the same distribution. However, there are many problems when this assumption is grossly violated, as in bio-medical applications where different acquisitions can generate drastic variations in the appearance of the data due to changing experimental conditions. This problem is accentuated with 3D data, for which annotation is very time-consuming, limiting the amount of data that can be labeled in new acquisitions for training. In this paper we present a multitask learning algorithm for domain adaptation based on boosting. Unlike previous approaches that learn task-specific decision boundaries, our method learns a single decision boundary in a shared feature space, common to all tasks. We use the boosting-trick to learn a non-linear mapping of the observations in each task, with no need for specific a-priori knowledge of its global analytical form. This yields a more parameter-free domain adaptation approach that successfully leverages learning on new tasks where labeled data is scarce. We evaluate our approach on two challenging bio-medical datasets and achieve a significant improvement over the state of the art.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2013/file/e96ed478dab8595a7dbda4cbcbee168f-Paper.pdf",
  "title": "One-shot learning and big data with n=2",
  "year": 2013,
  "venue": "NeurIPS",
  "abstract": "We model a “one-shot learning” situation, where very few observations y1, ..., yn ∈ R are available. Associated with each observation yi is a very highdimensional vector xi ∈ R, which provides context for yi and enables us to predict subsequent observations, given their own context. One of the salient features of our analysis is that the problems studied here are easier when the dimension of xi is large; in other words, prediction becomes easier when more context is provided. The proposed methodology is a variant of principal component regression (PCR). Our rigorous analysis sheds new light on PCR. For instance, we show that classical PCR estimators may be inconsistent in the specified setting, unless they are multiplied by a scalar c > 1; that is, unless the classical estimator is expanded. This expansion phenomenon appears to be somewhat novel and contrasts with shrinkage methods (c < 1), which are far more common in big data analyses.",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2014/file/8597a6cfa74defcbde3047c891d78f90-Paper.pdf",
  "title": "Feature Cross-Substitution in Adversarial Classification",
  "year": 2014,
  "venue": "NeurIPS",
  "abstract": "The success of machine learning, particularly in supervised settings, has led to numerous attempts to apply it in adversarial settings such as spam and malware detection. The core challenge in this class of applications is that adversaries are not static data generators, but make a deliberate effort to evade the classifiers deployed to detect them. We investigate both the problem of modeling the objectives of such adversaries, as well as the algorithmic problem of accounting for rational, objective-driven adversaries. In particular, we demonstrate severe shortcomings of feature reduction in adversarial settings using several natural adversarial objective functions, an observation that is particularly pronounced when the adversary is able to substitute across similar features (for example, replace words with synonyms or replace letters in words). We offer a simple heuristic method for making learning more robust to feature cross-substitution attacks. We then present a more general approach based on mixed-integer linear programming with constraint generation, which implicitly trades off overfitting and feature selection in an adversarial setting using a sparse regularizer along with an evasion model. Our approach is the first method for combining an adversarial classification algorithm with a very general class of models of adversarial classifier evasion. We show that our algorithmic approach significantly outperforms state-of-the-art alternatives.",
  "stance": 0.7
 },
 {
  "url": "https://papers.neurips.cc/paper/2014/file/aa2a77371374094fe9e0bc1de3f94ed9-Paper.pdf",
  "title": "Fast Multivariate Spatio-temporal Analysis via Low Rank Tensor Learning",
  "year": 2014,
  "venue": "NeurIPS",
  "abstract": "Accurate and efficient analysis of multivariate spatio-temporal data is critical in climatology, geology, and sociology applications. Existing models usually assume simple inter-dependence among variables, space, and time, and are computationally expensive. We propose a unified low rank tensor learning framework for multivariate spatio-temporal analysis, which can conveniently incorporate different properties in spatio-temporal data, such as spatial clustering and shared structure among variables. We demonstrate how the general framework can be applied to cokriging and forecasting tasks, and develop an efficient greedy algorithm to solve the resulting optimization problem with convergence guarantee. We conduct experiments on both synthetic datasets and real application datasets to demonstrate that our method is not only significantly faster than existing methods but also achieves lower estimation error.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2014/file/b86e8d03fe992d1b0e19656875ee557c-Paper.pdf",
  "title": "Do Convnets Learn Correspondence?",
  "year": 2014,
  "venue": "NeurIPS",
  "abstract": "Convolutional neural nets (convnets) trained from massive labeled datasets [1] have substantially improved the state-of-the-art in image classification [2] and object detection [3]. However, visual understanding requires establishing correspondence on a finer level than object category. Given their large pooling regions and training from whole-image labels, it is not clear that convnets derive their success from an accurate correspondence model which could be used for precise localization. In this paper, we study the effectiveness of convnet activation features for tasks requiring correspondence. We present evidence that convnet features localize at a much finer scale than their receptive field sizes, that they can be used to perform intraclass aligment as well as conventional hand-engineered features, and that they outperform conventional features in keypoint prediction on objects from PASCAL VOC 2011 [4].",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2014/file/c930eecd01935feef55942cc445f708f-Paper.pdf",
  "title": "Online combinatorial optimization with stochastic decision sets and adversarial losses",
  "year": 2014,
  "venue": "NeurIPS",
  "abstract": "Most work on sequential learning assumes a fixed set of actions that are available all the time. However, in practice, actions can consist of picking subsets of readings from sensors that may break from time to time, road segments that can be blocked or goods that are out of stock. In this paper we study learning algorithms that are able to deal with stochastic availability of such unreliable composite actions. We propose and analyze algorithms based on the Follow-The-PerturbedLeader prediction method for several learning settings differing in the feedback provided to the learner. Our algorithms rely on a novel loss estimation technique that we call Counting Asleep Times. We deliver regret bounds for our algorithms for the previously studied full information and (semi-)bandit settings, as well as a natural middle point between the two that we call the restricted information setting. A special consequence of our results is a significant improvement of the best known performance guarantees achieved by an efficient algorithm for the sleeping bandit problem with stochastic availability. Finally, we evaluate our algorithms empirically and show their improvement over the known approaches.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2014/file/d67d8ab4f4c10bf22aa353e27879133c-Paper.pdf",
  "title": "Robust Classification Under Sample Selection Bias",
  "year": 2014,
  "venue": "NeurIPS",
  "abstract": "In many important machine learning applications, the source distribution used to estimate a probabilistic classifier differs from the target distribution on which the classifier will be used to make predictions. Due to its asymptotic properties, sample reweighted empirical loss minimization is a commonly employed technique to deal with this difference. However, given finite amounts of labeled source data, this technique suffers from significant estimation errors in settings with large sample selection bias. We develop a framework for learning a robust bias-aware (RBA) probabilistic classifier that adapts to different sample selection biases using a minimax estimation formulation. Our approach requires only accurate estimates of statistics under the source distribution and is otherwise as robust as possible to unknown properties of the conditional label distribution, except when explicit generalization assumptions are incorporated. We demonstrate the behavior and effectiveness of our approach on binary classification tasks.",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2015/file/02522a2b2726fb0a03bb19f2d8d9524d-Paper.pdf",
  "title": "A fast, universal algorithm to learn parametric nonlinear embeddings",
  "year": 2015,
  "venue": "NeurIPS",
  "abstract": "Nonlinear embedding algorithms such as stochastic neighbor embedding do dimensionality reduction by optimizing an objective function involving similarities between pairs of input patterns. The result is a low-dimensional projection of each input pattern. A common way to define an out-of-sample mapping is to optimize the objective directly over a parametric mapping of the inputs, such as a neural net. This can be done using the chain rule and a nonlinear optimizer, but is very slow, because the objective involves a quadratic number of terms each dependent on the entire mapping’s parameters. Using the method of auxiliary coordinates, we derive a training algorithm that works by alternating steps that train an auxiliary embedding with steps that train the mapping. This has two advantages: 1) The algorithm is universal in that a specific learning algorithm for any choice of embedding and mapping can be constructed by simply reusing existing algorithms for the embedding and for the mapping. A user can then try possible mappings and embeddings with less effort. 2) The algorithm is fast, and it can reuse N -body methods developed for nonlinear embeddings, yielding linear-time iterations.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2015/file/0336dcbab05b9d5ad24f4333c7658a0e-Paper.pdf",
  "title": "Top-k Multiclass SVM",
  "year": 2015,
  "venue": "NeurIPS",
  "abstract": "Class ambiguity is typical in image classification problems with a large number of classes. When classes are difficult to discriminate, it makes sense to allow k guesses and evaluate classifiers based on the top-k error instead of the standard zero-one loss. We propose top-k multiclass SVM as a direct method to optimize for top-k performance. Our generalization of the well-known multiclass SVM is based on a tight convex upper bound of the top-k error. We propose a fast optimization scheme based on an efficient projection onto the top-k simplex, which is of its own interest. Experiments on five datasets show consistent improvements in top-k accuracy compared to various baselines.",
  "stance": 0.9
 },
 {
  "url": "https://papers.neurips.cc/paper/2015/file/17e62166fc8586dfa4d1bc0e1742c08b-Paper.pdf",
  "title": "Expressing an Image Stream with a Sequence of Natural Sentences",
  "year": 2015,
  "venue": "NeurIPS",
  "abstract": "We propose an approach for retrieving a sequence of natural sentences for an image stream. Since general users often take a series of pictures on their special moments, it would better take into consideration of the whole image stream to produce natural language descriptions. While almost all previous studies have dealt with the relation between a single image and a single natural sentence, our work extends both input and output dimension to a sequence of images and a sequence of sentences. To this end, we design a multimodal architecture called coherent recurrent convolutional network (CRCN), which consists of convolutional neural networks, bidirectional recurrent neural networks, and an entity-based local coherence model. Our approach directly learns from vast user-generated resource of blog posts as text-image parallel training data. We demonstrate that our approach outperforms other state-of-the-art candidate methods, using both quantitative measures (e.g. BLEU and top-K recall) and user studies via Amazon Mechanical Turk.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2015/file/2b38c2df6a49b97f706ec9148ce48d86-Paper.pdf",
  "title": "Learning Bayesian Networks with Thousands of Variables",
  "year": 2015,
  "venue": "NeurIPS",
  "abstract": "We present a method for learning Bayesian networks from data sets containing thousands of variables without the need for structure constraints. Our approach is made of two parts. The first is a novel algorithm that effectively explores the space of possible parent sets of a node. It guides the exploration towards the most promising parent sets on the basis of an approximated score function that is computed in constant time. The second part is an improvement of an existing ordering-based algorithm for structure optimization. The new algorithm provably achieves a higher score compared to its original formulation. Our novel approach consistently outperforms the state of the art on very large data sets.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2015/file/2e65f2f2fdaf6c699b223c61b1b5ab89-Paper.pdf",
  "title": "Scalable Adaptation of State Complexity for Nonparametric Hidden Markov Models",
  "year": 2015,
  "venue": "NeurIPS",
  "abstract": "Bayesian nonparametric hidden Markov models are typically learned via fixed truncations of the infinite state space or local Monte Carlo proposals that make small changes to the state space. We develop an inference algorithm for the sticky hierarchical Dirichlet process hidden Markov model that scales to big datasets by processing a few sequences at a time yet allows rapid adaptation of the state space cardinality. Unlike previous point-estimate methods, our novel variational bound penalizes redundant or irrelevant states and thus enables optimization of the state space. Our birth proposals use observed data statistics to create useful new states that escape local optima. Merge and delete proposals remove ineffective states to yield simpler models with more affordable future computations. Experiments on speaker diarization, motion capture, and epigenetic chromatin datasets discover models that are more compact, more interpretable, and better aligned to ground truth segmentations than competitors. We have released an open-source Python implementation which can parallelize local inference steps across sequences.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2015/file/4ca82782c5372a547c104929f03fe7a9-Paper.pdf",
  "title": "End-to-end Learning of LDA by Mirror-Descent Back Propagation over a Deep Architecture",
  "year": 2015,
  "venue": "NeurIPS",
  "abstract": "We develop a fully discriminative learning approach for supervised Latent Dirichlet Allocation (LDA) model using Back Propagation (i.e., BP-sLDA), which maximizes the posterior probability of the prediction variable given the input document. Different from traditional variational learning or Gibbs sampling approaches, the proposed learning method applies (i) the mirror descent algorithm for maximum a posterior inference and (ii) back propagation over a deep architecture together with stochastic gradient/mirror descent for model parameter estimation, leading to scalable and end-to-end discriminative learning of the model. As a byproduct, we also apply this technique to develop a new learning method for the traditional unsupervised LDA model (i.e., BP-LDA). Experimental results on three real-world regression and classification tasks show that the proposed methods significantly outperform the previous supervised topic models, neural networks, and is on par with deep neural networks.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2015/file/6974ce5ac660610b44d9b9fed0ff9548-Paper.pdf",
  "title": "The Pareto Regret Frontier for Bandits",
  "year": 2015,
  "venue": "NeurIPS",
  "abstract": "Given a multi-armed bandit problem it may be desirable to achieve a smallerthan-usual worst-case regret for some special actions. I show that the price for such unbalanced worst-case regret guarantees is rather high. Specifically, if an algorithm enjoys a worst-case regret of B with respect to some action, then there must exist another action for which the worst-case regret is at least Ω(nK/B), where n is the horizon and K the number of actions. I also give upper bounds in both the stochastic and adversarial settings showing that this result cannot be improved. For the stochastic case the pareto regret frontier is characterised exactly up to constant factors.",
  "stance": -0.4
 },
 {
  "url": "https://papers.neurips.cc/paper/2015/file/84438b7aae55a0638073ef798e50b4ef-Paper.pdf",
  "title": "An Active Learning Framework using Sparse-Graph Codes for Sparse Polynomials and Graph Sketching",
  "year": 2015,
  "venue": "NeurIPS",
  "abstract": "Let f : {−1, 1} → R be an n-variate polynomial consisting of 2 monomials, in which only s 2 coefficients are non-zero. The goal is to learn the polynomial by querying the values of f . We introduce an active learning framework that is associated with a low query cost and computational runtime. The significant savings are enabled by leveraging sampling strategies based on modern coding theory, specifically, the design and analysis of sparse-graph codes, such as Low-Density-Parity-Check (LDPC) codes, which represent the state-of-the-art of modern packet communications. More significantly, we show how this design perspective leads to exciting, and to the best of our knowledge, largely unexplored intellectual connections between learning and coding. The key is to relax the worst-case assumption with an ensemble-average setting, where the polynomial is assumed to be drawn uniformly at random from the ensemble of all polynomials (of a given size n and sparsity s). Our framework succeeds with high probability with respect to the polynomial ensemble with sparsity up to s = O(2) for any δ ∈ (0, 1), where f is exactly learned using O(ns) queries in time O(ns log s), even if the queries are perturbed by Gaussian noise. We further apply the proposed framework to graph sketching, which is the problem of inferring sparse graphs by querying graph cuts. By writing the cut function as a polynomial and exploiting the graph structure, we propose a sketching algorithm to learn the an arbitrary n-node unknown graph using only few cut queries, which scales almost linearly in the number of edges and sub-linearly in the graph size n. Experiments on real datasets show significant reductions in the runtime and query complexity compared with competitive schemes.",
  "stance": 0.4
 },
 {
  "url": "https://papers.neurips.cc/paper/2015/file/8f53295a73878494e9bc8dd6c3c7104f-Paper.pdf",
  "title": "Learning visual biases from human imagination",
  "year": 2015,
  "venue": "NeurIPS",
  "abstract": "Although the human visual system can recognize many concepts under challenging conditions, it still has some biases. In this paper, we investigate whether we can extract these biases and transfer them into a machine recognition system. We introduce a novel method that, inspired by well-known tools in human psychophysics, estimates the biases that the human visual system might use for recognition, but in computer vision feature spaces. Our experiments are surprising, and suggest that classifiers from the human visual system can be transferred into a machine with some success. Since these classifiers seem to capture favorable biases in the human visual system, we further present an SVM formulation that constrains the orientation of the SVM hyperplane to agree with the bias from human visual system. Our results suggest that transferring this human bias into machines may help object recognition systems generalize across datasets and perform better when very little training data is available.",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2015/file/a1afc58c6ca9540d057299ec3016d726-Paper.pdf",
  "title": "Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images",
  "year": 2015,
  "venue": "NeurIPS",
  "abstract": "We introduce Embed to Control (E2C), a method for model learning and control of non-linear dynamical systems from raw pixel images. E2C consists of a deep generative model, belonging to the family of variational autoencoders, that learns to generate image trajectories from a latent space in which the dynamics is constrained to be locally linear. Our model is derived directly from an optimal control formulation in latent space, supports long-term prediction of image sequences and exhibits strong performance on a variety of complex control problems.",
  "stance": 0.6
 },
 {
  "url": "https://papers.neurips.cc/paper/2015/file/aa169b49b583a2b5af89203c2b78c67c-Paper.pdf",
  "title": "Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks",
  "year": 2015,
  "venue": "NeurIPS",
  "abstract": "In this paper we introduce a generative parametric model capable of producing high quality samples of natural images. Our approach uses a cascade of convolutional networks within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion. At each level of the pyramid, a separate generative convnet model is trained using the Generative Adversarial Nets (GAN) approach [11]. Samples drawn from our model are of significantly higher quality than alternate approaches. In a quantitative assessment by human evaluators, our CIFAR10 samples were mistaken for real images around 40% of the time, compared to 10% for samples drawn from a GAN baseline model. We also show samples from models trained on the higher resolution images of the LSUN scene dataset.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2015/file/df6d2338b2b8fce1ec2f6dda0a630eb0-Paper.pdf",
  "title": "Adaptive Stochastic Optimization: From Sets to Paths",
  "year": 2015,
  "venue": "NeurIPS",
  "abstract": "Adaptive stochastic optimization (ASO) optimizes an objective function adaptively under uncertainty. It plays a crucial role in planning and learning under uncertainty, but is, unfortunately, computationally intractable in general. This paper introduces two conditions on the objective function, the marginal likelihood rate bound and the marginal likelihood bound, which, together with pointwise submodularity, enable efficient approximate solution of ASO. Several interesting classes of functions satisfy these conditions naturally, e.g., the version space reduction function for hypothesis learning. We describe Recursive Adaptive Coverage, a new ASO algorithm that exploits these conditions, and apply the algorithm to two robot planning tasks under uncertainty. In contrast to the earlier submodular optimization approach, our algorithm applies to ASO over both sets and paths.",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2015/file/dfa92d8f817e5b08fcaafb50d03763cf-Paper.pdf",
  "title": "Adversarial Prediction Games for Multivariate Losses",
  "year": 2015,
  "venue": "NeurIPS",
  "abstract": "Multivariate loss functions are used to assess performance in many modern prediction tasks, including information retrieval and ranking applications. Convex approximations are typically optimized in their place to avoid NP-hard empirical risk minimization problems. We propose to approximate the training data instead of the loss function by posing multivariate prediction as an adversarial game between a loss-minimizing prediction player and a loss-maximizing evaluation player constrained to match specified properties of training data. This avoids the non-convexity of empirical risk minimization, but game sizes are exponential in the number of predicted variables. We overcome this intractability using the double oracle constraint generation method. We demonstrate the efficiency and predictive performance of our approach on tasks evaluated using the precision at k, the F-score and the discounted cumulative gain.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2015/file/ec8956637a99787bd197eacd77acce5e-Paper.pdf",
  "title": "Where are they looking?",
  "year": 2015,
  "venue": "NeurIPS",
  "abstract": "Humans have the remarkable ability to follow the gaze of other people to identify what they are looking at. Following eye gaze, or gaze-following, is an important ability that allows us to understand what other people are thinking, the actions they are performing, and even predict what they might do next. Despite the importance of this topic, this problem has only been studied in limited scenarios within the computer vision community. In this paper, we propose a deep neural networkbased approach for gaze-following and a new benchmark dataset, GazeFollow, for thorough evaluation. Given an image and the location of a head, our approach follows the gaze of the person and identifies the object being looked at. Our deep network is able to discover how to extract head pose and gaze orientation, and to select objects in the scene that are in the predicted line of sight and likely to be looked at (such as televisions, balls and food). The quantitative evaluation shows that our approach produces reliable results, even when viewing only the back of the head. While our method outperforms several baseline approaches, we are still far from reaching human performance on this task. Overall, we believe that gazefollowing is a challenging and important problem that deserves more attention from the community.",
  "stance": 0.1
 },
 {
  "url": "https://papers.neurips.cc/paper/2015/file/f770b62bc8f42a0b66751fe636fc6eb0-Paper.pdf",
  "title": "Monotone k-Submodular Function Maximization with Size Constraints",
  "year": 2015,
  "venue": "NeurIPS",
  "abstract": "A k-submodular function is a generalization of a submodular function, where the input consists of k disjoint subsets, instead of a single subset, of the domain. Many machine learning problems, including influence maximization with k kinds of topics and sensor placement with k kinds of sensors, can be naturally modeled as the problem of maximizing monotone k-submodular functions. In this paper, we give constant-factor approximation algorithms for maximizing monotone ksubmodular functions subject to several size constraints. The running time of our algorithms are almost linear in the domain size. We experimentally demonstrate that our algorithms outperform baseline algorithms in terms of the solution quality.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/01eee509ee2f68dc6014898c309e86bf-Paper.pdf",
  "title": "Tagger: Deep Unsupervised Perceptual Grouping",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "We present a framework for efficient perceptual inference that explicitly reasons about the segmentation of its inputs and features. Rather than being trained for any specific segmentation, our framework learns the grouping process in an unsupervised manner or alongside any supervised task. We enable a neural network to group the representations of different objects in an iterative manner through a differentiable mechanism. We achieve very fast convergence by allowing the system to amortize the joint iterative inference of the groupings and their representations. In contrast to many other recently proposed methods for addressing multi-object scenes, our system does not assume the inputs to be images and can therefore directly handle other modalities. We evaluate our method on multi-digit classification of very cluttered images that require texture segmentation. Remarkably our method achieves improved classification performance over convolutional networks despite being fully connected, by making use of the grouping mechanism. Furthermore, we observe that our system greatly improves upon the semi-supervised result of a baseline Ladder network on our dataset. These results are evidence that grouping is a powerful tool that can help to improve sample efficiency.",
  "stance": 0.7
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/07cdfd23373b17c6b337251c22b7ea57-Paper.pdf",
  "title": "Budgeted stream-based active learning via adaptive submodular maximization",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "Active learning enables us to reduce the annotation cost by adaptively selecting unlabeled instances to be labeled. For pool-based active learning, several effective methods with theoretical guarantees have been developed through maximizing some utility function satisfying adaptive submodularity. In contrast, there have been few methods for stream-based active learning based on adaptive submodularity. In this paper, we propose a new class of utility functions, policy-adaptive submodular functions, which includes many existing adaptive submodular functions appearing in real world problems. We provide a general framework based on policy-adaptive submodularity that makes it possible to convert existing poolbased methods to stream-based methods and give theoretical guarantees on their performance. In addition we empirically demonstrate their effectiveness by comparing with existing heuristics on common benchmark datasets.",
  "stance": 0.7
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/0d73a25092e5c1c9769a9f3255caa65a-Paper.pdf",
  "title": "Maximal Sparsity with Deep Networks?",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "The iterations of many sparse estimation algorithms are comprised of a fixed linear filter cascaded with a thresholding nonlinearity, which collectively resemble a typical neural network layer. Consequently, a lengthy sequence of algorithm iterations can be viewed as a deep network with shared, hand-crafted layer weights. It is therefore quite natural to examine the degree to which a learned network model might act as a viable surrogate for traditional sparse estimation in domains where ample training data is available. While the possibility of a reduced computational budget is readily apparent when a ceiling is imposed on the number of layers, our work primarily focuses on estimation accuracy. In particular, it is well-known that when a signal dictionary has coherent columns, as quantified by a large RIP constant, then most tractable iterative algorithms are unable to find maximally sparse representations. In contrast, we demonstrate both theoretically and empirically the potential for a trained deep network to recover minimal `0-norm representations in regimes where existing methods fail. The resulting system, which can effectively learn novel iterative sparse estimation algorithms, is deployed on a practical photometric stereo estimation problem, where the goal is to remove sparse outliers that can disrupt the estimation of surface normals from a 3D scene.",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/0ed9422357395a0d4879191c66f4faa2-Paper.pdf",
  "title": "Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "In this paper, we propose a very deep fully convolutional encoding-decoding framework for image restoration such as denoising and super-resolution. The network is composed of multiple layers of convolution and deconvolution operators, learning end-to-end mappings from corrupted images to the original ones. The convolutional layers act as the feature extractor, which capture the abstraction of image contents while eliminating noises/corruptions. Deconvolutional layers are then used to recover the image details. We propose to symmetrically link convolutional and deconvolutional layers with skip-layer connections, with which the training converges much faster and attains a higher-quality local optimum. First, the skip connections allow the signal to be back-propagated to bottom layers directly, and thus tackles the problem of gradient vanishing, making training deep networks easier and achieving restoration performance gains consequently. Second, these skip connections pass image details from convolutional layers to deconvolutional layers, which is beneficial in recovering the original image. Significantly, with the large capacity, we can handle different levels of noises using a single model. Experimental results show that our network achieves better performance than recent state-of-the-art methods.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/140f6969d5213fd0ece03148e62e461e-Paper.pdf",
  "title": "Learning from Small Sample Sets by Combining Unsupervised Meta-Training with CNNs",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "This work explores CNNs for the recognition of novel categories from few examples. Inspired by the transferability properties of CNNs, we introduce an additional unsupervised meta-training stage that exposes multiple top layer units to a large amount of unlabeled real-world images. By encouraging these units to learn diverse sets of low-density separators across the unlabeled data, we capture a more generic, richer description of the visual world, which decouples these units from ties to a specific set of categories. We propose an unsupervised margin maximization that jointly estimates compact high-density regions and infers low-density separators. The low-density separator (LDS) modules can be plugged into any or all of the top layers of a standard CNN architecture. The resulting CNNs significantly improve the performance in scene classification, fine-grained recognition, and action recognition with small training samples.",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/14d9e8007c9b41f57891c48e07c23f57-Paper.pdf",
  "title": "Learning under uncertainty: a comparison between R-W and Bayesian approach",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "Accurately differentiating between what are truly unpredictably random and systematic changes that occur at random can have profound effect on affect and cognition. To examine the underlying computational principles that guide different learning behavior in an uncertain environment, we compared an R-W model and a Bayesian approach in a visual search task with different volatility levels. Both R-W model and the Bayesian approach reflected an individual’s estimation of the environmental volatility, and there is a strong correlation between the learning rate in R-W model and the belief of stationarity in the Bayesian approach in different volatility conditions. In a low volatility condition, R-W model indicates that learning rate positively correlates with lose-shift rate, but not choice optimality (inverted U shape). The Bayesian approach indicates that the belief of environmental stationarity positively correlates with choice optimality, but not lose-shift rate (inverted U shape). In addition, we showed that comparing to Expert learners, individuals with high lose-shift rate (sub-optimal learners) had significantly higher learning rate estimated from R-W model and lower belief of stationarity from the Bayesian model.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/1abb1e1ea5f481b589da52303b091cbb-Paper.pdf",
  "title": "A Probabilistic Model of Social Decision Making based on Reward Maximization",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "A fundamental problem in cognitive neuroscience is how humans make decisions, act, and behave in relation to other humans. Here we adopt the hypothesis that when we are in an interactive social setting, our brains perform Bayesian inference of the intentions and cooperativeness of others using probabilistic representations. We employ the framework of partially observable Markov decision processes (POMDPs) to model human decision making in a social context, focusing specifically on the volunteer’s dilemma in a version of the classic Public Goods Game. We show that the POMDP model explains both the behavior of subjects as well as neural activity recorded using fMRI during the game. The decisions of subjects can be modeled across all trials using two interpretable parameters. Furthermore, the expected reward predicted by the model for each subject was correlated with the activation of brain areas related to reward expectation in social interactions. Our results suggest a probabilistic basis for human social decision making within the framework of expected reward maximization.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/43351f7bf9a215be70c2c2caa7555002-Paper.pdf",
  "title": "Dual Space Gradient Descent for Online Learning",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "One crucial goal in kernel online learning is to bound the model size. Common approaches employ budget maintenance procedures to restrict the model sizes using removal, projection, or merging strategies. Although projection and merging, in the literature, are known to be the most effective strategies, they demand extensive computation whilst removal strategy fails to retain information of the removed vectors. An alternative way to address the model size problem is to apply random features to approximate the kernel function. This allows the model to be maintained directly in the random feature space, hence effectively resolve the curse of kernelization. However, this approach still suffers from a serious shortcoming as it needs to use a high dimensional random feature space to achieve a sufficiently accurate kernel approximation. Consequently, it leads to a significant increase in the computational cost. To address all of these aforementioned challenges, we present in this paper the Dual Space Gradient Descent (DualSGD), a novel framework that utilizes random features as an auxiliary space to maintain information from data points removed during budget maintenance. Consequently, our approach permits the budget to be maintained in a simple, direct and elegant way while simultaneously mitigating the impact of the dimensionality issue on learning performance. We further provide convergence analysis and extensively conduct experiments on five real-world datasets to demonstrate the predictive performance and scalability of our proposed method in comparison with the state-of-the-art baselines.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/44f683a84163b3523afe57c2e008bc8c-Paper.pdf",
  "title": "Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "We study the problem of 3D object generation. We propose a novel framework, namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets. The benefits of our model are three-fold: first, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3D objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of 3D objects, so that we can sample objects without a reference image or CAD models, and explore the 3D object manifold; third, the adversarial discriminator provides a powerful 3D shape descriptor which, learned without supervision, has wide applications in 3D object recognition. Experiments demonstrate that our method generates high-quality 3D objects, and our unsupervisedly learned features achieve impressive performance on 3D object recognition, comparable with those of supervised learning methods.",
  "stance": 0.6
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/49d4b2faeb4b7b9e745775793141e2b2-Paper.pdf",
  "title": "An Architecture for Deep, Hierarchical Generative Models",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "We present an architecture which lets us train deep, directed generative models with many layers of latent variables. We include deterministic paths between all latent variables and the generated output, and provide a richer set of connections between computations for inference and generation, which enables more effective communication of information throughout the model during training. To improve performance on natural images, we incorporate a lightweight autoregressive model in the reconstruction distribution. These techniques permit end-to-end training of models with 10+ layers of latent variables. Experiments show that our approach achieves state-of-the-art performance on standard image modelling benchmarks, can expose latent class structure in the absence of label information, and can provide convincing imputations of occluded regions in natural images.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/5d6646aad9bcc0be55b2c82f69750387-Paper.pdf",
  "title": "Stochastic Three-Composite Convex Minimization",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "We propose a stochastic optimization method for the minimization of the sum of three convex functions, one of which has Lipschitz continuous gradient as well as restricted strong convexity. Our approach is most suitable in the setting where it is computationally advantageous to process smooth term in the decomposition with its stochastic gradient estimate and the other two functions separately with their proximal operators, such as doubly regularized empirical risk minimization problems. We prove the convergence characterization of the proposed algorithm in expectation under the standard assumptions for the stochastic gradient estimate of the smooth term. Our method operates in the primal space and can be considered as a stochastic extension of the three-operator splitting method. Numerical evidence supports the effectiveness of our method in real-world problems.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/632cee946db83e7a52ce5e8d6f0fed35-Paper.pdf",
  "title": "Large Margin Discriminant Dimensionality Reduction in Prediction Space",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "In this paper we establish a duality between boosting and SVM, and use this to derive a novel discriminant dimensionality reduction algorithm. In particular, using the multiclass formulation of boosting and SVM we note that both use a combination of mapping and linear classification to maximize the multiclass margin. In SVM this is implemented using a pre-defined mapping (induced by the kernel) and optimizing the linear classifiers. In boosting the linear classifiers are pre-defined and the mapping (predictor) is learned through a combination of weak learners. We argue that the intermediate mapping, i.e. boosting predictor, is preserving the discriminant aspects of the data and that by controlling the dimension of this mapping it is possible to obtain discriminant low dimensional representations for the data. We use the aforementioned duality and propose a new method, Large Margin Discriminant Dimensionality Reduction (LADDER) that jointly learns the mapping and the linear classifiers in an efficient manner. This leads to a data-driven mapping which can embed data into any number of dimensions. Experimental results show that this embedding can significantly improve performance on tasks such as hashing and image/scene classification.",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/65ded5353c5ee48d0b7d48c591b8f430-Paper.pdf",
  "title": "SURGE: Surface Regularized Geometry Estimation from a Single Image",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "This paper introduces an approach to regularize 2.5D surface normal and depth predictions at each pixel given a single input image. The approach infers and reasons about the underlying 3D planar surfaces depicted in the image to snap predicted normals and depths to inferred planar surfaces, all while maintaining fine detail within objects. Our approach comprises two components: (i) a fourstream convolutional neural network (CNN) where depths, surface normals, and likelihoods of planar region and planar boundary are predicted at each pixel, followed by (ii) a dense conditional random field (DCRF) that integrates the four predictions such that the normals and depths are compatible with each other and regularized by the planar region and planar boundary information. The DCRF is formulated such that gradients can be passed to the surface normal and depth CNNs via backpropagation. In addition, we propose new planar-wise metrics to evaluate geometry consistency within planar surfaces, which are more tightly related to dependent 3D editing applications. We show that our regularization yields a 30% relative improvement in planar consistency on the NYU v2 dataset [24].",
  "stance": 0.7
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/6e7d2da6d3953058db75714ac400b584-Paper.pdf",
  "title": "Learning the Number of Neurons in Deep Networks",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "Nowadays, the number of layers and of neurons in each layer of a deep network are typically set manually. While very deep and wide networks have proven effective in general, they come at a high memory and computation cost, thus making them impractical for constrained platforms. These networks, however, are known to have many redundant parameters, and could thus, in principle, be replaced by more compact architectures. In this paper, we introduce an approach to automatically determining the number of neurons in each layer of a deep network during learning. To this end, we propose to make use of a group sparsity regularizer on the parameters of the network, where each group is defined to act on a single neuron. Starting from an overcomplete network, we show that our approach can reduce the number of parameters by up to 80% while retaining or even improving the network accuracy.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/bb7946e7d85c81a9e69fee1cea4a087c-Paper.pdf",
  "title": "Error Analysis of Generalized Nyström Kernel Regression",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "Nyström method has been successfully used to improve the computational efficiency of kernel ridge regression (KRR). Recently, theoretical analysis of Nyström KRR, including generalization bound and convergence rate, has been established based on reproducing kernel Hilbert space (RKHS) associated with the symmetric positive semi-definite kernel. However, in real world applications, RKHS is not always optimal and kernel function is not necessary to be symmetric or positive semi-definite. In this paper, we consider the generalized Nyström kernel regression (GNKR) with `2 coefficient regularization, where the kernel just requires the continuity and boundedness. Error analysis is provided to characterize its generalization performance and the column norm sampling strategy is introduced to construct the refined hypothesis space. In particular, the fast learning rate with polynomial decay is reached for the GNKR. Experimental analysis demonstrates the satisfactory performance of GNKR with the column norm sampling.",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/c4492cbe90fbdbf88a5aec486aa81ed5-Paper.pdf",
  "title": "Strategic Attentive Writer for Learning Macro-Actions",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "We present a novel deep recurrent neural network architecture that learns to build implicit plans in an end-to-end manner purely by interacting with an environment in reinforcement learning setting. The network builds an internal plan, which is continuously updated upon observation of the next input from the environment. It can also partition this internal representation into contiguous sub-sequences by learning for how long the plan can be committed to – i.e. followed without replaning. Combining these properties, the proposed model, dubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally abstracted macro-actions of varying lengths that are solely learnt from data without any prior information. These macro-actions enable both structured exploration and economic computation. We experimentally demonstrate that STRAW delivers strong improvements on several ATARI games by employing temporally extended planning strategies (e.g. Ms. Pacman and Frostbite). It is at the same time a general algorithm that can be applied on any sequence data. To that end, we also show that when trained on text prediction task, STRAW naturally predicts frequent n-grams (instead of macro-actions), demonstrating the generality of the approach.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/dc4c44f624d600aa568390f1f1104aa0-Paper.pdf",
  "title": "Satisfying Real-world Goals with Dataset Constraints",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "The goal of minimizing misclassification error on a training set is often just one of several real-world goals that might be defined on different datasets. For example, one may require a classifier to also make positive predictions at some specified rate for some subpopulation (fairness), or to achieve a specified empirical recall. Other real-world goals include reducing churn with respect to a previously deployed model, or stabilizing online training. In this paper we propose handling multiple goals on multiple datasets by training with dataset constraints, using the ramp penalty to accurately quantify costs, and present an efficient algorithm to approximately optimize the resulting non-convex constrained optimization problem. Experiments on both benchmark and real-world industry datasets demonstrate the effectiveness of our approach.",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/dc5c768b5dc76a084531934b34601977-Paper.pdf",
  "title": "Launch and Iterate: Reducing Prediction Churn",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "Practical applications of machine learning often involve successive training iterations with changes to features and training examples. Ideally, changes in the output of any new model should only be improvements (wins) over the previous iteration, but in practice the predictions may change neutrally for many examples, resulting in extra net-zero wins and losses, referred to as unnecessary churn. These changes in the predictions are problematic for usability for some applications, and make it harder and more expensive to measure if a change is statistically significant positive. In this paper, we formulate the problem and present a stabilization operator to regularize a classifier towards a previous classifier. We use a Markov chain Monte Carlo stabilization operator to produce a model with more consistent predictions without adversely affecting accuracy. We investigate the properties of the proposal with theoretical analysis. Experiments on benchmark datasets for different classification algorithms demonstrate the method and the resulting reduction in churn.",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/e2a2dcc36a08a345332c751b2f2e476c-Paper.pdf",
  "title": "Learning Treewidth-Bounded Bayesian Networks with Thousands of Variables",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "We present a method for learning treewidth-bounded Bayesian networks from data sets containing thousands of variables. Bounding the treewidth of a Bayesian network greatly reduces the complexity of inferences. Yet, being a global property of the graph, it considerably increases the difficulty of the learning process. Our novel algorithm accomplishes this task, scaling both to large domains and to large treewidths. Our novel approach consistently outperforms the state of the art on experiments with up to thousands of variables.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2016/file/f0adc8838f4bdedde4ec2cfad0515589-Paper.pdf",
  "title": "Adaptive Neural Compilation",
  "year": 2016,
  "venue": "NeurIPS",
  "abstract": "This paper proposes an adaptive neural-compilation framework to address the problem of learning efficient programs. Traditional code optimisation strategies used in compilers are based on applying pre-specified set of transformations that make the code faster to execute without changing its semantics. In contrast, our work involves adapting programs to make them more efficient while considering correctness only on a target input distribution. Our approach is inspired by the recent works on differentiable representations of programs. We show that it is possible to compile programs written in a low-level language to a differentiable representation. We also show how programs in this representation can be optimised to make them efficient on a target input distribution. Experimental results demonstrate that our approach enables learning specifically-tuned algorithms for given data distributions with a high success rate.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/1680e9fa7b4dd5d62ece800239bb53bd-Paper.pdf",
  "title": "Efficient Modeling of Latent Information in Supervised Learning using Gaussian Processes",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "Often in machine learning, data are collected as a combination of multiple conditions, e.g., the voice recordings of multiple persons, each labeled with an ID. How could we build a model that captures the latent information related to these conditions and generalize to a new one with few data? We present a new model called Latent Variable Multiple Output Gaussian Processes (LVMOGP) that allows to jointly model multiple conditions for regression and generalize to a new condition with a few data points at test time. LVMOGP infers the posteriors of Gaussian processes together with a latent space representing the information about different conditions. We derive an efficient variational inference method for LVMOGP for which the computational complexity is as low as sparse Gaussian processes. We show that LVMOGP significantly outperforms related Gaussian process methods on various tasks with both synthetic and real data.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/253614bbac999b38b5b60cae531c4969-Paper.pdf",
  "title": "Collecting Telemetry Data Privately",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "The collection and analysis of telemetry data from user’s devices is routinely performed by many software companies. Telemetry collection leads to improved user experience but poses significant risks to users’ privacy. Locally differentially private (LDP) algorithms have recently emerged as the main tool that allows data collectors to estimate various population statistics, while preserving privacy. The guarantees provided by such algorithms are typically very strong for a single round of telemetry collection, but degrade rapidly when telemetry is collected regularly. In particular, existing LDP algorithms are not suitable for repeated collection of counter data such as daily app usage statistics. In this paper, we develop new LDP mechanisms geared towards repeated collection of counter data, with formal privacy guarantees even after being executed for an arbitrarily long period of time. For two basic analytical tasks, mean estimation and histogram estimation, our LDP mechanisms for repeated data collection provide estimates with comparable or even the same accuracy as existing single-round LDP collection mechanisms. We conduct empirical evaluation on real-world counter datasets to verify our theoretical results. Our mechanisms have been deployed by Microsoft to collect telemetry across millions of devices.",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/2650d6089a6d640c5e85b2b88265dc2b-Paper.pdf",
  "title": "What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model – uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/2cd4e8a2ce081c3d7c32c3cde4312ef7-Paper.pdf",
  "title": "InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "The goal of imitation learning is to mimic expert behavior without access to an explicit reward signal. Expert demonstrations provided by humans, however, often show significant variability due to latent factors that are typically not explicitly modeled. In this paper, we propose a new algorithm that can infer the latent structure of expert demonstrations in an unsupervised way. Our method, built on top of Generative Adversarial Imitation Learning, can not only imitate complex behaviors, but also learn interpretable and meaningful representations of complex behavioral data, including visual demonstrations. In the driving domain, we show that a model learned from human demonstrations is able to both accurately reproduce a variety of behaviors and accurately anticipate human actions using raw visual inputs. Compared with various baselines, our method can better capture the latent structure underlying expert demonstrations, often recovering semantically meaningful factors of variation in the data.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/3fb451ca2e89b3a13095b059d8705b15-Paper.pdf",
  "title": "ELF: An Extensive, Lightweight and Flexible Research Platform for Real-time Strategy Games",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "In this paper, we propose ELF, an Extensive, Lightweight and Flexible platform for fundamental reinforcement learning research. Using ELF, we implement a highly customizable real-time strategy (RTS) engine with three game environments (Mini-RTS, Capture the Flag and Tower Defense). Mini-RTS, as a miniature version of StarCraft, captures key game dynamics and runs at 40K frameper-second (FPS) per core on a laptop. When coupled with modern reinforcement learning methods, the system can train a full-game bot against built-in AIs endto-end in one day with 6 CPUs and 1 GPU. In addition, our platform is flexible in terms of environment-agent communication topologies, choices of RL methods, changes in game parameters, and can host existing C/C++-based game environments like ALE [4]. Using ELF, we thoroughly explore training parameters and show that a network with Leaky ReLU [17] and Batch Normalization [11] coupled with long-horizon training and progressive curriculum beats the rule-based built-in AI more than 70% of the time in the full game of Mini-RTS. Strong performance is also achieved on the other two games. In game replays, we show our agents learn interesting strategies. ELF, along with its RL platform, is open sourced at https://github.com/facebookresearch/ELF.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/643de7cf7ba769c7466ccbc4adfd7fac-Paper.pdf",
  "title": "Acceleration and Averaging in Stochastic Descent Dynamics",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "We formulate and study a general family of (continuous-time) stochastic dynamics for accelerated first-order minimization of smooth convex functions. Building on an averaging formulation of accelerated mirror descent, we propose a stochastic variant in which the gradient is contaminated by noise, and study the resulting stochastic differential equation. We prove a bound on the rate of change of an energy function associated with the problem, then use it to derive estimates of convergence rates of the function values (almost surely and in expectation), both for persistent and asymptotically vanishing noise. We discuss the interaction between the parameters of the dynamics (learning rate and averaging rates) and the covariation of the noise process. In particular, we show how the asymptotic rate of covariation affects the choice of parameters and, ultimately, the convergence rate.",
  "stance": 0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/7cc234202e98d2722580858573fd0817-Paper.pdf",
  "title": "Thy Friend is My Friend: Iterative Collaborative Filtering for Sparse Matrix Estimation",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "The sparse matrix estimation problem consists of estimating the distribution of an n× n matrix Y , from a sparsely observed single instance of this matrix where the entries of Y are independent random variables. This captures a wide array of problems; special instances include matrix completion in the context of recommendation systems, graphon estimation, and community detection in (mixed membership) stochastic block models. Inspired by classical collaborative filtering for recommendation systems, we propose a novel iterative, collaborative filteringstyle algorithm for matrix estimation in this generic setting. We show that the mean squared error (MSE) of our estimator converges to 0 at the rate of O(d2(pn)−2/5) as long as ω(dn) random entries from a total of n entries of Y are observed (uniformly sampled), E[Y ] has rank d, and the entries of Y have bounded support. The maximum squared error across all entries converges to 0 with high probability as long as we observe a little more, Ω(dn ln(n)) entries. Our results are the best known sample complexity results in this generality.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/81b3833e2504647f9d794f7d7b9bf341-Paper.pdf",
  "title": "The Marginal Value of Adaptive Gradient Methods in Machine Learning",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "Adaptive optimization methods, which perform local optimization with a metric constructed from the history of iterates, are becoming increasingly popular for training deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We show that for simple overparameterized problems, adaptive methods often find drastically different solutions than gradient descent (GD) or stochastic gradient descent (SGD). We construct an illustrative binary classification problem where the data is linearly separable, GD and SGD achieve zero test error, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to half. We additionally study the empirical generalization capability of adaptive methods on several stateof-the-art deep learning models. We observe that the solutions found by adaptive methods generalize worse (often significantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.",
  "stance": -1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/8597a6cfa74defcbde3047c891d78f90-Paper.pdf",
  "title": "Learned D-AMP: Principled Neural Network based Compressive Image Recovery",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "Compressive image recovery is a challenging problem that requires fast and accurate algorithms. Recently, neural networks have been applied to this problem with promising results. By exploiting massively parallel GPU processing architectures and oodles of training data, they can run orders of magnitude faster than existing techniques. However, these methods are largely unprincipled black boxes that are difficult to train and often-times specific to a single measurement matrix. It was recently demonstrated that iterative sparse-signal-recovery algorithms can be “unrolled” to form interpretable deep networks. Taking inspiration from this work, we develop a novel neural network architecture that mimics the behavior of the denoising-based approximate message passing (D-AMP) algorithm. We call this new network Learned D-AMP (LDAMP). The LDAMP network is easy to train, can be applied to a variety of different measurement matrices, and comes with a state-evolution heuristic that accurately predicts its performance. Most importantly, it outperforms the state-of-the-art BM3D-AMP and NLR-CS algorithms in terms of both accuracy and run time. At high resolutions, and when used with sensing matrices that have fast implementations, LDAMP runs over 50× faster than BM3D-AMP and hundreds of times faster than NLR-CS.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/8edd72158ccd2a879f79cb2538568fdc-Paper.pdf",
  "title": "Associative Embedding: End-to-End Learning for Joint Detection and Grouping",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "We introduce associative embedding, a novel method for supervising convolutional neural networks for the task of detection and grouping. A number of computer vision problems can be framed in this manner including multi-person pose estimation, instance segmentation, and multi-object tracking. Usually the grouping of detections is achieved with multi-stage pipelines, instead we propose an approach that teaches a network to simultaneously output detections and group assignments. This technique can be easily integrated into any state-of-the-art network architecture that produces pixel-wise predictions. We show how to apply this method to multi-person pose estimation and report state-of-the-art performance on the MPII and MS-COCO datasets.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/934815ad542a4a7c5e8a2dfa04fea9f5-Paper.pdf",
  "title": "Nearest-Neighbor Sample Compression: Efficiency, Consistency, Infinite Dimensions",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "We examine the Bayes-consistency of a recently proposed 1-nearest-neighbor-based multiclass learning algorithm. This algorithm is derived from sample compression bounds and enjoys the statistical advantages of tight, fully empirical generalization bounds, as well as the algorithmic advantages of a faster runtime and memory savings. We prove that this algorithm is strongly Bayes-consistent in metric spaces with finite doubling dimension — the first consistency result for an efficient nearest-neighbor sample compression scheme. Rather surprisingly, we discover that this algorithm continues to be Bayes-consistent even in a certain infinitedimensional setting, in which the basic measure-theoretic conditions on which classic consistency proofs hinge are violated. This is all the more surprising, since it is known that k-NN is not Bayes-consistent in this setting. We pose several challenging open problems for future research.",
  "stance": -0.3
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/a8baa56554f96369ab93e4f3bb068c22-Paper.pdf",
  "title": "Label Efficient Learning of Transferable Representations acrosss Domains and Tasks",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "We propose a framework that learns a representation transferable across different domains and tasks in a label efficient manner. Our approach battles domain shift with a domain adversarial loss, and generalizes the embedding to novel task using a metric learning-based approach. Our model is simultaneously optimized on labeled source data and unlabeled or sparsely labeled data in the target domain. Our method shows compelling results on novel classes within a new domain even when only a few labeled examples per class are available, outperforming the prevalent fine-tuning approach. In addition, we demonstrate the effectiveness of our framework on the transfer learning task from image object recognition to video action recognition.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/bdc4626aa1d1df8e14d80d345b2a442d-Paper.pdf",
  "title": "Conservative Contextual Linear Bandits",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "Safety is a desirable property that can immensely increase the applicability of learning algorithms in real-world decision-making problems. It is much easier for a company to deploy an algorithm that is safe, i.e., guaranteed to perform at least as well as a baseline. In this paper, we study the issue of safety in contextual linear bandits that have application in many different fields including personalized recommendation. We formulate a notion of safety for this class of algorithms. We develop a safe contextual linear bandit algorithm, called conservative linear UCB (CLUCB), that simultaneously minimizes its regret and satisfies the safety constraint, i.e., maintains its performance above a fixed percentage of the performance of a baseline strategy, uniformly over time. We prove an upper-bound on the regret of CLUCB and show that it can be decomposed into two terms: 1) an upper-bound for the regret of the standard linear UCB algorithm that grows with the time horizon and 2) a constant term that accounts for the loss of being conservative in order to satisfy the safety constraint. We empirically show that our algorithm is safe and validate our theoretical analysis.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/bf424cb7b0dea050a42b9739eb261a3a-Paper.pdf",
  "title": "Diving into the shallows: a computational perspective on large-scale shallow learning",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "Remarkable recent success of deep neural networks has not been easy to analyze theoretically. It has been particularly hard to disentangle relative significance of architecture and optimization in achieving accurate classification on large datasets. On the flip side, shallow methods (such as kernel methods) have encountered obstacles in scaling to large data, despite excellent performance on smaller datasets, and extensive theoretical analysis. Practical methods, such as variants of gradient descent used so successfully in deep learning, seem to perform below par when applied to kernel methods. This difficulty has sometimes been attributed to the limitations of shallow architecture. In this paper we identify a basic limitation in gradient descent-based optimization methods when used in conjunctions with smooth kernels. Our analysis demonstrates that only a vanishingly small fraction of the function space is reachable after a polynomial number of gradient descent iterations. That drastically limits the approximating power of gradient descent leading to over-regularization. The issue is purely algorithmic, persisting even in the limit of infinite data. To address this shortcoming in practice, we introduce EigenPro iteration, a simple and direct preconditioning scheme using a small number of approximately computed eigenvectors. It can also be viewed as learning a kernel optimized for gradient descent. Injecting this small, computationally inexpensive and SGD-compatible, amount of approximate second-order information leads to major improvements in convergence. For large data, this leads to a significant performance boost over the state-of-the-art kernel methods. In particular, we are able to match or improve the results reported in the literature at a small fraction of their computational budget. For complete version of this paper see https://arxiv.org/abs/1703.10622.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/c22abfa379f38b5b0411bc11fa9bf92f-Paper.pdf",
  "title": "Learning Affinity via Spatial Propagation Networks",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "In this paper, we propose spatial propagation networks for learning the affinity matrix for vision tasks. We show that by constructing a row/column linear propagation model, the spatially varying transformation matrix exactly constitutes an affinity matrix that models dense, global pairwise relationships of an image. Specifically, we develop a three-way connection for the linear propagation model, which (a) formulates a sparse transformation matrix, where all elements can be outputs from a deep CNN, but (b) results in a dense affinity matrix that effectively models any task-specific pairwise similarity matrix. Instead of designing the similarity kernels according to image features of two points, we can directly output all the similarities in a purely data-driven manner. The spatial propagation network is a generic framework that can be applied to many affinity-related tasks, such as image matting, segmentation and colorization, to name a few. Essentially, the model can learn semantically-aware affinity values for high-level vision tasks due to the powerful learning capability of deep CNNs. We validate the framework on the task of refinement of image segmentation boundaries. Experiments on the HELEN face parsing and PASCAL VOC-2012 semantic segmentation tasks show that the spatial propagation network provides a general, effective and efficient solution for generating high-quality segmentation results.",
  "stance": 0.9
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/c59b469d724f7919b7d35514184fdc0f-Paper.pdf",
  "title": "Deep Voice 2: Multi-Speaker Neural Text-to-Speech",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "We introduce a technique for augmenting neural text-to-speech (TTS) with lowdimensional trainable speaker embeddings to generate different voices from a single model. As a starting point, we show improvements over the two state-ofthe-art approaches for single-speaker neural TTS: Deep Voice 1 and Tacotron. We introduce Deep Voice 2, which is based on a similar pipeline with Deep Voice 1, but constructed with higher performance building blocks and demonstrates a significant audio quality improvement over Deep Voice 1. We improve Tacotron by introducing a post-processing neural vocoder, and demonstrate a significant audio quality improvement. We then demonstrate our technique for multi-speaker speech synthesis for both Deep Voice 2 and Tacotron on two multi-speaker TTS datasets. We show that a single neural TTS system can learn hundreds of unique voices from less than half an hour of data per speaker, while achieving high audio quality synthesis and preserving the speaker identities almost perfectly.",
  "stance": 0.9
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/c6036a69be21cb660499b75718a3ef24-Paper.pdf",
  "title": "Deliberation Networks: Sequence Generation Beyond One-Pass Decoding",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "The encoder-decoder framework has achieved promising progress for many sequence generation tasks, including machine translation, text summarization, dialog system, image captioning, etc. Such a framework adopts an one-pass forward process while decoding and generating a sequence, but lacks the deliberation process: A generated sequence is directly used as final output without further polishing. However, deliberation is a common behavior in human’s daily life like reading news and writing papers/articles/books. In this work, we introduce the deliberation process into the encoder-decoder framework and propose deliberation networks for sequence generation. A deliberation network has two levels of decoders, where the first-pass decoder generates a raw sequence and the second-pass decoder polishes and refines the raw sentence with deliberation. Since the second-pass deliberation decoder has global information about what the sequence to be generated might be, it has the potential to generate a better sequence by looking into future words in the raw sentence. Experiments on neural machine translation and text summarization demonstrate the effectiveness of the proposed deliberation networks. On the WMT 2014 English-to-French translation task, our model establishes a new state-of-the-art BLEU score of 41.5.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/c7558e9d1f956b016d1fdba7ea132378-Paper.pdf",
  "title": "Extracting low-dimensional dynamics from multiple large-scale neural population recordings by learning to predict correlations",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "A powerful approach for understanding neural population dynamics is to extract low-dimensional trajectories from population recordings using dimensionality reduction methods. Current approaches for dimensionality reduction on neural data are limited to single population recordings, and can not identify dynamics embedded across multiple measurements. We propose an approach for extracting low-dimensional dynamics from multiple, sequential recordings. Our algorithm scales to data comprising millions of observed dimensions, making it possible to access dynamics distributed across large populations or multiple brain areas. Building on subspace-identification approaches for dynamical systems, we perform parameter estimation by minimizing a moment-matching objective using a scalable stochastic gradient descent algorithm: The model is optimized to predict temporal covariations across neurons and across time. We show how this approach naturally handles missing data and multiple partial recordings, and can identify dynamics and predict correlations even in the presence of severe subsampling and small overlap between recordings. We demonstrate the effectiveness of the approach both on simulated data and a whole-brain larval zebrafish imaging dataset.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/dab49080d80c724aad5ebf158d63df41-Paper.pdf",
  "title": "Structured Bayesian Pruning via Log-Normal Multiplicative Noise",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "Dropout-based regularization methods can be regarded as injecting random noise with pre-defined magnitude to different parts of the neural network during training. It was recently shown that Bayesian dropout procedure not only improves generalization but also leads to extremely sparse neural architectures by automatically setting the individual noise magnitude per weight. However, this sparsity can hardly be used for acceleration since it is unstructured. In the paper, we propose a new Bayesian model that takes into account the computational structure of neural networks and provides structured sparsity, e.g. removes neurons and/or convolutional channels in CNNs. To do this we inject noise to the neurons outputs while keeping the weights unregularized. We establish the probabilistic model with a proper truncated log-uniform prior over the noise and truncated log-normal variational approximation that ensures that the KL-term in the evidence lower bound is computed in closed-form. The model leads to structured sparsity by removing elements with a low SNR from the computation graph and provides significant acceleration on a number of deep neural architectures. The model is easy to implement as it can be formulated as a separate dropout-like layer.",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/efdf562ce2fb0ad460fd8e9d33e57f57-Paper.pdf",
  "title": "Reconstructing perceived faces from brain activations with deep adversarial neural decoding",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "Here, we present a novel approach to solve the problem of reconstructing perceived stimuli from brain responses by combining probabilistic inference with deep learning. Our approach first inverts the linear transformation from latent features to brain responses with maximum a posteriori estimation and then inverts the nonlinear transformation from perceived stimuli to latent features with adversarial training of convolutional neural networks. We test our approach with a functional magnetic resonance imaging experiment and show that it can generate state-of-the-art reconstructions of perceived faces from brain activations. ConvNet (pretrained) + PCA ConvNet (adversarial training) la te nt fe at . prior (Gaussian) maximum a posteriori likelihood (Gaussian) posterior (Gaussian) pe rc ei ve d st im . br ai n re sp . *reconstruction *from brain resp. Figure 1: An illustration of our approach to solve the problem of reconstructing perceived stimuli from brain responses by combining probabilistic inference with deep learning.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/f22e4747da1aa27e363d86d40ff442fe-Paper.pdf",
  "title": "Deep Sets",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "We study the problem of designing models for machine learning tasks defined on sets. In contrast to traditional approach of operating on fixed dimensional vectors, we consider objective functions defined on sets that are invariant to permutations. Such problems are widespread, ranging from estimation of population statistics [1], to anomaly detection in piezometer data of embankment dams [2], to cosmology [3, 4]. Our main theorem characterizes the permutation invariant functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We also derive the necessary and sufficient conditions for permutation equivariance in deep models. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2017/file/f708f064faaf32a43e4d3c784e6af9ea-Paper.pdf",
  "title": "Overcoming Catastrophic Forgetting by Incremental Moment Matching",
  "year": 2017,
  "venue": "NeurIPS",
  "abstract": "Catastrophic forgetting is a problem of neural networks that loses the information of the first task after training the second task. Here, we propose a method, i.e. incremental moment matching (IMM), to resolve this problem. IMM incrementally matches the moment of the posterior distribution of the neural network which is trained on the first and the second task, respectively. To make the search space of posterior parameter smooth, the IMM procedure is complemented by various transfer learning techniques including weight transfer, L2-norm of the old and the new parameter, and a variant of dropout with the old parameter. We analyze our approach on a variety of datasets including the MNIST, CIFAR-10, Caltech-UCSDBirds, and Lifelog datasets. The experimental results show that IMM achieves state-of-the-art performance by balancing the information between an old and a new network.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2018/file/01161aaa0b6d1345dd8fe4e481144d84-Paper.pdf",
  "title": "Self-Supervised Generation of Spatial Audio for 360° Video",
  "year": 2018,
  "venue": "NeurIPS",
  "abstract": "We introduce an approach to convert mono audio recorded by a 360◦ video camera into spatial audio, a representation of the distribution of sound over the full viewing sphere. Spatial audio is an important component of immersive 360◦ video viewing, but spatial audio microphones are still rare in current 360◦ video production. Our system consists of end-to-end trainable neural networks that separate individual sound sources and localize them on the viewing sphere, conditioned on multi-modal analysis of audio and 360◦ video frames. We introduce several datasets, including one filmed ourselves, and one collected in-the-wild from YouTube, consisting of 360◦ videos uploaded with spatial audio. During training, ground-truth spatial audio serves as self-supervision and a mixed down mono track forms the input to our network. Using our approach, we show that it is possible to infer the spatial location of sound sources based only on 360◦ video and a mono audio track.",
  "stance": 0.7
 },
 {
  "url": "https://papers.neurips.cc/paper/2018/file/09060616068d2b9544dc33f2fbe4ce2d-Paper.pdf",
  "title": "Unsupervised Learning of Artistic Styles with Archetypal Style Analysis",
  "year": 2018,
  "venue": "NeurIPS",
  "abstract": "In this paper, we introduce an unsupervised learning approach to automatically discover, summarize, and manipulate artistic styles from large collections of paintings. Our method is based on archetypal analysis, which is an unsupervised learning technique akin to sparse coding with a geometric interpretation. When applied to neural style representations from a collection of artworks, it learns a dictionary of archetypal styles, which can be easily visualized. After training the model, the style of a new image, which is characterized by local statistics of deep visual features, is approximated by a sparse convex combination of archetypes. This enables us to interpret which archetypal styles are present in the input image, and in which proportion. Finally, our approach allows us to manipulate the coefficients of the latent archetypal decomposition, and achieve various special effects such as style enhancement, transfer, and interpolation between multiple archetypes.",
  "stance": 0.6
 },
 {
  "url": "https://papers.neurips.cc/paper/2018/file/12b1e42dc0746f22cf361267de07073f-Paper.pdf",
  "title": "Semi-Supervised Learning with Declaratively Specified Entropy Constraints",
  "year": 2018,
  "venue": "NeurIPS",
  "abstract": "We propose a technique for declaratively specifying strategies for semi-supervised learning (SSL). SSL methods based on different assumptions perform differently on different tasks, which leads to difficulties applying them in practice. In this paper, we propose to use entropy to unify many types of constraints. Our method can be used to easily specify ensembles of semi-supervised learners, as well as agreement constraints and entropic regularization constraints between these learners, and can be used to model both well-known heuristics such as co-training, and novel domain-specific heuristics. Besides, our model is flexible as to the underlying learning mechanism. Compared to prior frameworks for specifying SSL techniques, our technique achieves consistent improvements on a suite of well-studied SSL benchmarks, and obtains a new state-of-the-art result on a difficult relation extraction task.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2018/file/1f36c15d6a3d18d52e8d493bc8187cb9-Paper.pdf",
  "title": "Unsupervised Learning of Object Landmarks through Conditional Image Generation",
  "year": 2018,
  "venue": "NeurIPS",
  "abstract": "We propose a method for learning landmark detectors for visual objects (such as the eyes and the nose in a face) without any manual supervision. We cast this as the problem of generating images that combine the appearance of the object as seen in a first example image with the geometry of the object as seen in a second example image, where the two examples differ by a viewpoint change and/or an object deformation. In order to factorize appearance and geometry, we introduce a tight bottleneck in the geometry-extraction process that selects and distils geometryrelated features. Compared to standard image generation problems, which often use generative adversarial networks, our generation task is conditioned on both appearance and geometry and thus is significantly less ambiguous, to the point that adopting a simple perceptual loss formulation is sufficient. We demonstrate that our approach can learn object landmarks from synthetic image deformations or videos, all without manual supervision, while outperforming state-of-the-art unsupervised landmark detectors. We further show that our method is applicable to a large variety of datasets — faces, people, 3D objects, and digits — without any modifications.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2018/file/201e5bacd665709851b77148e225b332-Paper.pdf",
  "title": "Simple, Distributed, and Accelerated Probabilistic Programming",
  "year": 2018,
  "venue": "NeurIPS",
  "abstract": "We describe a simple, low-level approach for embedding probabilistic programming in a deep learning ecosystem. In particular, we distill probabilistic programming down to a single abstraction—the random variable. Our lightweight implementation in TensorFlow enables numerous applications: a model-parallel variational auto-encoder (VAE) with 2nd-generation tensor processing units (TPUv2s); a data-parallel autoregressive model (Image Transformer) with TPUv2s; and multiGPU No-U-Turn Sampler (NUTS). For both a state-of-the-art VAE on 64x64 ImageNet and Image Transformer on 256x256 CelebA-HQ, our approach achieves an optimal linear speedup from 1 to 256 TPUv2 chips. With NUTS, we see a 100x speedup on GPUs over Stan and 37x over PyMC3.1",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2018/file/204da255aea2cd4a75ace6018fad6b4d-Paper.pdf",
  "title": "When do random forests fail?",
  "year": 2018,
  "venue": "NeurIPS",
  "abstract": "Random forests are learning algorithms that build large collections of random trees and make predictions by averaging the individual tree predictions. In this paper, we consider various tree constructions and examine how the choice of parameters affects the generalization error of the resulting random forests as the sample size goes to infinity. We show that subsampling of data points during the tree construction phase is important: Forests can become inconsistent with either no subsampling or too severe subsampling. As a consequence, even highly randomized trees can lead to inconsistent forests if no subsampling is used, which implies that some of the commonly used setups for random forests can be inconsistent. As a second consequence we can show that trees that have good performance in nearest-neighbor search can be a poor choice for random forests.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2018/file/22fb0cee7e1f3bde58293de743871417-Paper.pdf",
  "title": "Distilled Wasserstein Learning for Word Embedding and Topic Modeling",
  "year": 2018,
  "venue": "NeurIPS",
  "abstract": "We propose a novel Wasserstein method with a distillation mechanism, yielding joint learning of word embeddings and topics. The proposed method is based on the fact that the Euclidean distance between word embeddings may be employed as the underlying distance in the Wasserstein topic model. The word distributions of topics, their optimal transports to the word distributions of documents, and the embeddings of words are learned in a unified framework. When learning the topic model, we leverage a distilled underlying distance matrix to update the topic distributions and smoothly calculate the corresponding optimal transports. Such a strategy provides the updating of word embeddings with robust guidance, improving the algorithmic convergence. As an application, we focus on patient admission records, in which the proposed method embeds the codes of diseases and procedures and learns the topics of admissions, obtaining superior performance on clinically-meaningful disease network construction, mortality prediction as a function of admission codes, and procedure recommendation.",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2018/file/25db67c5657914454081c6a18e93d6dd-Paper.pdf",
  "title": "Importance Weighting and Variational Inference",
  "year": 2018,
  "venue": "NeurIPS",
  "abstract": "Recent work used importance sampling ideas for better variational bounds on likelihoods. We clarify the applicability of these ideas to pure probabilistic inference, by showing the resulting Importance Weighted Variational Inference (IWVI) technique is an instance of augmented variational inference, thus identifying the looseness in previous work. Experiments confirm IWVI’s practicality for probabilistic inference. As a second contribution, we investigate inference with elliptical distributions, which improves accuracy in low dimensions, and convergence in high dimensions.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2018/file/27e8e17134dd7083b050476733207ea1-Paper.pdf",
  "title": "GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration",
  "year": 2018,
  "venue": "NeurIPS",
  "abstract": "Despite advances in scalable models, the inference tools used for Gaussian processes (GPs) have yet to fully capitalize on developments in computing hardware. We present an efficient and general approach to GP inference based on Blackbox Matrix-Matrix multiplication (BBMM). BBMM inference uses a modified batched version of the conjugate gradients algorithm to derive all terms for training and inference in a single call. BBMM reduces the asymptotic complexity of exact GP inference from O(n3) to O(n2). Adapting this algorithm to scalable approximations and complex GP models simply requires a routine for efficient matrix-matrix multiplication with the kernel and its derivative. In addition, BBMM uses a specialized preconditioner to substantially speed up convergence. In experiments we show that BBMM effectively uses GPU hardware to dramatically accelerate both exact GP inference and scalable approximations. Additionally, we provide GPyTorch, a software platform for scalable GP inference via BBMM, built on PyTorch.",
  "stance": 0.6
 },
 {
  "url": "https://papers.neurips.cc/paper/2018/file/287e041302f34b11ddfb57afc8048cd8-Paper.pdf",
  "title": "Forward Modeling for Partial Observation Strategy Games - A StarCraft Defogger",
  "year": 2018,
  "venue": "NeurIPS",
  "abstract": "We formulate the problem of defogging as state estimation and future state prediction from previous, partial observations in the context of real-time strategy games. We propose to employ encoder-decoder neural networks for this task, and introduce proxy tasks and baselines for evaluation to assess their ability of capturing basic game rules and high-level dynamics. By combining convolutional neural networks and recurrent networks, we exploit spatial and sequential correlations and train well-performing models on a large dataset of human games of StarCraft R : Brood War R . Finally, we demonstrate the relevance of our models to downstream tasks by applying them for enemy unit prediction in a state-of-the-art, rule-based StarCraft bot. We observe improvements in win rates against several strong community bots.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2018/file/294a8ed24b1ad22ec2e7efea049b8737-Paper.pdf",
  "title": "Sanity Checks for Saliency Maps",
  "year": 2018,
  "venue": "NeurIPS",
  "abstract": "Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings2.",
  "stance": 0.3
 },
 {
  "url": "https://papers.neurips.cc/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf",
  "title": "Are Sixteen Heads Really Better than One?",
  "year": 2019,
  "venue": "NeurIPS",
  "abstract": "Attention is a powerful and ubiquitous mechanism for allowing neural models to focus on particular salient pieces of information by taking their weighted average when making predictions. In particular, multi-headed attention is a driving force behind many recent state-of-the-art natural language processing (NLP) models such as Transformer-based MT models and BERT. These models apply multiple attention mechanisms in parallel, with each attention “head” potentially focusing on different parts of the input, which makes it possible to express sophisticated functions beyond the simple weighted average. In this paper we make the surprising observation that even if models have been trained using multiple heads, in practice, a large percentage of attention heads can be removed at test time without significantly impacting performance. In fact, some layers can even be reduced to a single head. We further examine greedy algorithms for pruning down models, and the potential speed, memory efficiency, and accuracy improvements obtainable therefrom. Finally, we analyze the results with respect to which parts of the model are more reliant on having multiple heads, and provide precursory evidence that training dynamics play a role in the gains provided by multi-head attention.",
  "stance": -0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/2019/file/3001ef257407d5a371a96dcd947c7d93-Paper.pdf",
  "title": "Generative Modeling by Estimating Gradients of the Data Distribution",
  "year": 2019,
  "venue": "NeurIPS",
  "abstract": "We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2019/file/3a24b25a7b092a252166a1641ae953e7-Paper.pdf",
  "title": "Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers",
  "year": 2019,
  "venue": "NeurIPS",
  "abstract": "Recent works have shown the effectiveness of randomized smoothing as a scalable technique for building neural network-based classifiers that are provably robust to `2-norm adversarial perturbations. In this paper, we employ adversarial training to improve the performance of randomized smoothing. We design an adapted attack for smoothed classifiers, and we show how this attack can be used in an adversarial training setting to boost the provable robustness of smoothed classifiers. We demonstrate through extensive experimentation that our method consistently outperforms all existing provably `2-robust classifiers by a significant margin on ImageNet and CIFAR-10, establishing the state-of-the-art for provable `2-defenses. Moreover, we find that pre-training and semi-supervised learning boost adversarially trained smoothed classifiers even further. Our code and trained models are available at http://github.com/Hadisalman/smoothing-adversarial2.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2019/file/44885837c518b06e3f98b41ab8cedc0f-Paper.pdf",
  "title": "Invariance and identifiability issues for word embeddings",
  "year": 2019,
  "venue": "NeurIPS",
  "abstract": "Word embeddings are commonly obtained as optimizers of a criterion function f of a text corpus, but assessed on word-task performance using a different evaluation function g of the test data. We contend that a possible source of disparity in performance on tasks is the incompatibility between classes of transformations that leave f and g invariant. In particular, word embeddings defined by f are not unique; they are defined only up to a class of transformations to which f is invariant, and this class is larger than the class to which g is invariant. One implication of this is that the apparent superiority of one word embedding over another, as measured by word task performance, may largely be a consequence of the arbitrary elements selected from the respective solution sets. We provide a formal treatment of the above identifiability issue, present some numerical examples, and discuss possible resolutions.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2019/file/46a558d97954d0692411c861cf78ef79-Paper.pdf",
  "title": "Limitations of the empirical Fisher approximation for natural gradient descent",
  "year": 2019,
  "venue": "NeurIPS",
  "abstract": "Natural gradient descent, which preconditions a gradient descent update with the Fisher information matrix of the underlying statistical model, is a way to capture partial second-order information. Several highly visible works have advocated an approximation known as the empirical Fisher, drawing connections between approximate second-order methods and heuristics like Adam. We dispute this argument by showing that the empirical Fisher—unlike the Fisher—does not generally capture second-order information. We further argue that the conditions under which the empirical Fisher approaches the Fisher (and the Hessian) are unlikely to be met in practice, and that, even on simple optimization problems, the pathologies of the empirical Fisher can have undesirable effects.",
  "stance": -1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2019/file/48aedb8880cab8c45637abc7493ecddd-Paper.pdf",
  "title": "vGraph: A Generative Model for Joint Community Detection and Node Representation Learning",
  "year": 2019,
  "venue": "NeurIPS",
  "abstract": "This paper focuses on two fundamental tasks of graph analysis: community detection and node representation learning, which capture the global and local structures of graphs, respectively. In the current literature, these two tasks are usually independently studied while they are actually highly correlated. We propose a probabilistic generative model called vGraph to learn community membership and node representation collaboratively. Specifically, we assume that each node can be represented as a mixture of communities, and each community is defined as a multinomial distribution over nodes. Both the mixing coefficients and the community distribution are parameterized by the low-dimensional representations of the nodes and communities. We designed an effective variational inference algorithm which regularizes the community membership of neighboring nodes to be similar in the latent space. Experimental results on multiple real-world graphs show that vGraph is very effective in both community detection and node representation learning, outperforming many competitive baselines in both tasks. We show that the framework of vGraph is quite flexible and can be easily extended to detect hierarchical communities.",
  "stance": 0.9
 },
 {
  "url": "https://papers.neurips.cc/paper/2019/file/48c8c3963853fff20bd9e8bee9bd4c07-Paper.pdf",
  "title": "Can Unconditional Language Models Recover Arbitrary Sentences?",
  "year": 2019,
  "venue": "NeurIPS",
  "abstract": "Neural network-based generative language models like ELMo and BERT can work effectively as general purpose sentence encoders in text classification without further fine-tuning. Is it possible to adapt them in a similar way for use as generalpurpose decoders? For this to be possible, it would need to be the case that for any target sentence of interest, there is some continuous representation that can be passed to the language model to cause it to reproduce that sentence. We set aside the difficult problem of designing an encoder that can produce such representations and, instead, ask directly whether such representations exist at all. To do this, we introduce a pair of effective, complementary methods for feeding representations into pretrained unconditional language models and a corresponding set of methods to map sentences into and out of this representation space, the reparametrized sentence space. We then investigate the conditions under which a language model can be made to generate a sentence through the identification of a point in such a space and find that it is possible to recover arbitrary sentences nearly perfectly with language models and representations of moderate size without modifying any model parameters.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2019/file/51c68dc084cb0b8467eafad1330bce66-Paper.pdf",
  "title": "Cold Case: The Lost MNIST Digits",
  "year": 2019,
  "venue": "NeurIPS",
  "abstract": "Although the popular MNIST dataset [LeCun et al., 1994] is derived from the NIST database [Grother and Hanaoka, 1995], the precise processing steps for this derivation have been lost to time. We propose a reconstruction that is accurate enough to serve as a replacement for the MNIST dataset, with insignificant changes in accuracy. We trace each MNIST digit to its NIST source and its rich metadata such as writer identifier, partition identifier, etc. We also reconstruct the complete MNIST test set with 60,000 samples instead of the usual 10,000. Since the balance 50,000 were never distributed, they can be used to investigate the impact of twenty-five years of MNIST experiments on the reported testing performances. Our limited results unambiguously confirm the trends observed by Recht et al. [2018, 2019]: although the misclassification rates are slightly off, classifier ordering and model selection remain broadly reliable. We attribute this phenomenon to the pairing benefits of comparing classifiers on the same digits.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2019/file/5481b2f34a74e427a2818014b8e103b0-Paper.pdf",
  "title": "On the Power and Limitations of Random Features for Understanding Neural Networks",
  "year": 2019,
  "venue": "NeurIPS",
  "abstract": "Recently, a spate of papers have provided positive theoretical results for training over-parameterized neural networks (where the network size is larger than what is needed to achieve low error). The key insight is that with sufficient overparameterization, gradient-based methods will implicitly leave some components of the network relatively unchanged, so the optimization dynamics will behave as if those components are essentially fixed at their initial random values. In fact, fixing these explicitly leads to the well-known approach of learning with random features (e.g. [27, 29]). In other words, these techniques imply that we can successfully learn with neural networks, whenever we can successfully learn with random features. In this paper, we formalize the link between existing results and random features, and argue that despite the impressive positive results, random feature approaches are also inherently limited in what they can explain. In particular, we prove that random features cannot be used to learn even a single ReLU neuron (over standard Gaussian inputs in R and poly(d) weights), unless the network size (or magnitude of its weights) is exponentially large in d. Since a single neuron is known to be learnable with gradient-based methods, we conclude that we are still far from a satisfying general explanation for the empirical success of neural networks. For completeness we also provide a simple self-contained proof, using a random features technique, that one-hidden-layer neural networks can learn low-degree polynomials.",
  "stance": -1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2019/file/56a3107cad6611c8337ee36d178ca129-Paper.pdf",
  "title": "SHE: A Fast and Accurate Deep Neural Network for Encrypted Data",
  "year": 2019,
  "venue": "NeurIPS",
  "abstract": "We present a novel nonnegative tensor decomposition method, called Legendre decomposition, which factorizes an input tensor into a multiplicative combination of parameters. Thanks to the well-developed theory of information geometry, the reconstructed tensor is unique and always minimizes the KL divergence from an input tensor. We empirically show that Legendre decomposition can more accurately reconstruct tensors than other nonnegative tensor decomposition methods.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/1fc30b9d4319760b04fab735fbfed9a9-Paper.pdf",
  "title": "Learning Dynamic Belief Graphs to Generalize on Text-Based Games",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "Playing text-based games requires skills in processing natural language and sequential decision making. Achieving human-level performance on text-based games remains an open challenge, and prior research has largely relied on hand-crafted structured representations and heuristics. In this work, we investigate how an agent can plan and generalize in text-based games using graph-structured representations learned end-to-end from raw text. We propose a novel graph-aided transformer agent (GATA) that infers and updates latent belief graphs during planning to enable effective action selection by capturing the underlying game dynamics. GATA is trained using a combination of reinforcement and self-supervised learning. Our work demonstrates that the learned graph-based representations help agents converge to better policies than their text-only counterparts and facilitate effective generalization across game configurations. Experiments on 500+ unique games from the TextWorld suite show that our best agent outperforms text-based baselines by an average of 24.2%.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/216f44e2d28d4e175a194492bde9148f-Paper.pdf",
  "title": "Reinforcement Learning for Control with Multiple Frequencies",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "Many real-world sequential decision problems involve multiple action variables whose control frequencies are different, such that actions take their effects at different periods. While these problems can be formulated with the notion of multiple action persistences in factored-action MDP (FA-MDP), it is non-trivial to solve them efficiently since an action-persistent policy constructed from a stationary policy can be arbitrarily suboptimal, rendering solution methods for the standard FA-MDPs hardly applicable. In this paper, we formalize the problem of multiple control frequencies in RL and provide its efficient solution method. Our proposed method, Action-Persistent Policy Iteration (AP-PI), provides a theoretical guarantee on the convergence to an optimal solution while incurring only a factor of |A| increase in time complexity during policy improvement step, compared to the standard policy iteration for FA-MDPs. Extending this result, we present ActionPersistent Actor-Critic (AP-AC), a scalable RL algorithm for high-dimensional control tasks. In the experiments, we demonstrate that AP-AC significantly outperforms the baselines on several continuous control tasks and a traffic control simulation, which highlights the effectiveness of our method that directly optimizes the periodic non-stationary policy for tasks with multiple control frequencies.",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/240ac9371ec2671ae99847c3ae2e6384-Paper.pdf",
  "title": "Restoring Negative Information in Few-Shot Object Detection",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "Few-shot learning has recently emerged as a new challenge in the deep learning field: unlike conventional methods that train the deep neural networks (DNNs) with a large number of labeled data, it asks for the generalization of DNNs on new classes with few annotated samples. Recent advances in few-shot learning mainly focus on image classification while in this paper we focus on object detection. The initial explorations in few-shot object detection tend to simulate a classification scenario by using the positive proposals in images with respect to certain object class while discarding the negative proposals of that class. Negatives, especially hard negatives, however, are essential to the embedding space learning in few-shot object detection. In this paper, we restore the negative information in few-shot object detection by introducing a new negativeand positive-representative based metric learning framework and a new inference scheme with negative and positive representatives. We build our work on a recent few-shot pipeline RepMet [1] with several new modules to encode negative information for both training and testing. Extensive experiments on ImageNet-LOC and PASCAL VOC show our method substantially improves the state-of-the-art few-shot object detection solutions. Our code is available at https://github.com/yang-yk/NP-RepMet.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/24bea84d52e6a1f8025e313c2ffff50a-Paper.pdf",
  "title": "Diverse Image Captioning with Context-Object Split Latent Spaces",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "Diverse image captioning models aim to learn one-to-many mappings that are innate to cross-domain datasets, such as of images and texts. Current methods for this task are based on generative latent variable models, e.g. VAEs with structured latent spaces. Yet, the amount of multimodality captured by prior work is limited to that of the paired training data – the true diversity of the underlying generative process is not fully captured. To address this limitation, we leverage the contextual descriptions in the dataset that explain similar contexts in different visual scenes. To this end, we introduce a novel factorization of the latent space, termed contextobject split, to model diversity in contextual descriptions across images and texts within the dataset. Our framework1 not only enables diverse captioning through context-based pseudo supervision, but extends this to images with novel objects and without paired captions in the training data. We evaluate our COS-CVAE approach on the standard COCO dataset and on the held-out COCO dataset consisting of images with novel objects, showing significant gains in accuracy and diversity.",
  "stance": 0.6
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/258be18e31c8188555c2ff05b4d542c3-Paper.pdf",
  "title": "Continual Learning with Node-Importance based Adaptive Group Sparse Regularization",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "We propose a novel regularization-based continual learning method, dubbed as Adaptive Group Sparsity based Continual Learning (AGS-CL), using two group sparsity-based penalties. Our method selectively employs the two penalties when learning each neural network node based on its the importance, which is adaptively updated after learning each task. By utilizing the proximal gradient descent method, the exact sparsity and freezing of the model is guaranteed during the learning process, and thus, the learner explicitly controls the model capacity. Furthermore, as a critical detail, we re-initialize the weights associated with unimportant nodes after learning each task in order to facilitate efficient learning and prevent the negative transfer. Throughout the extensive experimental results, we show that our AGS-CL uses orders of magnitude less memory space for storing the regularization parameters, and it significantly outperforms several state-of-the-art baselines on representative benchmarks for both supervised and reinforcement learning.",
  "stance": 0.9
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/26b58a41da329e0cbde0cbf956640a58-Paper.pdf",
  "title": "On ranking via sorting by estimated expected utility",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "Ranking tasks are defined through losses that measure trade-offs between different desiderata such as the relevance and the diversity of the items at the top of the list. This paper addresses the question of which of these tasks are asymptotically solved by sorting by decreasing order of expected utility, for some suitable notion of utility, or, equivalently, when is square loss regression consistent for ranking via score-andsort? We answer to this question by finding a characterization of ranking losses for which a suitable regression is consistent. This characterization has two strong corollaries. First, whenever there exists a consistent approach based on convex risk minimization, there also is a consistent approach based on regression. Second, when regression is not consistent, there are data distributions for which consistent surrogate approaches necessarily have non-trivial local minima, and for which optimal scoring function are necessarily discontinuous, even when the underlying data distribution is regular. In addition to providing a better understanding of surrogate approaches for ranking, these results illustrate the intrinsic difficulty of solving general ranking problems with the score-and-sort approach.",
  "stance": -0.3
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/293835c2cc75b585649498ee74b395f5-Paper.pdf",
  "title": "Dissecting Neural ODEs",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "Continuous deep learning architectures have recently re–emerged as Neural Ordinary Differential Equations (Neural ODEs). This infinite–depth approach theoretically bridges the gap between deep learning and dynamical systems, offering a novel perspective. However, deciphering the inner working of these models is still an open challenge, as most applications apply them as generic black–box modules. In this work we “open the box”, further developing the continuous–depth formulation with the aim of clarifying the influence of several design choices on the underlying dynamics.",
  "stance": 0.4
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/37bc5e7fb6931a50b3464ec66179085f-Paper.pdf",
  "title": "Learning Physical Constraints with Neural Projections",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "We propose a new family of neural networks to predict the behaviors of physical systems by learning their underpinning constraints. A neural projection operator lies at the heart of our approach, composed of a lightweight network with an embedded recursive architecture that interactively enforces learned underpinning constraints and predicts the various governed behaviors of different physical systems. Our neural projection operator is motivated by the position-based dynamics model that has been used widely in game and visual effects industries to unify the various fast physics simulators. Our method can automatically and effectively uncover a broad range of constraints from observation point data, such as length, angle, bending, collision, boundary effects, and their arbitrary combinations, without any connectivity priors. We provide a multi-group point representation in conjunction with a configurable network connection mechanism to incorporate prior inputs for processing complex physical systems. We demonstrated the efficacy of our approach by learning a set of challenging physical systems all in a unified and simple fashion including: rigid bodies with complex geometries, ropes with varying length and bending, articulated soft and rigid bodies, and multi-object collisions with complex boundaries.",
  "stance": 0.7
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/3f13cf4ddf6fc50c0d39a1d5aeb57dd8-Paper.pdf",
  "title": "Bayesian Bits: Unifying Quantization and Pruning",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "We introduce Bayesian Bits, a practical method for joint mixed precision quantization and pruning through gradient based optimization. Bayesian Bits employs a novel decomposition of the quantization operation, which sequentially considers doubling the bit width. At each new bit width, the residual error between the full precision value and the previously rounded value is quantized. We then decide whether or not to add this quantized residual error for a higher effective bit width and lower quantization noise. By starting with a power-of-two bit width, this decomposition will always produce hardware-friendly configurations, and through an additional 0-bit option, serves as a unified view of pruning and quantization. Bayesian Bits then introduces learnable stochastic gates, which collectively control the bit width of the given tensor. As a result, we can obtain low bit solutions by performing approximate inference over the gates, with prior distributions that encourage most of them to be switched off. We experimentally validate our proposed method on several benchmark datasets and show that we can learn pruned, mixed precision networks that provide a better trade-off between accuracy and efficiency than their static bit width equivalents.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/438124b4c06f3a5caffab2c07863b617-Paper.pdf",
  "title": "Meta-learning from Tasks with Heterogeneous Attribute Spaces",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "We propose a heterogeneous meta-learning method that trains a model on tasks with various attribute spaces, such that it can solve unseen tasks whose attribute spaces are different from the training tasks given a few labeled instances. Although many meta-learning methods have been proposed, they assume that all training and target tasks share the same attribute space, and they are inapplicable when attribute sizes are different across tasks. Our model infers latent representations of each attribute and each response from a few labeled instances using an inference network. Then, responses of unlabeled instances are predicted with the inferred representations using a prediction network. The attribute and response representations enable us to make predictions based on the task-specific properties of attributes and responses even when attribute and response sizes are different across tasks. In our experiments with synthetic datasets and 59 datasets in OpenML, we demonstrate that our proposed method can predict the responses given a few labeled instances in new tasks after being trained with tasks with heterogeneous attribute spaces.",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/47d40767c7e9df50249ebfd9c7cfff77-Paper.pdf",
  "title": "SMYRF - Efficient Attention using Asymmetric Clustering",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "We propose a novel type of balanced clustering algorithm to approximate attention. Attention complexity is reduced from O(N) to O(N logN), where N is the sequence length. Our algorithm, SMYRF, uses Locality Sensitive Hashing (LSH) in a novel way by defining new Asymmetric transformations and an adaptive scheme that produces balanced clusters. The biggest advantage of SMYRF is that it can be used as a drop-in replacement for dense attention layers without any retraining. On the contrary, prior fast attention methods impose constraints (e.g. queries and keys share the same vector representations) and require re-training from scratch. We apply our method to pre-trained state-of-the-art Natural Language Processing and Computer Vision models and we report significant memory and speed benefits. Notably, SMYRF-BERT outperforms (slightly) BERT on GLUE, while using 50% less memory. We also show that SMYRF can be used interchangeably with dense attention before and after training. Finally, we use SMYRF to train GANs with attention in high resolutions. Using a single TPU, we were able to scale attention to 128x128=16k and 256x256=65k tokens on BigGAN on CelebA-HQ.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/4b29fa4efe4fb7bc667c7b301b74d52d-Paper.pdf",
  "title": "BlockGAN: Learning 3D Object-aware Scene Representations from Unlabelled Images",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "We present BlockGAN, an image generative model that learns object-aware 3D scene representations directly from unlabelled 2D images. Current work on scene representation learning either ignores scene background or treats the whole scene as one object. Meanwhile, work that considers scene compositionality treats scene objects only as image patches or 2D layers with alpha maps. Inspired by the computer graphics pipeline, we design BlockGAN to learn to first generate 3D features of background and foreground objects, then combine them into 3D features for the whole scene, and finally render them into realistic images. This allows BlockGAN to reason over occlusion and interaction between objects’ appearance, such as shadow and lighting, and provides control over each object’s 3D pose and identity, while maintaining image realism. BlockGAN is trained end-to-end, using only unlabelled single images, without the need for 3D geometry, pose labels, object masks, or multiple views of the same scene. Our experiments show that using explicit 3D features to represent objects allows BlockGAN to learn disentangled representations both in terms of objects (foreground and background) and their properties (pose and identity). Our code is available at https://github.com/thunguyenphuoc/BlockGAN.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/518a38cc9a0173d0b2dc088166981cf8-Paper.pdf",
  "title": "Understanding the Role of Training Regimes in Continual Learning",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "Catastrophic forgetting affects the training of neural networks, limiting their ability to learn multiple tasks sequentially. From the perspective of the well established plasticity-stability dilemma, neural networks tend to be overly plastic, lacking the stability necessary to prevent the forgetting of previous knowledge, which means that as learning progresses, networks tend to forget previously seen tasks. This phenomenon coined in the continual learning literature, has attracted much attention lately, and several families of approaches have been proposed with different degrees of success. However, there has been limited prior work extensively analyzing the impact that different training regimes – learning rate, batch size, regularization method– can have on forgetting. In this work, we depart from the typical approach of altering the learning algorithm to improve stability. Instead, we hypothesize that the geometrical properties of the local minima found for each task play an important role in the overall degree of forgetting. In particular, we study the effect of dropout, learning rate decay, and batch size, on forming training regimes that widen the tasks’ local minima and consequently, on helping it not to forget catastrophically. Our study provides practical insights to improve stability via simple yet effective techniques that outperform alternative baselines.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/55d491cf951b1b920900684d71419282-Paper.pdf",
  "title": "Learning Certified Individually Fair Representations",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "Fair representation learning provides an effective way of enforcing fairness constraints without compromising utility for downstream users. A desirable family of such fairness constraints, each requiring similar treatment for similar individuals, is known as individual fairness. In this work, we introduce the first method that enables data consumers to obtain certificates of individual fairness for existing and new data points. The key idea is to map similar individuals to close latent representations and leverage this latent proximity to certify individual fairness. That is, our method enables the data producer to learn and certify a representation where for a data point all similar individuals are at `∞-distance at most , thus allowing data consumers to certify individual fairness by proving -robustness of their classifier. Our experimental evaluation on five real-world datasets and several fairness constraints demonstrates the expressivity and scalability of our approach.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/57cd30d9088b0185cf0ebca1a472ff1d-Paper.pdf",
  "title": "Can Implicit Bias Explain Generalization? Stochastic Convex Optimization as a Case Study",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "The notion of implicit bias, or implicit regularization, has been suggested as a means to explain the surprising generalization ability of modern-days overparameterized learning algorithms. This notion refers to the tendency of the optimization algorithm towards a certain structured solution that often generalizes well. Recently, several papers have studied implicit regularization and were able to identify this phenomenon in various scenarios. We revisit this paradigm in arguably the simplest non-trivial setup, and study the implicit bias of Stochastic Gradient Descent (SGD) in the context of Stochastic ConvexOptimization. As a first step, we provide a simple construction that rules out the existence of a distribution-independent implicit regularizer that governs the generalization ability of SGD. We then demonstrate a learning problem that rules out a very general class of distribution-dependent implicit regularizers from explaining generalization, which includes strongly convex regularizers as well as non-degenerate norm-based regularizations. Certain aspects of our constructions point out to significant difficulties in providing a comprehensive explanation of an algorithm’s generalization performance by solely arguing about its implicit regularization properties.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/588cb956d6bbe67078f29f8de420a13d-Paper.pdf",
  "title": "Critic Regularized Regression",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "Offline reinforcement learning (RL), also known as batch RL, offers the prospect of policy optimization from large pre-recorded datasets without online environment interaction. It addresses challenges with regard to the cost of data collection and safety, both of which are particularly pertinent to real-world applications of RL. Unfortunately, most off-policy algorithms perform poorly when learning from a fixed dataset. In this paper, we propose a novel offline RL algorithm to learn policies from data using a form of critic-regularized regression (CRR). We find that CRR performs surprisingly well and scales to tasks with high-dimensional state and action spaces – outperforming several state-of-the-art offline RL algorithms by a significant margin on a wide range of benchmark tasks.",
  "stance": 0.8
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/5bf8aaef51c6e0d363cbe554acaf3f20-Paper.pdf",
  "title": "A Scalable Approach for Privacy-Preserving Collaborative Machine Learning",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "We consider a collaborative learning scenario in which multiple data-owners wish to jointly train a logistic regression model, while keeping their individual datasets private from the other parties. We propose COPML, a fully-decentralized training framework that achieves scalability and privacy-protection simultaneously. The key idea of COPML is to securely encode the individual datasets to distribute the computation load effectively across many parties and to perform the training computations as well as the model updates in a distributed manner on the securely encoded data. We provide the privacy analysis of COPML and prove its convergence. Furthermore, we experimentally demonstrate that COPML can achieve significant speedup in training over the benchmark protocols. Our protocol provides strong statistical privacy guarantees against colluding parties (adversaries) with unbounded computational power, while achieving up to 16× speedup in the training time against the benchmark protocols.",
  "stance": 0.4
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/5ef20b89bab8fed38253e98a12f26316-Paper.pdf",
  "title": "Greedy inference with structure-exploiting lazy maps",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "We propose a framework for solving high-dimensional Bayesian inference problems using structure-exploiting low-dimensional transport maps or flows. These maps are confined to a low-dimensional subspace (hence, lazy), and the subspace is identified by minimizing an upper bound on the Kullback–Leibler divergence (hence, structured). Our framework provides a principled way of identifying and exploiting low-dimensional structure in an inference problem. It focuses the expressiveness of a transport map along the directions of most significant discrepancy from the posterior, and can be used to build deep compositions of lazy maps, where low-dimensional projections of the parameters are iteratively transformed to match the posterior. We prove weak convergence of the generated sequence of distributions to the posterior, and we demonstrate the benefits of the framework on challenging inference problems in machine learning and differential equations, using inverse autoregressive flows and polynomial maps as examples of the underlying density estimators.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/618491e20a9b686b79e158c293ab4f91-Paper.pdf",
  "title": "Bad Global Minima Exist and SGD Can Reach Them",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "Several works have aimed to explain why overparameterized neural networks generalize well when trained by Stochastic Gradient Descent (SGD). The consensus explanation that has emerged credits the randomized nature of SGD for the bias of the training process towards low-complexity models and, thus, for implicit regularization. We take a careful look at this explanation in the context of image classification with common deep neural network architectures. We find that if we do not regularize explicitly, then SGD can be easily made to converge to poorlygeneralizing, high-complexity models: all it takes is to first train on a random labeling on the data, before switching to properly training with the correct labels. In contrast, we find that in the presence of explicit regularization, pretraining with random labels has no detrimental effect on SGD. We believe that our results give evidence that explicit regularization plays a far more important role in the success of overparameterized neural networks than what has been understood until now. Specifically, by penalizing complicated models independently of their fit to the data, regularization affects training dynamics also far away from optima, making simple models that fit the data well discoverable by local methods, such as SGD.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/6217b2f7e4634fa665d31d3b4df81b56-Paper.pdf",
  "title": "Spin-Weighted Spherical CNNs",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "Learning equivariant representations is a promising way to reduce sample and model complexity and improve the generalization performance of deep neural networks. The spherical CNNs are successful examples, producing SO(3)-equivariant representations of spherical inputs. There are two main types of spherical CNNs. The first type lifts the inputs to functions on the rotation group SO(3) and applies convolutions on the group, which are computationally expensive since SO(3) has one extra dimension. The second type applies convolutions directly on the sphere, which are limited to zonal (isotropic) filters, and thus have limited expressivity. In this paper, we present a new type of spherical CNN that allows anisotropic filters in an efficient way, without ever leaving the spherical domain. The key idea is to consider spin-weighted spherical functions, which were introduced in physics in the study of gravitational waves. These are complex-valued functions on the sphere whose phases change upon rotation. We define a convolution between spin-weighted functions and build a CNN based on it. The spin-weighted functions can also be interpreted as spherical vector fields, allowing applications to tasks where the inputs or outputs are vector fields. Experiments show that our method outperforms previous methods on tasks like classification of spherical images, classification of 3D shapes and semantic segmentation of spherical panoramas.",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/2020/file/690f44c8c2b7ded579d01abe8fdb6110-Paper.pdf",
  "title": "Geo-PIFu: Geometry and Pixel Aligned Implicit Functions for Single-view Human Reconstruction",
  "year": 2020,
  "venue": "NeurIPS",
  "abstract": "We propose Geo-PIFu, a method to recover a 3D mesh from a monocular color image of a clothed person. Our method is based on a deep implicit function-based representation to learn latent voxel features using a structure-aware 3D U-Net, to constrain the model in two ways: first, to resolve feature ambiguities in query point encoding, second, to serve as a coarse human shape proxy to regularize the high-resolution mesh and encourage global shape regularity. We show that, by both encoding query points and constraining global shape using latent voxel features, the reconstruction we obtain for clothed human meshes exhibits less shape distortion and improved surface details compared to competing methods. We evaluate Geo-PIFu on a recent human mesh public dataset that is 10× larger than the private commercial dataset used in PIFu and previous derivative work. On average, we exceed the state of the art by 42.7% reduction in Chamfer and Point-to-Surface Distances, and 19.4% reduction in normal estimation errors.",
  "stance": 1.0
 }
]