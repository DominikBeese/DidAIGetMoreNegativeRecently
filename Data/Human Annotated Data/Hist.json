[
 {
  "url": "https://www.aaai.org/Papers/AAAI/1992/AAAI92-119.pdf",
  "title": "An Empirical Analysis of Terminological Representation Systems",
  "year": 1992,
  "venue": "AAAI",
  "abstract": "The family of terminological representation systems has its roots in the representation system KLONE. Since the development of this system more than a dozen similar representation systems have been developed by various research groups. These systems vary along a number of dimensions. In this paper, we present the results of an empirical analysis of six such systems. Surprisingly, the systems turned out to be quite diverse leading to problems when transporting knowledge bases from one system to another. Additionally, the runtime performance between different systems and knowledge bases varied more than we expected. Finally, our empirical runtime performance results give an idea of what runtime performance to expect from such representation systems. These findings complement previously reported analytical results about the computational complexity of reasoning in such systems.",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1994/AAAI94-014.pdf",
  "title": "Experimentally Evaluating Communicative Strategies: The Effect of the Task",
  "year": 1994,
  "venue": "AAAI",
  "abstract": "Effective problem solving among multiple agents requires a better understanding of the role of communication in collaboration. In this paper we show that there are communicative strategies that greatly improve the performance of resource-bounded agents, but that these strategies are highly sensitive to the task requirements, situation parameters and agents’ resource limitations. We base our argument on two sources of evidence: (1) an analysis of a corpus of 55 problem solving dialogues, and (2) experimental simulations of collaborative problem solving dialogues in an experimental world, Design-World, where we parameterize task requirements, agents’ resources and communicative strategies.",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1994/AAAI94-129.pdf",
  "title": "Unclear Distinctions Lead to Unnecessary Shortcomings: Examining the Rule Versus Fact, Role versus Filler, and Type Versus Predicate Distinctions from a Connectionist Representation and Reasoning Perspective",
  "year": 1994,
  "venue": "AAAI",
  "abstract": "This paper deals with three distinctions pertaining to knowledge representation, namely, the rules vs facts distinction, roles vs fillers distinction, and predicates vs types distinction. Though these distinctions may indeed have some intuitive appeal, the exact natures of these distinctions are not entirely clear. This paper discusses some of the problems that arise when one accords these distinctions a prominent status in a connectionist system by choosing the representational structures so as to reflect these distinctions. The example we will look at in this paper is the connectionist reasoning system developed by Ajjanagadde & Shastri(Ajjanagadde & Shastri 1991; Shastri & Ajjanagadde 1993). Their1 system performs an interesting class of inferences using activation synchrony to represent dynamic bindings. The rule/fact, role/filler, type/predicate distinctions figure predominantly in the way knowledge is encoded in their system. We will discuss some significant shortcomings this leads to. Then, we will propose a much more uniform scheme for representing knowledge. The resulting system enjoys some significant advantages over Ajjanagadde & Shastri’s system, while retaining the idea of using synchrony to represent bindings.",
  "stance": 0.5
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1994/AAAI94-139.pdf",
  "title": "On the Relation between the Coherence and Foundations Theories of Belief Revision",
  "year": 1994,
  "venue": "AAAI",
  "abstract": "Two recent, papers, (Ggrdenfors 1990; Doyle 1992), try to assess the relative merits of the two main approaches to belief revision, the foundations and coherence theories, but leave open the question of the mathematical connections between them. We answer this question by showing that the foundations and coherence theories of belief revision are mathematically equivalent. The result also has consequences for nonmonotonic reasoning, as it, entails that Poole’s system of default, reasoning and Shoham’s preferential logic are expressively equivalent, in that they can represent the same set of non monotonic consequence relations.",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1994/AAAI94-160.pdf",
  "title": "The First Law of Robotics (A Call to Arms)",
  "year": 1994,
  "venue": "AAAI",
  "abstract": "Even before the advent of Artificial Intelligence, science fiction writer Isaac Asimov recognized that an agent must place the protection of humans from harm at a higher priority than obeying human orders. Inspired by Asimov, we pose the following fundamental questions: (1) How should one formalize the rich, but informal, notion of “harm”? (2) How can an agent avoid performing harmful actions, and do so in a computationally tractable manner? (3) How should an agent resolve conflict between its goals and the need to avoid harm? (4) When should an agent prevent a human from harming herself? While we address some of these questions in technical detail, the primary goal of this paper is to focus attention on Asimov’s concern: society will reject autonomous agents unless we have some credible means of making them safe! The Three Laws of Robotics: A robot may not injure a human being, or, through inaction, allow a human being to come to harm. A robot must obey orders given it by human beings except where such orders would conflict with the First Law. A robot must protect its own existence as long as such protection does not conflict with the First or Second Law. Isaac Asimov (Asimov 1942): -.. Motiyation ~ In 1940, Isaac Asimov stated the First Law of Robotics, capturing an essential insight: an intelligent agent1 *We thank Steve Hanks, Nick Kushmerick, Neal Lesh, Kevin Sullivan, and Mike Williamson for helpful discussions. This research was funded in part by the University of Washington Royalty Research Fund, by Office of Naval Research Grants 90-J-1904 and 92-J-1946, and by National Science Foundation Grants IRI-8957302, IRI-9211045, and IRI-9357772. ‘Since the field of robotics now concerns itself primarily with kinematics, dynamics, path planning, and low level control issues, this paper might be better titled “The First Law of Agenthood.” However, we keep the reference to “Robotics” as a historical tribute to Asimov. should not slavishly obey human commands its foremost goal should be to avoid harming humans. Consider the following scenarios: A construction robot is instructed to fill a pothole in the road. Although the robot repairs the cavity, it leaves the steam roller, chunks of tar, and an oil slick in the middle of a busy highway. A softbot (software robot 4 is instructed to reduce disk utilization below 90 o. It succeeds, but inspection reveals that the agent deleted irreplaceable I4TEX files without backing them up to tape. While less dramatic than Asimov’s stories, the sce_ . __ _-. narios illustrate his point: not all ways of satisfying a human order are equally good; in fact, sometimes it is better not to satisfy the order at all. As we begin to deploy agents in environments where they can do some real damage, the time has come to revisit Asimov’s Laws. This paper explores the following fundamental questions: How should one formalize the notion of “harm”? We define dont-disturb and restoretwo domain-independent primitives that capture aspects of Asimov’s rich but informal notion of harm within the classical planning framework. How can an agent avoid performing harmful actions, and do so in a computationally tractable manner? We leverage and extend the familiar mechanisms of planning with subgoal interactions (Tate 1977; Chapman 1987; McAllester & Rosenblitt 1991; Penberthy & Weld 1992) to detect potential harm in polynomial time. In addition, we explain how the agent can avoid harm using tactics such as confrontation and evasion (executing subplans to defuse the threat of harm). How should an agent resolve conflict between its goals and the need to avoid harm? We impose a strict hierarchy where dont-disturb constraints override planners goals, but restore constraints do not. When should an agent prevent a human from harming herself ? At the end of the paper, we show how our framework could be extended to partially address this question. 1042 Planning and Scheduling From: AAAI-94 Proceedings. Copyright © 1994, AAAI (www.aaai.org). All rights reserved.",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1994/AAAI94-162.pdf",
  "title": "On the Nature of Modal Truth Criteria in Planning",
  "year": 1994,
  "venue": "AAAI",
  "abstract": "Chapman’s paper, “Planning for Conjunctive Goals,” has been widely acknowledged for its contribution toward understanding the nature of nonlinear (partial-order) planning, and it has been one of the bases of later work by others---but it is not free of problems. This paper addresses some problems involving modal truth and the Modal Truth Criterion (MTC). Our results are as follows: Even though modal duality is a fundamental axiom of classical modal logics, it does not hold for modal truth in Chapman’s plans; i.e., “necessarily p” is not equivalent to “not possibly lp.” Although the MTC for necessary truth is correct, the MTC for possible truth is incorrect: it provides necessary but insz#kient conditions for ensuring possible truth. Furthermore, even though necessary truth can be determined in polynomial time, possible truth is NP-hard. If we rewrite the MTC to talk about modal conditional truth (i.e., modal truth conditional on executability) rather than modal truth, then both the MTC for necessary conditional truth and the MTC for possible conditional truth are correct; and both can be computed in polynomial time.",
  "stance": -0.8
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1994/AAAI94-223.pdf",
  "title": "Decision-Theoretic Plan Failure Debugging and Repair",
  "year": 1994,
  "venue": "AAAI",
  "abstract": "A number of strategies exist for the recovery from execution-time plan failures. One manner in which these strategies difFer is the degree of dependence on the reliability and availability of the planner’s knowledge. The best strategy, however, may be dependent on a number of considerations, including the type of plan failure, the criticality of the failure, the availability of resources, and the reliability and availability of the knowledge involved in a given plan failure instance. We are examining a decision-theoretic approach to diagnose plan fakes and to dynamically select fkom multiple failure recovery strategies when an execution-time plan failure occurs. Existing failure recovery strategies generally classify, with assumed or proven certainty, the type of error that occurred during plan execution and then select a fixed strategy to recover from that error. Assumptions regarding the accuracy and completeness of the planner’s domain model vary. On one end are approaches that use deterministic heuristics or purely syntactic analyses to debug and repair. These approaches are efficient and require limited knowledge, but generally are limited in the level of diagnosis and repair they can perform. On the other end are logic-based approaches, that are robust, but are knowledge and resource intensive. Such approaches are not always feasible. Incomplete or uncertain knowledge of previous planning actions, as in multi-agent planners, prechrdes a complete logical analysis. Even when possible, the costs of collecting and reasoning with complete information may be intractable, especially given time pressures and other resource constraints. The goal of our work is the development of an approach that can intelligently select and apply ftilure recovery strategies that are appropriate to the situation and that can cope with uncertainty. There are three primary components of our research: diagnosis of plan failures, plan repair and planner modification. When a failure is detected, we use a",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1994/AAAI94-255.pdf",
  "title": "Preliminary Studies in Agent Design in Simulated Environments",
  "year": 1994,
  "venue": "AAAI",
  "abstract": "It is known that, in general, the point along the purely-reactive/classical-planning axis of the controller spectrum that is most appropriate for a particular environment/task (E/T) p air will be determined by characteristics of the environment, the agent’s perceptual and effectual capabilities, and the task. Instead of proposing another hybrid architecture, we want to determine criteria for determining which architectural compromise is best suited for a given E/T. Our goal is to understand relationships between E/T pairs and the agent architecture, so that we can predict the performance of the architecture under parametric variations of the environment and/or the architecture. This is a first step toward constructing methods for automatic synthesis of agents as in (Ros89). Our example of a domain where the choice of architectural basis is not so clear is the game of XChomp (programmed by Jerry J. Shekhel), a close relative of the commercial game PacMan. This domain allows for easy change of parameters to simulate a number of discrete combinatorial problem domains. Interesting characteristics of the game that make it different from the E/Ts considered in (AC87), (Bro86), (ChaSl), (Sch87) among others, are: There are no+EocaE taslcs. By non-local, we refer to not only spatial and temporal extents, but also universal quantification of parameters of the task. Hostile aspects of the environment may be temporarily made not only benign, but positive concrete goals; these conversions are under the control of the player. Classification of objects changes over time, complicating the decision of how to respond to such objects, as these are based on projections of possible futures. There is not much flexibility with respect to movement. Not only is movement within this environment restricted to the four cardinal directions, but it is a maze, so that in most locations, only two of those four may be used. When the cost of making mistakes is high, the extra effort to get it right the first time is (possibly) justified. There are multiple conflicting objectives. While having multiple objectives is not particularly novel, those in this environment have a nasty habit of pulling their acquisitions at cross purposes. Any designer must answer the following questions: On what informational basis does an agent make its action selection choice ? What aspects of the world does it perform forward projection on, what aspects does it sense, and what is the map from external and internal state of the agent to an action (or sequence) that maximizes the objective function of the agent? We need to be able to construct a solution and justify it using methods other than pointing to the constructed solution as an existence proof. This follows in the spirit of work done in (Hor93) for a mobile robot. To enable us to study these questions, we isolate a range of E/T combinations based on the XChomp game. We implement a range of controllers that exploit the information needed for “optimal” play and test their task performance experimentally. We then vary the performance requirements and environmental specifications in a form of perturbation analysis to determine how robust the agents are; and how to modify them to be effective in new situations. In addition, starting from a very simplified version of this E/T, we are developing a theoretical basis upon which to justify the agents we develop. This basis is expected to not only be used in determining how an agent should behave, but also what a designer should not be concerned about.",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1994/AAAI94-258.pdf",
  "title": "When the Best Move Isn’t Optimal: Q-learning with Exploration",
  "year": 1994,
  "venue": "AAAI",
  "abstract": "The most popular delayed reinforcement learning technique, Q-learning (Watkins 1989)) estimates the future reward expected from executing each action in every state. If these estimates are correct, then an agent can use them to select the action with maximal expected future reward in each state, and thus perform optimally. Watkins has proved that Q-learning produces an optimal policy (the function mapping states to actions) and that these estimates converge to the correct values given the optimal policy. However, often the agent does not follow the optimal policy faithfully the agent must also explore the world, taking suboptimal actions in order to learn more about its environment. The “optimal” policy produced by Q-learning is no longer optimal if its prescriptions are only followed occasionally. In many situations (e.g., dynamic environments), the agent never stops exploring. In such domains Q-learning converges to policies which are suboptimal in the sense that there exists a different policy which would achieve higher reward when combined with exploration. A bit of notation: &(z, a) is the expected future reward received after taking action a in state x. V(x) is the expected future reward received after starting in state x. 0 and v are used to denote the approximations kept by the algorithm. Each time the agent takes an action a moving it from state x to state y and generating a reward T, Q-learning updates the approximations according to the following rules:",
  "stance": -0.6
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1994/AAAI94-270.pdf",
  "title": "Model-Based Sensor Diagnosis: When Monitoring Should be Monitored",
  "year": 1994,
  "venue": "AAAI",
  "abstract": "A complex industrial plant, such as a nuclear power plant, is monitored thanks to a number of sensors. The instrumentation may be itself a complex system liable to failures. We propose a model-based sensor diagnosis system which relies on the topological description of the plant and on a set of component models. This model implicitly conceals relations involving only sensor data. Such relations must always be verified if components behave normally; thus, the detection task consists of verifying these relations. So, this work is a first step in extending the scope of model-based diagnosis, since we question here the information stemming from the plant and normally considered as safe. As further studies, we wish to monitor this detection system itself; i.e., whenever the instrumentation is supposed to behave correctly, nonverified constraints point out to errors in the plant model. Questioning the model-based diagnosis A model-based diagnosis (Reiter 1987) relies on structural and behavioural knowledge and on observations. Observations in a plant stem from sensors. Since the instrumentation is liable to failures, sensor data are questionable. On the other hand, component models in a thermohydraulic circuit are very crude and the topological database describing the plant must be updated after each human intervention on the plant. So, when a model-based reasoning system provides a result, three assumptions must be taken into account: there is no sensor failure, component models are accurate enough, and the topological database rigorously describes the plant. Sensor failure in a thermohydraulic circuit Fluid behaviour is described by a set of equations of different kinds stemming from the plant model. As some of the variables are measured by sensors, we seek to exhibit, when they exist, algebraic relationships between them by eliminating the variables which are not measured. Such constraints should be verified at each step if every component behaves properly. Constraint violation is equivalent to a malfunction and is seen as a sensor failure. 1476 Student Abstracts Constraints can be found in two steps. First, a qualitative model of constraint existence is set up by means of structural analysis. Secondly, models are formally handled as according to the structural analysis results in order to establish the constraints on sensor data. The set of equations is turned into a structural matrix (Iwasaki & Simon 1986) in which each variable v is characterized with respect to each equation E by only two pieces of information: whether v is involved in E and whether E can be solved with respect to v. Constraints are found by triangulation of a part of this matrix. Further research direction Whereas the operator may check the installation thanks to the instrumentation, the present system aims at providing a diagnosis on the instrumentation itself, rather than on the installation. We wish to check the sensors with their own values. Sensor data, thus validated, may be used in other monitoring systems. Sensor diagnosis may be seen as a part of the diagnosis of a monitoring system. On the other hand, this sensor diagnosis system may itself be faulty, and should also be monitored. This system is based on four sources of knowledge and data, namely: the topological database describing the installation (TDB), the models library (ML), the research and generation algorithm (A), and sensors data. If no sensor is assumed to be faulty, then constraints violation is seen as a set of malfuctions of (TDB), (ML), or even (A).",
  "stance": 0.3
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1996/AAAI96-013.pdf",
  "title": "Nearly Monotonic Problems: A Key to Effective FA/C Distributed Sensor Interpretation?",
  "year": 1996,
  "venue": "AAAI",
  "abstract": "The fesractioncslly-Qcczdrrcate, cooperative (FA/C) distributed problem-solving paradigm is one approach for organizing distributed problem solving among homogeneous, cooperating agents. A key assumption of the FA/C model has been that the agents’ local solutions can substitute for the raw data in determining the global solutions. This is not the case in general, however. Does this mean that researchers’ intuitions have been wrong and/or that FA/C problem solving is not likely to be effective ? We suggest that some domains have a characteristic that can account for the success of exchanging mainly local solutions. We call such problems nearly monotonic. This concept is discussed in the context of FA/C-based distributed sensor",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1996/AAAI96-097.pdf",
  "title": "Formalizing Narratives Using Nested Circumscription",
  "year": 1996,
  "venue": "AAAI",
  "abstract": "The representation of narratives of actions and observations is a current issue in Knowledge Representation, where traditional plan-oriented treatments of action seem to fall short. To address narratives, Pinto and Reiter have extended Situation Calculus axioms, Kowalski and Sergot have introduced the Event Calculus in Logic Programming, and Baral et al. have defined the specification language C which allows to express actual and hypothetical situations in a uniform setting. The L entailment relation can formalize several forms of reasoning about actions and change. In this paper we illustrate a translation of L theories into Nested Abnormality Theories, a novel form of circumscription. The proof of soundness and completeness of the translation is the main technical result of the paper, but attention is also devoted to the features of Nested Abnormality Theories to capture commonsense reasoning in general and to clarify which assumptions a logical formalization forces upon a domain. These results also help clarifying the relationship between L and other recent circumscriptive formalizations for narratives, such as Miller and Shanahan’s. Content Areas Temporal Reasoning, Nonmonotonic Reasoning, Knowledge Representation.",
  "stance": 0.5
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1996/AAAI96-099.pdf",
  "title": "On the Range of Applicability of Baker’s Approach to the Frame Problem",
  "year": 1996,
  "venue": "AAAI",
  "abstract": "We investigate the range of applicability of Baker’s approach to the frame problem using an action language. We show that for temporal projection and deterministic domains, Baker’s approach gives the intuitively expected results. Introduction Baker’s circumscriptive approach to the frame problem (Baker 1991) h as b een employed in several studies of reasoning about action (for instance, (Lifschitz 1991; Kartha 1993; Shanahan 1995)) because of its simplicity and its ability to deal with domain constraints. However, it has recently been pointed out (Crawford & Etherington 1992; Kartha 1994) that Baker’s approach may sometimes lead to unintuitive conclusions. This paper addresses the following question: Under what circumstances does Baker’s approach give intuitively correct conclusions? Of course, the question as phrased above is imprecisewe have not characterized what we mean by the “intuitively correct” conclusions obtainable from the action domain under consideration. To do this, we follow the approach suggested in (Gelfond & Lifschitz 1993) of defining the syntax and semantics of an action language and identifying the “intuitively correct” conclusions with those entailed by the encoding of the action domain in the language. So the question we address becomes: Can we identify classes of domains in an action language that can be faithfully encoded using Baker’s approach? We answer this question in the affirmative as follows. First we define the syntax and semantics of an action language AR-. Then we identify two classes of domains expressible in this language, present a translation that employs Baker’s approach and prove that the translation is sound and complete for these classes. As will be seen, these two classes correspond to deterministic and temporal projection action domains. The rest of the paper is organized as follows. We first define the action language dRand present a brief review of Baker’s approach. We then give a translation from ARand characterize the two classes of domains 664 Knowledge Representation for which the translation is sound and complete. We conclude by discussing related work and the significance of the results presented in this paper. The Language ARIn this section, we define the syntax and semantics of an action language AR. The language ARis a subset of the language d7& introduced in (Kartha & Lifschitz 1994)-the latter has one additional kind of propositions called release propositions. Syntax of ARFormulae and Propositions To be precise, ARis not a single language, but rather a family of languages. A particular language in this group is characterized by e a nonempty set of symbols, that are called Auent names, or Auents, m a subset of fluent names, that is called the frame, o a nonempty set of symbols, that are called action names, or actions. A formula is a propositional combination of fluents. There are three types of propositions in AR--value propositions, effect propositions and constraints. A value proposition is an expression of the form C after A, 0) where C is a formula, and ‘7? is a string of actions. Informally, (1) asserts that C holds after the sequence of actions x is performed in the initial situation. For instance, OnRedBus after BuyTicket; GetOnBoard is a value proposition, where OnRedBus is a fluent and BuyTicket and GetOnBoard are actions. An effect proposition is an expression of the form A causes C if P, (2) where A is an action, and C and P are formulae. Intuitively, (2) asserts that A, if executed in a situation in which the precondition P is true, makes C true. For instance, BuyTicket causes HasTicket if -HasTicket From: AAAI-96 Proceedings. Copyright © 1996, AAAI (www.aaai.org). All rights reserved. is an effect proposition, where BuyTicket is an action, and HasTicket is a fluent. Finally, a constraint is a proposition of the form",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1996/AAAI96-118.pdf",
  "title": "Testing the Robustness of the Genetic Algorithm on the Floating Building Block Representation",
  "year": 1996,
  "venue": "AAAI",
  "abstract": "Recent studies on a floating building block representation for the genetic algorithm (GA) suggest that there are many advantages to using the floating representation. This paper investigates the behavior of the GA on floating representation problems in response to three different types of pressures: (1) a reduction in the amount of genetic material available to the GA during the problem solving process, (2) functions which have negative-valued building blocks, and (3) randomizing non-coding segments. Results indicate that the GA’s performance on floating representation problems is very robust. Significant reductions in genetic material (genome length) may be made with relatively small decrease in performance. The GA can effectively solve problems with negative building blocks. Randomizing non-coding segments appears to improve rather than harm GA performance. more fundamental AI problem is how ever more complex organisms can evolve, whether or not they are optimizers.",
  "stance": 0.6
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1996/AAAI96-168.pdf",
  "title": "A Bias towards Relevance: Recognizing Plans where Goal Minimization Fails",
  "year": 1996,
  "venue": "AAAI",
  "abstract": "Domains such as multiple trauma management, in which there are multiple interacting goals that change over time, are ones in which plan recognition’s standard inductive bias towards a single explanatory goal is inappropriate. In this paper we define and argue for an alternative bias based on identifying contextually “relevant” goals. We support this claim by showing how a complementary planning system in TraumAID 2.0, a decision-support system for the management of multiple trauma, allows us to define a four-level scale of relevance and therefore, of measurable deviations from relevance. This in turn allows definition of a bias towards relevance in the incremental recognition of physician plans by TraumAID’s critiquing interface, TraumaTIQ.",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1996/AAAI96-175.pdf",
  "title": "On the Size of Reactive Plans",
  "year": 1996,
  "venue": "AAAI",
  "abstract": "One of the most widespread approaches to reactive planning is Schoppers’ universal plans. We propose a stricter definition of universal plans which guarantees a weak notion of soundness not present in the original definition. Furthermore, we isolate three different types of completeness which capture different behaviours exhibited by universal plans. We show that universal plans which run in polynomial time and are of polynomial size cannot satisfy even the weakest type of completeness unless the polynomial hierarchy collapses. However, by relaxing either the polynomial time or the polynomial space requirement, the construction of universal plans satisfying the strongest type of completeness becomes trivial.",
  "stance": 0.5
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1996/AAAI96-194.pdf",
  "title": "A Counterexample to Theorems of Cox and Fine",
  "year": 1996,
  "venue": "AAAI",
  "abstract": "Cox’s well-known theorem justifying the use of probability is shown not to hold in finite domains. The counterexample also suggests that Cox’s assumptions are insufficient to prove the result even in infinite domains. The same counterexample is used to disprove a result of Fine on comparative conditional probability.",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1996/AAAI96-221.pdf",
  "title": "Symptom Management for Schizophrenic Agents",
  "year": 1996,
  "venue": "AAAI",
  "abstract": "Behavior-based paradigms are a promising avenue towards creating full-blown integrated autonomous agents. However, until now they have had a major stumbling block: programmers can create robust, subtle, and expressive behaviors, but the agent’s overall behavior gradually falls apart as these behaviors are combined. For small numbers of behaviors, this disintegration can be managed by the programmer, but as more behaviors are combined their interactions become so complex that they become at least time-consuming and at worst impossible to manage.",
  "stance": -1.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1998/AAAI98-042.pdf",
  "title": "The Branching Factor of Regular Search Spaces",
  "year": 1998,
  "venue": "AAAI",
  "abstract": "Many problems, such as the sliding-tile puzzles, generate search trees where different nodes have different numbers of children, in this case depending on the position of the blank. We show how to calculate the asymptotic branching factors of such problems, and how to efficiently compute the exact numbers of nodes at a given depth. This information is important for determining the complexity of various search algorithms on these problems. In addition to the sliding-tile puzzles, we also apply our technique to Rubik’s Cube. While our techniques are fairly straightforward, the literature is full of incorrect branching factors for these problems, and the errors in several incorrect methods are fairly subtle.",
  "stance": -1.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1998/AAAI98-078.pdf",
  "title": "Cooperating with People: The Intelligent Classroom",
  "year": 1998,
  "venue": "AAAI",
  "abstract": "People frequently complain that it is too difficult to figure out how to get computers to do what they want. However, with a computer system that actually tries to understand what its users are doing, people can interact in ways that are more natural to them. We have been developing a system, the Intelligent Classroom, that does exactly this. The Intelligent Classroom uses cameras and microphones to sense a speaker’s actions and then infers his intentions from those actions. Finally, it uses these intentions to decide what to do to best cooperate with the speaker. In the Intelligent Classroom, the speaker need not worry about how to operate the Classroom; he may simply go about his lecture and trust the Classroom to assist him at the appropriate moments.",
  "stance": 0.5
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1998/AAAI98-120.pdf",
  "title": "Experimenting with Power Default Reasoning",
  "year": 1998,
  "venue": "AAAI",
  "abstract": "In this paper we explore the computational aspects of Propositional Power Default Reasoning (PDR), a form of non-monotonic reasoning which the underlying logic is Kleene’s 3-valued propositional logic. PDR leads to a concise meaning of the problem of skeptical entailment which has better complexity characteristics than the usual formalisms (co-NP(3)-Complete instead [[p-Complete). We take advantage of this in an implementation called powder to encode and solve hard graph problems and explore randomly generated instances of skeptical entailment.",
  "stance": 0.0
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1998/AAAI98-168.pdf",
  "title": "Optimizing Initial Configurations of Neural Networks for the Task of Natural Language Learning",
  "year": 1998,
  "venue": "AAAI",
  "abstract": "One approach used to develop computer systems for natuidentifying phrases, e.g.<the boy> is a noun phrase; and (3) ral language processing (NLP) is that of Artificial Neural Netidentifying the relationships among phrases, e.g. <the boy> is works (NNs). Because of the large number of parameters a the agent of <runs>. This is similar to Jain (1991). NN has (e.g. network topology, learning algorithm, transfer The NN has 75 hidden nodes, divided into between 1 and functions) and the way they interact, finding optimal parame30 layers, as determined by the GA. The GA also determines ter values for any particular task can be extremely difficult. how these layers are connected to each other, and which Topology can greatly affect the performance of a NN. learning algorithm and transfer functions to use. Stolcke (1990) found that simple sentences could be proceThe NN is tested on a language of 508 sentences with three ssed by NNs with one hidden layer. Performance degraded, different complexity levels. The basic training set is 20% of however, when the NN was presented with embedded sententhe complete language. The GA makes three major decisions ces, for which more complex topologies were needed. What about training: (1) whether to train with sentences from all topology to use for any particular task is still an open quescomplexity levels at once, or with sentences from the simpler tion. Most decisions regarding NN topology are based on idelevel first and more complex sentences later, (2) whether to as of how the problem should be tackled. Jain (1991) used a train with sentences from all complexity levels in the same NN that first decomposed a sentence into phrases and later proportion, or with more sentences from one level or another; defined relationships between the phrases. Miikkulainen (3) whether the training set should be increased past the basic (1996) divided a NN into a parser, a segmenter, and a stack. 20% . If it is, the NN's fitness is decreased by a factor equal These NN vary in the number of layers, the number of nodes to the increase in the training set. in each layer, and how these layers are connected to each By studying the configurations chosen by the GA, I hope to other. identify which parameters are critical for the NLP task and Another aspect affecting performance is the corpus used to which values for these parameters produce the best perfortrain the NN. Nenov and Dyer (1994) found that training with mance. These results will provide a better understanding of individual words before showing sentences increased perforNN behavior and point to improved NN configurations. mance. Elman (1993) found that training a NN with simple sentences first and complex sentences later produced better results than presenting all sentences in one session. Although researchers have been successful in using NN for NLP, choosing an initial configuration is quite complex. An automated process that can find an optimal set of parameters would be helpful in designing such a system. In my research I use Genetic Algorithms (GAs) to find what NN parameter values produce better performance for a particular NL task. There is one input node for each word in the vocabulary. A sentence is presented to the NN one word at a time by activating the corresponding node at the input layer. The NN incrementally generates a description of the input by correctly identifying the parts of the sentence and how they relate to each other. For example, in the sentence <the boy runs> the NN should respond by (1) identifying each word in the sentence, e.g. <runs> is a movement verb; (2) References",
  "stance": 0.5
 },
 {
  "url": "https://www.aaai.org/Papers/AAAI/1998/AAAI98-180.pdf",
  "title": "A Script-Based Approach to Modifying Knowledge-Based Systems",
  "year": 1998,
  "venue": "AAAI",
  "abstract": "Modifying knowledge-based systems (KBSs) is a complex activity that needs to be performed very often. The occurrence of changes in a KBSs environment, requests for extending the system's functionality, and the debugging of inadequate knowledge are some of the events that demand modi cations to a KBSs. One of the di culties of KBS modi cations is that they might require the modi cation of several related portions of the system. Determining what portions have to be changed and how to change them requires a deep understanding of how the elements of the KBS interact. This requirement is especially hard for users when the rationale behind the design of a KBS or the details of its implementation are unknown.",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/P86-1017",
  "title": "Encoding and Acquiring Meanings for Figurative Phrases",
  "year": 1986,
  "venue": "ACL",
  "abstract": "1.1 The Task D o m a i n Here we address the problem of mapping phrase meanings into their conceptual representations. Figurative phrases are pervasive in human communication, yet they are difficult to explain theoretically. In fact, the ability to handle idiosyncratic behavior of phrases should be a criterion for any theory of lexical representation. Due to the huge number of such phrases in the English language, phrase representation must be amenable to parsing, generation, and also to learning. In this paper we demonstrate a semantic representation which facilitates, for a wide variety of phrases, both learning and parsing.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/P86-1033",
  "title": "Linguistic Coherence: A Plan-Based Alternative",
  "year": 1986,
  "venue": "ACL",
  "abstract": "To fully unders tand a sequence of u t terances , one must be able to infer implici t re la t ionships be tween the ut terances . Al though the ident i f ica t ion of sets of u t terance relat ionships forms the basis for many theories of discourse, the formal iza t ion and recognition of such relat ionships has proven to be an ext remely difficult computa t iona l task. This paper presents a plan-based approach to the representa t ion and recognit ion of implici t re la t ionships be tween ut terances. Relat ionships are formulated as discourse plans, which allows their representation in terms of planning opera tors and their computation via a plan recogni t ion process. By incorpora t ing complex inferent ia l processes relat ing u t te rances into a plan-based f ramework , a formal iza t ion and computability not available in the ear l ie r works is provided.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/P86-1039",
  "title": "What Should Machine Translation Be?",
  "year": 1986,
  "venue": "ACL",
  "abstract": "After a considerable hiatus of interest and funding, machine translation has come in recent years to occupy a significant place in the discipline of natural language processing. It has also become one of the most visible representations of natural language processing to the outside world. Machine translation systems are relatively unique with respect to the extent of the coverage they at tempt , and, correspondingly, the size of the grammatical and lexicaI corpora involved. Adding to this the complexity introduced by multiple language directions into the same system design (and the enormous procedural problems imposed by simultaneous development in several sites) gives some clue as to the opt imism which presently exists for machine translation.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P86-1040",
  "title": "Machine Translation will not Work",
  "year": 1986,
  "venue": "ACL",
  "abstract": "Large expenditures on fundamental scientific research are usually limited to the hard sciences. It is therefore entirely reasonable to suppose that, if large sums of money are spent on machine translations, it will be with the clear expectation that what is being purchased is principally development and engineering, and that the result will contribute substantially to the solution of some pressing problem.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P86-1041",
  "title": "Machine Translation already does Work",
  "year": 1986,
  "venue": "ACL",
  "abstract": "The first difficulty in answering a question like \"Does machine translation work is that the question itself is illposed. It takes for granted that there is one single thing called machine translation and that everyone is agreed about what it is. But in fact, even a cursory glance at the systems already around, either in regular operational use or under development, will reveal a wide range of different types of systems.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P87-1013",
  "title": "A Logical Version of Functional Grammar",
  "year": 1987,
  "venue": "ACL",
  "abstract": "Kay's functional-unification grammar notation [5] is a way of expressing grammars which relies on very few primitive notions. The primary syntactic structure is the feature structure, which can be visualised as a directed graph with arcs labeled by at t r ibutes of a constituent, and the primary structure-building operation is unification. In this paper we propose a mathematical formulation of FUG, using logic to give a precise account of the strings and the structures defined by any grammar written in this notation.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/P88-1017",
  "title": "Parsing Japanese Honorifics in Unification-Based Grammar",
  "year": 1988,
  "venue": "ACL",
  "abstract": "This paper presents a unification-based approach to Japanese honorifics based on a version of HPSG (Head-driven Phrase Structure Grammar)ll]121. Utterance parsing is based on lexical specifications of each lexical item, including honorifics, and a few general PSG rules using a parser capable of unifying cyclic feature structures. It is shown that the possible word orders of Japanese honori f ic predicate constituents can be automatically deduced in the proposed f ramework w i thou t independent ly specifying them. Discourse Information Change Rules (DICRs) that a l low resolving a class of anaphors in honorific contexts are also formulated.",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/P89-1024",
  "title": "A Hybrid Approach to Representation in the Janus Natural Language Processor",
  "year": 1989,
  "venue": "ACL",
  "abstract": "In BBN's natural language understanding and generation system (Janus), we have used a hybrid approach to representation, employing an intensional logic for the representation of the semantics of utterances and a taxonomic language with formal semantics for specification of descriptive constants and axioms relating them. Remarkably, 99.9% of 7,000 vocabulary items in our natural language applications could be adequately axiomatlzed in the taxonomic language.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/P89-1026",
  "title": "Two Constraints on Speech Act Ambiguity",
  "year": 1989,
  "venue": "ACL",
  "abstract": "Existing plan-based theories of speech act interpretation do not account for the conventional aspect of speech acts. We use patterns of linguistic features (e.g. mood, verb form, sentence adverbials, thematic roles) to suggest a range of speech act interpretations for the utterance. These are filtered using plan-bused conversational implicatures to eliminate inappropriate ones. Extended plan reasoning is available but not necessary for familiar forms. Taking speech act ambiguity seriously, with these two constraints, explains how \"Can you pass the salt?\" is a typical indirect request while \"Are you able to pass the salt?\" is not.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/P89-1027",
  "title": "Treatment of Long Distance Dependencies in LFG and TAG: Functional Uncertainty in LFG Is a Corollary in TAG",
  "year": 1989,
  "venue": "ACL",
  "abstract": "In this paper the functional uncertainty machinery in LFG is compared with the treatment of long distance dependencies in TAG. It is shown that the functional uncertainty machinery is redundant in TAG, i.e., what functional uncertainty accomplishes for LFG follows f~om the TAG formalism itself and some aspects of the linguistic theory instantiated in TAG. It is also shown that the analyses provided by the functional uncertainty machinery can be obtained without requiring power beyond mildly context-sensitive grammars. Some linguistic and computational aspects of these results have been briefly discussed also. 1 I N T R O D U C T I O N The so-called long distance dependencies are characterized in Lexical Functional Grammars (LFG) by the use of the formal device of functional uncertainty, as defined by Kaplan and Zaenan [3] and Kaplan and Maxwell [2]. In this paper, we relate this characterization to that provided by Tree ~,djoining Grammars (TAG), showing a direct correspondence between the functional uncertainty equations in LFG analyses and the elementary trees in TAGs that give analyses for \"long distance\" dependencies. We show that the functional uncertainty machinery is redundant in TAG, i.e., what functional uncertainty accomplishes for LFG follows from the TAG formalism itself and some fundamental aspects of the linguistic theory instantiated in TAG. We thus show that these analyses can be obtained without requiring power beyond mildly context-sensitive grammars. We also *Thi s work was par t ia l ly suppo r t ed (for the first author) by the D R R P A gran t N00014-85-K0018, A l tO gran t DAA29-84-9-0027, a n d NSF gran t IRI84-10413-A02. T h e first a u t h o r also benef i ted f rom some discuss ion wi th Mark Johnson a n d R on K a p l a n a t the Tit isee Workshop on Unif ication G r a m m a r s , March, 1988. briefly discuss the linguistic and computational significance of these results. Long distance phenomena are associated with the so-called movement. The following examples, 1. Mary Henry telephoned. 2. Mary Bill said that Henry telephoned. 3. Mary John claimed that Bill said that Henry telephoned. illustrate the long distance dependencies due to topicalization, where the verb telephoned and its object Mary can be arbitrarily apart. It is difficult to state generalizations about these phenomena if one relies entirely on the surface structure (as defined in CFG based frameworks) since these phenomena cannot be localized at this level. Kaplan and Zaenan [3] note that, in LFG, rather than stating the generalizations on the c-structure, they must be stated on f-structures, since long distance dependencies are predicate argument dependencies, and such functional dependencies are represented in the f-structures. Thus, as stated in [2, 3], in the sentences (1), (2), and (3) above, the dependencies are captured by the equations (in the LFG notation 1) by 1\" T O P I C =T OBJ, T T O P I C =T C O M P OBJ, and 1\" T O P I C =T C O M P C O M P OBJ, respectively, which state that. the topic Mary is also the object of tele. phoned. In general, since any number of additional complement predicates may be introduced, these equations will have the general form \"f T O P I C =T C O M P C O M P ... OBJ Kaplan and Zaenen [3] introduced the formal device of functional unc'ertainty, in which this general case is stated by the equation 1 Because of lack of space, we will no t define the LFG nota t ion . We a s s u m e tha t the reader is famil iar wi th it.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/P89-1031",
  "title": "Evaluating Discourse Processing Algorithms",
  "year": 1989,
  "venue": "ACL",
  "abstract": "In order to take steps towards establishing a methodology for evaluating Natural Language systems, we conducted a case study. We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues. We present the quantitative results of handsimulating these algorithms, but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general. We illustrate the general difficulties encountered with quantitative evaluation. These are problems with: (a) allowing for underlying assumptions, (b) determining how to handle underspecifications, and (c) evaluating the contribution of false positives and error chaining.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/P90-1006",
  "title": "Memory Capacity and Sentence Processing",
  "year": 1990,
  "venue": "ACL",
  "abstract": "The limited capacity of working memory is intrinsic to human sentence processing, and therefore must be addressed by any theory of human sentence processing. This paper gives a theory of garden-path effects and processing overload that is based on simple assumptions about human short term memory capacity.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/P91-1019",
  "title": "Subject-Dependent Co-Occurrence and Word Sense Disambiguation",
  "year": 1991,
  "venue": "ACL",
  "abstract": "We describe a method for obtaining subject-dependent word sets relative to some (subjecO domain. Using the subject classifications given in the machine-readable version of Longman's Dictionary of Contemporary English, we established subject-dependent cooccurrence links between words of the defining vocabulary to construct these \"neighborhoods\". Here, we describe the application of these neighborhoods to information retrieval, and present a method of word sense disambiguation based on these co-occurrences, an extension of previous work.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P91-1038",
  "title": "A Preference-first Language Processor: Integrating the Unification Grammar and Markov Language Model for Speech Recognition Applications",
  "year": 1991,
  "venue": "ACL",
  "abstract": "A language processor is to find out a most promising sentence hypothesis for a given word lattice obtained from acoustic signal recognition. In this paper a new language processor is proposed, in which unification granunar and Markov language model are integrated in a word lattice parsing algorithm based on an augmented chart, and the island-driven parsing concept is combined with various preference-first parsing strategies defined by different construction principles and decision rules. Test results\"show that significant improvements in both correct rate of recognition and computation speed can be achieved .",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P91-1049",
  "title": "Non-Literal Word Sense Identification Through Semantic Network Path Schemata",
  "year": 1991,
  "venue": "ACL",
  "abstract": "When computer programs disambiguate words in a sentence, they often encounter non-literal or novel usages not included in their lexicon. In a recent study, Georgia Green (personal communication) estimated that 17% to 20% of the content word senses encountered in various types of normal English text are not fisted in the dictionary. While these novel word senses are generally valid, they occur in such great numbers, and with such little individual frequency that it is impractical to explic i ty include them all within the lexicon. Instead, mechanisms are needed which can derive novel senses from existing ones; thus allowing a program to recognize a significant set of potential word senses while keeping its lexicon within a reasonable size.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P92-1017",
  "title": "Inside-Outside Reestimation From Partially Bracketed Corpora",
  "year": 1992,
  "venue": "ACL",
  "abstract": "1. M O T I V A T I O N The most successful stochastic language models have been based on finite-state descriptions such as n-grams or hidden Markov models (HMMs) (Jelinek et al., 1992). However, finite-state models cannot represent the hierarchical structure of natural language and are thus ill-suited to tasks in which that structure is essential, such as language understanding or translation. It is then natural to consider stochastic versions of more powerful grammar formalisms and their grammatical inference problems. For instance, Baker (1979) generalized the parameter estimation methods for HMMs to stochastic context-free grammars (SCFGs) (Booth, 1969) as the inside-outside algorithm. Unfortunately, the application of SCFGs and the original inside-outside algorithm to natural-language modeling has been so far inconclusive (Lari and Young, 1990; Jelinek et al., 1990; Lari and Young, 1991). Several reasons can be adduced for the difficulties. First, each iteration of the inside-outside algorithm on a grammar with n nonterminals may require O(n3[wl 3) time per training sentence w, 128 while each iteration of its finite-state counterpart training an HMM with s states requires at worst O(s2lwl) time per training sentence. That complexity makes the training of suffÉciently large grammars computationally impractical. Second, the convergence properties of the algorithm sharply deteriorate as the number of nonterminal symbols increases. This fact can be intuitively understood by observing that the algorithm searches for the maximum of a function whose number of local maxima grows with the number of nonterminals. Finally, while SCFGs do provide a hierarchical model of the language, that structure is undetermined by raw text and only by chance will the inferred grammar agree with qualitative linguistic judgments of sentence structure. For example, since in English texts pronouns are very likely to immediately precede a verb, a grammar inferred from raw text will tend to make a constituent of a subject pronoun and the following verb. We describe here an extension of the inside-outside algorithm that infers the parameters of a stochastic context-free grammar from a partially parsed corpus, thus providing a tighter connection between the hierarchical structure of the inferred SCFG and that of the training corpus. The algorithm takes advantage of whatever constituent information is provided by the training corpus bracketing, ranging from a complete constituent analysis of the training sentences to the unparsed corpus used for the original inside-outside algorithm. In the latter case, the new algorithm reduces to the original one. Using a partially parsed corpus has several advantages. First, the the result grammars yield constituent boundaries that cannot be inferred from raw text. In addition, the number of iterations needed to reach a good grammar can be reduced; in extreme cases, a good solution is found from parsed text but not from raw text. Finally, the new algorithm has bet ter t ime complexity when sufficient bracketing information is provided. 2 . P A R T I A L L Y B R A C K E T E D T E X T Informally, a partially bracketed corpus is a set of sentences annota ted with parentheses marking constituent boundaries that any analysis of the corpus should respect. More precisely, we s tar t f rom a corpus C consisting of bracketed strings, which are pairs e = (w,B) where w is a string and B is a bracketing of w. For convenience, we will define the length of the bracketed string c by Icl = Iwl. Given a string w = wl ..-WlM, a span of w is a pair of integers ( i , j ) with 0 < i < j g [w[, which delimits a substring iwj = wi+y . . .wj of w. The abbreviation iw will s tand for iWl~ I. A bracketing B of a string w is a finite set of spans on w ( that is, a finite set of pairs or integers (i, j ) with 0 g i < j < [w[) satisfying a consistency condition tha t ensures that each span (i, j ) can be seen as delimiting a string iwj consisting of a sequence of one of more. The consistency condition is simply tha t no two spans in a bracketing may overlap, where two spans (i, j ) and (k, l) overlap if either i < k < j < l or k < i < l < j . Two bracketings of the same string are said to be compatible if their union is consistent. A span s is valid for a bracketing B if {s} is compatible with B. Note that there is no requirement tha t a bracketing of w describe fully a constituent s t ructure of w. In fact, some or all sentences in a corpus may have empty bracketings, in which case the new algori thm behaves like the original one. To present the notion of compatibil i ty between a derivation and a bracketed string, we need first to define the span of a symbol occurrence in a context-free derivation. Let (w,B) be a bracketed string, and c~0 ==~ a l :=¢, . . . =~ c~m = w be a derivation of w for (S)CFG G. The span of a symbol occurrence in (~1 is defined inductively as follows: • I f j -m, c U = w E E*, and the span of wi in ~j is ( i 1, i). • I f j < m, then a j : flAT, aj+l = /3XI\"'Xk')', where A -* X I \" . X k is a rule of G. Then the span of A in a j is ( i l , j k ) , where for each 1 < l < k, (iz,jt) is the span of Xl in a j+ l The spans in (~j of the symbol occurrences in/3 and 7 are the same as those of the corresponding symbols in ~ j+ l . A derivation of w is then compat ible with a bracketing B of w if the span of every symbol occurrence in the derivation is valid in B. 3 . G R A M M A R R E E S T I M A T I O N The inside-outside algori thm (Baker, 1979) is a reest imation procedure for the rule probabilit ies of a Chomsky normal-form (CNF) SCFG. It takes as inputs an initial CNF SCFG and a training corpus of sentences and it i teratively reest imates rule probabilities to maximize the probabil i ty tha t the g rammar used as a stochastic generator would produce the corpus. A reestimation algori thm can be used both to refine the parameter es t imates for a CNF SCFG derived by other means (Fujisaki et hi., 1989) or to infer a g rammar from scratch. In the lat ter case, the initial g r ammar for the inside-outside algor i thm consists of all possible CNF rules over given sets N of nonterrninals and E of terminals, with suitably assigned nonzero probabilities. In what follows, we will take N, ~ as fixed, n IN[, t = [El, and assume enumerat ions N {A1, . . . ,An} and E = {h i , . . . ,bt}, with A1 the g r a m m a r s ta r t symbol. A CNF SCFG over N , E can then be specified by the n~+ nt probabilities Bp,q,r of each possible binary rule Ap --* Aq Ar and Up,m of each possible unary rule Ap --* bin. Since for each p the parameters Bp,q,r and Up,rn are supposed to be the probabilities of different ways of expanding Ap, we must have for all 1 _< p _< n E Bp,q,r + E Up,m = 1 (7)",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P92-1023",
  "title": "GPSM: A Generalized Probabilistic Semantic Model for Ambiguity Resolution",
  "year": 1992,
  "venue": "ACL",
  "abstract": "In natural language processing, ambiguity resolution is a central issue, and can be regarded as a preference assignment problem. In this paper, a Generalized Probabilistic Semantic Model (GPSM) is proposed for preference computation. An effective semantic tagging procedure is proposed for tagging semantic features. A semantic score function is derived based on a score function, which integrates lexical, syntactic and semantic preference under a uniform formulation. The semantic score measure shows substantial improvement in structural disambiguation over a syntax-based approach.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P92-1032",
  "title": "Estimating Upper and Lower Bounds on the Performance of Word-Sense Disambiguation Programs",
  "year": 1992,
  "venue": "ACL",
  "abstract": "We have recently reported on two new word-sense disambiguation systems, one trained on bilingual material (the Canadian Hansards) and the other trained on monolingual material (Roget's Thesaurus and Grolier's Encyclopedia). After using both the monolingual and bilingual classifiers for a few months, we have convinced ourselves that the performance is remarkably good. Nevertheless, we would really like to be able to make a stronger statement, and therefore, we decided to try to develop some more objective evaluation measures. Although there has been a fair amount of literature on sense-disambiguation, the literature does not offer much guidance in how we might establish the success or failure of a proposed solution such as the two systems mentioned in the previous paragraph. Many papers avoid quantitative evaluations altogether, because it is so difficult to come up with credible estimates of performance. This paper will attempt to establish upper and lower bounds on the level of performance that can be expected in an evaluation. An estimate of the lower bound of 75% (averaged over ambiguous types) is obtained by measuring the performance produced by a baseline system that ignores context and simply assigns the most likely sense in all cases. An estimate of the upper bound is obtained by assuming that our ability to measure performance is largely limited by our ability obtain reliable judgments from human informants. Not surprisingly, the upper bound is very dependent on the instructions given to the judges. Jorgensen, for example, suspected that lexicographers tend to depend too much on judgments by a single informant and found considerable variation over judgments (only 68% agreement), as she had suspected. In our own experiments, we have set out to find word-sense disambiguation tasks where the judges can agree often enough so that we could show that they were outperforming the baseline system. Under quite different conditions, we have found 96.8% agreement over judges.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P92-1036",
  "title": "Some Problematic Cases of VP Ellipsis",
  "year": 1992,
  "venue": "ACL",
  "abstract": "It has been widely assumed that VP ellipsis is governed by an identity condition: the elided VP is interpreted as an identical copy of another expression in surrounding discourse. For example, Sag (76) imposes an identity condition on Logical Form representations of VP's. A basic feature of this account is the requirement that a syntactic VP be available as the antecedent. This requirement is reflected in most subsequent accounts as well. In this paper I examine three cases of VP ellipsis in which the antecedent cannot be identified with any VP. These cases, which are illustrated using naturallyoccurring examples, present a fundamental problem for any of the standard approaches. I will argue that they receive a natural treatment in the system I have developed, in which VP ellipsis is treated by storing VP meanings in a discourse model.",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/P92-1039",
  "title": "Right Association Revisited",
  "year": 1992,
  "venue": "ACL",
  "abstract": "Consideration of when Right Association works and when it fails lead to a restatement of this parsing principle in terms of the notion of heaviness. A computational investigation of a syntactically annotated corpus provides evidence for this proposal and suggest circumstances when RA is likely to make correct attachment predictions.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P92-1041",
  "title": "Incremental Dependency Parsing",
  "year": 1992,
  "venue": "ACL",
  "abstract": "The paper introduces a dependency-based grammar and the associated parser and focusses on the problem of determinism in parsing and recovery from errors. First, it is shown how dependency-based parsing can be afforded, by taking into account the suggestions coming from other approaches, and the preference criteria for parsing are briefly addressed. Second, the issues of the interconnection between the syntactic analysis and the semantic interpretation in incremental processing are discussed and the adoption of a TMS for the recovery of the processing errors is suggested. T H E B A S I C P A R S I N G A L G O R I T H M The parser has been devised for a system that works on the Italian language. The structure that results from the parsing process is a dependency tree, that exhibits syntactic and semantic information. The dependency structure: The structure combines the traditional view of dependency syntax with the feature terms of the unification based formalisms (Shieber 86): single attributes (like number or tense) appear inside the nodes of the tree, while complex attributes (like grammatical relations) are realized as relations between nodes. The choice of a dependency structure, which is very suitable for free word order languages (Sgall et al. 86), reflects the intuitive idea of a language with few constraints on the order of legal constructions. Actually, the flexibility of a partially configurational language like Italian (that can be considered at an intermediate level between the totally configurational languages like English and the totally inflected free-ordered Slavonic languages) can be accounted for with a relaxation of the strong constraints posed by a constituency grammar (Stock 1989) or by constraining to a certain level a dependency grammar. Cases of topicalization, like un dolce di frutta ha ordinato il maestro a cake with fruits has ordered the teacher and in general all the five permutations of the \"basic\" (i.e. more likely) SVO structure of the sentence are so common in Italian, that it seems much more economical to express the syntactic knowledge in terms of dependency relations. Every node in the structure is associated with a word in the sentence, in such a way that the relation between two nodes at any level is of a head&modifier type. The whole sentence has a head, namely the verb, and its roles (the subj is included) are its modifiers. Every modifier in turn has a head (a noun, which can be a proper, common or pro-noun, for participants not marked by a preposition, a preposition, or a verb, in case of subordinate sentences not preceded by a conjunction) and further modifiers. Hence the dependency tree gives an immediate representation of the thematic structure of the sentence, thus being very suitable for the semantic interpretation. Such a structure also allows the application of the rules, based on grammatical relations, that govern complex syntactic phenomena, as revealed by the extensive work on Relational Grammar. The dependency grammar is expressed declaratively via two tables, that represent the relations of immediate dominance and linear order for pairs of categories. The constraints on the order between a head and one of its modifiers and between two modifiers of the same head are reflected by the nodes in the dependency structure. The formation of the complex structure that is associated with the nodes is accomplished by means of unification: the basic terms are originated by the lexicon and associated with the nodes. There exist principles that govern the propagation of the features in the dependency tree expressed as analogous conventions to GPSG ones. The incremental parser: In the system, the semantic, as well as the contextual and the anaphoric binding analysis, is interleaved with the syntactic parsing. The analysis is incremental, in the sense that it is carried out in a piecemeal strategy, by taking care of partial results too. In order to accomplish the incremental parsing and to build a dependency representation of the sentence, the linguistic knowledge of the two tables is",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/P93-1007",
  "title": "A Speech-First Model for Repair Detection and Correction",
  "year": 1993,
  "venue": "ACL",
  "abstract": "Interpreting fully natural speech is an important goal for spoken language understanding systems. However, while corpus studies have shown that about 10% of spontaneous utterances contain self-corrections, or REPAIRS, little is known about the extent to which cues in the speech signal may facilitate repair processing. We identify several cues based on acoustic and prosodic analysis of repairs in a corpus of spontaneous speech, and propose methods for exploiting these cues to detect and correct repairs. We test our acoustic-prosodic cues with other lexical cues to repair identification and find that precision rates of 89-93% and recall of 78-83% can be achieved, depending upon the cues employed, from a prosodically labeled corpus.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/P93-1025",
  "title": "Transfers of Meaning",
  "year": 1993,
  "venue": "ACL",
  "abstract": "In one form or another, the phenomena associated with \"meaning transfer\" have become central issues in a lot of recent work on semantics. Speaking very roughly, we can partition approaches to the phenomenon along two dimensions, which yield four basic points of departure. In the first two, people have considered transfer in basically semantic or linguistic terms. Some have concentrated on what we might call the paradigmatic aspects of transfer, focusing on the productive lexical processes that map semantic features into features for example, the \"grinding\" rule that applies to turn the names of animals into mass terms denoting their meat or fur. This the approach that's involved in most recent work on \"regular polysemy,\" \"systematic polysemy,\" and the like, for example by Apresjan, Ostler and Atkins, Briscoe and Copestake, Nunberg and Zaenen, Wilensky, Kilgarriff and a number of other people. Other people have emphasized the syncategorematic aspects of transfer; that is, the ways meaning shifts and specifications are coerced in the course of semantic composition. This is an approach that hass been developed in particular by James Pustejovsky and his collaborators, building on earlier work on type shifting. As opposed to these, there are conceptual and pragmatic approaches to transfer, which focus on the extralinguistic circumstances that license transfers of various types. Here again there are both paradigmatic and syncategorematic approaches, loosely speaking. The first is exemplified in a lot of recent work on metaphor by people associated with the \"cognitive linguistics\" school, which has focused chiefly on the relations between domains of experience that metaphor variously exploits and imputes. The second is represented by work on indirect speech within Gricean pragmatics, Relevance Theory, and the like, which has been chiefly concerned with specifying the conversational conditions that give rise to metaphor, irony, and analogous phenomena. Of course this categorization is somewhat factitious. The borders between these approaches are highly porous, and most work on transfer overlaps several of them. This is entirely appropriate, since these are in no sense competing theories or accounts of the phenomena. Transfer is clearly a linguistic process, and in many of its most important forms a lexical one. But it just as clearly has its basis in very general cognitive and communicative principles. And while it's reasonable that people should choose to focus on one or another of these considerations relative to their immediate interests, it is also useful to keep the Big Picture in mind, lest we inadvertently ascribe to one domain of explanation a responsibility that more properly belongs to another. This is the picture I want to sketch out in this talk. A comprehensive account of transfer has to make appeal to three different kinds of regularities or rules. The first are nonlinguistic: the correspondences between domains, real or imputed, that transfer invokes, and the communicative interests that may make these invocations useful or instructive they enable us to identify one thing in virtue of its relation to another, explain an abstract domain by reference to a concrete one, and so forth. Second, there is the repertory of general linguistic processes of transfer that exploit these correspondences and principles. By these I have in mind not traditional categories like metaphor, synecdoche, and m e t o n y m y distinctions that have basically to do with the kinds of domain correspondences that transfer exploits but the various types of operations that make possible typeshifting and sortal reassignment of expressions, syntactic recategorizations, and deferred indexical reference. These processes may cross-cut the types of domain correspondences that they exploit, and I'll show that we often find a single type of domain correspondence underlying two or more distinct semantic processes of transfer. Third, there are the language-specific instantiations of these operations, for example in the form of constructions or lexical rules that license particular types or",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/P94-1005",
  "title": "From Strings to Trees to Strings to Trees ... (Abstract)",
  "year": 1994,
  "venue": "ACL",
  "abstract": "A certain amount of structure is necessary simply because a clause may embed another clause, or one clause may attach to another clause or parts of it. Leaving this need of structure aside, the question then is how much structure should a (minimal) clause have? Grammar formalisms can differ significantly on this issue. Minimal clauses can be just strings, or words linked by dependencies (dependency trees), or with rich phrase structure trees, or with flat (one level) phrase structure trees (almost strings) and so on. How much hierarchical structure is needed for a minimal clause is still an open question, that is being debated heatedly. How are clauses put together? Are these operations more like string manipulations (concatenation, insertion, or wrapping, for example) or are they more like tree transformations (generalized transformations of the early transformational grammars, for example)? Curiously, the early transformational grammars, although clearly using tree transformations, actually formulated the transformations as pseudo string-like operations! More recent non-transformational grammars differ significantly with respect to their use of string rewriting or tree rewriting operations. Grammar formalisms differ with respect to their stringiness or treeness. Also during their evolution, they have gone back and forth between string-like and tree-like representations, often combining them in different ways. These swings are a reflection of the complex interplay between aspects of language structure such as constituency, dependency, dominance, locality of predicates and their arguments, adjacency, order, and discontinuity. We will discuss these issues in an informal manner, in the context of a range of formalisms.",
  "stance": -0.1
 },
 {
  "url": "https://aclanthology.org/P94-1034",
  "title": "An Automatic Treebank Conversion Algorithm for Corpus Sharing",
  "year": 1994,
  "venue": "ACL",
  "abstract": "An automatic treebank conversion method is proposed in this paper to convert a treebank into another treebank. A new treebank associated with a different grammar can be generated automatically from the old one such that the information in the original treebank can be transformed to the new one and be shared among different research communities. The simple algorithm achieves conversion accuracy of 96.4% when tested on 8,867 sentences between two major grammar revisions of a large MT system.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/P94-1038",
  "title": "Similarity-Based Estimation of Word Cooccurrence Probabilities",
  "year": 1994,
  "venue": "ACL",
  "abstract": "In many applications of natural language processing it is necessary to determine the likelihood of a given word combination. For example, a speech recognizer may need to determine which of the two word combinations \"eat a peach\" and \"eat a beach\" is more likely. Statistical NLP methods determine the likelihood of a word combination according to its frequency in a training corpus. However, the nature of language is such that many word combinations are infrequent and do not occur in a given corpus. In this work we propose a method for est imat ing the probabili ty of such previously unseen word combinations using available information on \"most similar\" words. We describe a probabilistic word association model based on distributional word similarity, and apply it to improving probabil i ty estimates for unseen word bigrams in a variant of Katz 's back-off model. The similarity-based method yields a 20% perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P95-1012",
  "title": "Compiling HPSG type constraints into definite clause programs",
  "year": 1995,
  "venue": "ACL",
  "abstract": "We present a new approach to HPSG processing: compiling HPSG grammars expressed as type constraints into definite clause programs. This provides a clear and computationally useful correspondence between linguistic theories and their implementation. The compiler performs offline constraint inheritance and code optimization. As a result, we are able to efficiently process with HPSG grammars without haviog to hand-translate them into definite clause or phrase structure based systems. 1 I n t r o d u c t i o n The HPSG architecture as defined in (Pollard and Sag, 1994) (henceforth HPSGII) is being used by an increasing number of linguists, since the formally well-defined framework allows for a rigid and explicit formalization of a linguistic theory. At the same time, the feature logics which provide the formal foundation of HPSGII have been used as basis for several NLP systems, such as ALE (Carpenter, 1993), CUF (DSrre and Dorna, 1993), Troll (Gerdemann and King, 1993) or TFS (Emele and Zajac, 1990). These systems are at least partly intended as computational environments for the implementation of HPSG grammars. HPSG linguists use the description language of the logic to express their theories in the form of implicative constraints. On the other hand, most of the computational setups only allow feature descriptions as extra constraints with a phrase structure or definite clause based language. 1 From a computational point of view the latter setup has several advantages. It provides access to the pool of work done in the *The authors are listed alphabetically. 1One exception is the TFS system. However, the possibility to express recursive relations on the level of the description language leads to serious control problems in that system. area of natural language processing, e.g., to efficient control strategies for the definite clause level based on tabelling methods like Earley deduction, or different parsing strategies in the phrase structure setup. The result is a gap between the description language theories of HPSG linguists and the definite clause or phrase structure based NLP systems provided to implement these theories. Most grammars currently implemented therefore have no clear correspondence to the linguistic theories they originated from. To be able to use implemented grammars to provide feedback for a rigid and complete formalization of linguistic theories, a clear and computationMly useful correspondence has to be established. This link is also needed to stimulate further development of the computational systems. Finally, an HPSGII style setup is also interesting to model from a software engineering point of view, since it permits a modular development and testing of the grammar. The purpose of this paper is to provide the desired link, i.e., to show how a HPSG theory formulated as implicative constraints can be modelled on the level of the relational extension of the constraint language. More specifically, we define a compilation procedure which translates the type constraints of the linguistic theory into definite clauses runnable in systems such as Troll, ALE, or CUF. Thus, we perform constraint inheritance and code optimization off-line. This results in a considerable efficiency gain over a direct on-line treatment of type constraints as, e.g., in TFS. The structure of the paper is as follows: A short discussion of the logical setup for HPSGII provides the necessary formal background and terminology. Then the two possibilities for expressing a theory using the description language as in HPSGII or the relational level as in the computational architectures are introduced. The third section provides a simple picture of how HPSGII theories can be modelled on the relational level. This simple picture is then refined in the fourth section, where the compilation procedure and its implementation is discussed. A small example grammar is provided in the appendix.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P95-1020",
  "title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances",
  "year": 1995,
  "venue": "ACL",
  "abstract": "Drawing appropriate defeasible inferences has been proven to be one of the most pervasive puzzles of natural language processing and a recurrent problem in pragmatics. This paper provides a theoretical framework, called stratified logic, that can accommodate defeasible pragmatic inferences. The framework yields an algorithm that computes the conversational, conventional, scalar, clausal, and normal state implicatures; and the presuppositions that are associated with utterances. The algorithm applies equally to simple and complex utterances and sequences of utterances. 1 Pragmatics and Defeasibility It is widely acknowledged that a full account of natural language utterances cannot be given in terms of only syntactic or semantic phenomena. For example, Hirschberg (1985) has shown that in order to understand a scalar implicature, one must analyze the conversants' beliefs and intentions. To recognize normal state implicatures one must consider mutual beliefs and plans (Green, 1990). To understand conversationM implicatures associated with indirect replies one must consider discourse expectations, discourse plans, and discourse relations (Green, 1992; Green and Carberry, 1994). Some presuppositions are inferrable when certain lexical constructs (factives, aspectuals, etc) or syntactic constructs (cleft and pseudo-cleft sentences) are used. Despite all the complexities that individualize the recognition stage for each of these inferences, all of them can be defeated by context, by knowledge, beliefs, or plans of the agents that constitute part of the context, or by other pragmatic rules. Defeasibili~y is a notion that is tricky to deal with, and scholars in logics and pragmatics have learned to circumvent it or live with it. The first observers of the phenomenon preferred to keep defeasibility outside the mathematical world. For Frege (1892), Russell (1905), and Quine (1949) \"everything exists\"; therefore, in their logical systems, it is impossible to formalize the cancellation of the presupposition that definite referents exist (Hirst, 1991; Marcu and Hirst, 1994). We can taxonomize previous approaches to defea~ible pragmatic inferences into three categories (we omit here work on defeasibility related to linguistic phenomena such as discourse, anaphora, or speech acts). 1. Most linguistic approaches account for the defeasibility of pragmatic inferences by analyzing them in a context that consists of all or some of the previous utterances, including the current one. Context (Karttunen, 1974; Kay, 1992), procedural rules (Gazdar, 1979; Karttunen and Peters, 1979), lexical and syntactic structure (Weischedel, 1979), intentions (Hirschberg, 1985), or anaphoric constraints (Sandt, 1992; Zeevat, 1992) decide what presuppositions or implicatures are projected as pragmatic inferences for the utterance that is analyzed. The problem with these approaches is that they assign a dual life to pragmatic inferences: in the initial stage, as members of a simple or complex utterance, they are defeasible. However, after that utterance is analyzed, there is no possibility left of cancelling that inference. But it is natural to have implicatures and presuppositions that are inferred and cancelled as a sequence of utterances proceeds: research in conversation repairs (I-Iirst et M., 1994) abounds in such examples. We address this issue in more detail in section 3.3. 2. One way of accounting for cancellations that occur later in the analyzed text is simply to extend the boundaries within which pragmatic inferences are evaluated, i.e., to look ahead a few utterances. Green (1992) assumes that implicatures are connected to discourse entities and not to utterances, but her approach still does not allow cancellations across discourse units. 3. Another way of allowing pragmatic inferences to be cancelled is to assign them the status of defeasible information. Mercer (1987) formalizes pre-",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/P95-1040",
  "title": "The Effect of Pitch Accenting on Pronoun Referent Resolution",
  "year": 1995,
  "venue": "ACL",
  "abstract": "By strictest interpretation, theories of both centering and intonational meaning fail to predict the existence of pitch accented pronominals. Yet they occur felicitously in spoken discourse. To explain this, I emphasize the dual functions served by pitch accents, as markers of both propositional (semantic/pragmatic) and attentional salience. This distinction underlies my proposals about the attentional consequences of pitch accents when applied to pronominals, in particular, that while most pitch accents may weaken or reinforce a cospecifier's status as the center of attention, a contrastively stressed pronominal may force a shift, even when contraindicated by textual features. I n t r o d u c t i o n To predict and track the center of attention in discourse, theories of centering (Grosz et al., 1983; Brennan et al., 1987; Grosz et al., 1989) and immediate focus (Sidner, 1986) rely on syntactic and grammatical features of the text such as pronominalization and surface sentence position. This may be sufficient for written discourse. For oral discourse, however, we must also consider the way intonation affects the interpretation of a sentence, especially the cases in which it alters the predictions of centering theories. I investigate this via a phenomenon that, by the strictest interpretation of either centering or intonation theories, should not occur the case of pitch accented pronominals. Centering theories would be hard pressed to predict pitch accents on pronominals, on grounds of redundancy. To bestow an intonational marker of salience (the pitch accent) on a textual marker of salience (the pronominal) is unnecessarily redundant and especially when textual features correctly predict the focus of attention. Intonational theories would be similarly hard pressed, but on grounds of information quality and efficient use of limited resources. Given the serial and ephemeral nature of speech and the limits of working memory, it is most expedient to mark as salient the information-rich nonpronominals, rather than their semantically impoverished pronominal stand-ins. To do otherwise is an injudicious use of an attentional cue. However, when uttered with contrastive stress on the pronouns, (I) John introduced Bill as a psycholinguist and then HE insulted HIM. (after Lakoff, 1971) is felicitously understood to mean that after a slanderous introduction, Bill retaliated in kind against John. What makes (1) felicitous is tha t the pitch accents on the pronominals contribute at tentional information that cannot be gleaned from text alone. This suggests an attentional component to pitch accents, in addition to the propositional component explicated in Pierrehumbert and Hirschberg (1990). In this paper, I combine their account of pitch accent semantics with Grosz, Joshi and Weinstein's (1989) account of centering to yield insights into the phenomenon of pitch accented pronominals, and the attentional consequences of pitch accents in general. The relevant claims in PH90 and GJW89 are reviewed in the next two sections. P i t c h a c c e n t s e m a n t i c s A pitch accent is a distinctive intonational contour applied to a word to convey sentential stress (Bolinger, 1958; Pierrehumbert , 1980). PH90 catalogues six pitch accents, all combinations of high (H) and low (L) pitch targets, and structured as a main tone and an optional leading or trailing tone. The form of the accent L, H, L+H or H +L informs about the operation that would relate the salient i tem to the mutual beliefs 1 of the conversants; the main tone either commits (H*) or fails to commit 1 Mutual beliefs: propositions expressed or implied by the discourse, and which all conversants believe each other to accept as true and relevant same (Clark and Marshall, 1981).",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/P95-1046",
  "title": "Knowledge-based Automatic Topic Identification",
  "year": 1995,
  "venue": "ACL",
  "abstract": "As the first step in an automated text summarization algorithm, this work presents a new method for automatically identifying the central ideas in a text based on a knowledge-based concept counting paradigm. To represent and generalize concepts, we use the hierarchical concept taxonomy WordNet. By setting appropriate cutoff values for such parameters as concept generality and child-to-parent frequency ratio, we control the amount and level of generality of concepts extracted from the text. 1 1 I n t r o d u c t i o n As the amount of text available online keeps growing, it becomes increasingly difficult for people to keep track of and locate the information of interest to them. To remedy the problem of information overload, a robust and automated text summarizer or information extrator is needed. Topic identification is one of two very impor tant steps in the process of summarizing a text; the second step is summary text generation. A topic is a particular subject that we write about or discuss. (Sinclair et al., 1987). To identify the topics of texts, Information Retrieval (IR) researchers use word frequency, cue word, location, and title-keyword techniques (Paice, 1990). Among these techniques, only word frequency counting can be used robustly across different domains; the other techniques rely on stereotypical text structure or the functional structures of specific domains. Underlying the use of word frequency is the assumption that the more a word is used in a text, the more impor tant it is in that text. This method 1This research was funded in part by ARPA under order number 8073, issued as Maryland Procurement Contract # MDA904-91-C-5224 and in part by the National Science Foundation Grant No. MIP 8902426. recognizes only the literal word forms and nothing else. Some morphological processing may help, but pronominalization and other forms of coreferentiality defeat simple word counting. Furthermore, straightforward word counting can be misleading since it misses conceptual generalizations. For example: \"John bought some vegetables, fruit, bread, and milk.\" What would be the topic of this sentence? We can draw no conclusion by using word counting method; where the topic actually should be: \"John bought some groceries.\" The problem is that word counting method misses the impor tant concepts behind those words: vegetables, fruit, etc. relates to groceries at the deeper level of semantics. In recognizing the inherent problem of the word counting method, recently people have started to use artificial intelligence techniques (Jacobs and ttau, 1990; Mauldin, 1991) and statistical techniques (Salton et al., 1994; Grefenstette, 1994) to incorporate the sementic relations among words into their applications. Following this trend, we have developed a new way to identify topics by counting concepts instead of words. 2 T h e P o w e r o f G e n e r a l i z a t i o n In order to count concept frequency, we employ a concept generalization taxonomy. Figure 1 shows a possible hierarchy for the concept digital computer. According to this hierarchy, if we find iaptop and hand-held computer, in a text, we can infer that the text is about portable computers, which is their parent concept. And if in addition, the text also mentions workstation and mainframe, it is reasonable to say that the topic of the text is related to digital computer. Using a hierarchy, the question is now how to find the most appropriate generalization. Clearly we cannot just use the leaf concepts since at this level we have gained no power from generalization. On the other hand, neither can we use the very top concept everything is a thing. We need a method of identifying the most appropriate concepts somewhere in middle of the taxonomy. Our current solution uses",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/P96-1006",
  "title": "Integrating Multiple Knowledge Sources to Disambiguate Word Sense: An Exemplar-Based Approach",
  "year": 1996,
  "venue": "ACL",
  "abstract": "In this paper, we present a new approach for word sense disambiguation (WSD) using an exemplar-based learning algorithm. This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech of neighboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation. We tested our WSD program, named LEXAS, on both a common data set used in previous work, as well as on a large sense-tagged corpus that we separately constructed. LEXAS achieves a higher accuracy on the common data set, and performs better than the most frequent heuristic on the highly ambiguous words in the large corpus tagged with the refined senses of WoRDNET.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P96-1052",
  "title": "Towards Testing the Syntax of Punctuation",
  "year": 1996,
  "venue": "ACL",
  "abstract": "Little work has been done in NLP on the subject of punctuation, owing mainly to a lack of a good theory on which computational treatments could be based. This paper described early work in progress to try to construct such a theory. Two approaches to finding the syntactic function of punctuation marks are discussed, and procedures are described by which the results from these approaches can be tested and evaluated both against each other as well as against other work. Suggestions are made for the use of these results, and for future work. 1 B a c k g r o u n d The field of punctuation has been almost completely ignored within Natural Language Processing, with perhaps the exception of the sentence-final full-stop (period). This is because there is no coherent theory of punctuation on which a computational treatment could be based. As a result, most contemporary systems simply strip out punctuation in input text, and do not put any marks into generated texts. Intuitively, this seems very wrong, since punctuation is such an integral part of many written languages. If text in the real world (a newspaper, for example) were to appear without any punctuation marks, it would appear very stilted, ambiguous or infantile. Therefore it is likely that any computational system that ignores these extra textual cues will suffer a degradation in performance, or at the very least a great restriction in the class of linguistic data it is able to process. Several studies have already shown the potential for using punctuation within NLP. Dale (1991) has * This work was carried out under an award from the (UK) ESRC. Thanks are also due to Lex Holt, Henry Thompson, Ted Briscoe and anonymous reviewers. shown the benefits of using punctuation in the fields of discourse structure and semantics, and Jones (1994) has shown in the field of syntax that using a grammar that includes punctuation yields around two orders of magnitude fewer parses than one which does not. Further work has been carried out in this area, particularly by Briscoe and Carroll (1995), to show more accurately the contribution that usage of punctuation can make to the syntactic analysis of text. The main problem with these studies is that there is little available in terms of a theory of punctuation on which computational treatments could be based, and so they have somewhat ad hoc, idiosyncratic treatments. The only account of punctuation available is that of Nunberg (1990), which although it provides a useful basis for a theory is a little too vague to be used as the basis of any implementation. Therefore it seems necessary to develop a new theory of punctuation, that is suitable for computational implementation. Some work has already been carried out, showing the variety of punctuation marks and their orthographic interaction (Jones, 1995), but this paper describes the continuation of this research to determine the true syntactic function of punctuation marks in text. There are two possible angles to the problem of the syntactic function of punctuation: an observational one, and a theoretical one. Both approaches were adopted, in order to be be able to evaluate the results of each approach against those of the other, and in the hope that the results of both approaches could be combined. Thus the approaches are described one after the other here. 2 C o r p u s b a s e d A p p r o a c h The best data source for observation of grammatical punctuation usage is a large, parsed corpus. It ensures a wide range of real language is covered, and because of its size it should minimise the effect of any",
  "stance": -0.3
 },
 {
  "url": "https://aclanthology.org/P97-1002",
  "title": "Fast Context-Free Parsing Requires Fast Boolean Matrix Multiplication",
  "year": 1997,
  "venue": "ACL",
  "abstract": "Valiant showed that Boolean matrix multiplication (BMM) can be used for CFG parsing. We prove a dual result: CFG parsers running in time O([Gl[w[ 3-e) on a grammar G and a string w can be used to multiply m x m Boolean matrices in time O(m3-e/3). In the process we also provide a formal definition of parsing motivated by an informal notion due to Lang. Our result establishes one of the first limitations on general CFG parsing: a fast, practical CFG parser would yield a fast, practical BMM algorithm, which is not believed to exist. 1 I n t r o d u c t i o n The context-free grammar (CFG) formalism was developed during the birth of the field of computational linguistics. The standard methods for CFG parsing are the CKY algorithm (Kasami, 1965; Younger, 1967) and Earley's algorithm (Earley, 1970), both of which have a worst-case running time of O(gN 3) for a CFG (in Chomsky normal form) of size g and a string of length N. Graham et al. (1980) give a variant of Earley's algorithm which runs in time O(gN3/log N). Valiant's parsing method is the asymptotically fastest known (Valiant, 1975). It uses Boolean matrix multiplication (BMM) to speed up the dynamic programming in the CKY algorithm: its worst-case running time is O(gM(N)), where M(rn) is the time it takes to multiply two m x m Boolean matrices together. The standard method for multiplying matrices takes time O(m3). There exist matrix multiplication algorithms with time complexity O(m3-J); for instance, Strassen's has a worstcase running time of O(m 2\"sl) (Strassen, 1969), and the fastest currently known has a worst-case running time of O(m 2\"376) (Coppersmith and Winograd, 1990). Unfortunately, the constants involved are so large that these fast algorithms (with the possible exception of Strassen's) cannot be used in practice. As matrix multiplication is a very well-studied problem (see Strassen's historical account (Strassen, 1990, section 10)), it is highly unlikely that simple, practical fast matrix multiplication algorithms exist. Since the best BMM algorithms all rely on general matrix multiplication 1, it is widely believed that there are no practical O(m 3-~) BMM algorithms. One might therefore hope to find a way to speed up CFG parsing without relying on matrix multiplication. However, we show in this paper that fast CFG parsing requires fast Boolean matrix multiplication in a precise sense: any parser running in time O(gN 3-e) that represents parse data in a retrieval-efficient way can be converted with little computational overhead into a O(m 3-e/3) BMM algorithm. Since it is very improbable that practical fast matrix multiplication algorithms exist, we thus establish one of the first nontrivial limitations on practical CFG parsing. 1The \"four Russians\" algorithm (Arlazarov et al., 1970), the fastest BMM algorithm that does not simply use ordinary matrix multiplication, has worst-case running time O(mS/log m). Our technique, adapted from that used by Sat ta (1994) for tree-adjoining grammar (TAG) parsing, is to show that BMM can be efficiently reduced to CFG parsing. Satta's result does not apply to CFG parsing, since it explicitly relies on the properties of TAGs that allow them to generate non-context-free languages. 2 D e f i n i t i o n s A Boolean matrix is a matrix with entries from the set {0, 1}. A Boolean matrix multiplication algorithm takes as input two m x m Boolean matrices A and B and returns their Boolean product A x B , which is the m × m Boolean matrix C whose entries c~j are defined by",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/P97-1003",
  "title": "Three Generative, Lexicalised Models for Statistical Parsing",
  "year": 1997,
  "venue": "ACL",
  "abstract": "In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P97-1011",
  "title": "Learning Features that Predict Cue Usage",
  "year": 1997,
  "venue": "ACL",
  "abstract": "Our goal is to identify the features that predict the occurrence and placement of discourse cues in tutorial explanations in order to aid in the automatic generation of explanations. Previous at tempts to devise rules for text generation were based on intuition or small numbers of constructed examples. We apply a machine learning program, C4.5, to induce decision trees for cue occurrence and placement from a corpus of data coded for a variety of features previously thought to affect cue usage. Our experiments enable us to identify the features with most predictive power, and show that machine learning can be used to induce decision trees useful for text generation. 1 I n t r o d u c t i o n Discourse cues are words or phrases, such as because, first, and although, that mark structural and semantic relationships between discourse entities. They play a crucial role in many discourse processing tasks, including plan recognition (Litman and Allen, 1987), text comprehension (Cohen, 1984; Hobbs, 1985; Mann and Thompson, 1986; Reichman-Adar, 1984), and anaphora resolution (Grosz and Sidner, 1986). Moreover, research in reading comprehension indicates that felicitous use of cues improves comprehension and recall (Goldman, 1988), but that their indiscriminate use may have detrimental effects on recall (Millis, Graesser, and Haberlandt, 1993). Our goal is to identify general strategies for cue usage that can be implemented for automatic text generation. From the generation perspective, cue usage consists of three distinct, but interrelated problems: (1) occurrence: whether or not to include a cue in the generated text, (2) placement: where the cue should be placed in the text, and (3) selection: what lexical item(s) should be used. Prior work in text generation has focused on cue selection (McKeown and Elhadad, 1991; Elhadad and McKeown, 1990), or on the relation between *Learning Research & Development Center tComputer Science Department, and Learning Research ~z Development Center tlntelllgent Systems Program cue occurrence and placement and specific rhetorical structures (RSsner and Stede, 1992; Scott and de Souza, 1990; Vander Linden and Martin, 1995). Other hypotheses about cue usage derive from work on discourse coherence and structure. Previous research (Hobbs, 1985; Grosz and Sidner, 1986; Schiffrin, 1987; Mann and Thompson, 1988; Elhadad and McKeown, 1990), which has been largely descriptive, suggests factors such as structural features of the discourse (e.g., level of embedding and segment complexity), intentional and informational relations in that structure, ordering of relata, and syntactic form of discourse constituents. Moser and Moore (1995; 1997) coded a corpus of naturally occurring tutorial explanations for the range of features identified in prior work. Because they were also interested in the contrast between occurrence and non-occurrence of cues, they exhaustively coded for all of the factors thought to contribute to cue usage in all of the text. From their study, Moscr and Moore identified several interesting correlations between particular features and specific aspects of cue usage, and were able to test specific hypotheses from the hterature that were based on constructed examples. In this paper, we focus on cue occurrence and placement, and present an empirical study of the hypotheses provided by previous research, which have never been systematically evaluated with naturally occurring data. Wc use a machine learning program, C4.5 (Quinlan, 1993), on the tagged corpus of Moser and Moore to induce decision trees. The number of coded features and their interactions makes the manual construction of rules that predict cue occurrence and placement an intractable task. Our results largely confirm the suggestions from the hterature, and clarify them by highhghting the most influential features for a particular task. Discourse structure, in terms of both segment structure and levels of embedding, affects cue occurrence the most; intentional relations also play an important role. For cue placement, the most important factors are syntactic structure and segment complexity. The paper is organized as follows. In Section 2 we discuss previous research in more detail. Section 3 provides an overview of Moser and Moore's coding scheme. In Section 4 we present our learning experiments, and in Section 5 we discuss our results and conclude.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/P97-1037",
  "title": "A DP-based Search Using Monotone Alignments in Statistical Translation",
  "year": 1997,
  "venue": "ACL",
  "abstract": "In this paper, we describe a Dynamic Programming (DP) based search algorithm for statistical translation and present experimental results. The statistical translation uses two sources of information: a translation model and a language model. The language model used is a standard bigram model. For the translation lnodel, the alignment probabilities are made dependent on the differences in the alignment positions rather than on the absolute positions. Thus, the approach amounts to a first-order Hidden Markov model (HMM) as they are used successfully in speech recognition for the time alignment problem. Under the assumption that the alignment is monotone with respect to the word order in both languages, an efficient search strategy for translation can be formulated. The details of the search algorithm are described. Experiments on the EuTrans corpus produced a word error rate of 5.1(/~.. 1 O v e r v i e w : T h e S t a t i s t i c a l A p p r o a c h t o T r a n s l a t i o n The goal is the translation of a text given in some source language into a target language. We are given o J a source ( 'French') string fl = fl . . . fj . . . f . l , which is to be translated into a target ( 'English') string c~ = el...ei...el. Among all possible target strings, we will choose the one with the highest probability which is given by Bayes' decision rule (Brown e t al.. 1993): ,~ = argmax{P,'(e]~lfg~)} = argmax {P, ' (ef) . Pr(.f / lef)} Pr(e{) is the language model of the target language. whereas Pr(j'lale{) is the string translation model. The argmax operation denotes the search problem. In this paper, we address • the problem of introducing structures into the probabilistic dependencies in order to model the string translation probability Pr( f ] [e~). • the search procedure, i.e. an algorithm to perform the argmax operation in an efficient way. • transformation steps for both the source and the target languages in order to improve the translation process. The transformations are very much dependent on the language pair and the specific translation task and are therefore discussed in the context of the task description. We have to keep in mind that in the search procedure both the language and the translation model are applied after the text transformation steps. However, to keep the notation simple we will not make this explicit distinction in the subsequent exposition. The overall architecture of the statistical translation approach is summarized in Figure 1. 2 A l i g m n e n t M o d e l s A key issue in modeling the string translation probability Pr( f ( l e I) is the question of how we define the correspondence between the words of the target sentence and the words of the source sentence. In typical cases, we can assume a sort of pairwise dependence by considering all word pairs (f j ,ei) for a given sentence pair [f( ; el]. We further constrain this model by assigning each source word to exactly one target word. Models describing these types of dependencies are referred to as alignrnen.t models (Brown e t al., 1993), (Dagan e ta] . . 1993). (Kay & R6scheisen, 1993). (Fung & Church. 1994), (Vogel et al., 1996). In this section, we introduce a monotoue HMM based alignment and an associated DP based search algorithm for translation. Another approach to statistical machine translation using DP was presented in (Wu, 1996). The notational convention will be a,s follows. We use the symbol Pr(.) to denote general",
  "stance": 0.4
 },
 {
  "url": "https://aclanthology.org/P97-1061",
  "title": "Retrieving Collocations by Co-Occurrences and Word Order Constraints",
  "year": 1997,
  "venue": "ACL",
  "abstract": "In this paper, we describe a method for automatically retrieving collocations from large text corpora. This method retrieve collocations in the following stages: 1) extracting strings of characters as units of collocations 2) extracting recurrent combinations of strings in accordance with their word order in a corpus as collocations. Through the method, various range of collocations, especially domain specific collocations, are retrieved. The method is practical because it uses plain texts without any information dependent on a language such as lexical knowledge and parts of speech.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/P99-1008",
  "title": "Finding Parts in Very Large Corpora",
  "year": 1999,
  "venue": "ACL",
  "abstract": "We present a method for extracting parts of objects from wholes (e.g. \"speedometer\" from \"car\"). Given a very large corpus our method finds part words with 55% accuracy for the top 50 words as ranked by the system. The part list could be scanned by an end-user and added to an existing ontology (such as WordNet), or used as a part of a rough semantic lexicon.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/P99-1014",
  "title": "Inducing a Semantically Annotated Lexicon via EM-Based Clustering",
  "year": 1999,
  "venue": "ACL",
  "abstract": "We present a technique for automatic induction of slot annotations for subcategorization frames, based on induction of hidden classes in the EM framework of statistical estimation. The models are empirically evalutated by a general decision test. Induction of slot labeling for subcategorization frames is accomplished by a further application of EM, and applied experimentally on frame observations derived from parsing large corpora. We outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries.",
  "stance": 0.4
 },
 {
  "url": "https://aclanthology.org/P99-1043",
  "title": "Mixed Language Query Disambiguation",
  "year": 1999,
  "venue": "ACL",
  "abstract": "We propose a mixed language query disambiguation approach by using co-occurrence information from monolingual data only. A mixed language query consists of words in a primary language and a secondary language. Our method translates the query into monolingual queries in either language. Two novel features for disambiguation, namely contextual word voting and 1-best contextual word, are introduced and compared to a baseline feature, the nearest neighbor. Average query translation accuracy for the two features are 81.37% and 83.72%, compared to the baseline accuracy",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/P99-1047",
  "title": "A Decision-Based Approach to Rhetorical Parsing",
  "year": 1999,
  "venue": "ACL",
  "abstract": "We present a shift-reduce rhetorical parsing algorithm that learns to construct rhetorical structures of texts from a corpus of discourse-parse action sequences. The algorithm exploits robust lexical, syntactic, and semantic knowledge sources.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/P99-1053",
  "title": "A Syntactic Framework for Speech Repairs and Other Disruptions",
  "year": 1999,
  "venue": "ACL",
  "abstract": "This paper presents a grammatical and processing framework for handling the repairs, hesitations, and other interruptions in natural human dialog. The proposed framework has proved adequate for a collection of human-human task-oriented dialogs, both in a full manual examination of the corpus, and in tests with a parser capable of parsing some of that corpus. This parser can also correct a pre-parser speech repair identifier resulting in a 4.8% increase in recall.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P99-1057",
  "title": "Learning to Recognize Tables in Free Text",
  "year": 1999,
  "venue": "ACL",
  "abstract": "Many real-world texts contain tables. In order to process these texts correctly and extract the information contained within the tables, it is important to identify the presence and structure of tables. In this paper, we present a new approach that learns to recognize tables in free text, including the boundary, rows and columns of tables. When tested on Wall Street Journal news documents, our learning approach outperforms a deterministic table recognition algorithm that identifies tables based on a fixed set of conditions. Our learning approach is also more flexible and easily adaptable to texts in different domains with different table characteristics.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P99-1063",
  "title": "Lexical Semantics to Disambiguate Polysemous Phenomena of Japanese Adnominal Constituents",
  "year": 1999,
  "venue": "ACL",
  "abstract": "We exploit and extend the Generative Lexicon Theory to develop a formal description of adnominal constituents in a lexicon which can deal with linguistic phenomena found in Japanese adnominal constituents. We classify the problematic behavior into \"static disambiguation\" and \"dynamic disambiguation\" tasks. Static disambiguation can be done using lexical information in a dictionary, whereas dynamic disambiguation requires inferences at the knowledge representation level. 1 I n t r o d u c t i o n Natural language processing must disambiguate polysemous constituents in the input sentences. A good description of information necessary for disambiguation in the lexicon is crucial in high quality NLP systems. This paper discusses the treatment of linguistic phenomena in Japanese adnominM constituents and it focuses on how to generate the same semantic representation from different syntactic structures, and how to generate different semantic representations from a semantically ambiguous sentence. We exploit and extend the Generative Lexicon Theory (Pustejovsky, 1995; Bouillon, 1996) to develop a formal description of adnominal constituents in a lexicon which can offer a solution to these problems. We classify the problematic behavior of Japanese adnominal constituents into \"static disambiguation\" and \"dynamic disambiguation\" tasks. Whereas static disambiguation can be done using the lexical information in a dictionary, dynamic disambiguation needs inferences at the knowledge representation level. This paper mainly discusses dynamic disambiguation. 2 C l a s s i f i c a t i o n o f t h e U s a g e o f J a p a n e s e A d n o m i n a l C o n s t i t u e n t s On consideration of the syntactic relations between adnominal constituents and their head nouns, we find that some adnominal constituents can appear both in the attributive and predicative positions (Sakuma, 1967; Martin, 1975; Makino and Tsutsui, 1986). However, some adjectives express different meanings when they appear in one or the other position and some adjectives can appear only in one of these two positions (Hashimoto and Aoyama, 1992). We have classified the semantic relations between adnominal constituents and their modified nouns, based on whether the paraphrasing from attributive position to predicative position is possible or not. There are three possibilities: (Type A ) A paraphrase can be made without changing the modifying relations semantically. Ad. + N , N $~ (ga) Ad. (N is Ad.) Ad. = Adnominal constituent N = Head noun of noun phrase which is modified by Ad. (Type B) A paraphrase can be made only when a noun is restricted by its context: the presence of modifiers or determiners, e.g., articles. A d . + N --* ~:¢)(sono) N F~ (wa) Ad. (that N is Ad.) (Type C) A paraphrase cannot be made at all, i.e., only the attributive position is available.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/J86-1003",
  "title": "Is MT Linguistics?",
  "year": 1986,
  "venue": "CL",
  "abstract": "Reading Jonathan Slocum's survey article of machine translation in Computational Linguisties, Volume 11, No. 1, some points struck me. Slocum's paper aims to be an overview of the state of the art in MT. As such it is good work. But some methodological points of view could have been considered more accurately; for example, the relation of MT and the computational linguistic paradigm to linguistics (seen as a study of the nature of human language), and the notion \"fully automatic high quality translation\". It was a stunning surprise to learn that translation as a profession and discipline is so underweighted in the U.S.A. This, coupled with the state of linguistic theory and the computational devices at hand, surely explains almost in itself the failure of early attempts made in machine translation in the 1950s and 1960s in the U.S.A. If one's premises are bad, one's methods will not be any better and results still worse. And this really seems to be the situation in the history of MT (cf. Zarachnak 1979). The second point I want to address is Slocum's argumentation in favour of level of automaticity and quality of current MT systems. As Slocum puts it, it goes something like this: human translation is a many-step process, where it is not unusual for products to revised many times. Thus far we agree. Everyone with some experience in translation knows this. And this is no wonder, considering that linguistic competence is open, without definite limits. One never \"learns\" language totally, competence is never \"perfect\" whether we speak about mother tongue or some foreign language (cf, Sampson 1980, 1983). But Slocum takes this step-by-step revision style of human translation to mean at least it seems so to me that we actually already have fully automatic high quality MT systems in some sense. Following is a crucial passage in Slocum's argumentation (p. 2)",
  "stance": -0.3
 },
 {
  "url": "https://aclanthology.org/J86-1012",
  "title": "Abstracts of Current Literature",
  "year": 1986,
  "venue": "CL",
  "abstract": "S OF CURRENT LITERATURE Copies of the technical reports abstracted below are available from",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J88-3011",
  "title": "User Models and Discourse Models: United They Stand . . .",
  "year": 1988,
  "venue": "CL",
  "abstract": "Opinions on the relationship between discourse models (DMs) and user models (UMs) are obviously influenced by preassumptions about their respective contents. As far as DMs are concerned, two divergent views have been expressed in the discussion published here: I. The DM contains only representations of the objects mentioned so far in the discourse (i.e., a mentioned-object memory--see Schuster, this issue). The term \"object\" will be used here in the broad sense of Schuster, thus also denoting events, properties, etc. 2. The DM contains in addition a. a representation of the purpose underlying the segments of the dialog (i.e. a dialog purpose--see Grosz Sidner 1986, Chin, this issue). b. an attentional structure, which is a subset of the representations mentioned in (1) containing the currently focused objects which are ordered in a focus stack (Cohen, this issue; Chin, this issue, who requires only that the user must be familiar with these objects). Less disagreement seems to exist about the components of a UM. Generally, it is regarded as containing explicit representations of the system's assumptions about all relevant aspects of the user, i.e., assumptions about his/ her \"objective situation\" (e.g., marital status, number of children), as well as about his/her prior knowledge, goals, plans and false beliefs with respect to the domain of discourse. In order to meet Wahlster's personneldatabase counterexample, it must be further required that the user model be separable by the system from the rest of the system's knowledge. To discuss the relationship between DMs and UMs, a general belief, goal, and plan maintenance system (BGP-MS) will be presented here, the purpose of which is to store and update the beliefs, goals, and plans of both the system and an arbitrary number of other agents, including the system's current user. Specific subcomponents and subfunctions of this system hopefully capture the general consensus on what constitutes a discourse model and a user model, respectively. However, we will see that these subcomponents are strongly interwoven and that--apart from a few rarely occurring exceptions--the DM is part of the UM at least at the level of content. The question arises then, of course, whether it makes sense to separate these notions conceptually. The belief, goal, and plan maintenance system outlined here is being implemented (in a somewhat simplified form) in XTRA, a natural language access system to expert systems (Allgayer et al. 1988). A previous implementation was VIE-DPM (Kobsa 1985a,b). In the knowledge base of BGP-MS, the representation of the various types of (nested) beliefs and goals (Kobsa 1988) is separated into a number of hierarchically ordered partitions (see Figure 1). If it is shared knowledge between S and U that U possesses certain beliefs (knowledge), then this knowledge or these beliefs are represented in MB(UB). 1 MB(UW) contains those goals and plans of the user, MB(SB) those beliefs of the system, and MB(SW) those goals of the system for which the same holds true. \"Private\" beliefs of the system about the domain of discourse / about the user's beliefs / about the user's beliefs about the system's goals are represented in SB, SBUB, and SBUBSW, respectively. MB contains the mutual beliefs (knowledge) with respect to the domain, and MW the mutual goals and plans of S and U. The arrows between the partitions denote inheritance relationships. In the partitions of BGP-MS, the content of the individual beliefs, goals, and plans can be expressed through arbitrary representational structures (e.g., a KL-ONElike representation as used in XTRA). Various markers for non-belief and uncertainty can be added: For instance, in SBUB it can be expressed, among other",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J88-3012",
  "title": "Discourse Models, Dialog Memories, and User Models",
  "year": 1988,
  "venue": "CL",
  "abstract": "In this paper, we discuss some terminological issues related to the notions of discourse models, dialog memories, and user models. It is not our goal to show how discourse modeling and user modeling should actually interact in a cooperat ive system, but to show how the notions of discourse model, dialog memory, and user model can be defined and related in order to prevent misunderstandings and confusion. We argue that dialog memory may be subsumed under user model, as well as under discourse model, but that the three concepts should not be identified. Several separating criteria are discussed. We conclude that discourse modeling and user modeling are two lines of research that are orthogonal to each other.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/J88-3014",
  "title": "Distinguishing User Models From Discourse Models",
  "year": 1988,
  "venue": "CL",
  "abstract": "Of course, the interpretation of these positions depends on the definition of the terms involved and the underlying notion of the \"part-of\", \"intersect\", and \"distinct\" relations. The relationships cannot simply be interpreted in a set-theoretic sense, since all definitions for UMs and DMs proposed so far depend not only on representation structures, but also on processes used for the construction, maintenance, and exploitation of these structures. Since this is a terminological, and not an empirical, discussion, as I pointed out in Wahlster (1986), P1-P3 are primarily normative statements. So, P3, for instance, must be interpreted as \"The terms UM and DM should be defined in such a way, that they do not overlap\". This view seems not to be shared by all participants in the discussion. Schuster, for example, tries to prove her position (PI) in a set-theoretic sense. First, she argues that \"the user model contains information that does not appear in the discourse model\" and then she \"proves\" that \"any information in the discourse model is also in the user model\". I disagree not only with the form, but also with the content of Schuster's argumentation. She writes \"only if the discourse model is part of the user model can the system take it into account in its responses and its reasoning about the users\". By considering an isomorphic argumentation like \"only if a tomato is part of cheese, can one use it to prepare pizza\" it becomes clear that this proof is flawed. Also, Morik points out correctly that if one follows Schuster's argumentation one should \"view the grammar as part of the user model, because the grammar is necessary for understanding and producing utterances\". Today, it is a standard hypothesis in AI and computational linguistics that models for the language understanding and generation process must exploit various knowledge sources, including in many cases a DM and a UM. For example, in Jameson and Wahlster (1982) we described the NP generator of the HAM-ANS system, in which the generation of a definite or indefinite description was influenced both by the UM and the DM. But this in no way means that one must be included in the other. As long as there is no definitive evidence (e.g., from psychology or the neurosciences) for a particular structure, content, and use (or even existence) of UMs and DMs in the human information processing system, in AI the notions of UM and DM are concepts that help on the one hand to construct a theory of natural language dialog behavior, and on the other hand to structure the software systems that realize natural language systems. From the second point of view, which is the engineering perspective, the question of whether P1, P2, or P3 holds, is easy to decide so far. In most of the implemented systems the data structures and procedures labeled UM and DM are completely distinct. Even the recent GUMS package (Finin 1988), a general user modeling component, contains no specific representation structures or processes for discourse modeling. Since the discussion above suggests that we view the relation between the UM and the DM mainly as a terminological problem, in the next section we focus on possible definitions for UMs and DMs. Although often terminological discussions become quite tedious, at this point it seems to be important to define these concepts as precisely as possible, since many researchers are discovering interesting relationships between discourse and user models.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J90-2008",
  "title": "Briefly Noted",
  "year": 1990,
  "venue": "CL",
  "abstract": "indexed under \"Pr~iferenzsemantik,\" the other under \"preferences.\" A work as big as this is sure to contain something to offend everyone. Some of the articles, rather than being well-fitting pieces in structure, are perhaps a little too idiosyncratic; for example, Allen's lengthy complaints about the paucity of computer-assisted stylistic studies of Spanish text. And some authors are more adept than others at making their topic comprehensible to the newcomer; the same article by Allen is particularly reader-friendly. But despite the handbook's size, what I noticed most were the omissions. The short article on computer-assisted language teaching does little justice to current research into the application of sophisticated CL methods to the problem. I could find nothing on dealing with ill-formed input (or did the index let me down?). Transformational g rammar is mentioned a number of times, but government-binding theory is not. Research in discourse structure is hardly mentioned, except, unexpectedly, in the article on language generation. No form of the term \"anaphora\" appears in the index (though there is at least a passing mention of the problem (p. 270) in Lenders's introduction to the sections on processing). But such complaints should not be allowed to obscure the wealth that can be found in this handbook. I just wish that it were a little easier to find what one is looking for.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J90-3001",
  "title": "Resolving Quasi Logical Forms",
  "year": 1990,
  "venue": "CL",
  "abstract": "The paper describes intermediate and resolved logical form representations of sentences involving referring expressions and a reference resolution process for mapping between these representations. The intermediate representation, Quasi Logical Form (or QLF), may contain unresolved terms corresponding to anaphoric noun phrases covering bound variable anaphora, reflexives, and definite descriptions. Implict relations arising in constructs such as compound nominals appear in QLF as unresolved formulae. The QLF representation is also neutral with respect to ambiguities corresponding to quantifier scope and the collective/distributive distinction, the latter being treated as quantifier resolution. Reference candidates are proposed according to an ordered set of \"reference resolution rules\" producing possible resolved logical forms to which linguistic and pragmatic constraints are then applied.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/J90-4008",
  "title": "Briefly Noted",
  "year": 1990,
  "venue": "CL",
  "abstract": "M. Martin Taylor obtained his B.A.Sc. in engineering physics at the University of Toronto, his M.S.E. in industrial engineering at the Johns Hopkins University, and his Ph.D. in psychology at the Johns Hopkins University. He holds the position of Senior Experimental Psychologist at the Defence and Civil Institute of Environmental Medicine in Toronto. Insup Taylor obtained her B.A. at Seoul National University, and her M.A. and Ph.D. at the Johns Hopkins University, all in psychology. She is a Research Fellow at the McLuhan Centre for Culture and Technology, University of Toronto. They are the co-authors of The psychology of reading (Academic Press, 1983) and Psycholinguistics: Learning and using language (Prentice-Hall, 1990). Martin Taylor's address is: DCIEM, P.O. Box 2000, North York, Ontario, Canada M3M 3B9: Insup Taylor's address is: McLuhan Program, University of Toronto, 39A Queen's Park Crescent East, Toronto, Ontario, Canada M5S1A1. E-mail for both authors: mmt@ben.dciem.dnd.ca",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J91-1003",
  "title": "met*: A Method for Discriminating Metonymy and Metaphor by Computer",
  "year": 1991,
  "venue": "CL",
  "abstract": "The met* method distinguishes selected examples of metonymy from metaphor and from literalness and anomaly in short English sentences. In the met* method, literalness is distinguished because it satisfies contextual constraints that the nonliteral others all violate. Metonymy is discriminated from metaphor and anomaly in a way that [1] supports Lakoff and Johnson's (1980) view that in metonymy one entity stands for another whereas in metaphor one entity is viewed as another, [2] permits chains of metonymies (Reddy 1979), and [3] allows metonymies to co-occur with instances of either literalness, metaphor, or anomaly. Metaphor is distinguished from anomaly because the former contains a relevant analogy, unlike the latter. The met* method is part of Collative Semantics, a semantics for natural language processing, and has been implemented in a computer program called meta5. Some examples of meta5's analysis of metaphor and metonymy are given. The met* method is compared with approaches from artificial intelligence, linguistics, philosophy, and psychology.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J91-1007",
  "title": "A Computational Model of Metaphor Interpretation",
  "year": 1991,
  "venue": "CL",
  "abstract": "Section 6 (\"On Translation\") consists of a single paper by van der Korst in which he presents a FG MT system. The principles of the system are very simple. The predication underlying a linguistic expression is language-neutral; therefore the theory provides a ready-made interlingua. If it is possible to parse and to generate, then it is also possible to translate. Of course, this is an oversimplification, since different languages typically use different subsets of the set of possible predicates. Thus, paraphrasing relations between predications are necessary. Van der Korst provides a lot of useful examples to illustrate the problems and achievements of his system. The verdict: This is an important book, since it begins to sketch what a computational version of FG might look like. It is very important for people working in functional paradigms such as FG to bring their insights about language use to the design of NLP systems, which will have real users. However, the book is ultimately disappointing for a number of reasons. It has the feel of a collection of disparate papers that are united in their debt to Dik (1978) rather than by their participation in a coherent research program. The papers are inadequately cross-referenced and display many needless inconsistencies of style (e.g., \"PROLOG\" vs. \"Prolog'; endnotes vs. footnotes). The papers build very few bridges between computational FG and what is going on in the rest of NLP. As we have noted, some of the attempts to do so misfire. Perhaps most disappointing of all, the volume fails to raise what ought to be the most interesting question: what, if any, are the distinctive benefits of functional theories such as FG for NLP?",
  "stance": -0.3
 },
 {
  "url": "https://aclanthology.org/J91-2008",
  "title": "Antilinguistics: A Critical Assessment of Modern Linguistic Theory and Practice",
  "year": 1991,
  "venue": "CL",
  "abstract": "One danger attendant upon reviewing a book as bad as this knuckle-headed assault on linguists and all they stand for is that one may inadvertently suggest by the length and vehemence of one's commentary that the book is worthy of extended discussion. Another is that by quoting the most strikingly absurd parts of the book the review might give an exaggerated sense of its entertainment value (this is not really a funny book, despite a few howlers). Yet perhaps a brief comment is called for, lest silence imply assent. I would not dissent from the claim that there is plenty to be criticized in the present state of linguistics. The unjustified pretentions of much generative grammar and the inadequacies of some of its woeful attempts at description merit harsh criticism-maybe even as harsh as the sort of ridicule and cruelty that I dispense elsewhere (Pullum 1991). But there is no utility in a purported challenge to a whole discipline written by someone who deals in naive panaceas and seems unable to understand the primary literature. The production of this book is bad enough to make one suspect an amateur job: the page layout is often bad, the style is often awkward, and there are signs of carelessness (for example, beyond p. 180 every page number the index gives is incorrect). But the content is far worse. Sophomoric blunders abound: underlying constituent order confused with what speakers \"start with ... in their heads\" and surface structure with \"what they actually say\" (p. 30); innate general language acquisition mechanisms confused with innate rules for English grammar (p. 46); uncomprehended formulae botched (the arrowless \"transformation\" on p. 23, for example); well-known names are wrong (\"Deidre Wilson,\" pp. 28, 275; \"James Fodor,\" p. 195n; etc.); in a book on this level it is no surprise to find the old chestnut about the Eskimos' many words for snow turning up yet again (p. 221). Gethin argues that modern linguistics is a jumble of cabalistic nonsense, and can be swept away by the simple truth that the key to everything is meaning. Language acquisition is no puzzle: people learn meanings \"by observation and imitation,\" and they \"join individual meanings together so that they make larger meanings\" (p. 9). That is all one needs to know about language; but modern linguists are afflicted with a \"systematizing mania that pretends to discover new profundity in what everybody knows already\" (pp. 11-12); they fail to see that \"there is no such thing as structure in language\" (p. 93). \"There is no mystery\" (p. 108), there is only meaning. The targets of the book's ad hominem attacks include not only (of course) Noam Chomsky, but also an odd assortment of popularizers and interpreters. In fact, the bibliography is almost entirely restricted to secondary and tertiary sources like magazine articles, radio interviews, and pop science books, which exacerbates misreadings (see e.g.p. 187, where Gethin discusses a paraphrase by Jeremy Campbell of some remarks",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J91-3002",
  "title": "Chinese Number-Names, Tree Adjoining Languages, and Mild Context-Sensitivity",
  "year": 1991,
  "venue": "CL",
  "abstract": "The Tree Adjoining Grammar formalism, both its singleas well as multiple-component versions, has recently received attention as a basis for the description and explication of natural language. We show in this paper that the number-name system of Chinese is generated neither by this formalism nor by any other equivalent or weaker ones, suggesting that such a task might require the use of the more powerful Indexed Grammar formalism. Given that our formal results apply only to a proper subset of Chinese, we extensively discuss the issue of whether they have any implications for the whole of that natural language. We conclude that our results bear directly either on the syntax of Chinese or on the interface between Chinese and the cognitive component responsible for arithmetic reasoning. Consequently, either Tree Adjoining Grammars, as currently defined, fail to generate the class of natural languages in a way that discriminates between linguistically warranted sublanguages, or formalisms with generative power equivalent to Tree Adjoining Grammar cannot serve as a basis for the interface between the human linguistic and mathematical faculties.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/J91-3005",
  "title": "Erratum to: A Statistical Approach to Machine Translation",
  "year": 1991,
  "venue": "CL",
  "abstract": "In Section 6 of \"A statistical approach to machine translation\" (Computational Linguistics 16(2), 79-85), we reported the results of two experiments in which we estimated parameters of a statistical model of translation from English to French. In the first experiment, the English and French vocabularies each consisted of 9,000 common words, and the model parameters were estimated from 40,000 pairs of sentences 25 words or less in length. Words outside the 9,000-word vocabularies in these sentences were mapped to special unknown words. In the second experiment, the vocabularies were limited to 1,000 common English words and 1,700 common French words, and the model parameters were estimated from 117,000 pairs of sentences 10 words or less in length that were completely covered by the respective vocabularies. In Figures 4, 5, and 6 of the paper, we erroneously presented parameter estimates from the 1,000-word experiment, while claiming in the text that they were from the 9,000-word experiment. The parameter estimates for these two experiments differ considerably because of the restriction of the training corpus in the 1,000-word experiment to short, covered sentences. For example, the probability that hear is translated as bravo",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J91-3008",
  "title": "A Computational Model of First Language Acquisition",
  "year": 1991,
  "venue": "CL",
  "abstract": "In a recent survey of early language acquisition, Gleitman, Gleitman, Landau, and Wanner (1988) cite Leonard Bloomfield (1933, p. 29) as remarking that \"language learning is doubtless the greatest intellectual feat any one of us is ever required to perform.\" Given this, it is equally no small feat to attempt to build a computer model that does the same thing. Satake's book is one of but a handful of attempts in the computational linguistics tradition to take up this challengemsomewhat surprising given the vast range of linguistic and psychological literature on the subject. Perhaps it is because linguists and psychologists can try to digest just one piece of the acquisition puzzle, while a computational model must typically try to gobble a major chunk of language acquisition whole, or risk being called a mere toy. In this light, Satake should be congratulated for trying to present, in one brief volume, a computational model that attempts to handle facts about morpheme acquisition and intonation; varying word order across languages; verb subcategorization; and classic rule overgeneralization, while at the same time at least paying some attention to what psychologists know about child language. One then obviously runs the risk of stretching too thin, and in fact the volume under review runs far too short in large type. Readers looking for answers to these rich subjects mentioned just above will come away disappointed by a sketch that ultimately can only approximate what computer modeling did in this area more than ten years ago (work by Anderson 1977; Selfridge 1981; and Berwick 1979, 1985). The book exercises the model with a very limited range of sample sentences--just nine examples, with no recursion. More unfortunately, given the emphasis on free word order, no Japanese examples are included. The first quarter of the book is devoted to a rather thin outline of some of the basic psychological results on input available to the child and learnability theory, while the remainder is devoted to the three components of (sub)category generalization, a case analysis of the system working on the examples and a short study of over-regularization, and the use of teacher correction in a so-called \"production mode\" to repair mistakes. This last point is quite important, for Satake's intended novel contribution to this older literature is stated clearly at the outset: to build an empiricist model of acquisition that is cognitively faithful--that is, one where the structure of language is \"out there\" in the world and formed by inductive generalization, special properties of parental input (motherese), the order of examples (including negative examples), and the like rather than \"in there\"--the child's head. Satake means this of course as the polar opposite of \"innate\" acquisition procedures, which assume a richly structured knowledge of language to begin with. (Satake labels these as \"passive\" acquisition models",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J91-3009",
  "title": "As Time Goes By: Tense and Universal Grammar",
  "year": 1991,
  "venue": "CL",
  "abstract": "This book consists of an analysis, solidly within the framework of Government and Binding, of tense and its interaction with adverbials, temporal connectives, and complementation. There is both good news and bad news about the book; parts of it are carefully worked out and are therefore a valuable resource for the computational linguist, while other parts are less clearly argued. Like much current GB literature, Hornstein takes the goal of linguistic research to be the discovery of the nature of the innate language capacity; this is, of course, a laudable goal. Unfortunately, however, the argument that is presented takes a form that will no doubt prove bewildering to many. The argument, which pervades the book and forms the backbone of its first few chapters, resembles other arguments that have been presented within the same framework: a list of sentences is presented, some ungrammatical and starred; a formal model (one of many possible formal models) of some aspect of the structure of these sentences is presented, with rules for determining ill-formedness in the model and predicting ungrammaticality in the corresponding string; an argument is made that the nature of the formal model is such that it could not be acquired on the basis of primary data available to the child; and the claim is then made that the formal model that has been presented is (in fact, must be) a part of the innate linguistic competence that humans possess. There are several problems with this sort of argument. First, there are often a number of nonequivalent formal models that are consonant with the same set of data. Some of these models are more appealing than others; coming up with one model that covers the data does not in itself constitute proof that that particular model is the one that is innate and should be adopted. Indeed, an appeal to innateness has an insidious consequence: it essentially obviates the need for criteria for distinguishing between possible formal models of the same data. If any formal model can be claimed to be part of the innate language apparatus, there is no need to search for a simpler or cleaner or more parsimonious model than the one that has been proposed; any model is as good as any other, since all models can be innate. Second, the assumptions that are necessary to the claim that the model is unlearnable are not always thoroughly substantiated. In particular, Hornstein claims that \"of the data theoretically available to the child, it is likely that only the simple sentences can be absorbed . . . . The child.., is limited to an informationally restricted subset of the potentially relevant data\" (p. 2). This claim is made without citation of substantiating psycholinguistic or language acquisition literature; Hornstein stipulates, in effect, that the system he proposes is innate since it could not be learned on the basis of the subset of data he admits as relevant. Hornstein presents a 'neo-Reichenbachian' analysis of tense, by which he means past, present, and future tense as well as perfect aspect; other questions of the treatment",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J92-1008",
  "title": "Reference and Computation",
  "year": 1992,
  "venue": "CL",
  "abstract": "Kronfeld's book attacks the problem of referring: How do speakers reveal what entities they are talking about? How do they choose particular referring expressions in their utterances? These questions are asked from the perspective of a plan-based theory of speech acts and communication. Utterances are viewed as acts that are intended by speakers to have certain effects on the addressees, and on the world more generally. Utterances are therefore to be planned in broadly the same way as other types of acts are. Kronfeld's embedding of referring in speech act theory rests partly on a Gricean formulation of the literal goal of a referring expression. The relationship of this goal to the looser notion of the discourse purpose of the expression is discussed. Another aspect of the speech act theme is the discussion of two ways in which a referring expression can be intended by a speaker to be relevant: functionally relevant or conversationally relevant. Functionally relevant expressions are used primarily to lead the addressee to identify an object. Conversationally relevant descriptions are those that are intended to focus the addressee on a specific aspect of an object. Such descriptions are related to a specific type of Gricean conversational implicature. Kronfeld carefully draws a distinction between the problem of referring and the philosophical problem of reference. The latter problem is cast as the question, \"How can thoughts (and sentences that articulate them) be about objects?\" (p. 13). Kronfeld covers quite a lot of ground on the philosophical problem of reference, as the problem of referring is somewhat dependent on it. Kronfeld concentrates throughout on reference to physical objects, and on referring expressions that are noun phrases. Furthermore, almost all the book is about definite descriptions, names and other expressions that refer to single, specific, physical objects, rather than to indefinite objects, several objects, or sets of objects. Kronfeld also stresses that he is not directly concerned with the anaphoric linking of pronouns to other noun phrases. He is directly concerned only with links between noun phrases and the world. In fact, much of the book is a defense of the descriptive approach to the philosophical problem of reference in thought and in language. In this approach, to refer to an object is essentially to have or invoke a mental representation of that object. The relationship between a sentence or thought and the objects it is about is that of denotation, which in turn is a function of descriptive content. The descriptive approach",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J92-2006",
  "title": "Letters to the Editor",
  "year": 1992,
  "venue": "CL",
  "abstract": "I do not believe that Geoffrey Pullum, in his attack on my Antilinguistics (Computational Linguistics, 17(2), 240) is trying to protect his livelihood. I hardly think he is in any danger there. But I do think he is protecting the intellectual pride of himself and his colleagues. That is reasonable and to be expected. What is not reasonable is that in doing this he does not report, let alone try to answer, a single one of my general arguments or, with one possible exception, any of my specific points. His method of review is to mock remarks that he has stripped of all their reasoned context, and to scorn conclusions without even hinting at the existence of the large amounts of evidence I present in justification. (Luckily for my confidence there have been other reviewers who have shown greater appreciation of my rational powers.) Most serious of all Pullum's mistakes is perhaps that several of his complaints are based on the very assumptions that I question in the book. His talk of Quirk's \"monumental descriptive work on modern English\" without mentioning any part of my case against it is one example. His contempt for my failure to master primary sources is another. I should probably have emphasized my argument about this more. But it is there, on pages 2, 195n, and 258. It is surely significant that Pullum uses p. 195n, not to explain my position, but to attack the book's poor production. He is quite right to criticize the mistakes in names (I apologize to the people concerned), the faulty index, and missing arrows. It was careless; I should have checked the proofs better. Pullum is careless himself. He misquotes a sentence from p. 56; and in the index it is only the proper names that have two figures too high (and from p. 170, not p. 180 as he states)--all the other entries are correct. In the only case where Pullum really attempts a proper discussion of the material to show that \"when by chance Gethin gets hold of the linguistic ball for a moment he unfailingly drops it\") his criticism is typical of the barren formality that I complain of: I should have located the ambiguity of Flying planes can be dangerous in the transitive/intransitive contrast in fly, not in -ing. He thus abandons reality, the words actually used, for the sake of an abstraction. And I am not simply insisting on a principle here, for in the process Pullum gets it wrong. The transitive/intransitive difference in the meanings of -ing is not the crucial one, and may not be there at all, as can be seen in, for example, The burning sun.../Burning wood (is wasteful), where (burn)ing is transitive in both cases, but has different meanings. At the same time I cannot think of any sentence where there would be any transitive/intransitive confusion through the use of an infinitive, indicative, or imperative. Can Pullum? It is sadly revealing that he suspects that my attack on Quirk's grammar is prompted by a desire to settle scores with prestigious linguists at British universities. I have no scores to settle with anyone. Is he so used to academic in-fighting that he cannot believe that I have no personal quarrel, only a general quarrel with the attitudes and assumptions, purposes and pretentions, methods and thinking, of academic social 'science' ? I am impatient because while social 'scientists' claim authority, they have failed, I believe, to deliver real results, and yet at the same time exercise intellectual dominance over the rest of the community. I repeat something I say in my book. If academic experts think their work has any importance, that it can affect people's lives in any way,",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J92-3000",
  "title": "Computational Linguistics, Volume 18, Number 3, September 1992, Special Issue on Inheritance: II",
  "year": 1992,
  "venue": "CL",
  "abstract": "Book R e v i e w s Towards a Theory of Cognition and Computing J. Gerard Wolff Papers in Laboratory Phonology I: Between the Grammar and Physics of Speech John Kingston and Mary E. Beckman, eds. S4mantique et Recherches Cognitives Francois Rastier Computational Morphology: Practical Mechanisms for the English Lexicon Graeme D. Ritchie, Graham J. Russell, Alan W. Black, and Stephen G. Pulman Intelligent User Interfaces Joseph W. Sullivan and Sherman W. Tyler, eds. Adaptive Parsing: Self-Extending Natural Language Interfaces Jill Fain Lehman Generalized LR Parsing Masaru Tomita, ed. Literature and Cognition Jerry R. Hobbs",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J92-4006",
  "title": "Computer Rules, Conversational Rules",
  "year": 1992,
  "venue": "CL",
  "abstract": "There has been much controversy recently as to whether the rules of interaction discovered by conversation analysts are amenable to use by computers (Gilbert 1990; Hirst 1991; Luff, Gilbert, and Frohlich 1990). Button (1990) has argued that the rules of conversation are of a different ontological category than the rules used by computers, and that this means computers cannot be programmed to engage in conversation. Others (Fraser and Wooffitt 1990; Frohlich and Luff 1990; Gilbert, Wooffitt, and Fraser 1990) have argued to the contrary that the rules of conversation can be captured in a program, and indeed that some have been. I will argue for a third position. Button is right in his critique of existing attempts to import conversation analysis into computational linguistics and in his argument that there is a rule type mismatch. His arguments do not, however, show that computers cannot in principle be programmed to engage in conversation. I will argue by analogy to computer network protocols that an interactionist computational interpretation of the conversation analytical rules is possible, and that Button's critique can thereby be bypassed.",
  "stance": -0.5
 },
 {
  "url": "https://aclanthology.org/J92-4007",
  "title": "A Problem for RST: The Need for Multi-Level Discourse Analysis",
  "year": 1992,
  "venue": "CL",
  "abstract": "Rhetorical Structure Theory (RST) (Mann and Thompson 1987), argues that in most coherent discourse, consecutive discourse elements are related by a small set of rhetorical relations. Moreover, RST suggests that the information conveyed in a discourse over and above what is conveyed in its component clauses can be der ived from the rhetorical relation-based structure of the discourse. A large number of natural language generation systems rely on the rhetorical relations defined in RST to impose structure on multi-sentential text (Hovy 1991; Knott 1991; Moore and Paris 1989; Rosner and Stede 1992). In addition, many descriptive studies of discourse have employed RST (Fox 1987; Linden, Cumming, and Martin 1992; Matthiessen and Thompson 1988). However , recent work by Moore and Paris (1992) noted that RST cannot be used as the sole means of controlling discourse structure in an interactive dialogue system, because RST representations provide insufficient information to suppor t the generation of appropriate responses to \"fol low-up questions.\" The basic problem is that an RST representation of a discourse does not fully specify the intentional structure (Grosz and Sidner 1986) of that discourse. Intentional structure is crucial for responding effectively to questions that address a previous utterance: wi thout a record of what an utterance was intended to achieve, it is impossible to elaborate or clarify that utterance. 1 Further consideration has led us to conclude that the difficulty observed by Moore and Paris stems from a more fundamental problem with RST analyses. RST presumes that, in general, there will be a single, preferred rhetorical relation holding between consecutive discourse elements. In fact, as has been noted in other work on discourse structure (Grosz and Sidner 1986), discourse elements are related simultaneously on multiple levels. In this paper, we focus on two levels of analysis. The first involves the relation between the information conveyed in consecutive elements of a coherent discourse. Thus, for example, one utterance may describe an event that can be presumed to be the cause of another event described in the subsequent utterance. This causal relation is at what we will call the informational level. The second level of relation results from the fact that discourses are produced to effect changes in the mental state of the discourse participants. In coherent discourse, a speaker is carrying out a consistent plan to achieve the intended changes, and consecutive discourse elements are related to one another by means of the ways in which they participate in that plan. Thus, one utterance may be intended to increase the likelihood that the hearer will come to",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J93-1009",
  "title": "Issues in the choice of a source for Natural Language Generation",
  "year": 1993,
  "venue": "CL",
  "abstract": "The most vexing question in natural language generation is 'what is the source'-what do speakers start from when they begin to compose an utterance? Theories of generation in the literature differ markedly in their assumptions. A few start with an unanalyzed body of numerical data (e.g. Bourbeau et al. 1990; Kukich 1988). Most start with the structured objects that are used by a particular reasoning system or simulator and are cast in that system's representational formalism (e.g. Hovy 1990; Meteer 1992; R6sner 1988). A growing number of systems, largely focused on problems in machine translation or grammatical theory, take their input to be logical formulae based on lexical predicates (e.g. Wedekind 1988; Shieber et al. 1990). The lack of a consistent answer to the question of the generator's source has been at the heart of the problem of how to make research on generation intelligible and engaging for the rest of the computational linguistics community, and has complicated efforts to evaluate alternative treatments even for people in the field. Nevertheless, a source cannot be imposed by fiat. Differences in what information is assumed to be available, its relative decomposition when compared to the \"packaging\" available in the words or syntactic constructions of the language (linguistic resources), what amount and kinds of information are contained in the atomic units of the source, and what sorts of compositions and other larger scale organizations are possible--all these have an impact on what architectures are plausible for generation and what efficiencies they can achieve. Advances in the field often come precisely through insights into the representation of the source. Language comprehension research does not have this problem--its source is a text. Differences in methodology govern where this text comes from (e.g., single sentence vs. discourse, sample sentences vs. corpus study, written vs. spoken), but these aside there is no question of what the comprehension process starts with. Where comprehension \"ends\" is quite another matter. If we go back to some of the early comprehension systems, the end point of the process was an action, and there was linguistic processing at every stage (Winograd 1972). Some researchers, this author included, take the end point to be an elaboration of an already existing semantic model whereby some new individuals are added and new relations established between them and other individuals (Martin and Riesbeck 1986; McDonald 1992a). Today's dominant paradigm, however, stemming perhaps from the predominance of research on question-answering and following the lead of theoretical linguistics, is to take the end point to be a logical form: an expression that codifies the information in the text at a fairly shallow level, e.g., a first-order formula with content words mapped to predicates with the same spelling, and with individuals represented by quantified variables or constants.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J93-2013",
  "title": "Letters to the Editor",
  "year": 1993,
  "venue": "CL",
  "abstract": "Adaptive Parsing I am concerned by a comment made in Julia Johnson's review of my book Adaptive Parsing (Kluwer Academic Publishers, 1992) in Computational Linguistics 18(3) (September 1992). In the review, Dr. Johnson poses a number of thought-provoking questions that underscore open issues in this research, and seems to have been, overall, a thoughtful and attentive reader. Toward the end of the review, however, she states, \"The performance improvements realized with adaptive parsing over a particular kernel grammar without adaptation were not strong.\" This statement does not agree with the results in the book. As shown in the utility analysis on pages 194-200 and 207-210, performance of the system using the kernel grammar without adaptation gave an acceptance range from 7% to 24% of utterances; with adaptation, acceptance increased to 81% to 91%. I find it difficult to interpret this data as anything but a very strong performance improvement. Since the perceived usefulness of adaptation rests in great part on the performance improvements it affords over a static sublanguage, I am grateful for the opportunity to point out and correct this misperception.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J93-3007",
  "title": "Metaphor and Cognition: An Interactionist Approach",
  "year": 1993,
  "venue": "CL",
  "abstract": "Bipin Indurkhya has written a wide-ranging and interesting work that is easy to read. Although Indurkhya's starting point is the puzzle of similarity-creating metaphors, the book is really about cognition and conceptual structure. In particular, he is concerned with the philosophical problem of reconciling the constructive nature of our concepts with the notion of a pre-existing mind-independent structure of reality. While the book covers a great deal of ground and is well worth reading, I feel that the basic theory is flawed in that it rests on a common philosophical view of meaning that has been considered inadequate.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/J94-1002",
  "title": "A Hierarchical Stochastic Model for Automatic Prediction of Prosodic Boundary Location",
  "year": 1994,
  "venue": "CL",
  "abstract": "Prosodic phrase structure provides important information for the understanding and naturalness of synthetic speech, and a good model of prosodic phrases has applications in both speech synthesis and speech understanding. This work describes a statistical model of an embedded hierarchy of prosodic phrase structure, motivated by results in linguistic theory. Each level of the hierarchy is modeled as a sequence of subunits at the next level, with the lowest level of the hierarchy representing factors such as syntactic branching and prosodic constituent length using a binary tree classification. A maximum likelihood solution for parameter estimation is presented, allowing automatic training of different speaking styles. For predicting prosodic phrase breaks from text, a dynamic programming algorithm is given for finding the maximum probability prosodic parse. Experimental results on a corpus of radio news demonstrate a high rate of success for predicting major and minor phrase boundaries from text without syntactic information (81% correct prediction with 4% false prediction).",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/J94-1014",
  "title": "Briefly Noted",
  "year": 1994,
  "venue": "CL",
  "abstract": "The author hypothesizes that patterns of cohesion relations, displayed graphically, can illustrate the rudiments of texture. Thirty-five English texts taken from five genres (nonfiction, essays, biographies, novels, and short stories) are hand-labeled with three types of cohesion relations: definite articles, pronouns, and verbs with agent displacements (i.e., the verb's agent role is filled by a constituent not in the subject position). A text's \"cohesion map\" is created by assigning to each word a location on a two-dimensional grid corresponding to the word 's position in the text (roughly, each sentence corresponds to a row), and then drawing a line between the location of a cohesive element and the location of its original referent. The resulting map looks somewhat like a column of hanging pine-needle bunches; thus texts can be compared visually for properties such as burstiness, density, and connection span. Each kind of cohesive element is assigned its own map, although for one example all three cohesion maps are superimposed. The resulting maps seem to illustrate interesting differences among the texts. Unfortunately, the author neither describes how to analyze these maps nor explores the effects of multiple interacting types of cohesion elements. Instead, the analysis focuses on comparing text genres based on their overall \"relative cohesiveness.\" The leap from elucidating a text's style or texture to comparing texts for relative cohesion on the basis of three syntactic cues (one of which tends to occur only rarely and almost never extra-sententiaUy) is not well justified by the early parts of the book. Perhaps for this reason, there are two significant problems with the way relative cohesiveness is computed. First, the author assumes that nonfiction text is less cohesive than other genres such as biographies and fiction (footnote, p. 55). By far the most frequently occurring of the three cohesion relations that the author examines is pronominal reference (p. 68); lexical cohesion relations are excluded. However, as the author points out, biographies and fiction tend to have many pronominal references, whereas nonfiction texts tend to have few pronominal references but many lexical cohesion relations. So in effect the result of the comparison is pre-determined. Despite these problems, the author concludes that the definitions and procedures used are satisfactory (p. 96). The second problem with this analysis is that the comparison is based on a \"cohesion index,\" which is determined by multiplying the average number of cohesive elements corresponding to a referent by the average distance between the elements and their referent. This number is meant to indicate the relative cohesiveness of a text, but does not discriminate between a referent that has only a few, distant references and a referent that has a large number of nearby references. Bearing in mind that this is crossdisciplinary work (the author apparently originates from a literary field), a reader of Computational Linguistics may be put off by outdated references to the artificial intelligence literature and weak discussions of computational issues in general. The author is to be commended for working with a large number of lengthy texts, a rare precedent in discourse analysis. The idea of graphically displaying the interactions of the syntactic cohesive elements is a useful one. The next important steps are to explore how to represent multiple interacting cue types, and how to analyze or interpret the resulting illustrations; this may lead to a better understanding of texture and cohesion in written texts. --Marti Hearst, University of California, Berkeley",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J94-2008",
  "title": "Briefly Noted",
  "year": 1994,
  "venue": "CL",
  "abstract": "Most of the computer-based writing tools presently in use are quite dumb, relying on simple techniques to achieve their goals; however, development of intelligent writing aids is an important area in computational linguistics. Two recent books address many of the issues involved in the development of better writing tools. Computers and Writing: State of the Art (CWSA), and Computers and Writing: Issues and Implementations (CWII) contain papers from the Third International Conference on Computers and Writing and the Fourth Conference on Computers and the Writing Process, respectively. The second book contains selected papers, which were further developed by the authors for publication. It is not clear whether the papers in the first volume underwent the same selection process, but the time that elapsed between the conference and the publication of the book suggests that the papers might have been revised since their presentation at the conference. The papers cover a wide range of subtopics; as Sharples observes in his introduction, \"This volume is deliberately eclectic\" (CWII, p. 3). Research interests of participants include cognitive science, computer science, education, engineering, linguistics, and philosophy. The majority of the contributors are from the United Kingdom. CWII contains an introduction by the editor and 15 papers; CWSA contains a preface by Holt and 24 papers. Neither book has a concluding chapter to tie the strands together in any way, so the reader may go away with a sense of having been left hanging at the end. Perhaps the absence of concluding chapters is due to the difficulty of drawing any general conclusions from such a wide spectrum of topics. They range from designing writing tools to assessing educational tools, from observing writing practices to writing interactive fiction. Thus, many of the chapters might not be of interest to any one reader, unless that reader wants to gain an overview of the issues that are currently being explored. Most of the papers in the two books can be (very) loosely divided into three categories: the effects of the computer on writing practices; CAI for writing; and the development of computer writing tools. Neither of the books has a computational linguistics focus. About one-fifth of the papers in each describe the application of natural language processing techniques. For the most part, only current, reliable capabilities are applied to create working systems. I will briefly discuss a few of the papers that describe the implementation of NLP techniques, to give an idea of how they are being used. Kempen and Vosse (CWSA) present a \"language-sensitive\" text editor for Dutch. Their system focuses on detecting real-word spelling errors that are homophonous to the target word, as well as flagging inconsistent spellings and other orthographical errors. The text undergoes both word-level and sentence-level analyses. At the word level, triphone analysis is performed to select all possible homophonous word alternatives. Next, a shift-reduce parser analyzes each sentence. The parser, which contains about 500 augmented phrase structure rules, ignores aspects of syntactic structure that are not relevant to orthography. Feature matrices on the nonterminal symbols detect feature violations. The evaluation of the system is only preliminary, but it appears to be a practical solution to the problem of discovering many real-word errors using current parsing capabilities. Hoard, Wojcik, and Holzhauser (CWSA) report on a simplified English checker devel-",
  "stance": -1.0
 },
 {
  "url": "https://aclanthology.org/J94-3002",
  "title": "Commentary on Kaplan and Kay",
  "year": 1994,
  "venue": "CL",
  "abstract": "Anyone with a fundamental interest in morphology and phonology, either from a scientific or a computational perspective, will want to study this long-awaited paper carefully. Kaplan and Kay (henceforth K&K) announce two goals: \"to provide the core of a mathematical framework for phonology\" and \"to establish a solid basis for computation in the domain of phonological and orthographic systems.\" They show how the algebra of regular relations, with their corresponding automata, can be used to compile systems of phonological rules in the style of SPE, including directionality, optionality, and ordering. They sketch mechanisms for incorporating a lexicon and for dealing with exceptional forms, thus providing a complete treatment in a unified framework. This accomplishment in itself will not compel the attention of many working phonologists, who have found good reasons to replace the SPE framework (see Kenstowicz [1994] for a survey of modern practice), and whose efforts since 1975 have been aimed mainly at finding representational primitives to explain typological generalizations, support accounts of learning, generalization and change, and provide one end of the mapping between symbols and speech. In this effort, there has been little emphasis on SPE's goal of giving phonological descriptions an algorithmically specified denotation. Perhaps this paper, despite its superficial lack of connection to contemporary work in phonology, will set in motion a discussion that will ultimately redress the balance. On the computational side, practitioners of practical NLP will be happy to make extensive use of the algebra of regular relations, since it provides a truly elegant engineering solution to a wide range of problems. However, although direct interpretation of some simple FSTs can be efficient (e.g. Feigenbaum et al. 1991), and although Koskenniemi has documented efficient implementation techniques for his two-level systems, the overall architecture presented in this paper is not practically usable as written, because of either the size of the resulting automata or the time needed for (unwisely implemented) nondeterminism, or both. A range of well-known techniques enable programs based on the algebraic combination of (unary) FSAs to make efficient use of both time and space. Although these methods do not apply to FSTs in general, we may presume that K&K have developed analogous techniques for the crucial range of cases. With the growing interest in this technology, we can expect that either K&K will publish their work or others will recapitulate it, so that the algebra of regular relations can take its proper and prominent place in the toolkit of computational linguistics.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J94-3003",
  "title": "Commentary on Kaplan and Kay",
  "year": 1994,
  "venue": "CL",
  "abstract": "To appreciate this article fully, it is essential to understand the historical context into which it fits, and which it has to some extent created. Although formally published for the first time here, it is already an extremely influential and classic piece of work. Finite-state machines, in one form or another, have been used for the description of natural language since the early 1950s, with the extension to transducers appearing in the 1960s. After Chomsky's stern condemnation of the adequacy of finite-state machines for describing sentence structures, they virtually disappeared from mainstream theoretical linguistics. Within computer science, they continued to be a standard formalism, although transducers were not accorded the same detailed algebraic attention as simple automata. Phonologists, meanwhile, were inventing a variety of rule mechanisms that were (with rare exceptions) only partly formalized. Superficially, most of these systems (as typified by those of Chomsky and Halle) appeared to have little to do with finite-state machines. Indeed, their notations tended to suggest that the rules had much more than finite-state power. Kaplan and Kay have integrated these two streams of work--algebraic treatment of automata in computer science, and phonologically-motivated formalisms within linguistics--and their results should feed back productively into both subfields. The framework they have established allows the comparison of different competing formalisms in a rigorous manner, and permits the exploration of the formal limitations or capabilities of rule notations that were previously more like expository devices than formally defined systems. What may not be clear to the casual reader is that this work has been developed over many years, and early versions of it have already escaped into the computational linguistics community in less prominent forums. In this way it has already affected the course of research into phonological/morphological formalisms. Perhaps the most notable (and in its turn, influential) development has been Koskenniemi's two-level morphology, which has been successfully applied to the morphology of a very wide number of languages. Koskenniemi's ideas are a direct development of Kaplan and Kay's, as explained in Section 7 of the paper here. The theory of regular relations and finite-state transducers should not be viewed as a mere re-formalisation of 1960s linguistics. As well as its relevance to the two-level model, Kaplan and Kay suggest that it may also throw light on the formal properties of autosegmental phonology. Although the ideas were first circulated about 15 years ago, they are still of central relevance to computational phonology today.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J94-3011",
  "title": "Commentary on Bird and Klein",
  "year": 1994,
  "venue": "CL",
  "abstract": "Bird and Klein show us how various phonological constructs--feature geometry, the prosodic hierarchy, well-formedness constraints on strings of segments, templatic/autosegmental phonology, morphology-phonology interactions, and vowel-zero alternat i o n s m a y be treated in a rigorous fashion using the the formal resources of HPSG. To the incautious critic (or the skeptical phonologist) it may seem that they have given us merely a new notation in which to express conventional phonological analyses, such as Tranel's analysis of French schwa epenthesis or Goldsmith's textbook examples of Sierra Miwok. A central principle of Bird and Klein's approach that takes it beyond notation is Phonological Compositionality and the related concept of monotonicity, mentioned almost in passing in Section 3.3. Among the consequences of monotonicity are: (1) feature values may not be altered; (2) segments or other structural material may not be removed (i.e. delinking and restructuring, such as resyllabification, are prohibited); and (3) constraints are not extrinsically ordered; in fact, extrinsic rule ordering is inexpressible. An accommodation with orthodox generative phonology with regard to (1) may be found by employing underspecification, translating feature-changing rules into feature-filling constraints. Objections to extrinsic ordering have been raised at various times before, and many phonologists would like to be rid of it, so Bird and Klein's proposals regarding (3) are welcome. Many phonologists find (2) very unpalatable, however, despite the fact that Bird and Klein's example of French schwa insertion is also potentially applicable to some putative cases of deletion. A deletion rule operative in Welsh mutation, which removes / g / from the lexical representation of gardd in some environments, but leaves it present in the citation form, could be treated in HPSG phonology as consonant-zero alternation on a par with French schwa. This alternation is only treated as deletion because it is the noncitation form that lacks the initial consonant. Other apparent cases of deletion are more awkward for declarative approaches. In English trisyllabic shortening (e.g. profane --* profanity) and -ic shortening (e.g. t6ne t6nicity), orthodox analysis deletes a vowel slot and association lines incident to it. To treat these as instances of vowel-zero alternation would appear to require the representation of the stem to be sensitive to the presence of a very particular set of suffixes (-ic, -ity, etc.). HPSG phonology might either analyze these using different CV templates, like Sierra Miwok allomorphy, or reject the bisegmental analysis of phonological length, by treating shortening as the addition of a \"shortness\" feature (e.g. trisyllabic-shortening(V) --* I-long]). I do not expect either of these proposals to be popular. In any case, reconstructing morphophonological rules such as the above in declarative style will not satisfy some critics, for whom any suggestion of languagespecific rules is anathema.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J95-2002",
  "title": "An Efficient Probabilistic Context-Free Parsing Algorithm that Computes Prefix Probabilities",
  "year": 1995,
  "venue": "CL",
  "abstract": "We describe an extension of Earley's parser for stochastic context-free grammars that computes the following quantities given a stochastic context-free grammar and an input string: a) probabilities of successive prefixes being generated by the grammar; b) probabilities of substrings being generated by the nonterminals, including the entire string being generated by the grammar; c) most likely (Viterbi) parse of the string; d) posterior expected number of applications of each grammar production, as required for reestimating rule probabilities. Probabilities (a) and (b) are computed incrementally in a single left-to-right pass over the input. Our algorithm compares favorably to standard bottom-up parsing methods for SCFGs in that it works efficiently on sparse grammars by making use of Earley's top-down control structure. It can process any context-free rule format without conversion to some normal form, and combines computations for (a) through (d) in a single algorithm. Finally, the algorithm has simple extensions for processing partially bracketed inputs, and for finding partial parses and their likelihoods on ungrammatical inputs.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/J95-2008",
  "title": "Briefly Noted",
  "year": 1995,
  "venue": "CL",
  "abstract": "Verbmobil is a project sponsored by the German government, whose aim is to develop a translation system for face-to-face speech dialogue. This book is the report on a preliminary study of the project conducted by CSLI, Stanford University. The contents of the book are as follows: Chapter 1 contains introductory material; Chapter 2 and Chapter 3 are devoted to describing the current state-of-the-art of machine translation and speech recognition and speech synthesis technologies, respectively; and Chapter 4 reports a technical recommendation for the Verbmobil project based on the discussions in the preceding chapters. Chapter 2, which would be the most interesting for readers of Computational Linguistics, exhaustively covers various aspects of machine translation. First, the authors argue why machine translation is so difficult. They discuss the indeterminacy of language by referring to a number of examples: situatedness, mismatches between two languages in translation, and ambiguities of various levels. From these considerations, they suggest that good translation is not defined as preserving the meaning but as preserving the intention of the original (p. 27), sometimes by adding or deleting information (p. 26). Then, they review current technologies, historical perspectives, and theoretical issues such as syntax and grammar formalism. The most interesting and important discussion in this chapter is the section \"Translation Strategy.\" First, the authors refer to the controversial argument concerning the comparison between the transfer approach and the interlingual approach, but they claim that \"the issue of interest is not whether to pursue a transfer or an interlingual approach; the issue is which levels of analysis are necessary, and how to arrive at a representation suitable for generation of a target text\" (p. 82). From this point, and by considering many examples of translation mismatch that imply the impossibility of a naive interlingual scheme, the authors propose a new architecture called \"translation by negotiation.\" In this architecture, some interlingual representation is still supposed, but it does not require that an invariant representation that will be the same for the translation of a source sentence, or text, into any other language can be found at any time. There are three components: an analyzer, a generator, and a negotiator. The analyzer delivers to the negotiator an interlingual representation. The negotiator hands the interlingual representation to the generator. In some (or rare) cases, the generator can find the phrase in the target language that has exactly the same interlingual representation. In other cases, the generator reports to the negotiator error information such as the overspecification or underspecification of the input interlingual representation. Now, it is up to the negotiator to decide whether the error is fatal or not. When it is not so serious, the translation is accepted, but when it is serious, the negotiator has to solve it by some other means, such as referring to the context. In Chapter 2, the authors also discuss other topics: an overview of nonlinguistic translation approaches such as the stochastic approach and the example-based approach, and a comprehensive survey of current machine translation systems. These would also be quite informative to a reader. Chapter 3 is devoted to introducing speech-recognition technology and speechsynthesis technology, and Chapter 4 contains recommendations for the Verbmobil project. These might be less interesting for a reader of Computational Linguistics, so only a brief description is given below. In Chapter 3, general characteristics of speech are given, and current major speechrecognition technologies (the knowledgebased approach, the stochastic-based [HMMbased] approach, and the connectionismbased approach) are introduced, as well as a traditional template-based approach. These descriptions would be a good introduction for a reader who wants to see a general overview of speech-recognition technologies. As for speech synthesis, most pages are given",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J95-2010",
  "title": "Letter to the Editor",
  "year": 1995,
  "venue": "CL",
  "abstract": "Unlike Sidner, who used thematic roles to order the Cf (or potential foci), Kameyama determined that surface grammatical functions mattered to centering. This directly influenced the view expressed in Grosz, Joshi, and Weinstein (unpublished manuscript 1986) about the importance of grammatical subjects (Grosz, personal communication), on which the centering algorithm used in Walker, Iida, and Cote (1994) is based.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J96-1004",
  "title": "From Conceptual Time to Linguistic Time",
  "year": 1996,
  "venue": "CL",
  "abstract": "In this paper, we present a method for generating French texts conveying temporal information that integrates Discourse Representation Theory (DRT) and Systemic Grammar Theory. DRT is used to represent temporal information and an intermediate semantic level for the temporal localization expressed by temporal adverbial phrases and verb phrases. This representation is then translated into a syntactic form using Systemic Grammar Theory. We have implemented this method in a working prototype called Prdtexte.",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/J96-2004",
  "title": "Assessing Agreement on Classification Tasks: The Kappa Statistic",
  "year": 1996,
  "venue": "CL",
  "abstract": "Currently, computational linguists and cognitive scientists working in the area of discourse and dialogue argue that their subjective judgments are reliable using several different statistics, none of which are easily interpretable or comparable to each other. Meanwhile, researchers in content analysis have already experienced the same difficulties and come up with a solution in the kappa statistic. We discuss what is wrong with reliability measures as they are currently used for discourse and dialogue work in computational linguistics and cognitive science, and argue that we would be better off as afield adopting techniques from content analysis.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/J97-3006",
  "title": "Current theories of centering for pronoun interpretation: a critical evaluation",
  "year": 1997,
  "venue": "CL",
  "abstract": "A central claim of centering theory (Grosz, Joshi, and Weinstein, 1995 henceforth GJW) is that certain entities ment ioned in an utterance are more central than others, and that this proper ty imposes constraints on a speaker 's use of different types of expressions to refer to them. To articulate some of these constraints, they define several fundamental centering concepts and propose rules based on them that should be followed by a speaker in producing coherent discourse. This work has led to several analyses employing centering theory and extensions of it, particularly in the area of p ronoun interpretation (Kameyama 1986; Brennan, Friedman, and Pollard 1987; Di Eugenio 1990, 1996; Walker, Iida, and Cote 1994; Strube and Hahn 1996, inter alia; see also citations within GJW, forthcoming papers in Walker, Joshi, and Prince in press, and psycholinguistic studies described in Hudson-D 'Zmura 1989, Gordon, Grosz, and Gilliom 1993, and Brennan 1995). 1 In this squib, we discuss some facets of the pronoun interpretation problem that motivate a centering-style analysis, and demonstra te some problems with a popular centering-based approach with respect to these motivations.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J98-1002",
  "title": "Similarity-based Word Sense Disambiguation",
  "year": 1998,
  "venue": "CL",
  "abstract": "We describe a method for automatic word sense disambiguation using a text corpus and a machinereadable dictionary (MRD). The method is based on word similarity and context similarity measures. Words are considered similar if they appear in similar contexts; contexts are similar if they contain similar words. The circularity of this definition is resolved by an iterative, converging process, in which the system learns from the corpus a set of typical usages for each of the senses of the polysemous word listed in the MRD. A new instance of a polysemous word is assigned the sense associated with the typical usage most similar to its context. Experiments show that this method can learn even from very sparse training data, achieving over 92% correct disambiguation performance.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/J98-1004",
  "title": "Automatic Word Sense Discrimination",
  "year": 1998,
  "venue": "CL",
  "abstract": "This paper presents context-group discrimination, a disambiguation algorithm based on clustering. Senses are interpreted as groups (or clusters) of similar contexts of the ambiguous word. Words, contexts, and senses are represented in Word Space, a high-dimensional, real-valued space in which closeness corresponds to semantic similarity. Similarity in Word Space is based on second-order co-occurrence: two tokens (or contexts) of the ambiguous word are assigned to the same sense cluster if the words they co-occur with in turn occur with similar words in a training corpus. The algorithm is automatic and unsupervised in both training and application: senses are induced from a corpus without labeled training instances or other external knowledge sources. The paper demonstrates good performance of context-group discrimination for a sample of natural and artificial ambiguous words.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/J98-2002",
  "title": "Generalizing Case Frames Using a Thesaurus and the MDL Principle",
  "year": 1998,
  "venue": "CL",
  "abstract": "A new method for automatically acquiring case frame patterns from large corpora is proposed. In particular, the problem of generalizing values of a case frame slot for a verb is viewed as that of estimating a conditional probability distribution over a partition of words, and a new generalization method based on the Minimum Description Length (MDL) principle is proposed. In order to assist with efficiency, the proposed method makes use of an existing thesaurus and restricts its attention to those partitions that are present as \"cuts\" in the thesaurus tree, thus reducing the generalization problem to that of estimating a \"tree cut model\" of the thesaurus tree. An efficient algorithm is given, which provably obtains the optimal tree cut model for the given frequency data of a case slot, in the sense of MDL. Case frame patterns obtained by the method were used to resolve PP-attachment ambiguity. Experimental results indicate that the proposed method improves upon or is at least comparable with existing methods.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/J98-3001",
  "title": "Introduction to the Special Issue on Natural Language Generation",
  "year": 1998,
  "venue": "CL",
  "abstract": "There are two sides to natural language processing. On the one hand, work in natural language understanding is concerned with the mapping from some surface representation of linguistic material expressed as speech or text--to an underlying representation of the meaning carried by that surface representation. But there is also the question of how one maps from some underlying representation of meaning into text or speech: this is the domain of natural language generation. Whether our end-goal is the construction of artifacts that use natural languages intelligently, the formal characterization of phenomena in human languages, or the computational modeling of the human language processing mechanism, we cannot ignore the fact that language is both spoken (or written) and heard (or read). Both are equally large and important problems, but the literature contains much less work on natural language generation (NLG) than it does on natural language understanding (NLU). There are many reasons why this might be so, although clearly an important one is that researchers in natural language understanding in some sense start out with a more well-defined task: the input is known, and there is a lot of it around. This is not the case in natural language generation: there, it is the desired output that is known, but the input is an unknown; and while the world is awash with text waiting to be processed, there are fewer instances of what we might consider appropriate inputs for the process of natural language generation. For researchers in the field, this highlights the fundamental question that always has to be asked: What do we generate from? Despite this problem, the natural language generation community is a thriving one, with a research base that has been developing steadily--although perhaps at a slower pace because of the smaller size of the community--for just as long as work in natural language understanding. It should not be forgotten that much of NLP has its origins in the early work on machine translation in the 1950s; and that to carry out machine translation, one has to not only analyze existing texts but also to generate new ones. The early machine translation experiments, however, did not recognize the problems that give modern work in NLG its particular character. The first significant pieces of work in the field appeared during the 1970s; in particular, Goldman's work on the problem of lexicalizing underlying conceptual material (Goldman 1974) and",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J98-3006",
  "title": "Do the Right Thing {\\ldots} but Expect the Unexpected",
  "year": 1998,
  "venue": "CL",
  "abstract": "Dale and Reiter (1995) have recently discussed the nature of referring expression generation, focusing on the case of definite noun phrases. In particular, they consider Gricean approaches, whereby the speaker is supposed to take into account likely inferences by the hearer, in accord with Gricean maxims (Grice 1989), and select the generated NP accordingly, so as to avoid false or misleading inferences (Joshi 1982). They observe that previous accounts (including their own) have attempted to optimiz e the generated noun phrase, making it as brief as possible, within the constraints of accurately distinguishing the intended referent from any other candidate referents. For instance, consider a situation containing three animals: one small white cat and two dogs, one large and black and the other small and white. It is usually assumed that an optimal description of the first dog is either the large dog or the black dog, whereas the large black dog will be suboptimal, since it contains two adjectives where one will do; it is longer than strictly necessary, and suffers from a degree of redundancy (Dale 1992; Reiter 1990). However, Dale and Reiter argue that the previous algorithms proposed for this task are computationally inefficient, and that the task itself must be reconsidered. In particular, they suggest that there is substantial psycholinguistic evidence that people don't generate the shortest, most efficient NPs, and that this behavior is regarded as perfectly natural (see Levelt [1989] for a survey). Hence, generation algorithms need not optimize their descriptions either. Dale and Reiter go further; they state that:",
  "stance": -0.5
 },
 {
  "url": "https://aclanthology.org/J98-4002",
  "title": "Selective Sampling for Example-based Word Sense Disambiguation",
  "year": 1998,
  "venue": "CL",
  "abstract": "This paper proposes an efficient example sampling method for example-based word sense disambiguation systems. To construct a database of practical size, a considerable overhead for manual sense disambiguation (overhead for supervision) is required. In addition, the time complexity of searching a large-sized database poses a considerable problem (overhead for search). To counter these problems, our method selectively samples a smaller-sized effective subset from a given example set for use in word sense disambiguation. Our method is characterized by the reliance on the notion of training utility: the degree to which each example is informative for future example sampling when used for the training of the system. The system progressively collects examples by selecting those with greatest utility. The paper reports the effectiveness of our method through experiments on about one thousand sentences. Compared to experiments with other example sampling methods, our method reduced both the overhead for supervision and the overhead for search, without the degeneration of the performance of the system.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/J98-4005",
  "title": "Letter to the Editor: Clues from the Depth Hypothesis: A Reply to Geoffrey Sampson's Review",
  "year": 1998,
  "venue": "CL",
  "abstract": "In linguistics it has not been possible to use the standard criteria and assumptions of science because the ancients placed our discipline not in the physical domain but in the logical domain where concepts and theories do not represent parts of the natural world. Many of the problems facing linguistics follow inevitably, for example the difficulties that linguistics experiences in agreeing on grammatical theory. One symptom is the long-standing difficulty in testing the depth hypothesis, which came out of early MT research. Sampson (1997) attempted recently to test the depth hypothesis by a computer analysis of a grammatically annotated corpus of English. It is shown that this attempted test and his attempt at defending the testability of the dept h hypothesis are invalid. But clues from the depth hypothesis have led to new foundations for general linguistics put forth in the book (Yngve 1996) that Sampson (1998) reviewed. This work reconstitutes linguistics in the physical domain where the criteria and assumptions of science can be applied. Sampson's review of this book contains a number of serious errors and inaccuracies.",
  "stance": -0.5
 },
 {
  "url": "https://aclanthology.org/J98-4011",
  "title": "Briefly Noted",
  "year": 1998,
  "venue": "CL",
  "abstract": "This book is a diverse collection of ten presentations given at an International Summer School on Information Extraction in Rome, 1997. The goal of information extraction (IE) is selective, task-driven interpretation of text narrative in order to fill out templates with information about a particular scenario. I was disappointed to find that only five of the articles were actually about IE research. The other half of the articles addressed issues peripheral to IE, such as information retrieval (IR) and text classification. The first two articles are by Yorick Wilks and by Ralph Grishrnan, who are prominent IE researchers in the UK and the US, respectively. Each gives a high-level discussion of IE, its successes and limitations. Wilks makes the observation that IE's strength comes from its modular architecture. Individual modules such as part-of-speech tagging or morphology analysis can be constructed and optimized independently and reused in a variety of applications. He sees the primary limitation of IE to be the template representation that restricts the type of information that can be extracted. Grishman describes the typical architecture of IE systems whose modules include lexical analysis, name recognition, shallow syntactic parsing, task-specific pattern matching, coreference analysis, event merging, and finally template generation. He identifies the main challenges to IE as the cost of adapting a system to a new domain or scenario and a ceiling on performance, which is closely related to the issue of knowledge acquisition and difficulty handling complex syntactic structures. Three other articles deal with more specialized topics within IE. Robert Gaizauskas, Kevin Humphreys, Saliha Azzam, and Yorick Wilks describe a system for multilingual IE with some language-independent modules that are indexed by language-specific lexicons. Roberto Basili and Maria Teresa Pazienza discuss corpus-driven lexical acquisition, in particular for the \"foreground\" lexicon of words that support a particular IE task. Branimir Boguraev and Christopher Kennedy present work in technicalterm recognition and how this can be a step towards document summarization. The remaining articles concern IR, text dassification, or heterogeneous database techniques, and are only tangentially related to IE. Gregory Grefenstette presents an NLPbased strategy for suggesting additional IR query terms to a user. Alan Smeaton gives a tutorial on uses of NLP in IR. Nicola Guarino discusses formal ontologies and how these can enhance IR with semantic matching. Filippo Neri and Lorenza Saitta give a tutorial on machine learning that briefly touches on text classification. Sophie Cluet describes database techniques for querying semi-structured Web pages.--Stephen Soderland, Children's Hospital, Seattle",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J99-2012",
  "title": "Briefly Noted",
  "year": 1999,
  "venue": "CL",
  "abstract": "It may seem unusual for the author of a directly competing textbook (Covington 1994) to review this one---but in fact the competition is not head-on. My book introduces Prolog programmers to natural language, whereas this book introduces linguists to Prolog programming. As such, it helps solve the problem that there is no easy way for noncomputational linguists to get started in computing, and I may well use it in a course. The book covers most of Prolog but only a small and central part of computational linguistics, namely parsing. The first few chapters are a conventional introduction to Prolog except that the examples are chosen to be of interest to linguists (e.g., databases listing what language is spoken where) and little background in formal logic is presumed. Knowledge representation is introduced slowly to keep students from getting lost. The author demonstrates a sureness of touch derived, no doubt, from classroom experience. Compared to another competitor (Dougherty 1994), Matthews focuses more on Prolog and on parsing in general rather than a specific linguistic theory. The only section that may go awry in the classroom is that on structured objects (p. 61 ff.), where Prolog terms like language(uk) and queen(england) are described as involving \"functions.\" Students are likely to think they are functions that return values--that queen(england) evaluates to something identifying the Queen of England. It does not; it's just a data structure with queen in the functor position and england in the argument position. I find that I have to emphasize this in the classroom. Still, this is only a small part of an otherwise fine presentation, and it may be that Matthews can steer his students clear of this misunderstanding. The latter part of the book introduces transition networks, DCGs, and (briefly) leftcomer and chart parsing. The presentation is clear but, of course, is not a complete course in computational linguistics. (Neither is my book or Dougherty's.) But it is enough to get students started, and it is probably as much as those without previous programming experience can be expected to absorb in a single course.--Michael A. Covington, University of Georgia",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/J99-3011",
  "title": "Letter to the Editor: Grammatical Depth: A Rejoinder",
  "year": 1999,
  "venue": "CL",
  "abstract": "I was a little startled to find Victor Yngve (1998) using my review of his book From Grammar to Science as a peg on which to hang a sustained critique of my article \"Depth in English grammar\" (Sampson 1997), an item that was mentioned only tangentially in the book review. My 1997 article tested Yngve's famous depth hypothesis of forty years ago against data of a kind richer than was available then. It showed that the hypothesis as formulated by Yngve was not precisely the correct generalization about English, but it also showed that there does exist an invariant quantitative property of English syntactic structures that is closely related to Yngve's hypothesis. Yngve regards this demonstration as empty, on the grounds that there are different schemes for representing English grammatical structure, and hence my finding could have been merely the result of an arbitrary choice of analyses. \"The fact that [Sampson] could not have anticipated the result he found does not validate the work as empirical scientific research; any nonsensical result would be equally unanticipated\" (Yngve 1998, p. 635). Of course there are alternative schemes of grammatical analysis. My article drew attention to this, and surmised that the finding might be robust with respect to choice of analytic scheme. (This surmise could prove mistaken, but Yngve does nothing to suggest that it was in fact mistaken.) The point that seems to be lost on Yngve, though, is that there was no a priori reason to expect the data to yield any result as specific and precise as the result that emerged. It was a surprise to me to find, long after publication of the SUSANNE Corpus, that its structures displayed a quantitative property whose distribution possesses so low a standard deviation round its mean as does the property of \"raw production-based sentence depth\" defined in Sampson (1997). Yngve seems to imagine that the team responsible for the SUSANNE Corpus brought about this outcome by making arbitrary analytic decisions. Protestations about our research ethics would be redundant here; we would not have known how to cook the books that way even if we wanted to. No member of the team that produced the SUSANNE Corpus was aware of the measure that later turned out to be highly invariant. (The team had scattered by the time I engaged in the research reported in Sampson [1997], so most members are very likely unaware of it even now.) If we had been conscious of it, I cannot imagine how we could have gone about forcing our analyses to conform to the quantitative invariant, while achieving consistency with published analytic guidelines defined in great detail and in entirely nonquantitative terms. All that effort, just to manufacture a basis for one 20-page journal article? I don't think so. The invariant was there in the language samples; we didn't put it there. On the wider issue, whether empirical science can--never mind should--be",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/J99-4006",
  "title": "Conceptions of limited attention and discourse focus",
  "year": 1999,
  "venue": "CL",
  "abstract": "Walker (1996) presents a cache model of the operation of attention in the processing of discourse as an alternative to the focus space stack that was proposed previously by Grosz and Sidner (Grosz 1977a; Grosz and Sidner 1986). In this squib, we present a critical analysis of the cache model and of Walker's supporting evidence from anaphora in discourses with interruptions and from informationally redundant utterances. We argue that the cache model is underdetermined in several ways that are crucial to a comparison of the two models and conclude that Walker has not established the superiority of the cache model. We also argue that psycholinguistic evidence does not support the cache model over the focus stack model.",
  "stance": -0.9
 },
 {
  "url": "https://aclanthology.org/P84-1040",
  "title": "There Still Is Gold in the Database Mine",
  "year": 1984,
  "venue": "COLING",
  "abstract": "Let me state clearly at the outset that I disagree with the premise that the problem of interfacing to database systems has outlived its usefulness as a productive environment for NL research. But I can take this stand strongly only by being very liberal in defining both \"natural language interface\" and \"database systems\". same as \"Are there any vice presidents who are either male or female\". This same system, when asked for all the Michigan doctors and Pennsylvania dentists, produced a list of all the people who were either doctors or dentists and who lived in either Michigan or Pennsylvania. This is the state of our art?",
  "stance": -0.5
 },
 {
  "url": "https://aclanthology.org/P84-1041",
  "title": "Is There Natural Language after Data Bases?",
  "year": 1984,
  "venue": "COLING",
  "abstract": "1. Why Not Data Base Query? The undisputed favorite application for natural language interfaces has been data base query. Why? The reasons range from the relative simplicity of the task, including shallow semantic processing, to the potential real-world utility of the resultant system. Because of such reasons, the data base query task was an excellent paradigmatic problem for computational linguistics, and for the very same reasons it is now time for the field to abandon its protective cocoon and progress beyond this rather limiting task. But, one may ask, what task shall then become the new paradigmatic problem? Alas, such question presupposes that a single, universally acceptable, syntactically and semantically challenging task exists. I will argue that better progress can be made by diversification and focusing on different theoretically meaningful problems, with some research groups opting to investigate issues arisinq from the development of integrated multi-purpose systems.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P84-1054",
  "title": "On Parsing Preferences",
  "year": 1984,
  "venue": "COLING",
  "abstract": "It is argued that syntactic preference principles such as Right Association and Minimal Attachment are unsatisfactory as usually formulated. Among the difficulties are: (I) dependence on ill-specified or implausible principles of parser operation; (2) dependence on questionable assumptions about syntax; (3) lack Of provision, even in principle, for integration with semantic and pragmatic preference principles; and (4) apparent counterexamples, even when discounting (I)-(3). A possible approach to a solution is sketched. I. Some preference principles The following are some standard kinds of sentences illustrating the role of syntactic preferences. (I) John bought the book which I had selected for Mary (2) John promised to visit frequently (3) The girl in the chair with the spindly legs looks bored (4) John carried the groceries for Mary (5) She wanted the dress on that rack (6) The horse raced past the Darn fell (7) The boy got fat melted (I) (3) illustrate Right Association of PP's and adverbs, i.e., the preferred association of these modifiers with the rightmost verb (phrase) or noun (phrase) they can modify (Kimball 1973). Some variants of Right Association (also characterized as Late Closure or Low Attachment) which have Dean proposed are Final Arguments (Ford et al. 1982) and Shifting Preference (Shieber 1983); the former is roughly Late Closure restricted to the last obligatory constituent and any following optional constituents of verb phrases, while the latter is Late Closure within the context of an LR(1) shiftreduce parser. Regarding (4), it would seem that according to Right Association the PP for Mar~ should be preferred as postmodifier of groceries rather than carried; yet the opposite is the case. Frazier & Fodor's (1979) explanation is based on the assumed phrase structure rules VP -> V NP PP, and NP -> NP PP: attachment of the PP into the VP minimizes the resultant number of nodes. This principle of Minimal Attachment is assumed to take precedence over Right Association. Ford et al's (1982) variant is Invoked Attachment, and Shieber's (1983) variant is Maximal Reduction; roughly speaking, the former amounts to early closure of no___nn-final constituents, while the latter chooses the longest reduction among those possible reductions whose initial constituent is \"strongest\" (e.g., reducing V NP PP to VP is preferred to reducing NP PP to PP). In (5), Minimal Attachment would predict association of the PP on that rack with wanted, while the actual preference is for association with dress. Both Ford et al. and Shieber account for this fact by appeal to lexical preferences: for Ford et al., the strongest form of want takes an NP complement only, so that Final Arguments prevails; for Shieber, the NP the dress is stronger than wanted, viewed as a V requiring NP and PP complements, so that the shorter reduction prevails. sentence (6) leads most people \"down the garden path\", a fact explainable in terms of Minimal Attachment or its variants. The explanation also works for (7) (in the case of Ford et al. with appeal to the additional principle that re-analysis of complete phrases requiring re-categorization of lexical constituents is not possible). Purportedly, this is an advantage over Marcus' (1980) parsing model, whose three-phrase buffer should allow trouble-free parsing of (7). 2. Problems with the preference principles 2.1 Dependence on ill-specified or implausible principles of parser operation. Frazier & Fodor's (1979) model does not completely specify what structures are built as each new word is accommodated. Consequently it is hard to tell exactly what the effects Of their preference principles are. Shieber's (1983) shift-reduce parser is welldefined. However, it postulates complete phrases only, whereas human parsing appears to involve integration of completely analyzed phrases into larger, incomplete phrases. Consider for example the following sentence Deginnings: (8) So I says to the ... (9) The man reconciled herself to the ... (10) The news announced on the ... (11) The reporter announced on the ... (12) John beat a rather hasty and undignified ... People presented with complete, spoken sentences beginning like (8) and (9) are able to signal detection of the errors about two or three syllables after their occurrence. Thus agreement",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/P84-1071",
  "title": "What Not to Say",
  "year": 1984,
  "venue": "COLING",
  "abstract": "A problem with most text production and language generation systems is that they tend to become rather verbose. This may be due to negleetion of the pragmatic factors involved in communication. In this paper, a text production system, COMMENTATOR, is described and taken as a starting point for a more general discussion of some problems in Computational Pragmatics. A new line of research is suggested, based on the concept of unification.",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/C86-1034",
  "title": "Parsing Without (Much) Phrase Structure",
  "year": 1986,
  "venue": "COLING",
  "abstract": "Approaches to NL syntax conform in varying degrees to the older relational/dependency model, (essentially that assumed in traditional grammar), which treats a sentence as a group of words united by various relations, and the newer constituent model. Some modern approaches have nonetheless involved shifts away from essentially constituent-based models of the sort associated with Bloomfield and Chomsky to more relation-based ones (e.g. case grammar, relational grammar, daughter-dependency and word grammar, corepresentational grammar) while some others, notably lexical-functional grammar, have nonetheless continued to rely crucially on certain techniques inherited from constituency-based grammar, particularly context-free grammar. In computational linguistics there is a strong (if not universal) reliance on phrase structure as the medium via which to represent syntactic structure; call this the CONSENSUS VIEW. A significant amount of effort has accordingly been invested in techniques by which to build such a representation efficiently, which has in turn led to considerable work on the formal and computational properties of context-free gramamrs (or natural extensions of them) and of the associated languages. In its strongest form, the consensus view says that the recovery of a fully specified parse tree is an essential step in computational language processing, and would, if correct, provide important support for the constituent model. In this paper, we shall critically examine the rationale for this view, and will sketch (informally) an alternative view which we find more defensible. The actual position we shall take for this discussion, however, is conservative in that we will not argue that there is no place whatever for constituent analysis in parsing or in syntactic analysis generally. What we WILL argue is that phrase structure is at least partly redundant in that a direct leap to the composition of some semantic units is possible from a relatively underspecified syntactic representation (as opposed to a complete parse tree). However, see Rindflesch forthcoming for an approach to.parsing which entails a much stronger denial of the consensus view.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/C86-1038",
  "title": "Q&A: Already a Success?",
  "year": 1986,
  "venue": "COLING",
  "abstract": "When Prof. Wolfgang Wahlster (the organizer of this COLING-86 panel on \"Natural Language Interfaces: Ready for Commercial Success?\") sent out invitations to panelists, he stated that his goals were \"to evaluate three natural language interfaces which were introduced to the commercial market in 1985 and to relate them to current research in computational linguistics.\" For comparison, he has asked each of us panelists to answer a standard set of questions. These I will answer, but first let me set the stage by answering two logically prior queries.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/C86-1067",
  "title": "A Kana-Kanji Translation System for Non-Segmented Input Sentences Based on Syntactic and Semantic Analysis",
  "year": 1986,
  "venue": "COLING",
  "abstract": "This paper presents a disambiguation approach for t ransla t ing non-segmented-Kana into Kanji. The method consists of two steps. In the first step, an input sentence is analyzed morphologically and ambiguous morphemes are stored in a network form. In the second step, the best path, which is a string of morphemes, is selected by syntactic and semantic analysis based on case grammar. In order to avoid the combinatorial explosion of possible paths, the following heuristic search method is adopted. First, a path that contains the smallest number of weighted-morphemes is chosen as the quasi-best path by a best-first-search technique. Next, the restricted range of morphemes near the quasi-best path is extracted from the morpheme network to construct preferential paths. An experimental system incorporating large dictionaries has been developed and evaluated. A translat ion accracy of 90.5% was obtained. This can be improved to about 95% by optimizing the dictionaries.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/C86-1084",
  "title": "Linguistic Bases For Machine Translation",
  "year": 1986,
  "venue": "COLING",
  "abstract": "Researchers in MT do not work with linguistic theories which are 'on vogue' today. The two special issues on MT of the journal Computational Linguistics (CL 1985) contain eight contributions of the leading teams. In the bibliography of these articles you don't find names like Chomsky, Montague, Bresnan, Gazdar, Kamp, Barwise, Perry etc.[2] Syntactic theories like GB, GPSG, LFG are not mentioned (with one exception: R. Johnson et al. (1985 0.165) praise I.FG for its 'perspicuous notation', but do not (or not yet) incorporate ideas from LFG into their theory of MT). There arc no references whatsoever to recent semantic theories.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/C86-1128",
  "title": "A PROLOG Implementation of Government-Binding Theory",
  "year": 1986,
  "venue": "COLING",
  "abstract": "A number of recent research efforts have explicitly grounded parser design on linguistic theory (e.g., Bayer et al. (1985), Berwick and Weinberg (1984), Marcus (1980), Reyle and Frey (1983), and Wehrli (1983)). Although many of these parsers are based on generative grammar, and transformational grammar in particular, with few exceptions (Wehrli (1983)) the modular approach as suggested by this theory has been lagging (Barton (1984)). Moreover, Chomsky (1986) has recently suggested that rule-based parsers are implausible and that parsers could be based on lexical properties and structure determining principles.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/C88-1011",
  "title": "A System for Creating and Manipulating Generalized Wordclass Transition Matrices From Large Labelled Text-Corpora",
  "year": 1988,
  "venue": "COLING",
  "abstract": "This paper deals with the training phase of a Markov-type linguistic model that is based on transition probabilities between pvirs and triplets of syntactic categories. To determine the o?timal level of detail for a set of syntactic classes we developed a systetn that uses a set-theoretical formalism to defiue such sets mid has some measm~s to comp~uce and c,ptimize them fildividually. In section two we describe the optimizafiou problem (hi terms of piediction, infoimation and economy requilements) and our approach to its solution. Section three introduces the system dlat will assist a lhlguist in h,'mdling the prediction and economy criteria and in the last section we plesent some slunple lemtlts that can be achieved with it.",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/C88-1040",
  "title": "Robust parsing of severely corrupted spoken utterances",
  "year": 1988,
  "venue": "COLING",
  "abstract": "This paper describes a technique for enabling a speech understanding system to deal with sentences for which some monosyllabic words are not recognized. Such words are supposed to act as mere syntactic markers within the system linguistic domain. This result is achieved by combining a modified caseframe approach to linguistic knowledge representation with a parsing strategy able to integra te expectat ions from the language model and predictions from words. Experimental results show that the proposed technique permits to greatly increase the quota of corrupted sentences correctly understandable without sensibly decreasing parsing efficiency.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C88-1049",
  "title": "Chart Parsing According to the Slot and Filler Principle",
  "year": 1988,
  "venue": "COLING",
  "abstract": "A parser is an algorithm that assigns a structural description to a string according to a grammar. It follows from this definition that there are three general issues in parser design: the structure to be assigned, the type of grammar, the recognition algo~ rithm. Common parsers employ phrase structure descriptions, rule-based grammars, and derivation or transition oriented recognition. The following choices result in a new parser: The structure to be assigned to the input is a dependency tree with lexical, morpho-syntactic and functional-syntactic information associated with each node and coded by complex categories which are subject to unification. The grammar is lexicalized, i.e. the syntactical relationships are stated as part of the lexical descriptions of the elements of the language. The algorithm relies on the slot and filler principle in order to draw up complex structures. It utilizes a well-formed substring table (chart) which allows for discontinuous segments. 1. D e p e n d e n c y S t r u c t u r e The structuring principle of constituency trees is concatenation and the part-whole -relationship. The structuring principle of dependency trees is the relationship between lexemes and their complements. Note: It is not correct (or at least misleading) to define dependency as a relationship between words, as it is often done. The possibility and necessity of complements depend on the lexical meaning of words, i.e. a word which denotes a relationship asks for entities which it relates, a word which denotes a modification asks for an entity which it modifies etc. While it is awkward to associate functions (deep cases, roles, grammatical relationships) with phrase structures, it is not difficult to paraphrase the functions of complements on a lexical basis. For example, the argument of the predicate \"sleep\" denotes the sleeper; the meaning of \"persuade\" includes the persuader, the persuaded person and the contents of the persuasion. In a next step, one can abstract from the concrete function of dependents and arrive at abstract functions like subject, object, adjunct etc. Of course, the complements covering these roles can be single words as well as large phrases; for example \"John\", \"my father\", \"the president of the United States\" can all fill the role of the sleeper with respect to the predicate \"sleep\". However, phrases need not be represented by separate nodes in dependency trees (as they do in phrase markers) because their internal structure is again a question of dependency between lexemes and their complements. In a dependency tree, phrases are represented directly by their internal structure, which results in an arc between the superodinated head and the head within the complementary phrase. Nevertheless, the real principle of depen242 dency is a relationship between words and structures, or, formally, between single nodes and trees. Taking this into account, dependency trees are much more appealing than has been recognized so far. In order to restrict linguistic structures according to syntactic and semantic requirements, the use of complex categories is state of the art. Complex categories are sets of parameters (attributes) and values (features). Agreement between entities can be formulated in a general way in terms of parameters; the assignment of actual feature values is achieved by the mechanism of unification. If dependency J.s the relationship along which the catagories are unified, functional=syntactic and mo~ho-syntactic features can be handeled completely in parallel, as opposed to the two-phase mechanism which, for example, characterizes Lexical Functional Grammar. Each element in the dependency tree carries three labels: a role (which applies to the (sub)tree of which the element is the head), a lexeme, and a set of grammatical features. Constituency and dependency both have to be represented somehow or other in the syntactic description. As a consequence, recent developments have led to a convergence of formalisms of both origins with respect to their contents. (A good example is the similarity between Head-Driven Phrase Structure Grammar /Pollard, Sag 1987/ and Dependency Unification Grammar /Hellwig 1986/.) If phrase structure trees are used, the difference between governor and dependent must be denoted by the categories that label the nodes, e.g. by a x-bar notation. If dependency trees are used, the concatenation relationship must be denoted by positional features which are part of the complex morpho-svntactic category. 2. C h a r t p a r s i n g b a s e d on a l ex ica l i zed g r a m m a r The structure to be associated with a wellformed string can be defined in two ways: either by a set of abstract rules which describe the possible constructions of the ~language or by a description of the combi-. nation capabilities of the basic elements. The latter fits with the dependency approach. Given a lexical item and its morphosyntactic properties, it is relatively easy to give a precise description of its possible complements. The main advantage of this lexicalistic approach is the fact that augmenting or changing the description of an item normally does not interfere with the rest while any change in a rule-based grammar might produce unforeseen side effects with regard to the whole. The prerequisite for a lexicalized dependency grammar are trees that comprise slots. A slot is a description of the head of a tree that fits into another tree. Formally, a slot is a direct dependent of a head with a role associated to it, with a variable in the lexeme position, and with a categorization that covers all of the morpho-syntactic properties of the appertaining complement. If cross categorization does not allow all of the p~ssible properties of a complement within one category to be stated, a disjunction of slots is used to express the alternatives. The only mechanism needed for draw-ing up complex structures is the unification of slots and potential fillers. The control of the parsing process is achieved by means of a well-formed substring table ((]hart). It is widely accepted that chart parsing is superior to backtracking or to parallel processing of every path. A common version of a chart can be vizualized as a network of vertices representing points in the input, linked by edges representing segments. The edges are labelled with the categories that the parser has assigned to the constituents concerned. Alternatively, each edge is associated with a complete structural descrLption, including the information which is carried by the covered edges. In this case, a chart is simply a collect]on of trees (implemented as lists) projected on the various segments in the input. The innovation with regard to chart parsing th~vt is proposed in this paper is a label.ling of edges by trees that comprise slots. At the beginning, an edge for each word is entered into the chart. Each edge is label] o~ ed with a tree. The head of this tree contains the lexeme that is associated with the word according to the ].exicon; it carries a morpho-syntactic category according to the morphological properties of the word in question: it normally contains a variab].e as a role l~arker, since the syntagmatic function of the corresponding segment is still unknown. A slot is subordinated to the head for each element that is to be dependent in the resulting structure, if any. Slots are added to a lexical item according to c~>mpletion patterns refered to in the lexicon. (We can not qo into details here.) Subsequently, each tree in the chart looks for a slot in a \"tree that is associated with annother edge. If the head of the searching tree fitn the description in the slot then a new edge is drawn and labelled with the compound tree that results from inserting the first tree into the second. The categories of the ~ew tree are the result of unifying the categories of the slot tree and the filler tree. Special features state the positional re~/irements, e.g. whether the segment corresponding to the filler has to preceed or to follow of the segment corresponding to the element dominating the slot. This process continues until no new tree is produced. Parsing was successful if at ].east one edge covers the whole input. The dependency tr~e associated with this edge is the desired structural description. The fo].lowing example illustrates the m e -",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/C88-1070",
  "title": "Schema Method: A Framework for Correcting Grammatically Ill-formed Input",
  "year": 1988,
  "venue": "COLING",
  "abstract": "The schema method is a framework for correcting grammatically ill-formed input. In a natural language processing system ill-formed input cannot be overlooked. A computer assisted instruction (CAD system, in particular, needs to show the user's errors. This framework diagnoses ill-formed input, corrects it and explains the error, if an input is ill-~'ormed. The framework recognizes a sentence at two steps: first parses weak grammar, and then strongly filters the parsed sentence. When it is known what sentences are passed by the filter, it can be used even if it is imperfect. As the strong filter, a new method is used: an interpretation schema and an interpretation rule. An interpretation schema collects input information schemata and then an interpretat ion rule judges whether the collected schemata are correct or incorrect. This approach overcomes the problem of relaxation control, the major drawback of the previous syntactically-oriented methods, and is also more efficient. 1° In t roduct ion Ill-formed input cannot be ignored when a natural language processing system such as a computer assisted instruction (CAD system or a machine translation system is built. Particularly in a CAI System, students often make mistakes, such as mispunctuation, lack of agreement, misplaced/improperly-used words, etc. In these cases, a CAI system needs to point out input errors, and show why the input it~ wrong. In order to do so, the system needs to diagnose and correct ill-formed input to explain the errors. The schema method as a f ramework for correct ing grammatical ly ill-formed input is suggested and the diagnosis and correction of errors is discussed. There have been many studies for processing ill-formed input for English. The point of those studi.es is the diagnosis: how does the system find an error? The approaches are c lass i f ied in to two g roups : the syntactically-oriented group and the frame-based group. The syntact ical ly-oriented group includes robust parsers based on Augmented Transition Networks (ATN) which Use the relaxation technique/Kwansny 1981./or the meta-rule/Weisehedel 1980, 82, 87/, and the EPISTLE system which addresses the problems of the checking grammar and style of texts, such as letters, reports and manuals, written in ordinary Engl ish/Heidorn 1982/, /Jensen 1983/. The f r ame-based group a t t emp t s to deal wi th ungramnmtical input through extensions to pa t te rn matching parsing/Hayes 1981/, through conceptual case frame instantiation/Schank 1980/and through approaches involving mult iple coopera t ing pa r s ing s t r a t eg ies /Carbonell 1983/. The target of that study is dialogue phenomena in communicat ion with l imi ted-domain systems, such as data-base systems, electronic mail systems, etc. The aim of this study is error-correction of non-native speakers wri t ten Engl i sh text. This approach is syntactically oriented. The syntactically-oriented approaches/Kwansny 1981/ /Weischedel 1980,82,87/,/Heidorn 1982/,/Jensen 1983/are very similar. Their basic idea is relaxation. They first attempt to parse the input, using fully grammatical rules. If the sentence is not parsed, some of the conditions are relaxed. However these approaches have two major drawback. (1)Relaxation control strategies: when inputs are illformed, some means of ranking alternatives is appropriate. The number of relaxed configurations may be large. One of the most critical problems is control. The need to relax the very rules that constrain the search for an interpretation is like opening Pandora's box./Weischedel 1987(PP.117)/ (2)Computational inefficiency: the relaxation approach cannot recognize ill-formed input before the analysis with well-formed grammar is finished. Furthermore, fully wellformed grammar is needed. To make fully well-formed grammar, subcategorization of parts of speech is needed and other conditions are added. As a result, there are too many rules. In comparison to previous approaches, this approach does not use the relaxation technique. The difference between previous approaches and this one is the method of recognizing an ill-formed sentence. Previous approaches first use a strong filter, then relax the conditions. This approach, however, first uses weak grammars, and then strongly filters the passed sentence. This approach recognizes a sentence at two steps. An at tempt is made to expand lexical-functional grammar (LFG) /Kaplan 1982/ to deal with ill-formed input. LFG has two drawbacks: (1) LFG can't deal with errors of omission and (2) LFG has no framework for error correction. If an input sentence is well-formed, this framework obtains an LFG f-structure. If not, the sentence is corrected. Examples of error correction are given in the next section. In the section following the basic idea is described",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C88-1082",
  "title": "Linguistic Processing Using a Dependency Structure Grammar for Speech Recognition and Understanding",
  "year": 1988,
  "venue": "COLING",
  "abstract": "This paper proposes an efficient linguistic processing strategy for speech recognition and understanding using a dependency structure grammar. The strategy includes parsing and phrase prediction algorithms. After speech processing and phrase recognition based on phoneme recognition, the parser extracts the sentence with the best likelihood taking account of the phonetic likelihood of phrase candidates and the linguistic likelihood of the semantic inter-phrase dependency relationships. A fast parsing algorithm using breadth-first search is also proposed. The predictor pre-selects the p}~.ase candidates using transition rules combined with a dependency structure to reduce the amount of phonetic processing. The proposed linguistic processor has been tested through speech recognition experiments. The experimental results show that it greatly increases the accuracy of speech recognitions, and the breadth-first parsing algorithm and predictor increase processing speed.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C88-2095",
  "title": "Reasons why I do not care grammar formalism",
  "year": 1988,
  "venue": "COLING",
  "abstract": "Computational linguistics (CL) has borrowed a lot of ideas from Theoretical Linguistics (TZ). We could not have developed even a simple parser without the research results in TL. It is obviously nonsense to claim that we, computational linguists, do not care research results in TL. llowever, the researchers in TL, it seems to me, are very fond of fighlinq~ especially, those who are called Synlaclicians. They always fight with e~h other by asserting that their grammar formalisms are superior to the others'. They are oversensitive and tend to distinguish people into two groups, the ally and the enemy.",
  "stance": -0.2
 },
 {
  "url": "https://aclanthology.org/C88-2111",
  "title": "Using a Logic Grammar to Learn a Lexicon",
  "year": 1988,
  "venue": "COLING",
  "abstract": "It is suggested that the concept of \"logic grammar\" as re la t ion be tween a str ing and a parset ree can be ex tended by admi t t i ng the lexicon as par t of the relation. This makes it possible to give a s imple and elegant formulat ion of the process of infering a lexicon f rom e x a m p l e sen tences in con junc t ion wi th a g r a m m a r . V a r i o u s p r o b l e m s a r i s i n g f r o m implementa t ion and complexity factors are considered, and examples are shown to support the claim that the method shows potential as a practical tool for automatic lexicon acquisition.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/C90-1016",
  "title": "Discourse Processing in MT: Problems in Pronominal Translation",
  "year": 1990,
  "venue": "COLING",
  "abstract": "1. Intrcnluction Translation of anaphoric expressions has been problematic in most of the MT systems (Key 1986). One ot' the main reasons for the difficulties lies in the lack of discourse information representation in the MT systems. In this paper, we report an implementation of the Discourse Representation Theory in an LFG-based Engl ishto-Japanese MT program, and discuss problems in translating anaphoric expressions in this system. 2. Problems in Translating Anaplmra Problems in translation of anaphoric expressions can be seen on three different but interactive levels of l inguistic information: lexical, syntact ic and pragmatic. The main problem on the lexical level is due to a difference in the language specific parameters in the pronominal system such as F features (person, gender, number, etc.). Surface forms of pronominals depend on the F features of their antecedents, so that the translation of a particular pronominal form cannot be determined sorely by the pronominal itself. For example, 'ship' in English is feminine, but its translation, 'hune', is neutral in Japanese. Thus, a preform 'she' for 'ship' should not be translated as 'kanojo' (3rd, sing, fem), but as 'sore' (3rd, sing, neut). Problems on the syntactic level are mainly due to a difference in the distribution of anaphoric expressions. Mapping relations between English pronominals and their Japanese counterparts are shown below:",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/C90-2015",
  "title": "A Multi-Level Account of Cleft Constructions in Discourse",
  "year": 1990,
  "venue": "COLING",
  "abstract": "This pal~;r presents an analysis and synthesis of the factors relevant to the decision to use a cleft construction in discourse. The model described, based on a corpus of 587 naturally-occurring cleft constructions in written and spoken discourse, consists of two stages. The first stage concerus the decision to use a cleft construction rather than a non-cleft; the second describes the factors relevant in deciding between three types of cleft: it-clefts, such as that in example (1), wh-clefts, such as (2), m~d reverse wh-clefts, such as (3):",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/C90-2026",
  "title": "GPSG Parsing, Bidirectional Charts, and Connection Graphs",
  "year": 1990,
  "venue": "COLING",
  "abstract": "Ttfs paper describes a tractable method for parsing GPSG grmnmars without ,altering the modularity and expressiveness of this tbnnalism. The proposed method is based on a constraint propagation mech,'ufism which reduces the number of unnecessary structures built at parse thne through the early detection of inadmissible local trees. The propagation of constraints is rendered efficient by indexing constraints and categories in a connection graph m~d by using a bidirectional chat1 pm~er together with a botlore-up strategy centered around head constituents.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C90-2061",
  "title": "A Type-theoretical Analysis of Complex Verb Generation",
  "year": 1990,
  "venue": "COLING",
  "abstract": "Tense and aspect, together with mood and modality, usually form the entangled structure of a complex verb. They are often hard to translate by machines, because of both syntactic and semantic differences between languages. This problem seriously affects upon the generation process because those verb components in interlingua are hardly rearranged correctly in the target language. We propose here a method in which each verb element is defined as a mathematical function according to its type of type theory. This formalism gives each element its legal position in the complex verb in the target language and certifies so-called partial translation. In addition, the generation algorithm is totally free from the stepwise calculation, and is available on parallel architecture.",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/C90-3016",
  "title": "Structured Meanings in Computational Linguistics",
  "year": 1990,
  "venue": "COLING",
  "abstract": "Many natural language processing systems employ t ruth conditional knowledge representations (%ret)resentations' , etc.) to represent meanings of nata rm language expressions. T-representations have their strong and their weak sides. A strong side is logic: a relation of logical consequence can be de-. fined between such representations. A weak side is e x p r e s s i v e p o w e r : the capacity of t-representations to convey the subtleties of natural language is limited. For instance, let SL be a sentence that is true on purely logical grounds; then it is predicted ttmt any sentence S is synonymous with \"S and SL\". This deficiency comes out clearest in propositional attitude constructions, i.e. constructions of the form 'x V that S'; where V is an epistemic verb ('knows', ~believes') and S a sentence. Truth conditional accounts of nleaning (including intensional ones such an [Montague 1974]) predict wrongly ~hat anybody who knows that S is bound to also know that :'S and SL\", since t;he two sentences are t-indistinguishable ([Peters and Saarinen 1982]). The same lack of expressive power dooms, for example, automatic translation on the basis of t-representations to failure: trepresentations contain only information that is relevant for the truth or falsity of a sentence, dismissing all other information, such an mood, topic-con:merit ,,,tructure, etc. ([van Deemter-89]).",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/C90-3019",
  "title": "Organizing linguistic knowledge for multilingual generation",
  "year": 1990,
  "venue": "COLING",
  "abstract": "Abs t rac t We propose an architecture for the organisation of linguistic knowledge which allows to (1) separately formulate generalizations for different types of linguistic information, and (2) state interrelations between partial information belonging to different levels of description. We use typed feature structures for encoding linguistic knowledge. We show the application of this representational device for the architecture of linguistic knowledge sources for nmltilingum generation. As an example, we describe the use of interacting collocational and syntactic constraints in the generation of French and German sentences.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/C90-3020",
  "title": "Linear Encodings of Linguistic Analyses",
  "year": 1990,
  "venue": "COLING",
  "abstract": "1. (1) is of course highly unnatural in a sense. However, it effectively isolates for study a phenomenon that is intrinsic to natural language. Similar observations apply to the examples below. 2. It is of course also the case that an exponentially long answer caunot be produced in polynomial time. If the problem cannot be reformulated so that answers are not exponentially long, the question of tractability does not arise. See [Garey and Johnson 79] and [Barton, Berwick, and Ristad 87] for related discussions. basis than exponential-space encodings for explanations of how humans process language.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/C90-3035",
  "title": "Expressive Power of Grammatical Formalisms",
  "year": 1990,
  "venue": "COLING",
  "abstract": "We propose formalisms and concepts which allow to make precise the m'gmnents in controversies over the adequacy of competing models of language, and over their formal equivalence.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/C90-3087",
  "title": "Corpus Work With Pc Beta",
  "year": 1990,
  "venue": "COLING",
  "abstract": "0. Abstract. PC Beta is a PC oriented tool for corpus work in this term's broadest possible sense. With PC Beta one can prepare texts for corpus work, e.g. standardize texts in different ways (very important when texts from different sources together will constitute a corpus), one can process texts, and one can analyze texts. Making ordinary concordances and similar things with PC Beta is, of course, very simple, and, in fact, PC Beta give, s \"concordance making\" a new dimension. One can perform morphological analyses, one can use PC Beta as a \"tagger\", i.e. provide the words with different kinds of tags. In all, PC Beta is a versatile program, and it is in many cases the only program needed (together with functions belonging to the MS/PC-DOS operative system) for pursuing a complete corpus project. The program's main distinctive feature is simplicity: it is rule controlled, and the rules adhere to a format that any linguist can learn to understand very quickly. But beware, in spite of its innocent appearence the program i,; a little tiger.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/C92-1009",
  "title": "Feature Structure Based Semantic Head Driven Generation",
  "year": 1992,
  "venue": "COLING",
  "abstract": "This paper proposes a genera t ion me thod for fea ture-s t ructured)ased unificat ion g rammars . As comlx~red with fixed ~ri ty t e rm nota t ion , feature s t ruc tu re nota t ion is more tlexible for represent ing knowledge needed to genera te idiom~ttic s t ruc tu res as well as genem~l construct ions . The method enables feature s t rnc tu re retr ieval via nml t ip le indices. The indexing mechanism, when used with a semant ic head driven generat ion algor i thm, a t t a ins efficient genera t ion even when a large amoun t of genera t ion knowledge mus t be considered. Our method can produce all possi ble s t ruc tu res in parNlet , using s t ruc ture shar ing among ambiguous subs t ruc tures . 1 I n t r o d u c t i o n Pract icM generat ion sys tems mus t lnwe l inguis t ic knowledge of both specilic expressions like id ioms and generM g r a m m a t i c a l construct ions , ;rod t tmy should efgtciently produce sm'face s t r ings applying t h a t knowledge [[][2]. In order to satisfy the first requi rement , our sys tem employs a set of t rees anno ta t ed with fe,~ture s t ruc tures to represent genera t ion knowledge. l:;ach tree represents a t?agment of a syntact ic s t rnc ture , and is paired with a semant ic feature s t ructure . We can describe id iomat ic eons t ruc t ions , by making a tree which cont~tins lexical specifications and is paired with a specilie ra ther than general semaut ic s t ructure . Because feature s t ruc tu res allow par t ia l speei i ica t iom we can encode generat ion knowledge r ;mgiug over mul t ip le levels of genera l i ty in a. uniform way. l lowever , notice tha t this p roper ty will be res t r ic ted if we use DCG or (tixed ar i ty) t e rm nota t ion 1 Suppose there is a genera t ion knowledge s t ruc ture whose syn tac t i c par t is \"go on foot\". ' r im feat, tu'e s t ruc ture nota t ion of its semant ic par t will be sonmthing like: ~The flexibility of structure notation colnpated Lo tetln notation is also discussed il~ [4]. [ [Rein GO] [Agent ?agent [] ] [Instrument FOOT]]. while the t e rm nota t ion is : (1) i n s t rumen t (go (Agen t ) , foot) (2) These two no ta t ions seem to be equivalent , but there is a cruciN diflerence. A genera t ion knowledge s t ruc tu re conta in ing the fe~tture-based selnan t ics will still be unifiable even i f the semantic input to be unified contains additional material. Thus the knowledge s t ruc tu re will be discovered and i ts syntac t ic in format ion can he used for generat ion. By cont ras t , a te rm-based inpu t wi th additiona.1 e lements would not unify with the te rm-based semant ic s t ruc tu re shown above. It would thus be necessary to create add i t ional generat ion s t ruc tures conta in ing d is t inc t ( though par t ly overlN)ping) t e rm-based semantic s t ructures . Such addi t iona l s t ruc tures are red u n d a n t ~tn(l cause superfluous ou tpu t . For example , consider the a,ugmented feature s t ruc tu re (3). [ [Rein ~o] [Agent Ken] [Instrument FOOT] [Time I0 : OOmn] ] (3) i t will indeed nnify with (1) above. But termbased input semant ic s t ruc ture (4) will not unify with te rm-based semant ic s t ruc tu re (2). i n s t r u m e n t ( t i m e ( g o ( k e n ) , 10 :00am) , f o o t ) . (4) To unifv (2), semant ic ,structure (5) would a.lso be required. t i m e ( i n s t z u m e n t ( g o ( k e n ) , f o o t ) , 10:00ma) . (5) AcrEs DE COLING-92. NANTES. 23 28 AOt~q\" 1992 3 2 PROC. OI; COLING 92. NANTES. AUG. 23 28. 1992 For this reason, our generation knowledge consists of trees represented as feature structures. A tree can be substituted for a leaf node of asother tree to form a larger structure. Thus, tile tree can be regarded as a rule in a context-free feature-structure-based unification grammar. The second requirement for a generation system is efficient creation of syntactic structures. This is the main topic of this paper. Our system is based upon Semantic }lead Driven Generation [6], which is an efficient algorithm for unilication based formalisms. However, this algorithm requires some additional mechanisms to efficiently retrieve relevant generation knowledge, because feature structures can not be easily indexed. The algorithm presented here uses a nmltiple index network of feature structures to efficiently choose relevant generation knowledge from the knowledge base. The algorithm \"also uses an hypothetical node so as to efficiently maintain ambiguous structures during generation. 2 Phrase Descr ipt ion(PD) Generation knowledge is represented as a set of trees aunotated with feature structures, l,',ach tree is called a Phrase Description (PD). ALl example of a l)D is shown in Figure.1. Structure: (S AUX (NP PRON) VP) Annotation: (S [[syn [[cat S] [inv +]]1 [sem [[reln REQUEST] [agon *SP*] [recp *HR*] [obje ?ACTION]]]]) (AUX [[syn [[cat AUX] [lex \"would\"] [v-morph PAST]]]]) (NP [[syn [[cat NP] [case NOM]]]]) (PRON [[syn [[cat PRON] [case NOM] [lex \"you\"]]]]) (VP [[syn [[cat VP][v-morph BSE]]] [sem ?ACTION]J) Figure 1: an example of a PD A PD consists of two parts: a structure definition and feature structure annotation (Structure a.nd Annotation in Figure 1). The structure definition defines tile structure of a tree by using a list in which the first element corresl)onds to the mother node and tile rest of the elements correspond to daughters. l';ach daughter may t)e a tree rather than a sin> pie node. Acres DE COLING-92, NANTes, 23-28 AO6-r 1992 3 3 The annotation part specifies the feature structure of each symhol appearing in the structure definition. A feature structure description can contain tags or variables (symbols with \"?\" as a prefix in the figure), The scope of a tag in a PD is the entire PD. Each node should have a semmltic and syntactic feature structure. The semantic feature on the root node of a PD represents the semaattics of the PD; thus we call it the semantic structure of the PD. Although the description represents a tree, it is the same ms for a (partial) derivation structure of a unification-l)ased CFG, because tile current system does not allow adjoining operations. If the structure definition of every PD is restricted to mother-daughter relations only, the PD set is strictly equivalent to a unification-based CFG. 3 Generat ion Algor i thm Our algorithm is aal efficient extension of Semaattic Head Driven Generation. 3?he major extensions are: 1) it handles feature structures directly, and 2) it creates all possible phrase structures in parallel. These extensions are embodied mainly in the t 'l) activation and ambiguity handling mechanisms discussed in this section. 3.1 O v e r v i e w o f t h e a l g o r i t h m The main part of the generation process is expansion process, which iterates through expanding node selection, activation, prccombination, and application, using an e~Tmnding node agenda. Input to the process is a feature structure conraining syntactic, semantic and pragmatic features as an initial constraint on the root node. q'he Cxl)auding node agenda contains tim unlexicalized leaf nodes of the tree under creation. At the beginning of the process, it conta.ins only one node, which has the feature structure giveu as an initial constraint. The expanding node selection step picks up one node, say expanding node, from the agenda. If no node is picked ill) , the expaa~sinn process stops. The PD activation step activates all PD's whose senlantic strlLetures s~tlJs~tme the semantic structure of the expanding node. The precombination step makes PD sequences from activated PD's to satisfy some constraints. The application step instantiates the PD sequence(s) and applies it to tile expanding node. Paoc. oe COLING-92, NAm'ES, AU~. 23-28, 1992",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/C92-1025",
  "title": "Two-Level Morphology with Composition",
  "year": 1992,
  "venue": "COLING",
  "abstract": "(1) Lexical representations tend to be arbitrary. Because it is difficult to write and test two-level systems that map between pairs of radically dissimilar forms, lexical representations in existing two-level analyzers tend to stay close to the surface forms. This is not a problem for morphologically simple languages like English because, for most words, inflected forms are very similar to the canonical dictionary entry. Except for a small number of irregular verbs and nouns, it is not difficult to create a two-level description for English in which lexical forms coincide with the canonical citation forms found in a dictionary.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/C92-1032",
  "title": "Left-Corner Parsing and Psychological Plausibility",
  "year": 1992,
  "venue": "COLING",
  "abstract": "It is well known that even extremely limited centerembedding causes people to have difficulty ill comprehension, but that leftand right-branching constractions produce no such effect. If the difficulty in comprehension is taken to be a result of processing load, as is widely assumed, then measuring the processing load induced by a parsing strategy on these constructions may help determine its plausibility as a psychological model. On this basis, it has been ~rgued [A J91, JL83] that by identifying processing load with space utilization, we can rule out both top-down and bottom-up parsing as viable candidates for the human sentence processing mechanism, attd that left-corner parsing represents a plausible Mternative. Examining their arguments in detail, we find difficulties with each presentation. In this paper we revise the argument and validate its central claim. In so doing, we discover that the key distinction between the parsing methods is not the form of prediction (top-down vs. bottom-up vs. leftcorner), but rather the ability to iastantiate the operation of composition.",
  "stance": -0.6
 },
 {
  "url": "https://aclanthology.org/C92-1033",
  "title": "TTP: A Fast and Robust Parser for Natural Language",
  "year": 1992,
  "venue": "COLING",
  "abstract": "In this paper we describe TI~ , a fast and robust natural language parser which can analyze written text and generate regularized parse structures for sentences and phrases at the speed of approximately 0.5 sec/sentence, or 44 word per second. The parser is based on a wide coverage grammar for English, developed by the New York University's Linguistic String Project, and it uses the machine-readable version of the Oxford Advanced lw~arner's Dictionary as a source of its basic vocabulary. The parser operates on stochastically tagged text, and contains a powerful skip-and-fit recovery mechanism that allows it to deal with extra-grammatical input and to operate effectively under a severe time pressure. Empirical experiments, testing parser's speed and accuracy, were performed on several collections: a collection of technical abstracts (CACM-3204), a corpus of news messages (MUC-3), a selection from ACM Computer Library database, and a collection of Wall Street Journal articles, approximately 50 million words in total.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/C92-1047",
  "title": "Une ontologie du temps pour le langage naturel",
  "year": 1992,
  "venue": "COLING",
  "abstract": "We propose a new ontology for the time in natural language which provides the following",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C92-1063",
  "title": "The Typology of Unknown Words: An Experimental Study of Two Corpora",
  "year": 1992,
  "venue": "COLING",
  "abstract": "Most current state-of-the-art natural language processing (NLP) systems, when presented with real-life texts, have problems recognizing each and every word present in the input. Depending on the application, the consequences can be severe. For example, in a machine translation system the quality of the processing may suffer and sometimes further processing may even be impossible.There are two main reasons why a word might not be recograzed and thus be considered unknown by the system:",
  "stance": -0.5
 },
 {
  "url": "https://aclanthology.org/C92-2071",
  "title": "Un Systeme Inferentiel Oriente Objet Pour Des Applications En Langues Naturelles",
  "year": 1992,
  "venue": "COLING",
  "abstract": "Up to now, there is still no specific model for solving the problem of natured language representation and reasoning. In this paper, we propose an object oriented form,'dism for supporting knowledge representation, extraction and exploitation in tile context of natural language processing. In the natural language analysis, this system is situated after the morpbo-syntax and file linguistic semantics. It represents two classes of concepts: objects of discourse and action schemata, the former resulting from nominal syntngms and the latter from the 'processes'. We are concerned here just by the representation of objects. In the natural language discourse, manipulated objects ,are complex objects ~md the reasoning is by uature first inferential and then deductive. To lake into account this kind of reasoning we need a suitable representation: a model of inferential objects. The theoretical foundations of the proposed model are Lesniewski's logical systems: tile Calculus of Names and the Mereology. The former is based on a primitive lunctor called \"epsilon\" interpreted a.s is-a, the latter is based on a par t -o f relation which is called the \"ingredience\". The whole system is supported by these two primitives and theirs derived functions. The concepts of our model result from a collaboration between linguists and computer scientists. The main concepts are the intensional and extensional universes, notions and types. The possible thferenti,'d reasoning can be of different types : it can concern the status, the denominations, the structures or the \"fonctifs\" of the objects. Key-words : Knowledge Representation, Inferential Reasoning, Object Oriented Modelling, Natural Language Processing, Language Parsing and Understanding. RESUME Duns ce papier, nous proposons un lonnalisme orient6 objet pour la reprtsentation, I'extraction et l'exploitation des connaissances duns le contexte du traitement des langues natmelles. Duns un discours en laugue naturelle, les objets manipults sont des objets complexes et le raisonnemeut est avant tout de type inftrentiel awmt d'etre dtductif. Pour pouvoir tenir compte de ce type de raisounement, nous avons besoin d'uue reprdsentation idoine : un modtle d'objets inftrentiels. Les foudemeuts thtoriques de notre modtle sont les syst~mes logiques de Lesniewski : le Calcul des Noms et la Mtrtologie. Le premier repose sur ua fonctear primitif appel6 \"epsilon\" interprtt6 comme est-un, le second sur la relation partie-de appelte \"ringredience\". Les concepts de notre modtle sont le fruit d'une collaboration entre linguistes et informaticiens. Les principaux concepts sont les univers intensionnel et extensionnel, les notions et les types. Les raisonnements infdrentiels possibles sont de difftrentes sortes : ils peuvent porter sur le statut, les dtnominations, les structurels ou les fonctifs. Mots-clts : Repr6sentation des Conmtissances, Raisonnement Inftrentiel, Mod61isation Orientte Objet, Traitement de la Langue Naturelle, Analyse morphosyntaxique et Comprdhension du langage. 1 I N T R O D U C T I O N Le syst~me prtsent6 ici a pour but la reprtsentation, rextraction et rexploitation des connaissances dans le contexte du traitement automatique des langues. On salt [Berrendonner 89] que les raisonnements reprtsent~s duns des \"discours\" en langue naturelle ne sont que rarement dtductifs et sont le plus souvent inftrentiels. Pour pouvoir tenir compte de ces misonnements, nous avons besom d'une reprtsentation idoine. I1 n'existe pas en effet ~t l'heure actuelle de modble sptcifique pour r6soudre le probl/~me de la reprtsentafiou A~'Es DE COLING-92, NANTES, 23-28 At(It 1992 4 6 l PROC. OF COLING-92, NANTES, AUG. 23-28, 1992 des connaissances et du raisonnement en langue naturelle. Dans ce document, nous d6crivons le formalisme de repr6sentatioo et certains raisonnements que notre syst~me autorise : c'est un module d'objets inf~rentiels. Ce module est lui-m~me fond6 sur les syst~mes logiques de Lesniewski [Lesniewski 89]. D,ans ces syst~mes, nous utilisons le Calcul des Noms (bas6 sur la primitive £ : \"est-un/est-le\") et la M6rdologie (dont le fonctear de base est \"pattie-tout\", appel6 ingr6dience). En d6finitive, le module objet, et tout le syst~me reposent sur ces deux seules primitives et leurs d¢rivdes. Darts une chaine d'analyse du franqais, ce syst~me se situe apr~s la morpho-syntaxe et la s6mantique linguistique. II repr6sente deux families de concepts : les objets du discours, issus de cert~dns des syntagmes nominaux, et les sch6mas d'action qui sont issus des proc~s. Nous ne nous int6ressons ici qu'h la repr6sentation des objets du discours. Une premiere partie est consacr~e au module hun niveau conceptnel : nons y donnous une pr6sentation g6ndrale, suivie des concepts sur lesquels repose notre syst~me et enfin l'unit6 de repr6sentation de connaismmce choisie. Les bases logiques permettant la formalisation, ainsi que des caract6ristiques propres au module sont pr6sent6es dans une deuxi~me section. On donne un exemple de formalisation. Les troisi~me et quatri~me parties exposent l'organisation des connaissarlces et les raisonnements possibles sur cette connaissance. La derni~re pattie consacrCe aux teclmiques d'impl6mentation est suivie d'une conclusion. 2 LE MODELE CONCEPTUEL 2.1 Presentation g~n~rale Notre module r6sulte d'une collaboration entre linguistes et informaticiens, ll s'appuie sur certains r6sultats de la psychologie cognitive.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/C92-2085",
  "title": "Linguistic Knowledge Generator",
  "year": 1992,
  "venue": "COLING",
  "abstract": "The difficulties in current NLP applications are seldom due to the lack of appropriate frameworks for encoding our linguistic or extra-linguistic knowledge, hut rather to the fact that we do not know in advance what actual znstances of knowledge should be, even though we know in advance what types of knowledge are required. It normally takes a long time and requires painful trial and error processes to adapt knowledge, for example, in existing MT systems in order to translate documents of a new text-type and of a new subject domain. Semantic classification schemes for words, for example, usually reflect ontologies of subject domains so that we cannot expect a single classification scheme to be effective across different domains. To treat different suhlanguages requires different word classification schemes. We have to construct appropriate schemes for given sublanguages from scratch [1]. It has also been reported that not only knowledge concerned with extra-linguistic domains but also syntactic knowledge, such as subcategorization frames of verbs (which is usually conceived as a par t of general language knowledge), often varies from one sublanguage to another [2]. Though re-usability of linguistic knowledge is currently and intensively prescribed [3], our contention is that the adaptation of existing knowledge requires processes beyond mere re-use. Tha t is,",
  "stance": -0.3
 },
 {
  "url": "https://aclanthology.org/C92-2087",
  "title": "Logical Form of Hierarchical Relation on Verbs and Extracting it from Definition Sentences in a Japanese Dictionary",
  "year": 1992,
  "venue": "COLING",
  "abstract": "We are studying how to extract hierarchical relation on verbs from definition sentences in a Japanese dictionary. The hierarchical relation on verbs has been dealt with as a binary relation on verbs, but it should be dealt with as logical relation on predicates. We will define the logical form of the hierarchical relation on verbs and then discuss which part of the syntactic structure of the definition sentence represents that relation. We will call the main predicate verb in this part the definition verb. Furthermore we will describe how to semiautomatically select the proper meaning of the definition verb and the proper correspondence between cases of an entry verb and the definition verb in order to extract the hierarchical relation as logical relation.",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/C92-2100",
  "title": "A Three-level Revision Model for Improving Japanese Bad-styled Expressions",
  "year": 1992,
  "venue": "COLING",
  "abstract": "This paper proposes a three-level revision model for improving badly-styled Japanese expressions, especially in the field of technical communication. The model is a mixture of the regeneration-based model and tile rewriting-based model. The first level divides tong sentences, while the second level improves several badly-styled expressions with iterative partial rewriting operations. The last level performs regeneration, in which word ordering and punctuation to reduce tile reading ambiguity are currently involvod. Expelimental results show that our model is effective in realizing practical revision support systems.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/C92-2121",
  "title": "Semantic Network Array Processor as a Massively Parallel Computing Platform for High Performance and Large-Scale Natural Language Processing",
  "year": 1992,
  "venue": "COLING",
  "abstract": "This paper demonstrates the utility of the Semantic Network Array Processor (SNAP) as a massively parallel platform for high performance and large-scale natural language processing systems. SNAP is an experimental massively parallel machine which is dedicated to, but not limited to, the natural language processing ushag semantic networks. In designing the SNAP, we have investigated various natural language processing systems and theories to determine the scope of the hardware support and a set of micro-coded instructions to be provided. As a resuit, SNAP employs an extended markerpassing model and a dynamically modifiable network model. A set of primitive instructions is micro-coded to directly support a parallel marker-passing, bitoperations, numeric operations, network modifications, and other essential functions for natural language processing. This paper demonstrates the utility of SNAP for various paradigms of natural language processing. We have discovered that the SNAP provides milliseconds or microseconds performance on several important applicatious such as the memory-based parsing and translation, classificatlon-based parsing, and VLKB search. Also, we argue that there are numerous opportunities in the NLP community to take advantages of the comlmtational power of the SNAP. 1. I n t r o d u c t i o n In order to accomplish the high-performance natural language processing, we have designed a highly parallel machine called Semantic Network Array Processor (SNAP) [Lee and Moldovan, 1990]. The goal of our project is to develop and test the validity of the massively parallel machine for high performance and larg-scale natural language processing. Thus, the architecture of the SNAP was determined reflecting extensive analysis of basic operations essential to the \"This research is Bupported by the National Science Foundation under grant MIP-9009111 and MIP-9009109, and conducted as a part of IMPACT (InternationM Consortium for Massively Parallel Advanced Computing Technologies) Dan Moldovan Department of Electrical Engineering Systems University of Southern California Los Ange le s , C A 90089-1115 U.S.A. natural language processing. As a result of the investigation, we have decided to employ an extended marker-passing model and a dynamically modifiable network. Also, a set of primitive instructions is microcoded to directly support essential operations in natural language systems. Several approach can be taken to use SNAP as a platform for natural language processing systems. We can fully implement NLP system on SNAP, or we can speed up existing systems by implementing computationally expensive part on SNAP. We have hnplemented some of these approaches on SNAP, mid obtained extremely high performance (order of milliseconds for given tasks). In this paper, we describe the design philosophy and architecture of SNAP, and present several approaches toward high performance natural language processing systems on SNAP. 2. S N A P A r c h i t e c t u r e 2.1. Des ign P h i l o s o p h y of S N A P The Semantic Network Array Processor (SNAP) is a highly parallel array processor fully optindzed tbr semantic network processing with a marker-passing mechanism. The fundermental design decisions arc (1) a semantic network as a knowledge representation scheme, and (2) parallel marker-passing as an inference mechauism. First, the use of a semantic network as a represem tation scheme can be justified from the fact that most of the representation schemes of current AI and NLP theories (such as frame, feature structure, sort hierarchy, systemic choice network, neural network, etc.) can be mapped onto semantic networks. Also, tlmre are numbers of systems and models which directly use semantic networks [Sown, 1991]. Second, the use of marker-passing can be justified from several aspects. Obviously, there are many AI and NLP models which use some form of marker-passing as the central computing principle. For example, there are significant number of research being done on word-sense disambiguation as scene in Waltz and Pollack 1985] Itendler, 1988], [Hirst, 1986, [Charniak, 1983], [Tomabechi, 1987, etc. All of them assume passing of markers or values among nodes interconnected via some types of links. There are studies to handle syntactic conACRES DE COLING-92. NANaXS, 23-28 AO~\" 1992 8 1 3 Paoc. ov COLING-92. NAI, rVES, AUG. 23-28, 1992 tsmlttamtm4 ~ e t t m m a n t |NAP.1 ~ Cam~ oan@~a¢ c . J ~ t~ s s ^ p Figure I: SNAP-1 Architecture straints using some type of networks which can be mapped onto semantic networks. Recent studies on the Classification-Based Parsing [Kasper, 1989] and the Systemic Choice Network [Carpenter and Pollard~ 1991] assume hierarchical networks to represent varions linguistic constraints, and the search on these networks can be done by marker-passing. Also, there are more radical approaches to implement entire natural language systems using parallel marker-passing as seen in [Norvig, 1986], [Riesbeck and Martin, 1985], [Tomabechi, 1987], and [Kitano, 1991]. There are, however, differences in types of information carried in each marker-passing model. We will describe our design decisions later. As reported in [Evett, at. al., 1990], however, serial machines are not suitable for such processing because it causes performance degradation as a size of semantic network increases. There are clear needs for highly parallel machines. The rest of this section provides a brief overview of the SNAP architecture. 2.2. The A r c h i t e c t u r e SNAP consists of a processor array and an array controller (Figure 1). The processor array has processing cells which contain the nodes and hnks of a semantic network. The SNAP array consists of 160 processing elements each of which consists of a TMS320C30 DSP chip, local SRAM, etc. Each processing elements stores 1024 nodes which act as virtual processors. They are interconnected via a modified hypercube network. The SNAP controller interfaces the SNAP array with a SUN 3/280 host and broadcasts instructions to control the operation of the array. The instructions for the array are distributed through a global bus by the controller. Propagation of markers and the execution of other instructions can be proceased simultaneously. 2.3. Parallel M a r k e r P a s s i n g In the SNAP, content of the marker are: (1) bitvector, (2) address, and (3) numeric value (integer or floating point). In SNAP, the size of the marker is fixed. According to the classification in [Blelloch, 1986], our model is a kind of Finite Message Passing. There are types of marker-, or message-, pa~ing that propagates feature structures (or graphs)~ which are called Unbounded Message Passing. Although we have extended our marker-passing model from the traditional bit marker-passing to the complex markerpassing which carries bits, address, and numeric values, we decided not to carry unbounded messages. This is because propagation of feature structures and heavy symbolic operations at each PE are not practical assumptions to make, at least, on current massively parallel machines due to processor power, memory capacity on each PE, and the communication bottleneck. Propagation of feature structures would impose serious hardware design problems since the size of the message is unbounded, which means that the designer can not be sure if the local memory size is sufficient or not until the machine actually runs some applications. Also, PEa capable of performing operations to manipulate these messages (such as unification) would be large in physical size which causes assembly problems when thousands of processors are to be assembled into one machine. Since we decide not to support unbounded message passing, we decide to support functionalities attained by the unbounded message passing by other means such as sophisticated marker control rules, dynamic network modifications, etc. 2.4. Ins truct ion Sets A set of 30 high-level instructions specific to semantic network processing are implemented directly in hardware. These include associative search, marker setting and propagation, logical/arithmetic operations involving markers, create and delete nodes and relations, and collect a list of nodes with a certain marker set. Currently, the instruction set can be called from C language so that users can develop applications with an extended version of C language. From the programming level, SNAP provides dataparallel programming environment similar to C* of the Connection Machine ]Thinking Machines Corp., 1989], but specialized for semantic network processing with marker passing. Particularly important is the marker propagation rules. Several marker propagation rules are provided to govern the movement of markers. Marker propagation rules enables us to implement guided, or constraint, marker passing as well as unguided marker passing. This is done by specifying the type of links that markers can propagate. The following are some of the propagation rules of SNAP: e Seq(rl, r~): The Seq (sequence) propagation rule allows the marker to propagate through rl once then to r~. • Spread(rl,r2) : The Spread propagation rule allows the marker to travel through a chain of r l links and then r~ links. * Comb(rl,r~) : The Comb (combine) propagation rule allows the marker to propagate to all rl and r~ links without limitation. 2.5. K n o w l e d g e R e p r e s e n t a t i o n on S N A P SNAP provides four knowledge representation elements: node, link, node color and link value. These elements offer a wide range of knowledge representation schemes to be mapped on SNAP. On SNAP) a concept is represented by a node. A relation can be represented by either a node called relation node or AcrEs DE COLING-92, NANTES. 23-28 AOt)r 1992 8 1 4 PROC. OF COLING-92, NANTES. AUG. 23-28. 1992 a link between two nodes. The node color indicates the type of node. For example, when representing USC i s in Los Angeles and CW0 ie in Pi t t sbnrgh~ we may assign a relation node for IN. The IN node is shared by the two facts. In order to prevent the wrong interpretations such as USC in P i t t s b u r g h and CSll in Lea Angeles, we assigu I N # I and IN#2 to two distinct IN relations, and group the two relation nodes by a node color IN. Each lhlk has assigned to it a link value which indicates the strength of interconcepts relations. This link value supports probabilistic reasoning and connectionist-like processing. These four basic elements allow SNAP to support virtually any kind of graph-based knowledge representation formalisms such as KL-ONE [Braehman and Schmolze, 1985], Conceptual Graphs [Sown, 1984], KODIAK [Wilensky, 1987], etc. 3 . T h e M e m o r y B a s e d N a t u r a l L a n g u a g e P r o c e s s i n g Memory-baaed NLP is an idea of viewing NLP as a memory activity. For example, parsing is considered as a memoryosearch process which identifies similar eases in the past from the memory, and to provide interpretation based on the identified case. I t can be considered as an application of Memory-Baaed l~.easoning (MBR) [Stm~fill and Waltz, 1986] and CaseBased Reasoning (CBR) [Riesbeck and Schank, 1989] to NLP. This view~ however, counters to traditional idea to view NLP as arl extensive rule application process to build up meaning representation. Some models has been proposed in this direction, such as Direct Memory Access Parsing (DMAP) [Riesbeck and Martin, 1985] and q~DMDIALOO [Kitano, 1991]. For arguments concerning superiority of the metnory-based approach over the traditional approach, ace [Nagao, 1984], [Riesbeck and Martin, 1985], and [Sumita and ][ida, 1991]. DMSNAP is a SNAP implementation of the (I)DMDIALOG speech-to-speech dialogue translation system which is based on, in part, the memory-based approach. Naturally, it inherits basic ideas and mechanisms of the ~DMDIALOG system such as a memorybased approach to natural language processing and parallel marker-passing. Syntactic constraint network is introduced in DMSNAP whereas ODMDIALOG has been assuming unification operation to handle linguistic processing. DMSNAP consists of the nlemory network, syntactic constraint network, and markers to carry out inference. The memory network and the syntactic constraint network are compiled from a set of grammar rules written for DMSNAP. M e m o r y N e t w o r k on S N A P The major types of knowledge required for language translation in DMSNAP are: a lexicon, a concept type hierarchy, concept sequences, and syntactic constraints. Among them, the syntactic constraints are represented in the syntactic constraint network, and the rest of the knowledge is represented in the memory network. The memory network consists of various types of nodes such as concept sequence class (CSC), lexical item node* (LEX), concept nodes (CC) and others. Nodes are connected by a number of different links such as concept ahstraction links (ISA), expression links for both source language and target language (ENG and JPN), Role links (ROLE), constraint links (CONSTRAINT) , contextual llnk~ (CONTEXT) and others. A part of the menmry network is shown in Figure",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/C92-3148",
  "title": "Multilinguisation d'un editeur de documents structures. Application a un dictionnaire trilingue",
  "year": 1992,
  "venue": "COLING",
  "abstract": "R~sum~ Pour \"multilingualiser\" (et non simplement \"localiser\") Gif, un 6diteur de documents structures, nous avons d6fini un langage de transcription, appel6 langage E, analogue aux autres langages (S, Pet T) de Grif. E est utilis6 pour compl6ter la description structurale d'une classe de documents, 6crite en S, par une description \"linguistique\" concemant les syst~mes d'6cnture utilis6s clans les diff6rentes sous-structures des documents de la classe. Gr/tce ~ cette extension multilingue, on a pu construire une premi6re structure de dictionnmre trilingue chinois-fran~;ais-vietnamien, et l'utiliser sur un dictionnaire rEduit. Mots-el6s",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/C92-3150",
  "title": "Surface Grammatical Analysis for the Extraction of Terminological Noun Phrases",
  "year": 1992,
  "venue": "COLING",
  "abstract": "LEXTER is a software package for extracting terminology. A corpus of French language texts on any subject field is fed in, and LEXTER produces a list of likely terminological units to be submitted to an expert to be validated. To identify the terminological units, LEXTER takes their form into account and proceeds in two main stages : analysis, parsing. In the first stage, LEXTER uses a base of rules designed to indentify frontier markers in view to analysing the texts and extracting maximallength noun phrases. In the second stage, LEXTER parses these maximal-length noun phrases to extract subgroups which by virtue of their grammatical structure and their place in the maximal-length noun phrases are likely to be terminological units. In this article, the type of analysis used (surface grammatical analysis) is highlighted, as the methodological approach adopted to adapt the rules (experimental approach). I ) C o n s t i t u t i n g Constituting a terminology of a subject field, that is to say establishing a list of the terminological units that represent the concepts of this field, is an oft-encountered problem. For the Research Development Division of Electricit6 de France (French Electricity Board), this problem arose in the in formation documentation sector. An automatic indexing system, using different thesauri according to the application, has been operational for three years or more [Monteil 1990]. The terminologists and information scientists need a terminology a t e r m i n o l o g y extraction tool in order to keep these thesauri up to date in constantly changing fields and to create \"ex nihilo\" thesauri for new fields. This is the reason why the terminological extracting software, LEXTER, was developed, forming the first link in the chain that goes to make up the thesaurus. A corpus of frenchlanguage texts is fed into LEXTER, which gives out a list of likely terminological units, which are then passed on to art expert for validation. AUlXS DE COLING-92, NANTES, 23-28 AO~r 1992 9 7 7 PROC. OF COLING-92, NANTES, AUG. 23-28, 1992 2) What is a terminological unit ? The main aim here is not to provide a rigorous definition of what a terminological unit is, but rather to outline its essential features, and thus to justify the hypotheses (concerning the form of terminological units) on which LEXTER is based. Semantic function : the representation of the concept The first characteristic of the terminological unit is its function as the representation of a concept. The terminological unit plays this role of representa t ion in the f ramework of a terminology, which is the linguistic evidence of the organisation of a field of knowledge in the form of a ne twork of concepts; the terminological unit represents a concept, uniquely and completely, taken out of any textual context. The existence of this one-to-one relationship between a linguistic expression and an extra-linguistic object is, as we shall see, a situation which particulary concerns the terminological units. The appearance of a new terminological unit is most often a parallel process to that of the birth of the concept which it represents. This \"birth\" is marked by the consensus of a certain scientific community. This consensus is attested only when the occurrences of this linguistic expression, or term-to-be, shows a stable correlation to the same object in the subject field, uniquely and completely, in the writings of the agents of this scientific community. When this is the case, the object in question takes its place in the network describing the subject field, and the expression takes on the status of a terminological unit. This referential function is, for E. Benveniste, the \"synaptic\" mark of a syntagm [Benveniste 1966]. It is thus because occurrences in text of a terminological unit systematically refer to a concept, that a relationship of representation is established, out of any textual context, between the terminological unit and the concept. This underp ins the specif ic status of the terminological unit as opposed to that of the word in language, a status close to that of a descriptor in Information Science ([Le Guern 1984]). Syntactic form : synaptic composition We put forward the hypothesis that this function of representing the concept out of context puts a certain number of constraints on the form that terminological units may take on. It has been seen that the construction of terminological units obey well-known rules of syntactic formation, called synaptic composit ion ([Benveniste 1966]). For example : terminological units are noun phrases, generally made up of nouns and adjectives, and pratically never containing conjugated verbs; the prepositions used most often are \"de\" and \"~\", rarely followed by a determiner. To illustrate this, take the concept of a \"screen belonging to a portable computer\". Without going in to the linguistic phenomena behind this, it can be said that, in context, both the syntagms \"l 'rcran d'un ordinateur portable\" (\"the screen of a portable computer\") and \"un 6cran d 'ordinateur portable\" (\"a portable computer screen\") can refer generically to the concept. However, if one wished to represent this concept out of any textual context, the chances are that one would reject the expression \"rcran d'un ordinateur portable\", for wich the interpretation of the article \"un\" may be ambiguous, and accept the expression \"6cran d'ordinateur portable\", more naturally used in isolation and thus more suitable to go into a terminology. From these considerations on the form and the function of terminological units, two mains ideas are relevant to developing a computer NLsed system of terminology constitution : 1) It is possible to devise an extraction program solely based on syntactic data, since the grammatical form of terminological units is relatively predictable; 2) It is not possible to expect this program to extract terminological units and nothing else, given the basically referential semantic function of occurrences of terminological units : this means that the results obtained can only be considered, a priori, as likely terminological units. ACRES DE COL1NG-92, NAN'~S, 23-28 Aotrr 1992 9 7 8 PROC. OF COLING-92, NANTES, AUG. 23-28, 1992 3) How LEXTER works : analysis and parsing To detect terminological units, LEXTER takes the form of these units into consideration, and works in two phases : analysis and parsing. LEXTER treats categorized texts, which have been submitted to a morphological analysis : each word is tagged with its grammatical category (noun, verb, adjective, etc.). Figure 1 : Simplified example of how LEXTER works I (categorized) texts I UN TRAITEMENT DE TEXTE EST INSTALLE SUR LE DISQUE DUR DE LA STATION DE TRAVAIL",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/C92-3161",
  "title": "Shalt2- a Symmetric Machine Translation System with Conceptual Transfer",
  "year": 1992,
  "venue": "COLING",
  "abstract": "Shal l2 is a knowledge-based machine translation system with a symmetric architecture. The grammar rules, mapping rules between syntactic and conceptual (semantic) representations, and transfer rules for conceptual paraphrasing are all bi-directional knowledge sources used by both a parser and a generator.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/C92-3162",
  "title": "A Knowledge-based Machine-aided System for Chinese Text Abstraction",
  "year": 1992,
  "venue": "COLING",
  "abstract": "Benjamin K Tsou Hing-cheung Ho Tom Bong-yeung Lai Caesar Suen Lun Hing-lung Lin City Polytechnic of Hong Kong",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/C92-4206",
  "title": "Multimodal Database Query",
  "year": 1992,
  "venue": "COLING",
  "abstract": "The paper proposes a mult imodal interface for a real sales database application. We show how natural language processing may be integrated with a visual, direct manipulat ion method of database query, to produce a user interface which supports a flexible form of query specification, provides implicit guidance about the coverage of the linguistic component, and allows more focused discourse reference.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/C94-1023",
  "title": "AUTOMATIC MODEL REFINEMENT - with an application to tagging",
  "year": 1994,
  "venue": "COLING",
  "abstract": "Statistical NLP models usually only consider coarse information and very restricted context to make the estimation of parameters feasible. To reduce the modeling error introduced by a simplified probabilistic model, the Classitication and Regression Tree (CART) method was adopted in this paper to select more discriminative features for automatic model refinement. Because the features are adopted dependently during splitting the classification tree in CART, the number of training data in each terminal node is small, which makes the labeling process of terminal nodes not robust. This over-tuning phenomenon cannot be completely removed by crossvalidation process (i.e., pruning process). A probabilistic classification model based on the selected discriminative features is thtls proposed to use the training data more efficiently. In tagging the Brown Corpus, our probabilistic classification model reduces the error rate of the top 10 error dominant words from 5.71% to 4.35%, which shows 23.82% improvement over the un-",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C94-1032",
  "title": "A Stochastic Japanese Morphological Analyzer Using a Forward-DP Backward-A* N-Best Search Algorithm",
  "year": 1994,
  "venue": "COLING",
  "abstract": "We present a novel method for segmenting the input sentence into words and assigning parts of speech to the words. It consists of a statistical language model and an efficient two-pa~qs N-best search algorithm. The algorithm does not require delimiters between words. Thus it is suitable for written Japanese. q'he proposed Japanese morphological analyzer achieved 95. l% recall and 94.6% precision for open text when it was trained and tested on the ATI'¢ Corpus.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C94-1075",
  "title": "A Modular Architecture for Constraint-Based Parsing",
  "year": 1994,
  "venue": "COLING",
  "abstract": "This paper presents a framework and a system for implementing, comparing and analyzing parsers for some classes of Constraint-Based Grammars. The framework consists in a uniform theoretic description of parsing algorithms, and provides the structure for decomposing the system into logical components, with possibly several interchangeable implementations. Many parsing algorithms can be obtained by compositi(m of the modules of our system. Modularity is also ,~ way of achieving code sharing for the common parts of these various algorithms. Furthermore, tile design lielpi~ reusing the existing modules when implementing other algorithms. The system uses the flexible modularity provided by the programmifig languages hleool-90, 1)ased on a type system that ensures the safety of module composition.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/C94-1079",
  "title": "PRINCIPAR - An Efficient, Broad-coverage, Principle-based Parser",
  "year": 1994,
  "venue": "COLING",
  "abstract": "We present an efI]cient, broad-coverage, principle-based parser for English. The parser has been implemented in C++ and runs on SUN Sparcstations with X-windows. It conrains a lexicon with over 90,000 entries, constructed automatically by applying a set of extraction and conversion rules to entries from machine readable dictionaries.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/C94-2106",
  "title": "A System of Verbal Semantic Attributes Focused on the Syntactic Correspondence between Japanese and English",
  "year": 1994,
  "venue": "COLING",
  "abstract": "This paper proposes a system of 97 verbal semantic attributes for Japanese verbs which considers both dynamic characteristics and the relationship of verbs to cases. These attribute values are used to disambiguate the meanings of all Japanese and English pattern pairs in a Japanese to English transfer pattern dictionary consisting of 15,000 pairs of Japanese valence patterns and equivalent English syntactic structures.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/C94-2138",
  "title": "A Reestimation Algorithm for Probabilistic ttecursive Transition Network",
  "year": 1994,
  "venue": "COLING",
  "abstract": "Prob~bilistic l{,ecursive Tr~msition Network(Pl~TN) is an elevated version of t{51'N to model and process lan-. guages in stoch~st, ic parameters. The representation is a direct derivation front the H,TN and keeps much the spirit of ltidden Markov Model at the same tint(,. We present a reestimation algorithm ['or Ptl,TN that is ~ variation of Inside-Ontside algorithm that comput, es the vMues of the probabilistic parameters from sample sentences (parsed or unparsed). 1. ln trodu( : t ion In this pal)er , we introduce a network representa t ion, Probabilistic Recursive Transitio. Network tha t is directly derived fl'Oln R'CN and I tMM, and present an est imat ion algori thm lot tile probabilistic paraHteters. PR;12N is a ][]TN mJgmented with probabil i t ies in the transi t ions ~md states and with the lexical distr ibutions in the transi-tions, or is the Hidden Markov Model augmented with a stack tha t makes some traltsit ions deter ministic. The paramete.r esthnat ion of PI{;I'N is devel oped as a wu'iation of Ins ide ( )u t s ide algorithm. The hlsidc ()utside algori thm has becn applied e(,10t , I;o ~, ,.~* recently by Jelinek (1{t9{/) and ],ari (1991). The algori thm was first introduced by Baker in 1.979 and is the context free lmtguage version o[ Forward-.Backw~rd algori thm in IIid-. *This research is partly supported by KOSEF (Km:ea Science altd Teclntology l\"oundation) under tit{= title \"A Study mt the Bnilding '[~echni(lues for [txdmst Km~wledge based Systems\" from 19911 through 1994. den Markov Models. Its theoret ical lbund~Ltion is laid by Baam aud Weh:h in the late 6l)'s, which in t a rn is a type of the F,M Mgorithm in statist ics (Rabiner, 1989). Kupiec (1991) introduced a trellis based es-. t imation Mgorithm of Hidden SCFG tha t ae commodates both ilnside-Outside ~dgorithm and l!brward-.Backward \",flgorithm. The meaning of our work can be sought from the use of more plain topology of I{TN, whereas Kupiec 's work is a unilied version of tbrward-.backword and Inside Outside ~lgorithms. Nonetheless, the implemen. rat ion of reest imat ion Mgorittun carries no more theoretical significance than the applicative efli ciency and variation for differing representat ions since B~ker first apt)lied it to CI\"Gs. 2. Probab i l i s t i c R e c u r s i v e Trans i t ion N e t w o r k A probabilistic ff.l.'N (PRTN, hereafter) denoted by A is ~ 4 tuple. A is ~ t ransi t ion m~trix containing tr~n.sition probabili t ies, ~tnd 13 is aiL observation mat r ix containing probabi l i ty dis tr ibut ion of the words ob servable at each terminM transi t ion where row and column correspond to terminM transi t ions and a list of words respective, ly. F specilies the types of transit ions, and D2 denotes a stack. The first two model parameters are the same as tha t of I[MM, thus typed transi t ions and the existence of a stack art', what distinguishes I ' t tTN fl'om t[MM.",
  "stance": 0.1
 },
 {
  "url": "https://aclanthology.org/C94-2149",
  "title": "XTAG System - A Wide Coverage Grammar for English",
  "year": 1994,
  "venue": "COLING",
  "abstract": "This paper presents the XTAG system, a grammar development tool based on the Tree Adjoining Grammar (TAG) formalism that includes a wide-coverage syntactic grammar for English. The various components of the system are discussed and preliminary evaluation results from the parsing of various corpora are given. Results from the comparison of X3AG against the IBM statistical parser and the Alvey Natural Language Tool parser are also given. 1 I N T R O D U C T I O N XTAG is a large on-going project to develop a widecoverage grammar for English, based on the l,exicalized Tree Adjoining Grammar (I 3\"AG) tbrmalism. LTAG is a lexicalized mildly-context sensitive tree rewriting system [Joshi et al., 1975; Schabes, 1990] that is closely related to Dependency Grammars and Categorial Grammars. Elementary trees in 13~AG provicle a larger domain of locality over which syntactic and semantic (predicate-argument) constraints are specified. XTAG also serves as an LTAG grammar development system consisting of a predictive left-to-right parser, an X-window interface, a roof phological analyzer, and a part-of-speech tagger (also referred to as simply 'tagger'). 2 S Y S T E M D E S C R I P T I O N Figure 1 shows the overall llow of the system when parsing a sentence. The input sentence is submitted to the Morphological Analyzer and the 3hgger. The morphological analyzer retrieves the morphological information for each individual word from the morphological database. This output is tiltered in the P.O.S Blender using the output of the trigram tagger to reduce the part-of-speech ambiguity of the *currently at BBN, Cambridge, MA, USA Input Sentence ~ ' p h Analy~ l_ 'l'~ger ,,__ _ _ ~ [ p.0.S Blender ~< 1 ̧ . . . . . . . i i _ _ / i ~ 1 % ~ TteeSdection , , ~ N y n O B ~ . . . . . . . . . . . . . . . . . . . . Derivation Structure Figure I : Overview of XTAG system words. The sentence, now annotated with part-ofspeech tags and morphological information for each word, is input to the Parser, which consults the syntactic database and tree database to retrieve the appropriate tree structures for each lexical item. A variety o[' heuristics arc used to reduce the number of trees selected. The parsertitan composes the structures to obtain the parse(s) of the sentence. 2.1 Morphological Analyzer The morphology database [Karp et al., 19921 was originally exlracted from 1979 edition of the Collins English Dictionary and Oxford Adwmced Learner's l)ictionary of Current English, and then cleaned up and auglnentcd by hand. It consists of approximately 317,000 inltected items, along with their root forms and intlectional intbrmalion (such as case, num-",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/C94-2153",
  "title": "An Efficient Syntactic Tagging Tool for Corpora",
  "year": 1994,
  "venue": "COLING",
  "abstract": "A BSTRA CT The tree bank is an important resources tbr MT and linguistics researches, but it requires that large number of sentences be annotated with syntactic information. It is time consuming and troublesome, and dil'ficult to keep consistency, if' annotation is done manually. In this paper, wc presented a new technique for the semi-automatic tagging of Chinese tcxt. The system takes as input Chinese text, and outputs the syntactically tagged sentence(dependency tree). We use dependency grammar and employ a stack based sh i f t / r educe context-dependent parser as the tagging mechanism. The system works in human-machine cooperative way, in which the machine can acquire tagging rules from human intervention. The automation level can be improved step by step by accumulating rules during annotation. In addition, good consistency of tagging is guaranteed.",
  "stance": 0.6
 },
 {
  "url": "https://aclanthology.org/C94-2177",
  "title": "Reverse Queries in DATR",
  "year": 1994,
  "venue": "COLING",
  "abstract": "I)ATI{ is a declara t ive re.presentat ion language ti)r lex-. ical i i f formation and as such, fit prin(:iple, neul;ral with resl)(;ct; 1;o i)arl;icul&r l)rocessing st,rat,egies. Previous D A T R (:l)mt)iler/inl;erI)ret(!r sy,qt(!ms suppor t only one al:l:e.4s ,%rat,egy ~hnt, closely resembles the set, of inti~r-. otlce rllleS of the procedm:sd s(mumti(:s of ])A.Tli (Evmls & C,~tz(lar 1989a). In this i/al)er w(! present, an alt,ern;> 1;ivc access st,r~tl;egy ('ri:'uc'l'.s('. q'ucr!/ .stral, cgy ) ~br a himtrivial subsel; o f I ) A ' F ] / . 1 The Reverse Query Prob lem D A T R (Evans & Gazdm\" 1989@ has l)ecome. Olte of the iiiosl; widely used fornlatl languages tin' the I'(~l)t'ese.tll;;tt,ion of lexicad infornlat,ion. ! )N[ ' l l ~q)plil:ations ha.re been (h~velol)ed for a wide variety of lmlguages (including English, .lat/mmse , Kikuyu, Arabi(:, l,at,in, and others) ;rod mmly different; subdonudns of le, xical rel)resentat,ion, including inilect,ional morphology, undt~rspecification l)honltlogy, nlm-(:onca.t,enative morphophono l ogy, lexicaI senlanti(:s, and tone sys tems I. We presutlI)OSe tha t the reader of the llresenl; paper is [)mlilia.r wi th the basic Datur(!s of ])AT[/. as spe(:ilie(1 in Ewms & Gazda.r [ 1 9 8 9 @ 7'he ;all;(tu;tcy of st lexi(:on repr(~se.nt,;~t;ion fornmlisln depends basical ly on two ma jo r fact,ors: • it,s declarative c:cpres.sivenes.s: is the ff)rmalism, in prin(:iple, i:al)al)le of rel)resent,ing l;he phenomena in • This research was partly SUpl)orted by the (~ermau l,'e(h!ral Ministry of Heseareh and Technology (BMfI', project VEI~P,MOBIl,) at the University of l~ielelk~ld. I would like tel thank 1)afy(Id Gibbon for vely ttseful COllltHeltL8 o11 ali earlier draft of I;his paper. 1 See Cahill [[9.93], Gibbon [ [9!)2], (lazdm\" [I 9921, ;rod Kilbm'y [1992] for reeelll; I)ATR applicatious in Lhese areas. An informal introducl, ion I,o I)ATR is given in (lazdar [19!10]. 'l'he sl.andatd syntax and semantics of I)ATI{ is defined in I,iwms gz (~az(lar [198!)a, 19891)]. hul)lementation issues are discussed iu (:libbon & Almua [1991], Jenkins [1990], aud in Gibbon [19931 . M,)ser [I 992a, 1992b, 1992(:, 1992d] provides interesting insights into the fl~rmal properties of I)N['I/(see also the I)A'['I/ represen/ations of finil,e state allLomal.a, dilI'e~ent kiiMs of logics, regisl, er operations ere. in Evans & (l~z(la,' [1990], and l,;ml;er [1993]). Andry et al. [19931 describe how I)ATR can lm used in speech-oriented ~tl)l)lieal.ion.~;. qll(*,st,ion~ &lid does it allow lbr a.n explicit t,re;~t,lllonl; of generalisat,ions, subgene, ral isat ions, ;rod ex--",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/C94-2212",
  "title": "NL Understanding with a Grammar of Constructions",
  "year": 1994,
  "venue": "COLING",
  "abstract": "We present an approach to na tura l language unders tanding based on a computable grammar of const~ctions. A construetionconsists of a set of features of form and a description of meaning in a context. A grammar is a set of constructions. This kind of grammar is the key element of MINCAL, an implemented natural language speech-enabled interface to an online calendar system. Tile architecture has two key aspects: (a) the use of constructions, integrat ing descriptions of form, meaning an(t context into one whole; and (b) the separation of domain knowledge (about calendars) from application kno'wledgt; (about the part icular on-line calendar). 1 I n t r o d u c t i o n : a n o v e r v i e w o f t h e s y s t e m We present an approach to natural language unders tanding based on a computablc gTummar of constructions. A construction consists of a set of features of form and a description of meaning in a context. A grammar is a set of constructions. This kind of grammar is the key clement of MINCAI,, an imt)lemented natural language speech-enabled interface t() an on-line calendar system. The system consists of a NL grammar, a parser, an on-line calendar, a domain knowledge base (about dates, t imes and meetings), an application knowledge base (about the calendar), a speech recognizer,",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/C96-1006",
  "title": "Extracting Word Correspondences from Bilingual Corpora Based on Word Co-occurrence Information",
  "year": 1996,
  "venue": "COLING",
  "abstract": "A new method has been developed for extracting word correspondences from a bilingual corpus. First, the co-occurrence infi~rmation for each word in both languages is extracted li'om the corpus. Then, the correlations between the co-occurrence features of the words are calculated pairwisely with tile assistance of a basic word bilingual dictionary. Finally, the pairs of words with the highest correlations are output selectively. This method is applicable to rather small, unaligned corpora; it can extract correspondences between compound words as well as simple words. An experiment using bilingual patent-specification corpora achieved 28% recall and 76% precision; this demonstrates that the method effectively reduces the cost of bilingual dictionary augmentation.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C96-1069",
  "title": "An Automatic Clustering of Articles Using Dictionary Definitions",
  "year": 1996,
  "venue": "COLING",
  "abstract": "In this paper, we propose a statistical approach for clustering of artMes using on-line dictionary definitions. One of the characteristics of our approach is that every sense of word in ar tMes is automatically disambiguated using dictionary definitions. The other is that in order to cope with the problem of a phrasal lexicon, linking which links words with their semantically similar words in articles is introduced in our method. The results of experiments demonstrate the effectiveness of the proposed method.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/C96-1086",
  "title": "Inherited Feature-based Similarity Measure Based on Large Semantic Hierarchy and Large Text Corpus",
  "year": 1996,
  "venue": "COLING",
  "abstract": "We describe a similarity calculation model called IFSM (Inherited Feature Similarity Measure) between objects (words/concepts) based on their common and distinctive features. We propose an implementation method for obtaining features based on abstracted triples extracted fi'om a large text eorpus utilizing taxonomical knowledge. This model represents an integration of traditional methods, i.e,. relation b~used sin> itarity measure and distribution based similarity measure. An experiment, using our new concept abstraction method which we <'all the fiat probability grouping method, over 80,000 surface triples, shows that the abstraction level of 3000 is a good basis for feature description.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/C96-1087",
  "title": "A Probabilistic Approach to Compound Noun Indexing in Korean Texts",
  "year": 1996,
  "venue": "COLING",
  "abstract": "In this paper we address the problem of compound noun indexing that is about segmenting or decomposing compound nouns into promising",
  "stance": -0.5
 },
 {
  "url": "https://aclanthology.org/C96-1089",
  "title": "Learning Bilingual Collocations by Word-Level Sorting",
  "year": 1996,
  "venue": "COLING",
  "abstract": "This paper I)roposes ;t new tnethod for learning bilingual colloca, tions from sentence-aligned paralM corpora. Our method COml)ris('s two steps: (1) extracting llseftll word chunks (n-grmns) by word-level sorting and (2) constructing bilingua,l ('ollocations t)y combining the word-(;hunl(s a(-quired iu stag(' (1). We apply the method to a very ('hallenging text l)~tir: a stock market 1)ullet;in in Japanese and il;s abstract in En-glish. I)om;tin sl)ecific collocations are well captured ewm if they were not conta.ined in the dictionaric's of economic",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/C96-2098",
  "title": "Extraction of Lexical Translations from Non-Aligned Corpora",
  "year": 1996,
  "venue": "COLING",
  "abstract": "A method for extracting lexical translations from non-aligned corpora is proposed to cope with the unavailability of large aligned corpus. The assumption that \"translations of two co-occurring words in a source language also co-occur in the target language\" is adopted and represented in the stochastic matrix formulation. The translation matrix provides the co-occurring information translated from the source into the target. This translated co-occurring information should resemble that of the original in the target when the ambiguity of the translational relation is resolved. An algorithm to obtain the best translation matrix is introduced. Some experiments were performed to evaluate the effectiveness of the ambiguity resolution and the refinement of the dictionary. 1 I n t r o d u c t i o n Alignment of corpora is now being actively studied to support example-based automatic translation and dictionary refinement. Focusing on the latter, in order to obtain lexical translations, the maximum likelihood method is applied to roughly aligned corpus. One of the problems of this method is that it needs a large amount of aligned corpus for training (Brown, 1993). When it exists, a qualified dictionary is also likely to exist, because it should have been created and used when the corpus in the source language was translated by hand to make the aligned corpus. There are few requirements to improve dictionaries in such a case. On the other hand, when a large amount of aligned corpus does not exist but only two independent corpora do, for example, the corpora between two 'not so international' *Author's current address: Department of Computer Science, Tokyo University of Agriculture and Technology. 2-24-16 Naka-machi, Koganei, Tokyo 184 JAPAN. languages or those in a constrained domain, the low quality dictionaries need to be improved. To make a new dictionary between two uncommon languages, it is often necessary to transform published dictionaries, one between the source and the international language, the other between the international and the target language. The problem in this process is to eliminate the irrelevant translations introduced by words with ambiguous meanings (Tanaka, 1994). This carl be thought of as choosing the translations from several candidates without aligned corpus. Note that adopting aligned corpus of insufficient size cause the same situation. We therefore propose a method to extract lexical translations using two corpora which are not aligned in the source and target language. Our method is proposed as the extension of the framework to solve the problem of choosing the translation according to the context. Thus, one of tile merits of our research is that two problems, looking for the translation according to the global and local context, are handled within the same framework. 2 A s s u m p t i o n a n d A m b i g u i t y",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C96-2102",
  "title": "Towards a Syntactic Account of Punctuation",
  "year": 1996,
  "venue": "COLING",
  "abstract": "Little notice has been taken of punctuation in the field of natural language processing, chiefly due to the lack of any coherent theory on which to base implementations. Some work has been carried out concerning punctuation and parsing, but much of it seems to have been rather ad-hoc and performance-motivated. This paper describes the first step towards the construction of a theoretically-motivated account of punctuation. Parsed corpora are processed to extract punctuation patterns, which are then checked and generalised to a small set of General Punctuation Rules. Their usage is discussed, and suggestions are made for possible methods of including punctuation information in grammars. 1 I n t r o d u c t i o n Ititherto, the field of punctuation has been almost completely ignored within Natural Language Processing, with perhaps the single exception of the sentence-final full-stop (period). The reason for this non-treatment has been the lack of any coherent theory Of punctuation on which a computational treatment could be based. As a result, most contemporary systems simply strip out punctuation in input text, and do not put any marks into generated texts. Intuitively, this s~ems very wrong, since punctuation is such an integral part of many written languages. If text in the real world (a newspaper, for example) were to appear without any punctuation marks, it would appear very stilted, ambiguous or infantile. Therefore it is likely that any computational system that ignores these extra textual cues will suffer a degradation in performance, or at the very least a great restriction in the class of linguistic data it is able to process. Several studies have already shown the potential for using punctuation within NLP. Dale (1991) has shown the positive benefits of using punctuation ill the fields of discourse structure and semantics, suggesting that it can be used to indicate degrees of rhetorical balance and aggregation between juxtaposed elements, and also that in certain cases a punctuation mark can determine the rhetorical relations that hold between two elements. In the field of syntax Jones (1994) has shown, through a comparison of the performance of a grammar that uses punctuation and one which does not, that for the more complex sentences of real language, parsing with a punctuated grammar yields around two orders of magnitude fewer parses than parsing with an nnpunctuated grammar, and that additionally the punctuated parses better reflect the linguistic structure of the sentences. Briscoe and Carroll (1995) extend this work to show the real contribution that usage of punctuation can make to the syntactic analysis of text. They also point out some fundamental problems of the approach adopted by Jones (1994). If, based on the conclusions of these studies, we are to include punctuation in NLP systems it is necessary to have some theory upon which a treatment can be based. Thus far, the only account available is that of Nunberg (1990), which although it provides a useful basis for a theory is a little too vague to be used as the basis of' any implementation. In addition, the basic implementation of Nunberg's punctuation linguistics seems untenable, certainly on a computational level, since it stipulates that punctuation phenomena should be treated on a seperate level to the lexical words in the sentence (Jones, 1994). It is also the case that Nunberg's t reatment of punctuation is",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/C96-2110",
  "title": "Identifying the Coding System and Language of On-line Documents on the Internet",
  "year": 1996,
  "venue": "COLING",
  "abstract": "This paper proposes a new algorithm that simultaneously identifies the coding system and language of a code string fetched from the Internet, especially World-Wide Web. The algorithm uses statistic language models to select the correctly decoded string as well as to determine the language. The proposed algorithm covers 9 languages and 11 coding systems used in Eastern Asia and Western Europe. Experimental results show that the level of accuracy of our algorithm is over 95% for 640 on-line documents.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C96-2114",
  "title": "Linguistic Indeterminacy as a Source of Errors in Tagging",
  "year": 1996,
  "venue": "COLING",
  "abstract": "Most evaluations of part-of-speech tagging compare the utput of an automatic tagger to some established standard, define the differences as tagging errors and try to remedy them by, e.g., more training of the tagger. The present article is based on a manual analysis of a large number of tagging errors. Some clear patterns among the errors can be discerned, and the sources of the errors as well as possible alternative methods of remedy are presented and discussed. In particular are the problems with undecidable",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/C96-2144",
  "title": "A Constraint-based Case Frame Lexicon",
  "year": 1996,
  "venue": "COLING",
  "abstract": "We present a constraint-based case f lame lexicon arctfitecture fbr bi-directionM mapping between a syntactic case Dame and a semantic Dame. The lexicon uses a semantic sense as the basic unit and employs a multi-tiered constraint structure for the resolution of syntactic information into the appropriate senses and/or idiomatic usage. VMency changing transfbrmations such as morptnologieally marked passivized or causativized forms are handled via le:xical rules that manipulate case Dames templates. The system has been implemented in a typedfeature system and applied to Turkish. 1 I n t r o d u c t i o n -Recent adwmces in theoreticM and practical aspects of feature and constraint-based tbrmMisms for representing linguistic information have fostered research on the use of such formMisms in the design and implementat ion of computat ional lexicons (Briscoe el al., 1993). Case frame approach has been the representation of choice especially for languages with free constituent order, explicit case marking of noun phrases and embedded clauses filling nominal syntactic roles. The semantics of such syntactic role fillers are usually determined by their lexicM, semantic and morplmsyntactic properties, instead of position in the sentence. In this paper, we present an approach to building a constraint-based case Dame lexicon for use in natural language processing in Turkish. A number of observations tha.t we have made on Turkish tmve indicated that we have to go beyond the tradit ional transitive and intransitive distinction, and utilize a Damework where verb valence is considered as the obligatory co-existence of an arbi trary subset of possible arguments along with the obligatory exclusion of certain others, relative to a verb seuse. A d d i t i o n a l morphosyn t~c t i c , lexical and semantic selectional constrMnts are utilized to map a given syntactic argument structure to a specific verb sense. In recent years, there have been several studies on constrmnt-based lexicons. iR,ussell el al. (1993) propose an approach to multiple default inheritance tbr unification-based lexicon. In another study by Lascarides et el. (1995), an ordered approach to default unification is suggested, de Paiva (1993) tbrmalizes the system of well-fornmd typed feature struetures. In this study, type hierarchies and relations are mathematically defined. They also formalize unification and generalization operators between tin(; featm:e structures, along with defining well-formedness notion that we use in our system. 2 Representing Case Frame I n f o r m a t i o n In rlhu'kish, (and possibly in many other languages) verbs often convey several meanings (some totally unrelated) when they are used with subjects, objects, oblique objects, adverbiM adjuncts, with certain lexical, morphological, and semantic features, and co occurrence restrictions. In addition to the usual sense wu:iations due to selectional restrictions on verbal arguments, in most cases, the meaning conveyed by a. case Dante is idiomatic, with subtle constrMnts. For example, the Turkisln verb ye (cat), when used with a direct object noun phrase whose head is: 1. para (money), with no case or possessive markings and a lmman subject, means to accept bribe, 2. pare (money), wittn a non-human subject, means to cost a lol, 3. para (or any other NP whose head is ontologically IS-A money, e.g., dolar, mark, etc.) with obligatory accusative markilig ~md optional possessive meriting, means to spend *~IO~tCy~ 4. kafa (head) with obligatory accusative marking and no possessive marking, means to get mentally deranged, 5. hak (right) with optionM accusative and possessive markings, mearls to be unfair,",
  "stance": 0.4
 },
 {
  "url": "https://aclanthology.org/C96-2145",
  "title": "Error-tolerant Tree Matching",
  "year": 1996,
  "venue": "COLING",
  "abstract": "This paper presents an efficient algor i thm for retrieving from a database of trees, all trees that match a given query tree appro,imately, that is, within a certain error tolerance. It has natural language processing applications in searching for matches in example-based translation systems, and retrieval from lexical databases containing entries of complex feature structures. The algorithm has been implemented on SparcStations, and for large randomly generated synthetic tree databases (some having tens of thousands of trees) it can associatively search [or trees with a small error, in a mat te r of tenths of a second to few seconds. 1 I n t r o d u c t i o n Recent approaches in machine translation known as example-based translation rely on searching a database of previous translations of sentences or fragments of sentences, and composing a translation from the translations of any matching examples (Sato and Nagao, 1!)90; Nirenburg, Beale and l)omasnhev, 1994). The example database may consist, of paired text fragments, or trees as in Sat() and Nagao (1990). Most often, exact matches for new sentences or fragments will not be in the database, and one has to consider exampies that are \"similar\" to the sentence or fragment in question. This involves associatively searching through the database, tbr trees that are \"close\" to the query tree. This paper addresses the computat ional problem o[ retrieving trees that are close to a given query tree in terms of a certain distance metric. The paper first presents the approximate tree matching problem in an abstract setting and presents an algorithm for approximate associative tree matching. The Mgorithm relies on linearizing the trees and then representing the complete database of trees as a t r i e structure which can be efficiently searched. The problem then reduces to sequence correction problem akin to s tandard spelling correction problem. The trie is then used with an approximate finite state recognition algori thm close to a query tree. Following some experimental results from a number of synthetic tree databases, the paper ends with conclusions. 2 A p p r o x i m a t e T r e e M a t c h i n g In this paper we consider the problem of searching in a database of trees, all trees that are \"close\" to a given query tree, where closeness is defined in terms of an error metric. The trees tha t we consider have labeled terminal and non-terminal nodes. We assume that all immediate children of a given node have unique labels, and that a total ordering on these labels is defined. We consider two trees close if we can • add/delete a small number of leaves to / f rom one of the trees, and/or • change the label of a small number of leaves in one of the trees to get the second tree. A pair of such \"close\" trees is depicted in Fignre 1. 2.1 Linearizat ion of trees Before proceeding any fllrther we would like to define the terminology we will be using in the fob lowing sections: We identify each leaf node in a tree with an ordered vertex list (re, vl, v2, . . . , vd) where each vi is the label of a vertex from the root v0 to the leaf Vd at depth d, and :{'or i > 0, vi is the parent of vi+ L. A tree with n leaves is represented by a vertex list sequence. V L S = . V i , V ' e , . . . , 1⁄4 , where each V~. = v3o, v{, v~, v~, . •., va,;, corresponds to a vertex list for a leaf at level dj. This sequence is constructed by taking into account the total order on the labels at every level, that is, 17i is lexico.qraphically less than Vi+l, based on the total ordering of the vertex labels. For instance, the first tree in Fignre 1 would be represented by the vertex list sequence:",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/C96-2172",
  "title": "Human Language Technology can modernize writing and grammar instruction",
  "year": 1996,
  "venue": "COLING",
  "abstract": "The recently published Survey of the State of the Art in Human Language Technology does not spend a single word on computer-aided language learning (CALL). Indeed, present-day CALL systems hardly employ Natural Language Processing (NLP) techniques. Reasons for this state of affairs are not hard to find. First of all, current language teaching methods tend to emphasize oral language skills, especially in second-language instruction. But automatic recognition of speech, in particular speech by non-natives, has only taken its first steps outside the laboratory; and many language teachers still judge synthesized speech of insufficient quality to serve as a model for language learners. Secondly, modern language pedagogy stresses communicative success rather than formal correctness. This, too, works against the profitable deployment of NLP tools in CALL because automatically generating non-trivial, communicatively interesting and instructive dialogues does not yet seem within reach of NLP technology, let alone the evaluation of student responses from the point of view of successful interpersonal communication. More congenial with these priorities were multimedia innovations. If anything has brought about a metamorphosis in second-language teaching practices, it was the introduction of affordable video, audio and other graphical and acoustic tools that, under the control of flexible software, can create an illusion of 'total immersion'--the supposedly ideal language learning situation. However, language proficiency includes more than conversational skills alone. Equally important are writing skills: orthography, formulating well-formed sentences, composing clear and well-organized texts. It is in the area of written language instruction that NLP technology can find a wealth of extremely valuable applications. Teaching firstand second-language writing skills is very laborintensive because teachers need to mark large numbers of test papers. NLP software holds the potential of alleviating this burden considerably, and even of outperforming teachers in the speed and quality of feedback to learners, and in the capability of generating well-targeted and attractive exercises. A particularly important reason why human language technology should begin to take written hmguage instruction seriously, derives from the following argument. Many instructional scientists subscribe to the view that language skills are best acquired in a situation similar to that of children learning their mother tongne. This explains not only the bias in favor of oral and conversational language skills in current hmguagc pedagogy but also the reluctance to work with explicit grammar rules. The negative attitude toward grammar is strengthened by the generally disappointing outcomes of grammarbased language teaching methods. However, these negative results may have a completely different origin. Although the reasoning that language acquisition can do without explicit rules may hold for oral language proficiency, there is no evidence that it generalizes to the acquisition of written language skills. If writing skills do require the application of explicit orthographic, morphological, syntactic, etc., rules by the learner, then antigrammar attitudes must be detrimental. Learners will be deprived of knowledge that in fact is essential to solving writing problems. More serious is the ensuing lack of interest in the improvement of grammar instruction methods. Taught by age-old methods, many learners only have an inkling of the meaning of important grammatical concepts. Grammar rules referencing these terms are hard to apply successfully in written composition. The response by most linguists and instructional scientists to this state of affairs has been misguided. Instead of initiating research into improved grammar teaching methods, they have tended to play down the importance of grammar rules and linguistic awareness in learning how to write.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/C96-2191",
  "title": "Spoken-Language Translation Method Using Examples",
  "year": 1996,
  "venue": "COLING",
  "abstract": "A certain recovery method is now under consideration: a re-entrizing model for phoneme candidates by means of searching the correct phonemes using modification depending on recognition error characteristics in an example-based framewbrk [Wakita95]. This approach provides a recovery effect in handling phoneme or syllable sequences, and the effect depends on the particular speakers because of individual error characteristics.",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/C98-1021",
  "title": "Spoken Dialogue Interpretation with the DOP Model",
  "year": 1998,
  "venue": "COLING",
  "abstract": "We show how the DOP model can be used for fast and robust processing of spoken input in a practical spoken dialogue system called OVIS. OVIS, Openbaar Vervoer Informatie Systeem (\"Public Transport Information System\"), is a Dutch spoken language information system which operates over ordinary telephone lines. The prototype system is the immediate goal of the NWO 1 Priority Programme \"Language and Speech Technology\". In this paper, we extend the original DOP model to context-sensit ive interpretation of spoken input. The system we describe uses the OVIS corpus (10,000 trees enriched with compositional semantics) to compute from an input word-graph the best utterance together with its meaning. Dialogue context is taken into account by dividing up the OVIS corpus into context-dependent subcorpora. Each system question triggers a subcorpus by which the user answer is analyzed and interpreted. Our experiments indicate that the context-sensitive DOP model obtains better accuracy than the original model, allowing for fast and robust processing of spoken input.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C98-1048",
  "title": "Experiments with Learning Parsing Heuristics",
  "year": 1998,
  "venue": "COLING",
  "abstract": "Any large language processing software relies in its operation on heuristic decisions concerning the strategy of processing. These decisions are usually \"hard-wired\" into the software in the form of handcrafted heuristic rules, independent of the nature of the processed texts. We propose an alternative, adaptive approach in which machine learning techniques learn the rules from examples of sentences in each class. We have experimented with a variety of learning techniques on a representative instance of this problem within the realm of parsing. Our approach lead to the discovery of new heuristics that perform significantly better than the current hand-crafted heuristic. We discuss the entire cycle of application of machine learning and suggest a methodology for the use of machine learning as a technique for the adaptive optimisation of language-processing software.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C98-1087",
  "title": "Long Distance Pronominalisation and Global Focus",
  "year": 1998,
  "venue": "COLING",
  "abstract": "1) Out corpus of descriptive text contains a significant number of long-distance pronominal references (8.4% of the total). In order to account for how these pronouns are interpreted, we re-examine Grosz and Sidner's theory of the attentional state, and in particular the use of the global focus to supplement centering theory. Our corpus evidence concerning these long-distance pronominal references, as well as studies of the use of descriptions, proper names and ambiguous uses of pronouns, lead us to conclude that a discourse focus stack mechanism of the type proposed by Sidner is essential to account for the use of these referring expressions. We suggest revising the Grosz & Sidner fl'amework by allowing for the possibility that an entity in a focus space may have special status.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/C98-1106",
  "title": "Term-list Translation using Mono-lingual Word Co-occurrence Vectors",
  "year": 1998,
  "venue": "COLING",
  "abstract": "A term-list is a list of content words that characterize a consistent text or a concept. This paper presents a new method for translating a term-list by using a corpus in the target language. The method first retrieves alternative translations for each input word from a bilingual dictionary. It then determines the most 'coherent' combination of alternative translations, where the coherence of a set of words is defined as the proximity among multi-dimensional vectors produced from the words on the basis of co-occurrence statistics. The method was applied to term-lists extracted from newspaper articles and achieved 81% translation accuracy for ambiguous words (i.e., words with multiple translations). 1 I n t r o d u c t i o n A list of content words, called a term-list, is widely used as a compact representation of documents in information retrieval and other document processing. Automatic translation of term-lists enables this processing to be cross-linguistic. This paper presents a new method for translating term-lists by using cooccurrence statistics in the target language. Although there is little study on automatic translation of term-lists, related studies are found in the area of target word selection (for content words) in conventional full-text machine translation (MT). Approaches for target word selection can be classified into two types. The first type, which has been adopted in many commercial MT systems, is based on hand assembled disambiguation rules, and/or dictionaries. The problem with this approach is that creating these rules requires much cost and that they are usually domain-dependent 1 The second type, called the statistics-based approach, learns disambiguation knowledge from large corpora. Brown et al. presented an algorithm that * This r e s e a r c h w a s done when the au thor w a s a t Center for the S tudy of Language and Information(CSLI) , Stanford University. 1In fact, this is par t ly shown by the fact tha t many MT s y s t e m s have subs t i tu tab le domain-dependen t (or \"user\" ) dict ionaries . relies on translation probabilities estimated from large bilingual corpora (Brown et al., 1990)(Brown et al., 1991). Dagan and Itai (1994) and Tanaka and Iwasaki (1996) proposed algorithms for selecting target words by using word co-occurrence statistics in the target language corpora. The latter algorithms using mono-lingual corpora are particularly important because, at present, we cannot always get a sufficient amount of bilingual or parallel corpora. Our method is closely related to (Tanaka and Iwasaki, 1996) from the viewpoint that they both rely on mono-lingual corpora only and do not require any syntactic analysis. The difference is that our method uses \"coherence scores\", which can capture associative relations between two words which do not co-occur in the training corpus. This paper is organized as follows, Section 2 describes the overall translation process. Section 3 presents a disambiguation algorithm, which is the core part of our translation Inethod. Section 4 and 5 give experimental results and discussion. 2 T e r m l i s t T r a n s l a t i o n Our term-list translation method consists of two steps called Dictionary Lookup and Disambiguation. 1. Dictionary Lookup: For each word in the given term-list, all the alternative translations are retrieved from a bilingual dictionary. A translation candidate is defined as a combination of one translation for each input word. For example, if the input term-list consists of two words, say Wl and w2, and their translation include Wll for Wl and W2a for w2, then (wu, w~a) is a translation candidate. If wl and w2 have two and three alternatives respectively then there are 6 possible translation candidates.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C98-2119",
  "title": "Word Clustering and Disambiguation Based on Co-occurrence Data",
  "year": 1998,
  "venue": "COLING",
  "abstract": "We address the problem of clustering words (or constructing a thesaurus) based on co-occurrence data, and using the acquired word classes to improve the accuracy of syntactic disambiguation. We view this problem as that of estimating a joint probability distribution specifying the joint probabilities of word pairs, such as noun verb pairs. We propose an efficient algorithm based on the Minimum Description Length (MDL) principle for estimating such a probability distribution. Our method is a natural extension of those proposed in (Brown et al., 1992) and (Li and Abe, 1996), and overcomes their drawbacks while retaining their advantages. We then coinbined this clustering method with the disamI)iguation method of (Li and Abe, 1995) to derive a disambiguation method that makes use of both automatically constructed thesauruses and a hand-made thesaurus. The overall disambiguation accuracy achieved by our method is 85.2%, which compares favorably against the accuracy (82.4%) obtained by the state-of-the-art disambiguation method of (Brill and Resnik, 1994).",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C98-2120",
  "title": "Identifying Syntactic Role of Antecedent in Korean Relative Clause Using Corpus and Thesaurus Information",
  "year": 1998,
  "venue": "COLING",
  "abstract": "This paper describes an approach to identifying the syntactic role of an antecedent in a Korean relative clause, which is essential to structural disambiguation and semantic analysis. In a learning phase, linguistic knowledge such as conceptual co-occurrence pat terns and syntactic role distribution of antecedents is extracted from a large-scale corpus. Then, in an application phase, the extracted knowledge is applied in determining the correct syntactic role of an antecedent in relative clauses. Unlike previous research based on co-occurrence pat terns at the lexical level, we represent co-occurrence pat terns with concept types in a thesaurus. In an experiment, the proposed method showed a high accuracy rate of 90.4% in resolving ambiguities of syntactic role determination of an-",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/C98-2145",
  "title": "An Estimate of Referent of Noun Phrases in Japanese Sentences",
  "year": 1998,
  "venue": "COLING",
  "abstract": "In machine translation and man-machine dialogue, it is important to clarify' referents of noun phrases. We present a method for determining the referents of noun phrases in Japanese sentences by using the referential properties, modifiers, and possessors 1 of noun phrases. Since the Japanese language has no articles, it is difficult to decide whether a noun phrase has an antecedent or not. We had previously estimated the referential properties of noun phrases that correspond to articles by using clue words in the sentences (Murata and Nagao 1993). By using these referential properties, our system determined tile referents of noun phrases ill Japanese se,ltences. Furthermore we used the modifiers and possessors of noun phrases in determining the referents of noun phrases. As a result, on training sentences we obtained a precision rate of 82% and a recall rate of 85% in the determination of the referents of noun phrases that have antecedents. On test sentences, we obtained a precision rate of 79% and a recall rate of 77%.",
  "stance": 0.7
 },
 {
  "url": "https://aclanthology.org/C98-2147",
  "title": "Japanese OCR Error Correction using Character Shape Similarity and Statistical Language Model",
  "year": 1998,
  "venue": "COLING",
  "abstract": "We present a novel OCR error correction method for languages without word delimiters that have a large character set, such as Japanese and Chinese. It consists of a statistical OCR model, an approximate word matching method using character shape similarity, and a word segmentation algorithm using a statistical language model. By using a statistical OCR model and character shape similarity, the proposed error corrector outperforms the previously published method. When the baseline character recognition accuracy is 90%, it achieves 97.4% character recognition accuracy. 1 I n t r o d u c t i o n As our society is becoming more computerized, people are getting enthusiastic about entering everything into computers. So the need for OCR in areas such as office automation and information retrieval is becoming larger, contrary to our expectation. In Japanese, although the accuracy of printed character OCR is about 98~, sources such as old books, poor quality photocopies, and faxes are still difficult to process and cause many errors. The accuracy of handwritten OCR is still about 90% (Hildebrandt and Liu, 1993), and it worsens dramatically when the input quality is poor. If NLP techniques could be used to boost the accuracy of handwriting and poor quality documents, we could enjoy a very large market for OCR related applications. OCR error correction can be thought of a spelling correction problem. Although spelling correction has been studied for several decades (Kukich, 1992), the traditional techniques are implicitly based on English and cannot be used for Asian languages such as Japanese and Chinese. The traditional strategy for English spelling correction is called isolated word error correction: Word boundaries are placed by white spaces. If the tokenized string is not in the dictionary, it is a nonword. For a non-word, correction candidates are retrieved fl'om the dictionary by approximate string match techniques using context-independent word distance measures such as edit distance (Wagner and Fischer, 1974) and ngram distance (Angell et al., 1983). Recently, statistical language models and featurebased method have been used for context-sensitive spelling correction, where errors are corrected considering the context in which the error occurs (Church and Gale, 1991; Mays et al., 1991; Golding and Schabes, 1996). Similar techniques are used for correcting the output of English OCRs (Tong and Evans, 1996) and English speech recognizers (Ringger and Allen, 1996). There are two problems in Japanese (and Chinese) spelling correction. The first is the word boundary problem. It is impossible to use isolated word error correction techniques because there are no delimiters between words. The second is the short word problem. Word distance measures are useless because the average word length is short (< 2), and the character set is large (> 3000). There are a much larger number of one edit distance neighbors for a word, compared with English. Recently, the first problem was solved by selecting the most likely word sequence from all combinations of exactly and approximately matched words using a Viterbi-like word segmentation algorithm and a statistical language model considering unknown words and non-words (Nagata, 1996). However, the second problem is not solved yet, at least elegantly. The solution presented in (Nagata, 1996) which sorts a list of one edit distance words considering the context in which it will be placed is inaccurate because the context itself might include some errors. In this paper, we present a context-independent approximate word match method using character shape similarity. This is suitable for languages with large character sets, such as Japanese and Chinese. We also present a method to build a statistical OCR model by smoothing the character confusion probability using character shape similarity. It seems previous NLP researchers are reluctant",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C98-2154",
  "title": "An Efficient Parallel Substrate for Typed Feature Structures on Shared Memory Parallel Machines",
  "year": 1998,
  "venue": "COLING",
  "abstract": "This paper describes an efficient parallel system for processing Typed Feature Structures (TFSs) on shared-memory parallel machines. We call the system Parallel Substrate for TFS (PSTFS). PSTFS is designed for parallel computing environments where a large number of agents are working and communicating with each other. Such agents use PSTFS as their low-level module for solving constraints on TFSs and sending/receiving TFSs to/fi 'om other agents in an efficient manner. From a programmers point of view. PSTFS provides a simple and unified mechanism for building high-level parallel NLP svstems. The performance and the flexibility of our PSTFS are shown through the experiments on two different types of parallel HPSG parsers. The speed-up was more than 10 times on both parsers. 1 I n t r o d u c t i o n The need for real-time NLP systems has been discussed for the last decade. The difficulty in implementing such a system is that people can not use sophisticated but computationally expensive methodologies. However, if we could provide an efficient tool/environment for developing parallel NLP systems, programmers would have to be less concerned about the issues related to efficiency of the system. This became possible due to recent developments of parallel machines with shared-memory architecture. We propose an efficient programming environment for developing parallel NLP systems on shared-memory parallel machines, called the Parallel Substrate for Typed Feature Structures (PSTFS). The environment is based on agentbased/object-oriented architecture. In other words, a system based on PSTFS has many computational agents running on different processors in parallel; those agents communicate with each other by using messages including TFSs. Tasks of the whole system, such as pars* This research is part ial ly founded by the project of JSPS(JSPS-RFTF96P00502). ] / Figure 1: Agent-based System with the PSTFS ing or semantic processing, are divided into several pieces which can be simultaneously computed by several agents. Several parallel NLP systems have been developed previously. But most of them have been neither efficient nor practical enough (Adriaens and Hahn, 1994). On the other hand, our PSTFS provides the following features. • An efficient communication scheme for messages including Typed Feature Structures (TFSs) (Carpenter , 1992). • Efficient t reatment of TFSs by an abstract machine (Makino et al., 1998). Another possible way to develop parallel NLP systems with TFSs is to use a full concurrent logic programming language (Clark and Gregory, 1986; Ueda, 1985). However, we have observed that it is necessary to control parallelism in a flexible way to achieve high-performance. (Fixed concurrency in a logic programming language does not provide sufficient flexibility.) Our agent-based architecture is suitable for accomplishing such flexibility in parallelism. The next section discusses PSTFS from a programmers ' point of view. Section 3 describes the PSTFS architecture in detail. Section 4 describes the performance of PSTFS on our HPSG parsers.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C98-2158",
  "title": "Recognition of the Coherence Relation between Te-linked Clauses",
  "year": 1998,
  "venue": "COLING",
  "abstract": "This paper describes a method for recognizing coherence relations between clauses which are linked by te in Japanese a translational equivalent of English and. We consider that the coherence relations are categories each of which has a prototype structure as well as the relationships among them. By utilizing this organization of the relations, we can infer an appropriate relation from the semantic structures of the clauses between which that relation holds. We carried out an experiment and obtained the correct recognition ratio of 82% for the 280 sentences.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/C98-2159",
  "title": "On the Evaluation and Comparison of Taggers: the Effect of Noise in Testing Corpora.",
  "year": 1998,
  "venue": "COLING",
  "abstract": "This paper addresses the issue of Pos tagger evaluation. Such evaluation is usually performed by comparing the tagger outt)ut with a reference test corpus, which is ~msumed to be error-free. Current ly used corpora contain noise which causes the obtained performance to be a distortion of the real value. We analyze to what extent this distortion may invalidate the comparison between taggers or the measure of the improve,nent given by a new system. The main conclusion is tha t a more rigorous testing experimentation set t ing/designing is needed to reliably evaluate and compare tagger accuracies. 1 Introduct ion and Motivat ion Par t of Speech (pos ) Tagging is a quite well defined NLP l>roblem, which consists of assigning to each word in a text the proper morphosyntaet ic tag for the given context. Although many words are ambiguous regarding their pos , in most cases they can be completely disambigua.ted taking into account an adequate context. Successful taggers have been built using several approaches, such ms statistical techniques, symbolic machine learning techniques, neural networks, etc. The accuracy reported by most current taggers ranges from 96-97% to almost 100~ in the linguistically motivated Constraint Grammar environment. Unfortunately, there have been very few direct comparisons of alternative taggers I on identical test data. However~ in most current papers it is argued that the performance of some taggers is bet ter than others as a result of some kind of indirect comparisons between them. We 1()no of the exceptions is the work by (Sanmelsson and Voutilainen, 1997), in which a very strict compm'ison bctwecn taggers is performed. think that there a.re a number of not enough controlled/considered factors tha t make these conchlsions dubious in most cases. In this direction, the present paper aims to point out some of tile difficulties arising when evaluating and comparing tagger performances against a reference test corpus, and to make. some criticism about common practices followed by the NLP re.searchers in this issue. The above mentioned factors can affect either the evaluation or the comparison process. Factors affecting the evaluation process are: (1) Training and test experiments are usually pertbrmed over noisy corpora which distorts the obtained results, (2) performance figures are too often calculated from only a single or very small number of trials, though average results from multiple trials are crucial to obtain reliable estimations of accuracy (Mooney, 1996), (3) testing experiments arc usually clone on corpora with the same characteristics as the training da ta -~usuMly a small fresh portion of the trMning corpusbut no serious a t t empts have been done in order to determine the reliability of the results when moving from one domain to another (Krovetz, 1997), and (4) no figures about computat ional effort space/ t ime complexi tyare usually reported, even from an empirical perspective. A factors affecting the comparison process is that comparisons between taggers are often indirect, while they should be compared under the same conditions in a multiple trial experiment with statistical tests of significance. For these reasons, this paper calls for a discussion on pos taggers evaluation, aiming to establish a more rigorous test experimentat ion setting/designing, indispensable to ext ract reliable conclusions. As a start ing point, we will focus only on how the noise in the test corpus can affect the obtained results.",
  "stance": -0.8
 },
 {
  "url": "https://aclanthology.org/C98-2162",
  "title": "Machine Aided Error-Correction Environment for Korean Morphological Analysis and Part-of-Speech Tagging",
  "year": 1998,
  "venue": "COLING",
  "abstract": "Statistical methods require very large corpus with high quality. But building large and faultless annota ted corpus is a very difficult job. This paper proposes an efficient method to construct part-of-speech tagged corpus. A rulebased error correction method is proposed to find and correct errors semi-automatical ly by user-defined rules. We also make use of user's correction log to reflect feedback. Experiments were carried out to show the efficiency of error correction process of this workbench. The result shows that about 63.2 % of tagging errors can be corrected.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C98-2176",
  "title": "Building Accurate Semantic Taxonomies Monolingual MRDs",
  "year": 1998,
  "venue": "COLING",
  "abstract": "This paper presents a method that conbines a set of unsupervised algorithms in order to accurately build large taxonomies from any machine-readable dict ionary (MRD). Our aim is to profi t from convent ional MRDs, with no explicit semantic coding. We propose a system that 1) performs fully automatic extraction of taxonomic links from MRD entries and 2) ranks the extracted relations in a way that selective manual refinement is allowed. Tested accuracy can reach around 100% depending on the degree of coverage selected, showing that taxonomy building is not limited to structured dictionaries such as LDOCE.",
  "stance": 0.9
 },
 {
  "url": "https://aclanthology.org/C98-2202",
  "title": "Keyword Extraction using Term-Domain Interdependence for Dictation of Radio News",
  "year": 1998,
  "venue": "COLING",
  "abstract": "In this paper, we propose keyword extraction method for dictation of radio news which consists of several domains. In our method, newspaper articles which are automatically classified into suitable domains are used in order to calculate feature vectors. The feature vectors shows term-domain interdependence and are used for selecting a suitable domain of each part of radio news.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/C98-2216",
  "title": "Modeling with Structures in Statistical Machine Translation",
  "year": 1998,
  "venue": "COLING",
  "abstract": "Most statistical machine translation systems employ a word-based alignment model. In this paper we demonstrate that word-based align: ment is a major cause of translation errors. We propose a new alignment model based on shallow phrase structures, and tile structures can be automatically acquired from parallel corpus. This new model achieved over 110% error reduction for our st)oken language translation task.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/C98-2223",
  "title": "Word Sense Disambiguation using Optimised Combinations of Knowledge Sources",
  "year": 1998,
  "venue": "COLING",
  "abstract": "Word sense disambiguation algorithms, with few exceptions, have made use of only one lexical knowledge source. We describe a system which t)erforms word sense disambiguation on all content words in free text by combining different knowledge sources: semantic preferences, dictionary definitions and subjec t /domain codes along with part-of-speech tags, optimised by means of a learning algorithm. We also describe the creation of a new sense tagged corpus by combining existing resources. Tested accuracy of our approach on this corpus exceeds 92%, demonstrating the viability of all-word disambiguation rather than restricting oneself to a small sample. 1 I n t r o d u c t i o n This paper describes a system that integrates a nmnber of partial sources of information to perform word sense disambiguation (WSD) of content words in general text at a high level of accuracy. The methodology and evaluation of WSD are somewhat different from those of other NLP modules, and one can distinguish three aspects of this difference, all of which come down to evaluation problelns, as does so much in NLP these days. First, researchers are divided between a general inethod (that a t tempts to apply WSD to all the content words of texts, the option taken in this paper) and one that is applied only to a small trial selection of texts words (for example (Schiitze, 1992) (Yarowsky, 1995)). These researchers have obtained very high levels of success, in excess of 95%, close to the figures for other \"solved\" NLP modules, the issue being whether these small word sample methods and techniques will transfer to general WSD over all content words. Others, (eg. (Mahesh et al., 1997) (Harley and Glennon, 1997)) have pursued the general option on the grounds that it is the real task and should be tackled directly, but with rather lower success rates. The division between the approaches probably comes down to no more than the availability of gold s tandard text in sufficient quantities, which is more costly to obtain for WSD than other tasks. In this paper we describe a method we have used for obtaining more test material by transforming one resource into another, an advance we believe is unique and helpful in this impasse. However, there have also been deeper problems about evaluation, which has led sceptics like (Kil~ garriff, 1993) to question the whole WSD enterprise, for example that it is harder for subjects to assign one and only one sense to a word in context (and hence the produce the test material itself) than to perform other NLP related tasks. One of the present authors has discussed Kilgarriff 's figures elsewhere (Wilks, 1997) and argued that they are not, in fact, as gloomy as he suggests. Again, this is probably an area where there is an \"expertise effect\": some subjects can almost certainly make finer, more intersubjective, sense distinctions than others in a reliable way, just as lexicographers do. But there is another, quite different, source of unease about the evaluation base: everyone agrees that new senses appear in corpora that cannot be assigned to any existing dictionary sense, and this is an issue of novelty, not just one of the difi%ulty of discrimination. If that is the case, it tends to undermine the s tandard mark-up-model-andtes t methodology of most recent NLP, since it will not then be possible to mark up sense assignment in advance against a dictionary if new senses are present. We shall not tackle this difficult issue fllrther here, but press on towards experiment. 2 K n o w l e d g e S o u r c e s a n d W o r d S e n s e D i s a m b i g u a t i o n One further issue must be mentioned, because it is unique to WSD as a task and is at tile core of our approach. Unlike other welt-known NLP modules, WSD seems to be implementable by a number of apparent ly different information sources. All the following have been implemented as tile basis of experimental WSD at various times: part-of-speech, semantic preferences, collocating items or classes, thesanral or subject areas, dictionary definitions, synonym lists, among others (such as bilingual equivalents in parallel texts). These phenomena seem",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P98-1004",
  "title": "A Simple Hybrid Aligner for Generating Lexical Correspondences in Parallel Texts",
  "year": 1998,
  "venue": "COLING",
  "abstract": "We present an algorithm for bilingual word alignment that extends previous work by treating multi-word candidates on a par with single words, and combining some simple assumptions about the translation process to capture alignments for low frequency words. As most other alignment algorithms it uses cooccurrence statistics as a basis, but differs in the assumptions it makes about the translation process. The algorithm has been implemented in a modular system that allows the user to experiment with different combinations and variants of these assumptions. We give performance results from two evaluations, which compare well with results reported in the literature.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P98-1018",
  "title": "Consonant Spreading in Arabic Stems",
  "year": 1998,
  "venue": "COLING",
  "abstract": "This paper examines the phenomenon of consonant spreading in Arabic stems. Each spreading involves a local surface copying of an underlying consonant, and, in certain phonological contexts, spreading alternates productively with consonant lengthening (or gemination). The morphophonemic triggers of spreading lie in the patterns or even in the roots themselves, and the combination of a spreading root and a spreading pat tern causes a consonant to be copied multiple times. The interdigitation of Arabic stems and the realization of consonant spreading are formalized using finite-state morphotactics and variation rules, and this approach has been successfully implemented in a large-scale Arabic morphological analyzer which is available for testing on the Internet. 1 I n t r o d u c t i o n Most formal analyses of Semitic languages, including Arabic, defend the reality of abstract, unpronounceable morphemes called ROOTS, consisting usually of three, but sometimes two or four. consonants called RADICALS. The classic examples include k t b (~. ,D ~)1, appearing in a number of words having to do with writing, books, schools, etc.; and d r s ( ~ 9 z), appearing in words having to do with studying, learning, teaching, etc. Roots combine nonconcatenatively with PATTERNS to form STEMS, a process known informally as INTERDIGITATION o r INTERCALATION. W e shall look first at Arabic stems in general before examining GEMINATION and SPREADING, related phenomena wherein a single underlying radical is real~The Arabic-script examples in this paper were produced using the ArabTeX package for TEX and DTEX by Prof. Dr. Klaus Lagally of the University of Stut tgart . daras duris darn'as duruus diraasa(t) darraas madrasa( t ) madaar is madras iyy tadri is 'study' 'be studied' 'teach' 'lessons' 's tudy' 'eager student ' 'school' 'schools' 'scholastic' ' instruction' verb verb verb n o u n noun n o u n noun n o u n adj-like noun Figure 1: Some stems built on root d r s ized multiple times in a surface string. Semitic morphology, including stem interdigitation and spreading, is adequately and elegantly formalizable using finite-state rules and operations. 1.1 A r a b i c S t e m s The stems in Figure 12 share the d r s root morpheme, and indeed they are traditionally organized under a d r s heading in printed lexicons like the authoritative Dictionary of Modern Written Arabic of Hans Wehr (1979). A root morpheme like d r s interdigitates with a pattern morpheme, or, in some analyses. with a pat tern and a separate vocalization morpheme, to form abstract stems. Because interdigitation involves pat tern elements being inserted between the radicals of the root morpheme, Semitic stem formation is a classic example of non-concatenative morphotactics. Separating and identifying the component morphemes of words is of course the core task of morphological analysis for any language, and analyzing Semitic stems is a classic challenge 2The taa~ marbuu.ta, notated here as ( t ) , is the feminine ending pronounced only in certain environments. Long consonants and long vowels are indicated here with gemination.",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/P98-1022",
  "title": "A Probabilistic Corpus-Driven Model for Lexical-Functional Analysis",
  "year": 1998,
  "venue": "COLING",
  "abstract": "We develop a Data-Oriented Parsing (DOP) model based on the syntactic representations of Lexical-Functional Grammar (LFG). We start by summarizing the original DOP model for tree representations and then show how it can be extended with corresponding functional structures. The resulting LFG-DOP model triggers a new, corpus-based notion of grammaticality, and its probability models exhibit interesting behavior with respect to specificity and the interpretation of ill-formed strings.",
  "stance": 0.1
 },
 {
  "url": "https://aclanthology.org/P98-1040",
  "title": "Dialogue Management in Vector-Based Call Routing",
  "year": 1998,
  "venue": "COLING",
  "abstract": "This paper describes a domain independent, automatically trained call router which directs customer calls based on their response to an open-ended \"How may I direct your call?\" query. Routing behavior is trained from a corpus of transcribed and hand-routed calls and then carried out using vector-based information retrieval techniques. Based on the statistical discriminating power of the n-gram terms extracted from the caller's request, the caller is 1) routed to the appropriate destination, 2) transferred to a human operator, or 3) asked a disambiguation question. In the last case, the system dynamically generates queries tailored to the caller's request and the destinations with which it is consistent. Our approach is domain independent and the training process is fully automatic. Evaluations over a financial services call center handling hundreds of activities with dozens of destinations demonstrate a substantial improvement on existing systems by correctly routing 93.8% of the calls after punting 10.2% of the calls to a human operator.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P98-1049",
  "title": "Expérimentation en apprentissage d'heuristiques pour l'analyse syntaxique",
  "year": 1998,
  "venue": "COLING",
  "abstract": "Les syst~mes ou programmes de traitement de la langue naturelle doivent prendre des drcisions quant au choix des meilleures stratrgies ou rrgles h appliquer en cours de rrsolution d 'un probl~me particulier. Pour un analyseur syntaxique constitu6 d 'une base de rrgles symboliques, le cas auquel nous nous intrressons ici, ces drcisions peuvent consister h srlectionner les rrgles ou l 'ordonnancement de celles-ci permettant de produire la plus rapide ou la plus prrcise analyse syntaxique pour un 6noncr, un type d ' rnonc6 ou m~me un corpus sprcifique. La complexit6 de telles bases de rrgles grammaticales et leurs subtilitrs computationnelles et linguistiques font en sorte que la prise de ces drcisions constitue un probl~me difficile. Nous nous sommes donc fix6 comme objectif de trouver des techniques qui permettraient d'apprendre des heuristiques performantes de prise de drcision afin de les incorporer ~ un analyseur syntaxique existant. Pour atteindre une telle adaptabilitr, nous avons adopt6 une approche d'apprentissage automatis6 supportre par l'utilisation de syst~mes de classification automatique.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P98-1061",
  "title": "A Structure-sharing Parser for Lexicalized Grammars",
  "year": 1998,
  "venue": "COLING",
  "abstract": "In wide-coverage lexicalized grammars many of the elementary structures have substructures in common. This means that in conventional parsing algorithms some of the computation associated with different structures is duplicated. In this paper we describe a precompilation technique for such grammars which allows some of this computation to be shared. In our approach the elementary structures of the grammar are transformed into finite state automata which can be merged and minimised using standard algorithms, and then parsed using an automatonbased parser. We present algorithms for constructing automata from elementary structures, merging and minimising them, and string recognition and parse recovery with the resulting grammar. 1 I n t r o d u c t i o n It is well-known that fully lexicalised grammar formalisms such as LTAG (Joshi and Schabes, 1991) are difficult to parse with efficiently. Each word in the parser's input string introduces an elementary tree into the parse table for each of its possible readings, and there is often a substantial overlap in structure between these trees. A conventional parsing algorithm (VijayShanker and Joshi, 1985) views the trees as independent, and so is likely to duplicate the processing of this common structure. Parsing could be made more efficient (empirically if not formally), if the shared structure could be identified and processed only once. Recent work by Evans and Weir (1997) and Chen and Vijay-Shanker (1997) addresses this problem from two different perspectives. Evans and Weir (1997) outline a technique for compiling LTAG grammars into automata which are then merged to introduce some sharing of structure. Chen and Vijay-Shanker (1997) use underspecified tree descriptions to represent sets of trees during parsing. The present paper takes the former approach, but extends our previous work by: • showing how merged automata can be minimised, so that they share as much structure as possible; • showing that by precompiling additional information, parsing can be broken down into recognition followed by parse recovery; • providing a formal treatment of the algorithms for transforming and minimising the grammar, recognition and parse recovery. In the following sections we outline the basic approach, and describe informally our improvements to the previous account. We then give a formal account of the optimisation process and a possible parsing algorithm that makes use of it 1 . 2 A u t o m a t o n b a s e d p a r s i n g Conventional LTAG parsers (Vijay-Shanker and Joshi, 1985; Schabes and Joshi, 1988; VijayShanker and Weir, 1993) maintain a pa r se table, a set of i t e m s corresponding to complete and partial constituents. Parsing proceeds by first seeding the table with items anchored on the input string, and then repeatedly scanning the table for p a r s e r ac t ions . Parser actions introduce new items into the table licensed by one or more items already in the table. The main types of parser actions are: 1. extending a constituent by incorporating a complete subconstituent (on the left or 1However, due to lack of space, no proofs and only minimal informal descriptions are given in this paper.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/P98-1105",
  "title": "A Statistical Analysis of Morphemes in Japanese Terminology",
  "year": 1998,
  "venue": "COLING",
  "abstract": "En este art~culo, esta informado el resultado de an~lisis estad~stico de la dinAmica de los e|ementos constitutivos de t~rminos japoneses. En t~rminos japoneses, la contribuciSn de |os elementos morfol6gicos es diferente segfin los tipos de origen (entre los elementos adoptados de lenguas occidentales y los elementos originMes incluso elementos adoptados de lengua china). Para analizar este punto, un m~todo cuantitativo esta applicado, que puede caracterizar propiamente la din£mica de los datos mofol6gicos de t~rminos en base a las muestras pequefias.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P98-1111",
  "title": "Unlimited Vocabulary Grapheme to Phoneme Conversion for Korean TTS",
  "year": 1998,
  "venue": "COLING",
  "abstract": "This paper describes a grapheme-to-phoneme conversion method using phoneme connectivity and CCV conversion rules. The method consists of mainly four modules including morpheme normalization, phrase-break detection, morpheme to phoneme conversion and phoneme connectivity check. The morpheme normalization is to replace non-Korean symbols into standard Korean graphemes. The phrase-break detector assigns phrase breaks using part-of-speech (POS) information. In the morpheme-to-phoneme conversion module, each morpheme in the phrase is converted into phonetic patterns by looking up the morpheme phonetic pattern dictionary which contains candidate phonological changes in boundaries of the morphemes. Graphemes within a morpheme are grouped into CCV patterns and converted into phonemes by the CCV conversion rules. The phoneme connectivity table supports grammaticality checking of the adjacent two phonetic morphemes. In the experiments with a corpus of 4,973 sentences, we achieved 99.9% of the graphemeto-phoneme conversion performance and 97.5% of the sentence conversion performance. The full Korean TTS system is now being implemented using this conversion method.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P98-1113",
  "title": "A Flexible Example-Based Parser Based on the SSTC",
  "year": 1998,
  "venue": "COLING",
  "abstract": "In this paper we sketch an approach for Natural Language parsing. Our approach is an example-based approach, which relies mainly on examples that already parsed to their representation structure, and on the knowledge that we can get from these examples the required information to parse a new input s e n t e n c e . In our approach, examples are annotated with the Structured String Tree Correspondence (SSTC) annotation schema where each SSTC describes a sentence, a representation tree as well as the correspondence between substrhzgs in the sentence and subtrees in the representation tree. In the process of parsing, we first try to build subtrees for phrases in the input sentence which have been successfully found in the example-base a bottom up approach. These subtrees will then be combined together to form a single rooted representation tree based on an example with similar representation structure a top down approach.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/P98-2145",
  "title": "Text Segmentation with Multiple Surface Linguistic Cues",
  "year": 1998,
  "venue": "COLING",
  "abstract": "In general, a certain range of sentences in a text, is widely assumed to form a coherent unit which is called a discourse segment. Identifying the segment boundaries is a first step to recognize the structure of a text. In this paper, we describe a method for identifying segment boundaries of a Japanese text with the aid of multiple surface linguistic cues, though our experiments might be small-scale. We also present a method of training the weights for multiple linguistic cues automatically without the overfitting problem.",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/P98-2148",
  "title": "A Stochastic Language Model using Dependency and its Improvement by Word Clustering",
  "year": 1998,
  "venue": "COLING",
  "abstract": "In this paper, we present a stochastic language model for Japanese using dependency. The prediction unit in this model is all attribute of \"bunsetsu\". This is represented by the product of the head of content words and that of function words. The relation between the attributes of \"bunsetsu\" is ruled by a context-free grammar. The word sequences axe predicted from the attribute using word n-gram model. The spell of Unknow word is predicted using character n-grain model. This model is robust in that it can compute the probability of an arbitrary string and is complete in that it models from unknown word to dependency at the same time.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/P98-2187",
  "title": "A Generative Lexicon Perspective for Adjectival Modification",
  "year": 1998,
  "venue": "COLING",
  "abstract": "This paper presents a semantic interpretation of adjectival modification in terms of the Generative Lexicon. It highlights the elements which can be borrowed from the GL and develops limitations and extensions. We show how elements of the Qualia structure can be incorporated into semantic composition rules to make explicit the semantics of the combination adjective + noun.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/P98-2226",
  "title": "Translating Idioms",
  "year": 1998,
  "venue": "COLING",
  "abstract": "This paper discusses the t reatment of fixed word expressions developed for our ITS-2 FrenchEnglish translation system. This t reatment makes a clear distinction between compounds i.e. mult iword expressions of X°-level in which the chunks are adjacent and idiomatic phrases i.e. mult iword expressions of phrasal categories, where the chunks are not necessarily adjacent. In our system, compounds are handled during the lexical analysis, while idioms are treated in the syntax, where they are t reated as \"specialized lexemes\". Once recognized, an idiom can be transfered according to the specifications of the bilingual dictionary. We will show several cases of transfer to corresponding idioms in the target language, or to simple lexemes. The complete system, including several hundreds of compounds and idioms can be consulted on the Internet (ht tp : / / lat l .unige.ch/i tsweb.html). 1 I n t r o d u c t i o n Multiword expressions (henceforth MWE), are known to constitute a serious problem for natural language processing (NLP) 1. In the case of translation, a proper t reatment of MWE is a fundamental requirement, as few customers would tolerate a literal translation of such common expressions as entrer en vigueur 'to come into effect', met t re en oeuvre 'to implement ' , faire preuve 'to show' or faire connaissance 'to meet '. \" I am grateful to Anne Vandeventer, Christopher Laenzlinger and Thierry Etchegoyhen for helpful comments. Part of the work described in this paper has been supported by a grant from CTI (grant no 2673.1). zCf. Abeill~ & Schabes (1989), Arnold et al. (1995), Laporte (1988), Schenk (1995), Stock (1989), among others. However, a simple glance at some of the current commercial translation systems shows that none of them can be said to handle MWEs in an appropriate fashion. As a mat te r of fact, some of them explicitely warn their users not to use multiword expressions. In this paper, we will first stress some fundamental properties of two classes of MWEs, c o m p o u n d s and i d i o m s , and then present the t reatment of idioms developed for our FrenchEnglish ITS-2 translation system (cf. Ramluckun & Wehrli, 1993). 2 Compounds and idioms A two-way part i t ion of MWEs in (i) compounds and (ii) idioms is both convenient and theoretically well-motivated 2. Compounds are defined as MWEs of X°-level (ie. word level), in which the chunks are adjacent, as exemplified in (1), while \"idiomatic expressions\" correspond to MWEs of phrasal level, where chunks may not be adjacent, and may undergo various syntactic operations, as exemplified in (2-3). (1)a. pomme de terre 'potato ' b. ~ cause de 'because of' c. d~s lors que 'as soon as' The compounds given in (1) function, respectively, as noun, preposition and conjunction. They correspond to a single unit , both syntactically and semantically. In contrast, idiomatic expressions do not generally constitute fixed, closed syntactic units. They do, however, behave as semantic units. For instance the complex syntactic expression casser du sucre sur le dos de quelqu'un, literally break some sugar on ~This distinction between compounds and idioms is also discussed in Wehrli (1997)",
  "stance": 0.4
 },
 {
  "url": "https://aclanthology.org/P98-2247",
  "title": "Detecting Verbal Participation in Diathesis Alternations",
  "year": 1998,
  "venue": "COLING",
  "abstract": "We present a method for automatically identifying verbal participation in diathesis alternations. Automatically acquired subcategorization frames are compared to a hand-crafted classification for selecting candidate verbs. The minimum description length principle is then used to produce a model and cost for storing the head noun instances from a training corpus at the relevant argument slots. Alternating subcategorization frames are identified where the data from corresponding argument slots in the respective frames can be combined to produce a cheaper model than that produced if the data is encoded separately. I.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/W98-1211",
  "title": "Linguistic Theory in Statistical Language Learning",
  "year": 1998,
  "venue": "CoNLL",
  "abstract": "This article attempts to determine what elements of linguistic theory are used in statistical language learning, and why the extracted language models look like they do. The study indicates that some linguistic elements, such as the notion of a word, are simply too useful to be ignored. The second most important factor seems to be features inherited from the original task for which the technique was used, for example using hidden Markov models for partof-speech tagging, rather than speech recognition. The two remaining important factors are properties of the runtime processing scheme employing the extracted language model, and the properties of the available corpus resources to which the statistical learning techniques are applied. Deliberate attempts to include linguistic theory seem to end up in a fifth place.",
  "stance": 0.2
 },
 {
  "url": "https://aclanthology.org/W98-1227",
  "title": "A Method of Incorporating Bigram Constraints into an LR Table and Its Effectiveness in Natural Language Processing",
  "year": 1998,
  "venue": "CoNLL",
  "abstract": "In this paper, we propose a method for constructing bigram LR tables by way of incorporating bigram constraints into an LR table. Using a bigram LR table, it is possible for a GLR parser to make use of both big'ram and CFG constraints in natural language processing. Applying bigram LR tables to our GLR method has the following advantages: (1) Language models utilizing bigzam LR tables have lower perplexity than simple bigram language models, since local constraints (higram) and global constraints (CFG) are combined in a single bigram LR table. (2) Bigram constraints are easily acquired from a given corpus. Therefore data sparseness is not likely to arise. (3) Separation of local and global constraints keeps down the number of CFG rules. The first advantage leads to a reduction in complexity, and as the result, better performance in GLR parsing. Our experiments demonstrate the effectiveness of our method.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/W98-1237",
  "title": "Position Paper on Appropriate Audio/Visual Turing Test",
  "year": 1998,
  "venue": "CoNLL",
  "abstract": "Dr. Hugh Loebner in his 1994 article in Communications [1] makes the statement regarding future LP competitions, \"the winner of the Loebner Grand Prize must develop a computer with associated hardware that can respond 'intelligently' to audio visual input in the manner that a human would .... Turing wrote, 'The question and answer method seems suitable for introducing any one of the fields of human endeavor that we wish to include.' Well, I would like to ask questions about images and pattern recognition. If the computer answers appropriately it is intelligent.\" Some have said that requiring competitors to submit their programs to this kind of audio/visual interrogation is going overboard for what AI and robotics technology are prepared to offer [2]. The audio/visual requirement may prevent competitors from achieving the Loebner Grand Prize in the next ten years (yet I doubt it would have to take that long). However, observe how successful the scientists at Carnegie Mellon were able to be with the Navlab automated driving project in ten years [3]. Improved technologies for object and speech recognition (major component parts of the imagined program/hardware) are continuely being developed [4]. Little work has been done to consolidate neural recogition technologies and computational linguistics, to my knowledge [5]. Still, I believe that the most uncharted ground remains with determining sofrware requirements for passing the conventional Turing Test. The 'thinking' element which should allow a TT to be successfully passed is still largely unknown; i.e. the modeling of executive cognitive functions.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/W98-1240",
  "title": "The segmentation problem in morphology learning",
  "year": 1998,
  "venue": "CoNLL",
  "abstract": "Recently there has been a large literature on various approaches to learning morphology, and the success and cognitive plausibility of different approaches (Rumelhart and McClelland (1986), MacWhinney and Leinbach (1991) arguing for connectionist models, Pinker and Prince (1988), Lachter and Bever (1988), Marcus et al. (1992) arguing against connectionist models, Ling and Marinov (1993), Ling (1994) using ID3/C4.5 decision trees, and Mooney and Califf (1995, 1996) using inductive logic programming/decision lists, among others). However except for a couple of forays into German this literature has been exclusively concerned with the learning of the English past tense. This has not worried some. Ling is happy to describe it as \"a landmark task\". But while the English past tense has some interesting features in its combination of regular rules with semi-productive strong verb patterns, it is in many other respects a very trivial morphological system reflecting the generally vestigal nature of inflectional morphology within modem English.",
  "stance": 0.0
 },
 {
  "url": "https://aclanthology.org/W99-0706",
  "title": "Learning Transformation Rules to Find Grammatical Relations",
  "year": 1999,
  "venue": "CoNLL",
  "abstract": "Grammatical relationships are an important level of natural language processing. We present a trainable approach to find these relationships through transformation sequences and-error-driven learning. Our approach finds grammatical relationships between core syntax groups and bypasses much of the parsing phase. On our training and test set, our procedure achieves 63.6% recall and 77.3% precision (f-score = 69.8).",
  "stance": 0.8
 },
 {
  "url": "https://aclanthology.org/W97-0321",
  "title": "Word Sense Disambiguation Based on Structured Semantic Space",
  "year": 1997,
  "venue": "EMNLP",
  "abstract": "In this paper, we propose a framework, structured semantic space, as a foundation for word sense disarnbiguation tasks, and present a strategy to identify the correct sense of a word in some context based on the space. The semantic space is a set of multidimensional real-valued vectors, which formally describe the contexts of words. Instead of locating all word senses in the space, we only make use of mono-sense words to outline it. We design a merging procedure to establish the dendrogram structure of the space and give an heuristic algorithm to find the nodes (sense clusters) corresponding with sets of similar senses in the dendrogram. Given a word in a particular context' the context would activate some clusters in the dendrogram, based on its similarity with the contexts of the words in the clusters, then the correct sense of the word could be determined by comparing its definitions with those of the words in the clusters.",
  "stance": 0.3
 },
 {
  "url": "https://aclanthology.org/W98-1503",
  "title": "Aligning Clattses in Parallel Texts",
  "year": 1998,
  "venue": "EMNLP",
  "abstract": "This paper describes a method for the automatic alignment of parallel texts at clause level. The method features statistical techniques coupled with shallow linguistic processing. It presupposes a parallel bilingual corpus and identifies alignments between the clauses of the source and target language sides of the corpus. Parallel texts are first statistically aligned at sentence level and then tagged with their part-of-speech categories. Regular grammars functioning on tags, recognize clauses on both sides of the parallel text. A probabilistic model is applied next, operating on the basis of word occurrence and co-occurrence probabilities and character lengths. Depending on sentence size, possible alignments arc fed into a dynamic progranuning framework or a simulated annealing system in order to find or approxim~te the best alignment. 1he method has been tested on a Small Eng~ lish-Greek corpus consisting of texts relevant to software systems and has produced promising results in terms of correctly identified clause alignments.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/W98-1509",
  "title": "An Empirical Approach to Text Categorization Based on Term Weight Learning",
  "year": 1998,
  "venue": "EMNLP",
  "abstract": "In this paper) we propose a method for text categorizaLion task using term weight learning. In our approach, learning is to learn true keywords from the error of clustering results. Parameters of term weighting are then estimated so as to maximize the true keywords and minimize the other words in the text. The characteristic of our approach is that the degree of context dependency is used in order to judge whether a word in a text is a true keyvv·ord or not. The experiments using Wall Street Journal corpus demonstrate the effectiveness of the method.",
  "stance": 0.5
 },
 {
  "url": "https://aclanthology.org/W99-0615",
  "title": "HMM Specialization with Selective Lexicalization",
  "year": 1999,
  "venue": "EMNLP",
  "abstract": "We present a technique which complements Hidden Markov Models by incorporating some lexicalized states representing syntactically uncommon words. 'Our approach examines the distribution of transitions, selects the uncommon words, and makes lexicalized states for the words. We perfor'med a part-of-speech tagging experiment on the Brown corpus to evaluate the resultant language model and discovered that this technique improved the tagging accuracy by 0.21% at the 95% level of confidence.",
  "stance": 1.0
 },
 {
  "url": "https://aclanthology.org/W99-0630",
  "title": "Automatically Merging Lexicons that have Incompatible Part-of-Speech Categories",
  "year": 1999,
  "venue": "EMNLP",
  "abstract": "We present a new method to automatically merge lexicons that employ different incompatible POS categories. Such incompatibilities have hindered efforts to combine lexicons to maximize coverage with reasonable human effort. Given an \"original lexicon\", our method is able to merge lexemes from an \"additional lexicon\" into the original lexicon, converting lexemes from the additional lexicon with about 89% precision. This level of precision is achieved with the aid of a device we introduce called an anti-lexicon, which neatly summarizes all the essential information we need about the co-occurrence of tags and lemmas. Our model is intuitive, fast, easy to implement, and does not require heavy computational resources nor training corpus. l e m m a I tag apple INN boy NN calculate VB Example entries in Brill lexicon",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1990/file/41ae36ecb9b3eee609d05b90c14222fb-Paper.pdf",
  "title": "A Short-Term Memory Architecture for the Learning of Morphophonemic Rules",
  "year": 1990,
  "venue": "NeurIPS",
  "abstract": "Despite its successes, Rumelhart and McClelland's (1986) well-known approach to the learning of morphophonemic rules suffers from two deficiencies: (1) It performs the artificial task of associating forms with forms rather than perception or production. (2) It is not constrained in ways that humans learners are. This paper describes a model which addresses both objections. Using a simple recurrent architecture which takes both forms and \"meanings\" as inputs, the model learns to generate verbs in one or another \"tense\", given arbitrary meanings, and to recognize the tenses of verbs. Furthermore, it fails to learn reversal processes unknown in human language.",
  "stance": -0.2
 },
 {
  "url": "https://papers.neurips.cc/paper/1990/file/5a4b25aaed25c2ee1b74de72dc03c14e-Paper.pdf",
  "title": "Designing Linear Threshold Based Neural Network Pattern Classifiers",
  "year": 1990,
  "venue": "NeurIPS",
  "abstract": "The three problems that concern us are identifying a natural domain of pattern classification applications of feed forward neural networks, selecting an appropriate feedforward network architecture, and assessing the tradeoff between network complexity, training set size, and statistical reliability as measured by the probability of incorrect classification. We close with some suggestions, for improving the bounds that come from VapnikChervonenkis theory, that can narrow, but not close, the chasm between theory and practice. 1 Speculations on Neural Network Pattern Classifiers (1) The goal is to provide rapid, reliable classification of new inputs from a pattern source. Neural networks are appropriate as pattern classifiers when the pattern sources are ones of which we have little understanding, beyond perhaps a nonparametric statistical model, but we have been provided with classified samples of features drawn from each of the pattern categories. Neural networks should be able to provide rapid and reliable computation of complex decision functions. The issue in doubt is their statistical response to new inputs. (2) The pursuit of optimality is misguided in the context of Point (1). Indeed, it is unclear what might be meant by 'optimality' in the absence of a more detailed mathematical framework for the pattern source. (3) The well-known, oft-cited 'curse of dimensionality' exposed by Richard Bellman may be a 'blessing' to neural networks. Individual network processing nodes (e.g., linear threshold units) become more powerful as the number of their inputs increases. For a large enough number n of points in an input space of d dimensions, the number of dichotomies that can be generated by such a node grows exponentially in d. This suggests that, unlike all previous efforts at pattern classification that required substantial effort directed at the selection of low-dimensional feature vectors so as to make the decision rule calculable, we may now be approaching a",
  "stance": 0.3
 },
 {
  "url": "https://papers.neurips.cc/paper/1990/file/9de6d14fff9806d4bcd1ef555be766cd-Paper.pdf",
  "title": "Simple Spin Models for the Development of Ocular Dominance Columns and Iso-Orientation Patches",
  "year": 1990,
  "venue": "NeurIPS",
  "abstract": "Simple classical spin models well-known to physicists as the ANNNI and Heisenberg XY Models. in which long-range interactions occur in a pattern given by the Mexican Hat operator. can generate many of the structural properties characteristic of the ocular dominance columns and iso-orientation patches seen in cat and primate visual cortex.",
  "stance": 0.1
 },
 {
  "url": "https://papers.neurips.cc/paper/1990/file/c058f544c737782deacefa532d9add4c-Paper.pdf",
  "title": "Connectionist Approaches to the Use of Markov Models for Speech Recognition",
  "year": 1990,
  "venue": "NeurIPS",
  "abstract": "Previous work has shown the ability of Multilayer Perceptrons (MLPs) to estimate emission probabilities for Hidden Markov Models (HMMs). The advantages of a speech recognition system incorporating both MLPs and HMMs are the best discrimination and the ability to incorporate multiple sources of evidence (features, temporal context) without restrictive assumptions of distributions or statistical independence. This paper presents results on the speaker-dependent portion of DARPA's English language Resource Management database. Results support the previously reported utility of MLP probability estimation for continuous speech recognition. An additional approach we are pursuing is to use MLPs as nonlinear predictors for autoregressive HMMs. While this is shown to be more compatible with the HMM formalism, it still suffers from several limitations. This approach is generalized to take account of time correlation between successive observations, without any restrictive assumptions about the driving noise.",
  "stance": -0.2
 },
 {
  "url": "https://papers.neurips.cc/paper/1990/file/eed5af6add95a9a6f1252739b1ad8c24-Paper.pdf",
  "title": "The Devil and the Network: What Sparsity Implies to Robustness and Memory",
  "year": 1990,
  "venue": "NeurIPS",
  "abstract": "Robustness is a commonly bruited property of neural networks; in particular, a folk theorem in neural computation asserts that neural networks-in contexts with large interconnectivity-continue to function efficiently, albeit with some degradation, in the presence of component damage or loss. A second folk theorem in such contexts asserts that dense interconnectivity between neural elements is a sine qua non for the efficient usage of resources. These premises are formally examined in this communication in a setting that invokes the notion of the \"devil\" 1 in the network as an agent that produces sparsity by snipping connections. 1 ON REMOVING THE FOLK FROM THE THEOREM Robustness in the presence of component damage is a property that is commonly attributed to neural networks. The content of the following statement embodies this sentiment. Folk Theorem 1: Computation in neural networks is not substantially affected by damage to network components. While such a statement is manifestly not true in general-witness networks with \"grandmother cells\" where damage to the critical cells fatally impairs the computational ability of the network-there is anecdotal evidence in support of it in 1 Well, maybe an imp.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1991/file/55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf",
  "title": "Fast Learning with Predictive Forward Models",
  "year": 1991,
  "venue": "NeurIPS",
  "abstract": "A method for transforming performance evaluation signals distal both in space and time into proximal signals usable by supervised learning algorithms, presented in [Jordan & Jacobs 90], is examined. A simple observation concerning differentiation through models trained with redundant inputs (as one of their networks is) explains a weakness in the original architecture and suggests a modification: an internal world model that encodes action-space exploration and, crucially, cancels input redundancy to the forward model is added. Learning time on an example task, cartpole balancing, is thereby reduced about 50 to 100 times.",
  "stance": 0.6
 },
 {
  "url": "https://papers.neurips.cc/paper/1991/file/8d34201a5b85900908db6cae92723617-Paper.pdf",
  "title": "Benchmarking Feed-Forward Neural Networks: Models and Measures",
  "year": 1991,
  "venue": "NeurIPS",
  "abstract": "Existing metrics for the learning performance of feed-forward neural networks do not provide a satisfactory basis for comparison because the choice of the training epoch limit can determine the results of the comparison. I propose new metrics which have the desirable property of being independent of the training epoch limit. The efficiency measures the yield of correct networks in proportion to the training effort expended. The optimal epoch limit provides the greatest efficiency. The learning performance is modelled statistically, and asymptotic performance is estimated. Implementation details may be found in (Harney, 1992).",
  "stance": 1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1992/file/48ab2f9b45957ab574cf005eb8a76760-Paper.pdf",
  "title": "Analogy-- Watershed or Waterloo? Structural alignment and the development of connectionist models of analogy",
  "year": 1992,
  "venue": "NeurIPS",
  "abstract": "Neural network models have been criticized for their inability to make use of compositional representations. In this paper, we describe a series of psychological phenomena that demonstrate the role of structured representations in cognition. These findings suggest that people compare relational representations via a process of structural alignment. This process will have to be captured by any model of cognition, symbolic or subsymbolic.",
  "stance": 0.3
 },
 {
  "url": "https://papers.neurips.cc/paper/1992/file/9cf81d8026a9018052c429cc4e56739b-Paper.pdf",
  "title": "Metamorphosis Networks: An Alternative to Constructive Models",
  "year": 1992,
  "venue": "NeurIPS",
  "abstract": "Given a set oft raining examples, determining the appropriate number of free parameters is a challenging problem. Constructive learning algorithms attempt to solve this problem automatically by adding hidden units, and therefore free parameters, during learning. We explore an alternative class of algorithms-called metamorphosis algorithms-in which the number of units is fixed, but the number of free parameters gradually increases during learning. The architecture we investigate is composed of RBF units on a lattice, which imposes flexible constraints on the parameters of the network. Virtues of this approach include variable subset selection, robust parameter selection, multiresolution processing, and interpolation of sparse training data.",
  "stance": 0.3
 },
 {
  "url": "https://papers.neurips.cc/paper/1992/file/a532400ed62e772b9dc0b86f46e583ff-Paper.pdf",
  "title": "Weight Space Probability Densities in Stochastic Learning: II. Transients and Basin Hopping Times",
  "year": 1992,
  "venue": "NeurIPS",
  "abstract": "In stochastic learning, weights are random variables whose time evolution is governed by a Markov process. At each time-step, n, the weights can be described by a probability density function pew, n). We summarize the theory of the time evolution of P, and give graphical examples of the time evolution that contrast the behavior of stochastic learning with true gradient descent (batch learning). Finally, we use the formalism to obtain predictions of the time required for noise-induced hopping between basins of different optima. We compare the theoretical predictions with simulations of large ensembles of networks for simple problems in supervised and unsupervised learning. 1 Weight-Space Probability Densities Despite the recent application of convergence theorems from stochastic approximation theory to neural network learning (Oja 1982, White 1989) there remain outstanding questions about the search dynamics in stochastic learning. For example, the convergence theorems do not tell us to which of several optima the algorithm",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1992/file/fae0b27c451c728867a567e8c1bb4e53-Paper.pdf",
  "title": "Transient Signal Detection with Neural Networks: The Search for the Desired Signal",
  "year": 1992,
  "venue": "NeurIPS",
  "abstract": "Matched filtering has been one of the most powerful techniques employed for transient detection. Here we will show that a dynamic neural network outperforms the conventional approach. When the artificial neural network (ANN) is trained with supervised learning schemes there is a need to supply the desired signal for all time, although we are only interested in detecting the transient. In this paper we also show the effects on the detection agreement of different strategies to construct the desired signal. The extension of the Bayes decision rule (011 desired signal), optimal in static classification, performs worse than desired signals constructed by random noise or prediction during the background.",
  "stance": -1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1993/file/0537fb40a68c18da59a35c2bfe1ca554-Paper.pdf",
  "title": "Putting It All Together: Methods for Combining Neural Networks",
  "year": 1993,
  "venue": "NeurIPS",
  "abstract": "The past several years have seen a tremendous growth in the complexity of the recognition, estimation and control tasks expected of neural networks. In solving these tasks, one is faced with a large variety of learning algorithms and a vast selection of possible network architectures. After all the training, how does one know which is the best network? This decision is further complicated by the fact that standard techniques can be severely limited by problems such as over-fitting, data sparsity and local optima. The usual solution to these problems is a winner-take-all cross-validatory model selection. However, recent experimental and theoretical work indicates that we can improve performance by considering methods for combining neural networks.",
  "stance": -0.6
 },
 {
  "url": "https://papers.neurips.cc/paper/1993/file/1efa39bcaec6f3900149160693694536-Paper.pdf",
  "title": "How to Describe Neuronal Activity: Spikes, Rates, or Assemblies?",
  "year": 1993,
  "venue": "NeurIPS",
  "abstract": "What is the 'correct' theoretical description of neuronal activity? The analysis of the dynamics of a globally connected network of spiking neurons (the Spike Response Model) shows that a description by mean firing rates is possible only if active neurons fire incoherently. If firing occurs coherently or with spatio-temporal correlations, the spike structure of the neural code becomes relevant. Alternatively, neurons can be gathered into local or distributed ensembles or 'assemblies'. A description based on the mean ensemble activity is, in principle, possible but the interaction between different assemblies becomes highly nonlinear. A description with spikes should therefore be preferred.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1993/file/22ac3c5a5bf0b520d281c122d1490650-Paper.pdf",
  "title": "Emergence of Global Structure from Local Associations",
  "year": 1993,
  "venue": "NeurIPS",
  "abstract": "A variant of the encoder architecture, where units at the input and output layers represent nodes on a graph. is applied to the task of mapping locations to sets of neighboring locations. The degree to which the resuIting internal (i.e. hidden unit) representations reflect global properties of the environment depends upon several parameters of the learning procedure. Architectural bottlenecks. noise. and incremental learning of landmarks are shown to be important factors in maintaining topographic relationships at a global scale.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1993/file/22fb0cee7e1f3bde58293de743871417-Paper.pdf",
  "title": "Robot Learning: Exploration and Continuous Domains",
  "year": 1993,
  "venue": "NeurIPS",
  "abstract": "The goal of this workshop was to discuss two major issues: efficient exploration of a learner's state space, and learning in continuous domains. The common themes that emerged in presentations and in discussion were the importance of choosing one's domain assumptions carefully, mixing controllers/strategies, avoidance of catastrophic failure, new approaches with difficulties with reinforcement learning, and the importance of task transfer.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1993/file/d4c2e4a3297fe25a71d030b67eb83bfc-Paper.pdf",
  "title": "Bayesian Backpropagation Over I-O Functions Rather Than Weights",
  "year": 1993,
  "venue": "NeurIPS",
  "abstract": "The conventional Bayesian justification of backprop is that it finds the MAP weight vector. As this paper shows, to find the MAP i-o function instead one must add a correction tenn to backprop. That tenn biases one towards i-o functions with small description lengths, and in particular favors (some kinds of) feature-selection, pruning, and weight-sharing.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1993/file/e0cf1f47118daebc5b16269099ad7347-Paper.pdf",
  "title": "Functional Models of Selective Attention and Context Dependency",
  "year": 1993,
  "venue": "NeurIPS",
  "abstract": "This workshop reviewed and classified the various models which have emerged from the general concept of selective attention and context dependency, and sought to identify their commonalities. It was concluded that the motivation and mechanism of these functional models are \"efficiency\" and ''factoring'', respectively. The workshop focused on computational models of selective attention and context dependency within the realm of neural networks. We treated only ''functional'' models; computational models of biological neural systems, and symbolic or rule-based systems were omitted from the discussion.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1993/file/e97ee2054defb209c35fe4dc94599061-Paper.pdf",
  "title": "Complexity Issues in Neural Computation and Learning",
  "year": 1993,
  "venue": "NeurIPS",
  "abstract": "The general goal of this workshop was to bring t.ogether researchers working toward developing a theoretical framework for the analysis and design of neural networks. The t.echnical focus of the workshop was to address recent. developments in understanding the capabilities and limitations of variolls modds for neural computation and learning. The primary topics addressed the following three areas: 1) Computational complexity issues in neural networks, 2) Complexity issues in learning, and 3) Convergence and numerical properties of learning algorit.hms. Other topics included experiment.al/simulat.ion results on neural llet.works, which seemed to pose some open problems in the areas of learning and generalizat.ion properties of feedforward networks.",
  "stance": -1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1993/file/e995f98d56967d946471af29d7bf99f1-Paper.pdf",
  "title": "Stability and Observability",
  "year": 1993,
  "venue": "NeurIPS",
  "abstract": "The theme was the effect of perturbations of the defining parameters of a neural network due to: 1) mea\"urement\" (particularly with analog networks); 2) di\"cretization due to a) digital implementation of analog nets; b) bounded-precision implementation of digital networks; or c) inaccurate evaluation of the transfer function(s}; 3) noise in or incomplete input and/or output of the net or individual cells (particularly with analog networks).",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1994/file/1e48c4420b7073bc11916c6c1de226bb-Paper.pdf",
  "title": "Interference in Learning Internal Models of Inverse Dynamics in Humans",
  "year": 1994,
  "venue": "NeurIPS",
  "abstract": "Experiments were performed to reveal some of the computational properties of the human motor memory system. We show that as humans practice reaching movements while interacting with a novel mechanical environment, they learn an internal model of the inverse dynamics of that environment. Subjects show recall of this model at testing sessions 24 hours after the initial practice. The representation of the internal model in memory is such that there is interference when there is an attempt to learn a new inverse dynamics map immediately after an anticorrelated mapping was learned. We suggest that this interference is an indication that the same computational elements used to encode the first inverse dynamics map are being used to learn the second mapping. We predict that this leads to a forgetting of the initially learned skill.",
  "stance": -0.5
 },
 {
  "url": "https://papers.neurips.cc/paper/1994/file/b56a18e0eacdf51aa2a5306b0f533204-Paper.pdf",
  "title": "A Study of Parallel Perturbative Gradient Descent",
  "year": 1994,
  "venue": "NeurIPS",
  "abstract": "We have continued our study of a parallel perturbative learning method [Alspector et al., 1993] and implications for its implementation in analog VLSI. Our new results indicate that, in most cases, a single parallel perturbation (per pattern presentation) of the function parameters (weights in a neural network) is theoretically the best course. This is not true, however, for certain problems and may not generally be true when faced with issues of implementation such as limited precision. In these cases, multiple parallel perturbations may be best as indicated in our previous results.",
  "stance": -1.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1994/file/e6cb2a3c14431b55aa50c06529eaa21b-Paper.pdf",
  "title": "Hyperparameters Evidence and Generalisation for an Unrealisable Rule",
  "year": 1994,
  "venue": "NeurIPS",
  "abstract": "Using a statistical mechanical formalism we calculate the evidence, generalisation error and consistency measure for a linear perceptron trained and tested on a set of examples generated by a non linear teacher. The teacher is said to be unrealisable because the student can never model it without error. Our model allows us to interpolate between the known case of a linear teacher, and an unrealisable, nonlinear teacher. A comparison of the hyperparameters which maximise the evidence with those that optimise the performance measures reveals that, in the non-linear case, the evidence procedure is a misleading guide to optimising performance. Finally, we explore the extent to which the evidence procedure is unreliable and find that, despite being sub-optimal, in some circumstances it might be a useful method for fixing the hyperparameters.",
  "stance": 0.3
 },
 {
  "url": "https://papers.neurips.cc/paper/1996/file/459a4ddcb586f24efd9395aa7662bc7c-Paper.pdf",
  "title": "Why did TD-Gammon Work?",
  "year": 1996,
  "venue": "NeurIPS",
  "abstract": "Although TD-Gammon is one of the major successes in machine learning, it has not led to similar impressive breakthroughs in temporal difference learning for other applications or even other games. We were able to replicate some of the success of TD-Gammon, developing a competitive evaluation function on a 4000 parameter feed-forward neural network, without using back-propagation, reinforcement or temporal difference learning methods. Instead we apply simple hill-climbing in a relative fitness environment. These results and further analysis suggest that the surprising success of Tesauro's program had more to do with the co-evolutionary structure of the learning task and the dynamics of the backgammon game itself.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1996/file/6c14da109e294d1e8155be8aa4b1ce8e-Paper.pdf",
  "title": "Promoting Poor Features to Supervisors: Some Inputs Work Better as Outputs",
  "year": 1996,
  "venue": "NeurIPS",
  "abstract": "In supervised learning there is usually a clear distinction between inputs and outputs inputs are what you will measure, outputs are what you will predict from those measurements. This paper shows that the distinction between inputs and outputs is not this simple. Some features are more useful as extra outputs than as inputs. By using a feature as an output we get more than just the case values but can. learn a mapping from the other inputs to that feature. For many features this mapping may be more useful than the feature value itself. We present two regression problems and one classification problem where performance improves if features that could have been used as inputs are used as extra outputs instead. This result is surprising since a feature used as an output is not used during testing.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1996/file/dc87c13749315c7217cdc4ac692e704c-Paper.pdf",
  "title": "Limitations of Self-organizing Maps for Vector Quantization and Multidimensional Scaling",
  "year": 1996,
  "venue": "NeurIPS",
  "abstract": "The limitations of using self-organizing maps (SaM) for either clustering/vector quantization (VQ) or multidimensional scaling (MDS) are being discussed by reviewing recent empirical findings and the relevant theory. SaM 's remaining ability of doing both VQ and MDS at the same time is challenged by a new combined technique of online K-means clustering plus Sammon mapping of the cluster centroids. SaM are shown to perform significantly worse in terms of quantization error , in recovering the structure of the clusters and in preserving the topology in a comprehensive empirical study using a series of multivariate normal clustering problems.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1996/file/f47330643ae134ca204bf6b2481fec47-Paper.pdf",
  "title": "Balancing Between Bagging and Bumping",
  "year": 1996,
  "venue": "NeurIPS",
  "abstract": "We compare different methods to combine predictions from neural networks trained on different bootstrap samples of a regression problem. One of these methods, introduced in [6] and which we here call balancing, is based on the analysis of the ensemble generalization error into an ambiguity term and a term incorporating generalization performances of individual networks. We show how to estimate these individual errors from the residuals on validation patterns. Weighting factors for the different networks follow from a quadratic programming problem. On a real-world problem concerning the prediction of sales figures and on the well-known Boston housing data set, balancing clearly outperforms other recently proposed alternatives as bagging [1] and bumping [8]. 1 EARLY STOPPING AND BOOTSTRAPPING Stopped training is a popular strategy to prevent overfitting in neural networks. The complete data set is split up into a training and a validation set . Through learning the weights are adapted in order to minimize the error on the training data. Training is stopped when the error on the validation data starts increasing. The final network depends on the accidental subdivision in training and validation set , and often also on the, usually random, initial weight configuration and chosen minimization procedure. In other words , early stopped neural networks are highly unstable: small changes in the data or different initial conditions can produce large changes in the estimate. As argued in [1 , 8], with unstable estimators it is advisable to resample, i.e ., to apply the same procedure several times using different subdivisions in training and validation set and perhaps starting from different initial RWCP: Real World Computing Partnership; SNN: Foundation for Neural Networks. Balancing Between Bagging and Bumping 467 configurations. In the neural network literature resampling is often referred to as training ensembles of neural networks [3, 6]. In this paper, we will discuss methods for combining the outputs of networks obtained through such a repetitive procedure. First, however, we have to choose how to generate the subdivisions in training and validation sets. Options are, among others, k-fold cross-validation, subsampling and bootstrapping. In this paper we will consider bootstrapping [2] which is based on the idea that the available data set is nothing but a particular realization of some probability distribution. In principle, one would like to do inference on this \"true\" yet unknown probability distribution. A natural thing to do is then to define an empirical distribution. With so-called naive bootstrapping the empirical distribution is a sum of delta peaks on the available data points, each with probability content l/Pdata with Pdata the number of patterns. A bootstrap sample is a collection of Pdata patterns drawn with replacement from this empirical probability distribution. Some of the data points will occur once, some twice and some even more than twice in this bootstrap sample. The bootstrap sample is taken to be the training set, all patterns that do not occur in a particular bootstrap sample constitute the validation set. For large Pdata, the probability that a pattern becomes part of the validation set is (1 l/Pdata)Pda.ta. ~ l/e ~ 0.368. An advantage of bootstrapping over other resampling techniques is that most statistical theory on resampling is nowadays based on the bootstrap. Using naive bootstrapping we generate nrun training and validation sets out of our complete data set of Pdata input-output combinations {iI', tl'}. In this paper we will restrict ourselves to regression problems with, for notational convenience, just one output variable. We keep track of a matrix with components q; indicating whether pattern p is part of the validation set for run i (q; = 1) or of the training set (qf = 0). On each subdivision we train and stop a neural network with one layer of nhidden hidden units. The output or of network i with weight vector w( i) on input il' reads",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1998/file/efb76cff97aaf057654ef2f38cd77d73-Paper.pdf",
  "title": "Utilizing lime: Asynchronous Binding",
  "year": 1998,
  "venue": "NeurIPS",
  "abstract": "Historically, connectionist systems have not excelled at representing and manipulating complex structures. How can a system composed of simple neuron-like computing elements encode complex relations? Recently, researchers have begun to appreciate that representations can extend in both time and space. Many researchers have proposed that the synchronous firing of units can encode complex representations. I identify the limitations of this approach and present an asynchronous model of binding that effectively represents complex structures. The asynchronous model extends the synchronous approach. I argue that our cognitive architecture utilizes a similar mechanism.",
  "stance": 0.2
 },
 {
  "url": "https://papers.neurips.cc/paper/1999/file/86d7c8a08b4aaa1bc7c599473f5dddda-Paper.pdf",
  "title": "Rules and Similarity in Concept Learning",
  "year": 1999,
  "venue": "NeurIPS",
  "abstract": "This paper argues that two apparently distinct modes of generalizing concepts abstracting rules and computing similarity to exemplars should both be seen as special cases of a more general Bayesian learning framework. Bayes explains the specific workings of these two modes which rules are abstracted, how similarity is measured as well as why generalization should appear ruleor similarity-based in different situations. This analysis also suggests why the rules/similarity distinction, even if not computationally fundamental, may still be useful at the algorithmic level as part of a principled approximation to fully Bayesian learning.",
  "stance": 0.0
 },
 {
  "url": "https://papers.neurips.cc/paper/1999/file/e9b73bccd1762555582b513ff9d02492-Paper.pdf",
  "title": "Acquisition in Autoshaping",
  "year": 1999,
  "venue": "NeurIPS",
  "abstract": "Quantitative data on the speed with which animals acquire behavioral responses during classical conditioning experiments should provide strong constraints on models of learning. However, most models have simply ignored these data; the few that have attempted to address them have failed by at least an order of magnitude. We discuss key data on the speed of acquisition, and show how to account for them using a statistically sound model of learning, in which differential reliabilities of stimuli playa crucial role.",
  "stance": 0.2
 }
]